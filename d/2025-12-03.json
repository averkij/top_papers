{
    "date": {
        "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 3",
        "zh": "12æœˆ3æ—¥"
    },
    "time_utc": "2025-12-03 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-03",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.02556",
            "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
            "url": "https://huggingface.co/papers/2512.02556",
            "abstract": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
            "score": 197,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "3761447727f4f3de",
            "authors": [
                "DeepSeek-AI",
                "Aixin Liu",
                "Aoxue Mei",
                "Bangcai Lin",
                "Bing Xue",
                "Bingxuan Wang",
                "Bingzheng Xu",
                "Bochao Wu",
                "Bowei Zhang",
                "Chaofan Lin",
                "Chen Dong",
                "Chengda Lu",
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chenhao Xu",
                "Chong Ruan",
                "Damai Dai",
                "Daya Guo",
                "Dejian Yang",
                "Deli Chen",
                "Erhang Li",
                "Fangqi Zhou",
                "Fangyun Lin",
                "Fucong Dai",
                "Guangbo Hao",
                "Guanting Chen",
                "Guowei Li",
                "H. Zhang",
                "Hanwei Xu",
                "Hao Li",
                "Haofen Liang",
                "Haoran Wei",
                "Haowei Zhang",
                "Haowen Luo",
                "Haozhe Ji",
                "Honghui Ding",
                "Hongxuan Tang",
                "Huanqi Cao",
                "Huazuo Gao",
                "Hui Qu",
                "Hui Zeng",
                "Jialiang Huang",
                "Jiashi Li",
                "Jiaxin Xu",
                "Jiewen Hu",
                "Jingchang Chen",
                "Jingting Xiang",
                "Jingyang Yuan",
                "Jingyuan Cheng",
                "Jinhua Zhu",
                "Jun Ran",
                "Junguang Jiang",
                "Junjie Qiu",
                "Junlong Li",
                "Junxiao Song",
                "Kai Dong",
                "Kaige Gao",
                "Kang Guan",
                "Kexin Huang",
                "Kexing Zhou",
                "Kezhao Huang",
                "Kuai Yu",
                "Lean Wang",
                "Lecong Zhang",
                "Lei Wang",
                "Liang Zhao",
                "Liangsheng Yin",
                "Lihua Guo",
                "Lingxiao Luo",
                "Linwang Ma",
                "Litong Wang",
                "Liyue Zhang",
                "M. S. Di",
                "M. Y Xu",
                "Mingchuan Zhang",
                "Minghua Zhang",
                "Minghui Tang",
                "Mingxu Zhou",
                "Panpan Huang",
                "Peixin Cong",
                "Peiyi Wang",
                "Qiancheng Wang",
                "Qihao Zhu",
                "Qingyang Li",
                "Qinyu Chen",
                "Qiushi Du",
                "Ruiling Xu",
                "Ruiqi Ge",
                "Ruisong Zhang",
                "Ruizhe Pan",
                "Runji Wang",
                "Runqiu Yin",
                "Runxin Xu",
                "Ruomeng Shen",
                "Ruoyu Zhang",
                "S. H. Liu",
                "Shanghao Lu",
                "Shangyan Zhou",
                "Shanhuang Chen",
                "Shaofei Cai",
                "Shaoyuan Chen",
                "Shengding Hu",
                "Shengyu Liu",
                "Shiqiang Hu",
                "Shirong Ma",
                "Shiyu Wang",
                "Shuiping Yu",
                "Shunfeng Zhou",
                "Shuting Pan",
                "Songyang Zhou",
                "Tao Ni",
                "Tao Yun",
                "Tian Pei",
                "Tian Ye",
                "Tianyuan Yue",
                "Wangding Zeng",
                "Wen Liu",
                "Wenfeng Liang",
                "Wenjie Pang",
                "Wenjing Luo",
                "Wenjun Gao",
                "Wentao Zhang",
                "Xi Gao",
                "Xiangwen Wang",
                "Xiao Bi",
                "Xiaodong Liu",
                "Xiaohan Wang",
                "Xiaokang Chen",
                "Xiaokang Zhang",
                "Xiaotao Nie",
                "Xin Cheng",
                "Xin Liu",
                "Xin Xie",
                "Xingchao Liu",
                "Xingkai Yu",
                "Xingyou Li",
                "Xinyu Yang",
                "Xinyuan Li",
                "Xu Chen",
                "Xuecheng Su",
                "Xuehai Pan",
                "Xuheng Lin",
                "Xuwei Fu",
                "Y. Q. Wang",
                "Yang Zhang",
                "Yanhong Xu",
                "Yanru Ma",
                "Yao Li",
                "Yao Li",
                "Yao Zhao",
                "Yaofeng Sun",
                "Yaohui Wang",
                "Yi Qian",
                "Yi Yu",
                "Yichao Zhang",
                "Yifan Ding",
                "Yifan Shi",
                "Yiliang Xiong",
                "Ying He",
                "Ying Zhou",
                "Yinmin Zhong",
                "Yishi Piao",
                "Yisong Wang",
                "Yixiao Chen",
                "Yixuan Tan",
                "Yixuan Wei",
                "Yiyang Ma",
                "Yiyuan Liu",
                "Yonglun Yang",
                "Yongqiang Guo",
                "Yongtong Wu",
                "Yu Wu",
                "Yuan Cheng",
                "Yuan Ou",
                "Yuanfan Xu",
                "Yuduan Wang",
                "Yue Gong",
                "Yuhan Wu",
                "Yuheng Zou",
                "Yukun Li",
                "Yunfan Xiong",
                "Yuxiang Luo",
                "Yuxiang You",
                "Yuxuan Liu",
                "Yuyang Zhou",
                "Z. F. Wu",
                "Z. Z. Ren",
                "Zehua Zhao",
                "Zehui Ren",
                "Zhangli Sha",
                "Zhe Fu",
                "Zhean Xu",
                "Zhenda Xie",
                "Zhengyan Zhang",
                "Zhewen Hao",
                "Zhibin Gou",
                "Zhicheng Ma",
                "Zhigang Yan",
                "Zhihong Shao",
                "Zhixian Huang",
                "Zhiyu Wu",
                "Zhuoshu Li",
                "Zhuping Zhang",
                "Zian Xu",
                "Zihao Wang",
                "Zihui Gu",
                "Zijia Zhu",
                "Zilin Li",
                "Zipeng Zhang",
                "Ziwei Xie",
                "Ziyi Gao",
                "Zizheng Pan",
                "Zongqing Yao",
                "Bei Feng",
                "Hui Li",
                "J. L. Cai",
                "Jiaqi Ni",
                "Lei Xu",
                "Meng Li",
                "Ning Tian",
                "R. J. Chen",
                "R. L. Jin",
                "S. S. Li",
                "Shuang Zhou",
                "Tianyu Sun",
                "X. Q. Li",
                "Xiangyue Jin",
                "Xiaojin Shen",
                "Xiaosha Chen",
                "Xinnan Song",
                "Xinyi Zhou",
                "Y. X. Zhu",
                "Yanping Huang",
                "Yaohui Li",
                "Yi Zheng",
                "Yuchen Zhu",
                "Yunxian Ma",
                "Zhen Huang",
                "Zhipeng Xu",
                "Zhongyu Zhang",
                "Dongjie Ji",
                "Jian Liang",
                "Jianzhong Guo",
                "Jin Chen",
                "Leyi Xia",
                "Miaojun Wang",
                "Mingming Li",
                "Peng Zhang",
                "Ruyi Chen",
                "Shangmian Sun",
                "Shaoqing Wu",
                "Shengfeng Ye",
                "T. Wang",
                "W. L. Xiao",
                "Wei An",
                "Xianzu Wang",
                "Xiaowen Sun",
                "Xiaoxiang Wang",
                "Ying Tang",
                "Yukun Zha",
                "Zekai Zhang",
                "Zhe Ju",
                "Zhen Zhang",
                "Zihua Qu"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02556.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#long_context",
                    "#agents",
                    "#rl",
                    "#architecture",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "DeepSeek-V3.2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ DeepSeek Sparse Attention, ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‰ĞµĞ¼Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ reinforcement learning Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ GPT-5 Ğ¸ Gemini-3.0-Pro Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ DeepSeek-V3.2-Speciale Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğµ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ Ğ¸ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ² 2025 Ğ³Ğ¾Ğ´Ñƒ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with DeepSeek-V3.2",
                    "desc": "DeepSeek-V3.2 is a cutting-edge model that enhances reasoning and performance in complex tasks through innovative techniques. It features DeepSeek Sparse Attention (DSA), which optimizes attention mechanisms to lower computational costs while maintaining effectiveness in processing long contexts. Additionally, it employs a scalable reinforcement learning framework that allows it to match and even exceed the capabilities of models like GPT-5 and Gemini-3.0-Pro. The model also includes a large-scale task synthesis pipeline that improves training data generation, leading to better generalization and robustness in interactive environments."
                },
                "zh": {
                    "title": "æ·±åº¦æ¨ç†ä¸é«˜æ•ˆå­¦ä¹ çš„å®Œç¾ç»“åˆ",
                    "desc": "DeepSeek-V3.2 æ˜¯ä¸€ç§æ–°æ¨¡å‹ï¼Œç»“åˆäº†é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å’Œå“è¶Šçš„æ¨ç†æ€§èƒ½ã€‚å®ƒå¼•å…¥äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆDSAï¼‰ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDeepSeek-V3.2 é‡‡ç”¨äº†å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äº GPT-5 å’Œ Gemini-3.0-Proã€‚æœ€åï¼Œæ¨¡å‹è¿˜å¼€å‘äº†å¤§è§„æ¨¡çš„ä»»åŠ¡åˆæˆç®¡é“ï¼Œä»¥ç³»ç»Ÿæ€§åœ°ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜äº†åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªçš„ç¨³å¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.21689",
            "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
            "url": "https://huggingface.co/papers/2511.21689",
            "abstract": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
            "score": 100,
            "issue_id": 1,
            "pub_date": "2025-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "62f5f6f47ce24234",
            "authors": [
                "Hongjin Su",
                "Shizhe Diao",
                "Ximing Lu",
                "Mingjie Liu",
                "Jiacheng Xu",
                "Xin Dong",
                "Yonggan Fu",
                "Peter Belcak",
                "Hanrong Ye",
                "Hongxu Yin",
                "Yi Dong",
                "Evelina Bakhturina",
                "Tao Yu",
                "Yejin Choi",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.21689.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ Ğ´Ğ¸Ñ€Ğ¸Ğ¶Ñ‘Ñ€ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ToolOrchestra Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Orchestrator Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ LLM, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Intelligence: Small Orchestrators, Big Results!",
                    "desc": "This paper presents a method called ToolOrchestra, which uses a small orchestrator to manage various intelligent tools through reinforcement learning. The orchestrator, named Orchestrator, is designed to solve complex tasks like Humanity's Last Exam more efficiently than larger models. By focusing on outcome, efficiency, and user preferences in its training rewards, Orchestrator achieves higher accuracy and lower costs compared to previous models like GPT-5. The results indicate that this approach not only enhances performance but also provides a scalable solution for integrating diverse tools in reasoning systems."
                },
                "zh": {
                    "title": "å°å‹åè°ƒå™¨ï¼Œæå‡æ™ºèƒ½å·¥å…·çš„æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºToolOrchestraçš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå°å‹åè°ƒå™¨æ¥ç®¡ç†å„ç§æ™ºèƒ½å·¥å…·ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæé«˜è§£å†³å¤æ‚ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†äººç±»æœ€åè€ƒè¯•ï¼ˆHLEï¼‰æ—¶è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ToolOrchestraçš„åè°ƒå™¨æ¨¡å‹åœ¨æˆæœ¬ä¸Šæ›´å…·ä¼˜åŠ¿ï¼ŒåŒæ—¶èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„å·¥å…·ä½¿ç”¨åå¥½ã€‚æœ€ç»ˆï¼Œè®ºæ–‡å±•ç¤ºäº†å°å‹åè°ƒå™¨ä¸å¤šæ ·åŒ–å·¥å…·ç»„åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®ç”¨å’Œå¯æ‰©å±•çš„å·¥å…·å¢å¼ºæ¨ç†ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03041",
            "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
            "url": "https://huggingface.co/papers/2512.03041",
            "abstract": "MultiShotMaster extends a single-shot model with novel RoPE variants for flexible and controllable multi-shot video generation, addressing data scarcity with an automated annotation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
            "score": 62,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "0c7595dc9becb97b",
            "authors": [
                "Qinghe Wang",
                "Xiaoyu Shi",
                "Baolu Li",
                "Weikang Bian",
                "Quande Liu",
                "Huchuan Lu",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Xu Jia"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kling Team, Kuaishou Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03041.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "MultiShotMaster â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ RoPE (Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ): Multi-Shot Narrative RoPE Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°, Ğ¸ Spatiotemporal Position-Aware RoPE Ğ´Ğ»Ñ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ pipeline Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, ĞºÑ€Ğ¾ÑÑ-ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑ†ĞµĞ½, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Multi-Shot Video Generation with MultiShotMaster",
                    "desc": "MultiShotMaster is a new framework designed to improve video generation by allowing the creation of multi-shot videos, which are more complex than single-shot clips. It enhances a pre-existing single-shot model by introducing two innovative RoPE variants that help maintain narrative coherence and flexibility in shot arrangement. The framework also includes an automated annotation pipeline to generate the necessary data for training, addressing the issue of data scarcity. With features like text-driven consistency and customizable subjects and scenes, MultiShotMaster offers significant advancements in controllable video generation."
                },
                "zh": {
                    "title": "çµæ´»å¯æ§çš„å¤šé•œå¤´è§†é¢‘ç”Ÿæˆ",
                    "desc": "MultiShotMaster æ˜¯ä¸€ä¸ªç”¨äºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ‰©å±•äº†å•é•œå¤´æ¨¡å‹å¹¶å¼•å…¥äº†æ–°å‹çš„ RoPE å˜ä½“ã€‚è¯¥æ¡†æ¶è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé‡‡ç”¨è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“æ¥æå–å¤šé•œå¤´è§†é¢‘åŠå…¶ç›¸å…³ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥å¤šé•œå¤´å™äº‹ RoPE å’Œæ—¶ç©ºä½ç½®æ„ŸçŸ¥ RoPEï¼ŒMultiShotMaster å®ç°äº†çµæ´»çš„é•œå¤´å®‰æ’å’Œå™äº‹ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šé•œå¤´è§†é¢‘ç”Ÿæˆæ–¹é¢å…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œå¯æ§æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02038",
            "title": "Deep Research: A Systematic Survey",
            "url": "https://huggingface.co/papers/2512.02038",
            "abstract": "Deep Research systems integrate LLMs with external tools to enhance problem-solving capabilities, involving query planning, information acquisition, memory management, and answer generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
            "score": 62,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "e0292d037e5533b5",
            "authors": [
                "Zhengliang Shi",
                "Yiqun Chen",
                "Haitao Li",
                "Weiwei Sun",
                "Shiyu Ni",
                "Yougang Lyu",
                "Run-Ze Fan",
                "Bowen Jin",
                "Yixuan Weng",
                "Minjun Zhu",
                "Qiujie Xie",
                "Xinyu Guo",
                "Qu Yang",
                "Jiayi Wu",
                "Jujia Zhao",
                "Xiaqiang Tang",
                "Xinbei Ma",
                "Cunxiang Wang",
                "Jiaxin Mao",
                "Qingyao Ai",
                "Jen-Tse Huang",
                "Wenxuan Wang",
                "Yue Zhang",
                "Yiming Yang",
                "Zhaopeng Tu",
                "Zhaochun Ren"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Johns Hopkins University",
                "Leiden University",
                "Renmin University of China",
                "Shandong University",
                "Tencent",
                "Tsinghua University",
                "UCAS",
                "University of Amsterdam",
                "University of Arizona",
                "University of Illinois Urbana-Champaign",
                "University of Massachusetts Amherst",
                "Westlake University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02038.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº."
                },
                "en": {
                    "title": "Empowering LLMs: The Future of Deep Research Systems",
                    "desc": "This paper discusses the integration of large language models (LLMs) with external tools to improve their problem-solving abilities. It highlights the limitations of traditional methods in handling complex tasks that require critical thinking and multi-source verification. The authors present a structured approach to Deep Research systems, outlining key components such as query planning, information acquisition, memory management, and answer generation. Additionally, they provide insights into optimization techniques and evaluation criteria to guide future advancements in this field."
                },
                "zh": {
                    "title": "æ·±åº¦ç ”ç©¶ï¼šèµ‹èƒ½LLMsçš„å…¨æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "æ·±åº¦ç ”ç©¶ç³»ç»Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤–éƒ¨å·¥å…·ç»“åˆï¼Œä»¥å¢å¼ºå…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚è¿™äº›ç³»ç»Ÿæ¶‰åŠæŸ¥è¯¢è§„åˆ’ã€ä¿¡æ¯è·å–ã€è®°å¿†ç®¡ç†å’Œç­”æ¡ˆç”Ÿæˆç­‰å…³é”®ç»„ä»¶ã€‚é€šè¿‡è¿™ç§é›†æˆï¼ŒLLMsèƒ½å¤Ÿä½œä¸ºç ”ç©¶ä»£ç†ï¼Œå®Œæˆå¤æ‚çš„å¼€æ”¾æ€§ä»»åŠ¡ã€‚æœ¬æ–‡æä¾›äº†æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„å…¨é¢æ¦‚è¿°ï¼ŒåŒ…æ‹¬å‘å±•è·¯çº¿å›¾ã€åŸºç¡€ç»„ä»¶ã€å®æ–½æŠ€æœ¯ã€é‡è¦æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02472",
            "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
            "url": "https://huggingface.co/papers/2512.02472",
            "abstract": "R-Few, a guided Self-Play Challenger-Solver framework, enables stable and controllable model self-evolution with minimal human supervision, achieving performance improvements on math and reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
            "score": 48,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "e111adf115c19e6b",
            "authors": [
                "Wenhao Yu",
                "Zhenwen Liang",
                "Chengsong Huang",
                "Kishan Panaganti",
                "Tianqing Fang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab in Seattle",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02472.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ",
                    "desc": "R-Few â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Challenger-Solver, Ğ³Ğ´Ğµ Challenger Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Solver Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ´Ñ€ĞµĞ¹Ñ„ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… R-Few Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Guided Self-Play for Stable AI Self-Evolution",
                    "desc": "R-Few is a new framework designed to help AI models improve themselves with less human help. It uses a method called guided Self-Play, where a Challenger generates questions based on a few human examples, and a Solver learns from both human and generated questions. This approach helps prevent common problems in self-evolving systems, like losing focus or reinforcing mistakes. As a result, R-Few shows better performance in math and reasoning tasks, even outperforming models that used much more human data."
                },
                "zh": {
                    "title": "å¼•å¯¼è‡ªæˆ‘è¿›åŒ–ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "R-Fewæ˜¯ä¸€ç§å¼•å¯¼å¼è‡ªæˆ‘å¯¹å¼ˆæŒ‘æˆ˜è€…-æ±‚è§£å™¨æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æœ€å°åŒ–äººç±»ç›‘ç£çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹çš„ç¨³å®šå’Œå¯æ§è‡ªæˆ‘è¿›åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§çš„äººç±»ç›‘ç£ï¼Œç»“åˆä¸Šä¸‹æ–‡å¼•å¯¼å’Œæ··åˆè®­ç»ƒï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ•°å­¦å’Œæ¨ç†åŸºå‡†ä¸Šå–å¾—æ€§èƒ½æå‡ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒæŒ‘æˆ˜è€…ä¼šæŠ½å–ä¸€å°éƒ¨åˆ†äººç±»æ ‡æ³¨çš„ç¤ºä¾‹æ¥æŒ‡å¯¼åˆæˆé—®é¢˜çš„ç”Ÿæˆï¼Œè€Œæ±‚è§£å™¨åˆ™åœ¨åœ¨çº¿çš„ã€åŸºäºéš¾åº¦çš„è¯¾ç¨‹ä¸‹å…±åŒè®­ç»ƒäººç±»å’Œåˆæˆç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Fewåœ¨æ•°å­¦ä»»åŠ¡ä¸Šæ¯”R-Zeroæé«˜äº†3.0åˆ†ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¸è®­ç»ƒäº†20å€äººç±»æ•°æ®çš„General-Reasonerç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22609",
            "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
            "url": "https://huggingface.co/papers/2511.22609",
            "abstract": "MG-Nav, a dual-scale framework for zero-shot visual navigation, combines global memory-guided planning with local geometry-enhanced control using a Sparse Spatial Memory Graph and a VGGT-adapter for robust navigation in unseen environments.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
            "score": 47,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "a01796622cfb7fd2",
            "authors": [
                "Bo Wang",
                "Jiehong Lin",
                "Chenzhi Liu",
                "Xinting Hu",
                "Yifei Yu",
                "Tianjia Liu",
                "Zhongrui Wang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Southern University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22609.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#robotics",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ: Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…",
                    "desc": "MG-Nav Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑƒĞ·ĞµĞ» Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ·Ğ»Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¾Ğ±ÑŠĞµĞºÑ‚, Ğ° Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½ÑƒÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ VGGT-adapter, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Navigating the Unknown: MG-Nav's Dual-Scale Approach to Visual Navigation",
                    "desc": "MG-Nav is a novel framework designed for zero-shot visual navigation, integrating both global planning and local control strategies. It utilizes a Sparse Spatial Memory Graph to store and manage spatial information, allowing the agent to plan routes based on visual inputs and semantic understanding. The framework operates at two levels: it plans long-term paths using global memory while executing immediate navigation tasks with local control that adapts to obstacles. Additionally, the VGGT-adapter enhances the system's ability to align visual observations with navigation goals, ensuring robust performance in unfamiliar environments."
                },
                "zh": {
                    "title": "MG-Navï¼šæ™ºèƒ½è§†è§‰å¯¼èˆªçš„æ–°çªç ´",
                    "desc": "MG-Navæ˜¯ä¸€ç§åŒå°ºåº¦æ¡†æ¶ï¼Œç”¨äºé›¶-shotè§†è§‰å¯¼èˆªï¼Œç»“åˆäº†å…¨çƒè®°å¿†å¼•å¯¼çš„è§„åˆ’å’Œå±€éƒ¨å‡ ä½•å¢å¼ºçš„æ§åˆ¶ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯ç¨€ç–ç©ºé—´è®°å¿†å›¾ï¼ˆSMGï¼‰ï¼Œè¯¥å›¾ä»¥åŒºåŸŸä¸ºä¸­å¿ƒï¼Œèšåˆå¤šè§†è§’å…³é”®å¸§å’Œç‰©ä½“è¯­ä¹‰ï¼Œæ•æ‰å¤–è§‚å’Œç©ºé—´ç»“æ„ã€‚å…¨çƒå±‚é¢ä¸Šï¼Œä»£ç†åœ¨SMGä¸Šå®šä½ï¼Œå¹¶é€šè¿‡å›¾åƒåˆ°å®ä¾‹çš„æ··åˆæ£€ç´¢è§„åˆ’ç›®æ ‡æ¡ä»¶èŠ‚ç‚¹è·¯å¾„ï¼Œç”Ÿæˆå¯è¾¾çš„èˆªç‚¹åºåˆ—ã€‚å±€éƒ¨å±‚é¢ä¸Šï¼Œå¯¼èˆªåŸºç¡€ç­–ç•¥åœ¨ç‚¹ç›®æ ‡æ¨¡å¼ä¸‹æ‰§è¡Œè¿™äº›èˆªç‚¹ï¼Œå¹¶åœ¨ä»æœ€ç»ˆèŠ‚ç‚¹å¯¼èˆªåˆ°è§†è§‰ç›®æ ‡æ—¶åˆ‡æ¢åˆ°å›¾åƒç›®æ ‡æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02395",
            "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
            "url": "https://huggingface.co/papers/2512.02395",
            "abstract": "Skywork-R1V4, a 30B parameter multimodal agentic model, achieves top performance in perception and multimodal search benchmarks through supervised fine-tuning and interleaved reasoning without reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
            "score": 46,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "9080bcc384ffe813",
            "authors": [
                "Yifan Zhang",
                "Liang Hu",
                "Haofeng Sun",
                "Peiyu Wang",
                "Yichen Wei",
                "Shukang Yin",
                "Jiangbo Pei",
                "Wei Shen",
                "Peng Xia",
                "Yi Peng",
                "Tianyidan Xie",
                "Eric Li",
                "Yang Liu",
                "Xuchen Song",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02395.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Skywork-R1V4 â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ÑÑÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Gemini 2.5 Flash Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Unifying Multimodal Intelligence Without Reinforcement Learning",
                    "desc": "Skywork-R1V4 is a 30 billion parameter multimodal agentic model that excels in perception and multimodal search tasks. It integrates image manipulation and web search capabilities, overcoming the limitations of previous models that treated these tasks separately and relied on reinforcement learning. The model is trained using supervised fine-tuning on a dataset of high-quality planning-execution trajectories, allowing it to perform interleaved reasoning effectively. As a result, Skywork-R1V4 achieves state-of-the-art performance in various benchmarks, demonstrating that advanced multimodal intelligence can be developed without reinforcement learning."
                },
                "zh": {
                    "title": "Skywork-R1V4ï¼šæ— å¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€æ™ºèƒ½æ–°çªç ´",
                    "desc": "Skywork-R1V4æ˜¯ä¸€ä¸ªæ‹¥æœ‰300äº¿å‚æ•°çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œäº¤é”™æ¨ç†åœ¨æ„ŸçŸ¥å’Œå¤šæ¨¡æ€æœç´¢åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¡¶å°–è¡¨ç°ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSkywork-R1V4å°†å¤šæ¨¡æ€è§„åˆ’ã€ä¸»åŠ¨å›¾åƒæ“ä½œå’Œæ·±åº¦å¤šæ¨¡æ€æœç´¢ç»Ÿä¸€åœ¨ä¸€èµ·ï¼Œé¿å…äº†å¯¹æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ çš„ä¾èµ–ã€‚è¯¥æ¨¡å‹ä»…é€šè¿‡å¯¹å°‘äº30,000æ¡é«˜è´¨é‡çš„è§„åˆ’æ‰§è¡Œä¸€è‡´è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡é€æ­¥ä¸€è‡´æ€§è¿‡æ»¤è¿›è¡ŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒSkywork-R1V4åœ¨æ¨ç†æ—¶å±•ç°å‡ºå¤æ‚çš„é•¿æ—¶é—´æ¨ç†èƒ½åŠ›ï¼ŒæˆåŠŸåè°ƒè¶…è¿‡10æ¬¡å·¥å…·è°ƒç”¨ä»¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23127",
            "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
            "url": "https://huggingface.co/papers/2511.23127",
            "abstract": "DualCamCtrl is a diffusion model for camera-controlled video generation that uses a dual-branch framework and Semantic Guided Mutual Alignment to improve consistency and disentangle appearance and geometry modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
            "score": 43,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "37a9eaf374ccf957",
            "authors": [
                "Hongfei Zhang",
                "Kanghao Chen",
                "Zixin Zhang",
                "Harold Haodong Chen",
                "Yuanhuiyi Lyu",
                "Yuqi Zhang",
                "Shuai Yang",
                "Kun Zhou",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Fudan University",
                "HKUST",
                "HKUST (GZ)",
                "Knowin",
                "Shenzhen University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23127.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹",
                    "desc": "DualCamCtrl Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ RGB Ğ¸ depth Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Semantic Guided Mutual Alignment (SIGMA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ RGB Ğ¸ depth Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 40% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Generation with Camera Control and Depth Awareness",
                    "desc": "DualCamCtrl is a cutting-edge diffusion model designed for generating videos that are controlled by camera movements. It employs a dual-branch framework to create both RGB (color) and depth sequences that are consistent with each other, addressing previous limitations in scene understanding. The model introduces a Semantic Guided Mutual Alignment (SIGMA) mechanism, which enhances the integration of RGB and depth data by using semantic information. Through extensive testing, DualCamCtrl demonstrates a significant improvement in video generation accuracy, reducing camera motion errors by over 40% compared to earlier methods."
                },
                "zh": {
                    "title": "DualCamCtrlï¼šæå‡ç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§",
                    "desc": "DualCamCtrlæ˜¯ä¸€ç§ç”¨äºç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨åŒåˆ†æ”¯æ¡†æ¶å’Œè¯­ä¹‰å¼•å¯¼çš„äº’å¯¹é½æœºåˆ¶ï¼Œä»¥æé«˜ä¸€è‡´æ€§å¹¶è§£è€¦å¤–è§‚å’Œå‡ ä½•å»ºæ¨¡ã€‚è¯¥æ¨¡å‹é€šè¿‡äº’ç›¸ç”Ÿæˆç›¸æœºä¸€è‡´çš„RGBå’Œæ·±åº¦åºåˆ—ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç†è§£å’Œå‡ ä½•æ„è¯†æ–¹é¢çš„ä¸è¶³ã€‚è¯­ä¹‰å¼•å¯¼çš„äº’å¯¹é½æœºåˆ¶ï¼ˆSIGMAï¼‰åœ¨è¯­ä¹‰å¼•å¯¼ä¸‹èåˆRGBå’Œæ·±åº¦ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºäº†ä¸¤ç§æ¨¡æ€çš„åè°ƒæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDualCamCtrlåœ¨ç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ä¸€è‡´æ€§ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæ‘„åƒæœºè¿åŠ¨è¯¯å·®å‡å°‘äº†è¶…è¿‡40%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23369",
            "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
            "url": "https://huggingface.co/papers/2511.23369",
            "abstract": "A simulation framework improves autonomous driving by generating diverse, high-fidelity driving scenarios, leading to better generalization and robustness in real-world testing.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "e1e00c15d362710c",
            "authors": [
                "Haochen Tian",
                "Tianyu Li",
                "Haochen Liu",
                "Jiazhi Yang",
                "Yihang Qiu",
                "Guang Li",
                "Junli Wang",
                "Yinfeng Gao",
                "Zhang Zhang",
                "Liang Wang",
                "Hangjun Ye",
                "Tieniu Tan",
                "Long Chen",
                "Hongyang Li"
            ],
            "affiliations": [
                "Huawei Noah's Ark Lab",
                "Huawei Technologies",
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23369.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞ³Ğ¾-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞŸÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ½Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ robustness Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² - Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ´Ğ¾ 6.8 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SimScale: Enhancing Autonomous Driving with Scalable Simulation",
                    "desc": "This paper presents a simulation framework designed to enhance autonomous driving systems by generating a wide variety of realistic driving scenarios. The framework addresses the challenge of limited data diversity in real-world driving logs, particularly for rare and critical situations. By employing advanced neural rendering techniques, it creates high-fidelity multi-view observations and simulates new driving states, which are then used to train decision-making policies. The results show that combining real and simulated data significantly improves the robustness and generalization of planning methods in real-world tests, demonstrating the effectiveness of this approach in scaling up training data."
                },
                "zh": {
                    "title": "æ¨¡æ‹Ÿæ¡†æ¶æå‡è‡ªåŠ¨é©¾é©¶çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜ä¿çœŸçš„é©¾é©¶åœºæ™¯æ¥æ”¹å–„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆæˆå¤§é‡æœªè§çŠ¶æ€ï¼Œä»¥è¡¥å……ç°å®ä¸–ç•Œæ•°æ®çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®å’Œåˆ†å¸ƒå¤–çš„åœºæ™¯ä¸­ã€‚é€šè¿‡ä½¿ç”¨å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å’Œååº”ç¯å¢ƒï¼Œç”Ÿæˆçš„å¤šè§†è§’è§‚å¯Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶è‡ªæˆ‘è½¨è¿¹çš„æ‰°åŠ¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä¼ªä¸“å®¶è½¨è¿¹ç”Ÿæˆæœºåˆ¶ï¼Œä¸ºæ–°æ¨¡æ‹ŸçŠ¶æ€æä¾›åŠ¨ä½œç›‘ç£ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01822",
            "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
            "url": "https://huggingface.co/papers/2512.01822",
            "abstract": "InnoGym is a benchmark and framework that evaluates the innovation potential of AI agents using performance gain and novelty metrics, highlighting a gap between creativity and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
            "score": 33,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "9839c65abaaa9000",
            "authors": [
                "Jintian Zhang",
                "Kewei Xu",
                "Jingsheng Zheng",
                "Zhuoyun Yu",
                "Yuqi Zhu",
                "Yujie Luo",
                "Lanning Wei",
                "Shuofei Qiao",
                "Lun Du",
                "Da Zheng",
                "Shumin Deng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "National University of Singapore",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01822.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "InnoGym â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 18 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ iGym Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ¾Ğ¼ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ AI-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Bridging Creativity and Effectiveness in AI Innovation",
                    "desc": "InnoGym is a new benchmark and framework designed to assess the innovation potential of AI agents by focusing on both performance gain and novelty metrics. It addresses the limitation of existing benchmarks that mainly evaluate correctness, ignoring the diversity of methods used to achieve solutions. The framework includes 18 tasks from real-world engineering and scientific fields, ensuring standardized evaluation through rigorous validation processes. The findings reveal a significant gap between creativity and effectiveness, emphasizing the importance of measuring both aspects in AI development."
                },
                "zh": {
                    "title": "è¯„ä¼°AIåˆ›æ–°æ½œåŠ›çš„æ–°åŸºå‡†â€”â€”InnoGym",
                    "desc": "InnoGymæ˜¯ä¸€ä¸ªåŸºå‡†å’Œæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†çš„åˆ›æ–°æ½œåŠ›ï¼Œé‡‡ç”¨æ€§èƒ½æå‡å’Œæ–°é¢–æ€§æŒ‡æ ‡ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨æ­£ç¡®æ€§ï¼Œå¿½è§†äº†è§£å†³æ–¹æ¡ˆèƒŒåçš„æ–¹æ³•å¤šæ ·æ€§ã€‚çœŸæ­£çš„åˆ›æ–°ä¸ä»…ä¾èµ–äºäº§ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜éœ€è¦æ–¹æ³•çš„ç‹¬åˆ›æ€§ã€‚InnoGymé€šè¿‡å¼•å…¥æ€§èƒ½æå‡å’Œæ–°é¢–æ€§ä¸¤ä¸ªäº’è¡¥æŒ‡æ ‡ï¼Œç³»ç»Ÿåœ°è¯„ä¼°AIä»£ç†çš„åˆ›æ–°èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01078",
            "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
            "url": "https://huggingface.co/papers/2512.01078",
            "abstract": "SimWorld, a new Unreal Engine 5-based simulator, enables the development and evaluation of LLM/VLM agents in realistic, real-world-like settings with diverse physical and social reasoning scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
            "score": 33,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "c4e6339aaca93f31",
            "authors": [
                "Jiawei Ren",
                "Yan Zhuang",
                "Xiaokang Ye",
                "Lingjun Mao",
                "Xuhong He",
                "Jianzhi Shen",
                "Mrinaal Dogra",
                "Yiming Liang",
                "Ruixuan Zhang",
                "Tianai Yue",
                "Yiqing Yang",
                "Eric Liu",
                "Ryan Wu",
                "Kevin Benavente",
                "Rajiv Mandya Nagaraju",
                "Muhammad Faayez",
                "Xiyan Zhang",
                "Dhruv Vivek Sharma",
                "Xianrui Zhong",
                "Ziqiao Ma",
                "Tianmin Shu",
                "Zhiting Hu",
                "Lianhui Qin"
            ],
            "affiliations": [
                "JHU",
                "PolyU",
                "Purdue",
                "UCSD",
                "UIUC",
                "UMich",
                "USC",
                "UVA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01078.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#robotics",
                    "#games",
                    "#dataset",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "SimWorld â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Unreal Engine 5, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ¸ VLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ… Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. SimWorld Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹Ğº-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (GPT-4o, Gemini, Claude, DeepSeek) Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "SimWorld: Bridging AI Agents with Real-World Challenges",
                    "desc": "SimWorld is a new simulator built on Unreal Engine 5 that allows for the development and testing of large language model (LLM) and vision-language model (VLM) agents in realistic environments. It addresses the challenges faced by AI agents in complex physical and social settings by providing a platform for extensive interaction and reasoning. The simulator features realistic world dynamics, a rich interface for agent interaction, and customizable scenarios for diverse reasoning tasks. By deploying advanced LLM agents in multi-agent tasks, SimWorld reveals unique reasoning patterns and limitations, paving the way for improved real-world AI applications."
                },
                "zh": {
                    "title": "SimWorldï¼šæ¨åŠ¨çœŸå®ä¸–ç•Œæ™ºèƒ½ä»£ç†çš„æœªæ¥",
                    "desc": "SimWorldæ˜¯ä¸€ä¸ªåŸºäºè™šå¹»å¼•æ“5çš„æ–°å‹æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨å¼€å‘å’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»£ç†ã€‚è¯¥æ¨¡æ‹Ÿå™¨æä¾›äº†çœŸå®çš„ä¸–ç•Œæ¨¡æ‹Ÿï¼ŒåŒ…æ‹¬å‡†ç¡®çš„ç‰©ç†å’Œç¤¾ä¼šåŠ¨æ€ï¼Œä»¥åŠåŸºäºè¯­è¨€çš„ç¨‹åºåŒ–ç¯å¢ƒç”Ÿæˆã€‚SimWorldè¿˜ä¸ºLLM/VLMä»£ç†æä¾›äº†ä¸°å¯Œçš„æ¥å£ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œå¼€æ”¾è¯æ±‡çš„åŠ¨ä½œé€‰æ‹©ã€‚é€šè¿‡åœ¨é•¿æ—¶é—´è·¨åº¦çš„å¤šä»£ç†äº¤ä»˜ä»»åŠ¡ä¸­éƒ¨ç½²å‰æ²¿LLMä»£ç†ï¼ŒSimWorldå±•ç¤ºäº†ä¸åŒæ¨¡å‹çš„æ¨ç†æ¨¡å¼å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20645",
            "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
            "url": "https://huggingface.co/papers/2511.20645",
            "abstract": "PixelDiT is a single-stage, end-to-end diffusion model that operates directly in pixel space, overcoming the limitations of latent-space modeling by using a dual-level transformer architecture and achieving competitive performance in image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "d71da82a71d1fdff",
            "authors": [
                "Yongsheng Yu",
                "Wei Xiong",
                "Weili Nie",
                "Yichen Sheng",
                "Shiqiu Liu",
                "Jiebo Luo"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Rochester"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20645.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ",
                    "desc": "PixelDiT â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°: Ğ¾Ğ´Ğ¸Ğ½ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ğ° Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. PixelDiT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "PixelDiT: Revolutionizing Image Generation in Pixel Space",
                    "desc": "PixelDiT is a novel diffusion model that operates directly in pixel space, eliminating the need for lossy latent-space modeling. By utilizing a dual-level transformer architecture, it effectively captures both global semantics and fine texture details in images. This single-stage, end-to-end approach allows for efficient training and joint optimization without the errors associated with autoencoders. PixelDiT demonstrates superior performance in image and text-to-image generation, achieving impressive metrics that surpass existing models."
                },
                "zh": {
                    "title": "PixelDiTï¼šç›´æ¥åœ¨åƒç´ ç©ºé—´ä¸­å®ç°é«˜æ•ˆç”Ÿæˆ",
                    "desc": "PixelDiTæ˜¯ä¸€ç§å•é˜¶æ®µã€ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥åœ¨åƒç´ ç©ºé—´ä¸­æ“ä½œï¼Œå…‹æœäº†æ½œåœ¨ç©ºé—´å»ºæ¨¡çš„å±€é™æ€§ã€‚å®ƒé‡‡ç”¨åŒå±‚å˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚é€šè¿‡æ¶ˆé™¤å¯¹è‡ªç¼–ç å™¨çš„éœ€æ±‚ï¼ŒPixelDiTç›´æ¥å­¦ä¹ æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œé¿å…äº†æŸå¤±é‡å»ºå¸¦æ¥çš„è¯¯å·®ç´¯ç§¯ã€‚è¯¥æ¨¡å‹åœ¨ImageNet 256x256ä¸Šè¾¾åˆ°äº†1.61çš„FIDï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åƒç´ ç”Ÿæˆæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02899",
            "title": "Glance: Accelerating Diffusion Models with 1 Sample",
            "url": "https://huggingface.co/papers/2512.02899",
            "abstract": "Using phase-aware LoRA adapters, diffusion models achieve efficient acceleration and strong generalization with minimal retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "fe6fbbc856be3ce1",
            "authors": [
                "Zhuobai Dong",
                "Rui Zhao",
                "Songjie Wu",
                "Junchao Yi",
                "Linjie Li",
                "Zhengyuan Yang",
                "Lijuan Wang",
                "Alex Jinpeng Wang"
            ],
            "affiliations": [
                "CSU",
                "Microsoft",
                "NUS",
                "UESTC",
                "WHU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02899.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ·Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ°Ğ·Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸: Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ·Ñ‹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ²Ğ° Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° (Slow-LoRA Ğ¸ Fast-LoRA) Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ÑÑ‚Ğ¸ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ - Ğ²ÑĞµĞ³Ğ¾ Ñ‡Ğ°Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ñ 1K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Smart Acceleration with Phase-Aware LoRA Adapters",
                    "desc": "This paper presents a novel approach to enhance the efficiency of diffusion models in image generation by using phase-aware LoRA adapters. Instead of retraining entire models, the authors propose a strategy that applies different levels of acceleration to various stages of the denoising process. By introducing Slow-LoRA and Fast-LoRA adapters, they achieve significant speed improvements while preserving high visual quality. Their method demonstrates that minimal retraining can lead to strong generalization on unseen data, achieving up to 5 times acceleration over the base model."
                },
                "zh": {
                    "title": "é˜¶æ®µæ„ŸçŸ¥LoRAé€‚é…å™¨ï¼šé«˜æ•ˆåŠ é€Ÿä¸å¼ºæ³›åŒ–çš„ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé˜¶æ®µæ„ŸçŸ¥çš„LoRAé€‚é…å™¨ï¼Œç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡åœ¨æ—©æœŸè¯­ä¹‰é˜¶æ®µåº”ç”¨è¾ƒå°çš„åŠ é€Ÿï¼Œè€Œåœ¨åæœŸå†—ä½™é˜¶æ®µåº”ç”¨è¾ƒå¤§çš„åŠ é€Ÿï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è½»é‡çº§çš„LoRAé€‚é…å™¨å¯ä»¥åœ¨ä¸è¿›è¡Œå¤§é‡é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜æ•ˆåŠ é€Ÿå’Œå¼ºæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†æ¨¡å‹çš„æ¨ç†é€Ÿåº¦æé«˜è‡³åŸºçº¿æ¨¡å‹çš„äº”å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02425",
            "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
            "url": "https://huggingface.co/papers/2512.02425",
            "abstract": "WorldMM, a novel multimodal memory agent with episodic, semantic, and visual memory, outperforms existing methods in long video question-answering by adaptively retrieving from multiple temporal scales and memory sources.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "318a17e40ff42f8c",
            "authors": [
                "Woongyeong Yeo",
                "Kangsan Kim",
                "Jaehong Yoon",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02425.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "WorldMM â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ (ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…), ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ (ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ) Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ (Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑÑ†ĞµĞ½). ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ñ‘Ñ€Ğ½Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. WorldMM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 8.4% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡Ğ°ÑÑ‹ Ğ¸ Ğ´Ğ½Ğ¸."
                },
                "en": {
                    "title": "WorldMM: Revolutionizing Long Video Understanding with Multimodal Memory",
                    "desc": "WorldMM is a new multimodal memory agent designed to improve long video question-answering by using different types of memory. It combines episodic, semantic, and visual memory to effectively retrieve information from various temporal scales. This approach allows it to adaptively select the most relevant memory source based on the query, ensuring that both textual and visual details are utilized. As a result, WorldMM outperforms existing methods, achieving a significant performance gain in understanding complex scenes in long videos."
                },
                "zh": {
                    "title": "WorldMMï¼šé•¿è§†é¢‘é—®ç­”çš„æ–°çºªå…ƒ",
                    "desc": "WorldMMæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€è®°å¿†ä»£ç†ï¼Œç»“åˆäº†æƒ…èŠ‚è®°å¿†ã€è¯­ä¹‰è®°å¿†å’Œè§†è§‰è®°å¿†ï¼Œèƒ½å¤Ÿåœ¨é•¿è§†é¢‘é—®ç­”ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»å¤šä¸ªæ—¶é—´å°ºåº¦å’Œè®°å¿†æºè‡ªé€‚åº”åœ°æ£€ç´¢ä¿¡æ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶çš„å±€é™æ€§ã€‚WorldMMçš„æƒ…èŠ‚è®°å¿†ç´¢å¼•å¤šä¸ªæ—¶é—´å°ºåº¦çš„äº‹å®äº‹ä»¶ï¼Œè¯­ä¹‰è®°å¿†æŒç»­æ›´æ–°é«˜å±‚æ¬¡æ¦‚å¿µçŸ¥è¯†ï¼Œè€Œè§†è§‰è®°å¿†åˆ™ä¿ç•™åœºæ™¯çš„è¯¦ç»†ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒWorldMMåœ¨äº”ä¸ªé•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹³å‡æå‡äº†8.4%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03036",
            "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
            "url": "https://huggingface.co/papers/2512.03036",
            "abstract": "ViSAudio, an end-to-end framework using conditional flow matching, generates high-quality binaural audio from silent video, providing spatial immersion and consistency across various acoustic conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "78221fe3c4c7b12b",
            "authors": [
                "Mengchen Zhang",
                "Qi Chen",
                "Tong Wu",
                "Zihan Liu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Beihang University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03036.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#open_source",
                    "#video",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ²ÑƒĞº Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ViSAudio â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ·Ğ²ÑƒĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ BiAudio Ñ 97 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ°Ñ€ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²ÑƒĞºĞ°, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ²ÑƒĞº, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ²ÑƒĞºĞ°."
                },
                "en": {
                    "title": "Transforming Silent Video into Immersive Binaural Audio",
                    "desc": "ViSAudio is a novel framework designed to generate high-quality binaural audio directly from silent video, enhancing spatial immersion. Unlike traditional methods that first create mono audio and then spatialize it, ViSAudio employs an end-to-end approach using conditional flow matching. This method utilizes a dual-branch architecture to model audio latent flows, ensuring precise alignment between audio and video. The framework is supported by the BiAudio dataset, which contains a large variety of video-audio pairs, allowing it to adapt to different acoustic conditions and viewpoint changes effectively."
                },
                "zh": {
                    "title": "ä»é™é»˜è§†é¢‘ç”Ÿæˆé«˜è´¨é‡åŒè€³éŸ³é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ViSAudioæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ¡ä»¶æµåŒ¹é…æŠ€æœ¯ï¼Œä»é™é»˜è§†é¢‘ç”Ÿæˆé«˜è´¨é‡çš„åŒè€³éŸ³é¢‘ã€‚ä¸ä¼ ç»Ÿçš„å•å£°é“è¾“å‡ºä¸åŒï¼ŒViSAudioç›´æ¥ç”ŸæˆåŒè€³ç©ºé—´éŸ³é¢‘ï¼Œè§£å†³äº†ä»¥å¾€ä¸¤é˜¶æ®µæµç¨‹ä¸­äº§ç”Ÿçš„è¯¯å·®ç´¯ç§¯å’Œæ—¶ç©ºä¸ä¸€è‡´é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒåˆ†æ”¯éŸ³é¢‘ç”Ÿæˆæ¶æ„ï¼Œç¡®ä¿éŸ³é¢‘ä¸è¾“å…¥è§†é¢‘ä¹‹é—´çš„æ—¶ç©ºå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViSAudioåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”è§†è§’å˜åŒ–å’Œå£°æºè¿åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00956",
            "title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
            "url": "https://huggingface.co/papers/2512.00956",
            "abstract": "Optimal linear blockwise transforms for joint weight-activation quantization improve upon standard orthogonal transforms like the Hadamard transform by incorporating data statistics.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "6230f23959717e27",
            "authors": [
                "Jiale Chen",
                "Vage Egiazarian",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "affiliations": [
                "ETH ZÃ¼rich",
                "Institute of Science and Technology Austria (ISTA)",
                "Red Hat AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00956.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ‚Ğ¸Ğ¿Ğ° ĞĞ´Ğ°Ğ¼Ğ°Ñ€Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ WUSH ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ĞĞ´Ğ°Ğ¼Ğ°Ñ€Ğ° Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞĞ´Ğ°Ğ¼Ğ°Ñ€Ğ° Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "WUSH: Optimal Transforms for Smarter Quantization",
                    "desc": "This paper presents a new method for quantizing weights and activations in machine learning models, which improves upon traditional methods like the Hadamard transform. The authors introduce optimal linear blockwise transforms that take into account the statistics of the data, addressing the limitations of fixed orthogonal transforms. They derive closed-form solutions for adaptive transforms that enhance quantization for both integer and floating-point formats. The proposed method, named WUSH, combines a Hadamard structure with data-aware components, leading to better performance in quantization tasks."
                },
                "zh": {
                    "title": "æ•°æ®é©±åŠ¨çš„æœ€ä¼˜é‡åŒ–å˜æ¢",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çº¿æ€§å—å˜æ¢æ–¹æ³•ï¼Œç”¨äºè”åˆæƒé‡å’Œæ¿€æ´»çš„é‡åŒ–ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ­£äº¤å˜æ¢å¦‚å“ˆè¾¾ç›å˜æ¢ã€‚é€šè¿‡è€ƒè™‘æ•°æ®ç»Ÿè®¡ç‰¹æ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¨å¯¼å‡ºé—­å¼å½¢å¼çš„æœ€ä¼˜å˜æ¢ï¼Œé€‚ç”¨äºå¸¸è§çš„æ•°å€¼æ ¼å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºWUSHï¼Œç»“åˆäº†å“ˆè¾¾ç›å˜æ¢çš„åŸºç¡€å’ŒåŸºäºäºŒé˜¶çŸ©çš„æ•°æ®ä¾èµ–ç»„ä»¶ï¼Œå½¢æˆäº†ä¸€ç§éæ­£äº¤å˜æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸¸è§æ ¼å¼ä¸‹çš„è¡¨ç°ä¼˜äºå“ˆè¾¾ç›å˜æ¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19433",
            "title": "Mixture of Horizons in Action Chunking",
            "url": "https://huggingface.co/papers/2511.19433",
            "abstract": "A mixture of horizons strategy in VLA models improves performance and generalizability by combining long-term foresight and short-term precision, achieving higher throughput and superior performance in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies Ï€_0, Ï€_{0.5}, and one-step regression policy Ï€_{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, Ï€_{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "e7d6e4a1a9871f0a",
            "authors": [
                "Dong Jing",
                "Gang Wang",
                "Jiaqi Liu",
                "Weiliang Tang",
                "Zelong Sun",
                "Yunchao Yao",
                "Zhenyu Wei",
                "Yunhui Liu",
                "Zhiwu Lu",
                "Mingyu Ding"
            ],
            "affiliations": [
                "CUHK",
                "RUC",
                "UNC"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19433.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² (MoH) Ğ´Ğ»Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ´Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MoH Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing the Power of Mixed Horizons for Superior Robotic Performance",
                    "desc": "This paper introduces a mixture of horizons (MoH) strategy for vision-language-action (VLA) models in robotic manipulation. The MoH approach combines different action chunk lengths, allowing the model to leverage both long-term foresight and short-term precision simultaneously. By processing these varied horizons in parallel and fusing their outputs, the model achieves improved performance and generalizability on complex tasks. The results show that MoH significantly enhances throughput and success rates in both simulations and real-world applications, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "æ··åˆè§†é‡ç­–ç•¥æå‡æœºå™¨äººæ“ä½œæ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆè§†é‡ç­–ç•¥ï¼ˆMoHï¼‰ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å°†åŠ¨ä½œå—åˆ†æˆä¸åŒçš„è§†é‡æ®µå¹¶å¹¶è¡Œå¤„ç†ï¼ŒMoHèƒ½å¤ŸåŒæ—¶åˆ©ç”¨é•¿è¿œçš„å‰ç»æ€§å’ŒçŸ­æœŸçš„ç²¾ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMoHåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ååé‡å’Œä¼˜è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ··åˆä»»åŠ¡è®¾ç½®ä¸‹ï¼Œè¾¾åˆ°äº†99%çš„æˆåŠŸç‡ã€‚è¯¥æ–¹æ³•å…·æœ‰å³æ’å³ç”¨çš„ç‰¹æ€§ï¼Œé€‚ç”¨äºå…¨æ³¨æ„åŠ›åŠ¨ä½œæ¨¡å—ï¼Œä¸”è®­ç»ƒå’Œæ¨ç†å¼€é”€æå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02581",
            "title": "GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies",
            "url": "https://huggingface.co/papers/2512.02581",
            "abstract": "GoRL separates optimization and generation to achieve stable and expressive policies in reinforcement learning, outperforming Gaussian and generative-policy baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "fd0f15771f8b2f71",
            "authors": [
                "Chubin Zhang",
                "Zhenglin Wan",
                "Feng Chen",
                "Xingrui Yu",
                "Ivor Tsang",
                "Bo An"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications, China",
                "Centre for Frontier AI Research, A*STAR, Singapore",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02581.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#rl"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞÑ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº",
                    "desc": "GoRL Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ - Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ Ñ‚Ñ€Ğ°ĞºÑ‚ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ, Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ²ÑƒÑ…ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾, Ğ¿Ğ¾ĞºĞ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Decoupling Optimization and Generation for Better RL Policies",
                    "desc": "GoRL (Generative Online Reinforcement Learning) is a new framework that improves reinforcement learning by separating the processes of optimization and action generation. Traditional methods struggle between stability and expressiveness, as stable policies often lack the complexity needed for diverse actions. GoRL uses a latent policy for stable optimization while employing a generative decoder to create more complex actions. This approach allows GoRL to outperform both Gaussian and generative-policy methods in various continuous-control tasks, achieving significantly higher performance."
                },
                "zh": {
                    "title": "åˆ†ç¦»ä¼˜åŒ–ä¸ç”Ÿæˆï¼Œå®ç°ç¨³å®šä¸è¡¨è¾¾èƒ½åŠ›çš„å¹³è¡¡",
                    "desc": "GoRLï¼ˆç”Ÿæˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼‰é€šè¿‡å°†ä¼˜åŒ–ä¸ç”Ÿæˆåˆ†ç¦»ï¼Œè§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­ç¨³å®šæ€§ä¸è¡¨è¾¾èƒ½åŠ›ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ–¹æ³•ä¼˜åŒ–ä¸€ä¸ªå¯å¤„ç†çš„æ½œåœ¨ç­–ç•¥ï¼ŒåŒæ—¶åˆ©ç”¨æ¡ä»¶ç”Ÿæˆè§£ç å™¨åˆæˆåŠ¨ä½œï¼Œä»è€Œæé«˜äº†ç­–ç•¥çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡ä¸¤ç§æ—¶é—´å°ºåº¦çš„æ›´æ–°æœºåˆ¶ï¼ŒGoRLèƒ½å¤Ÿåœ¨ä¿æŒæ½œåœ¨ç­–ç•¥ç¨³å®šå­¦ä¹ çš„åŒæ—¶ï¼Œé€æ­¥å¢å¼ºè§£ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGoRLåœ¨å¤šé¡¹è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†é«˜æ–¯ç­–ç•¥å’Œå…¶ä»–ç”Ÿæˆç­–ç•¥åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02457",
            "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
            "url": "https://huggingface.co/papers/2512.02457",
            "abstract": "Audio-video joint denoising improves video generation quality by regularizing video dynamics through audio as a privileged signal, as demonstrated by the AVFullDiT architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "6ecfda84208bf83c",
            "authors": [
                "Jianzong Wu",
                "Hao Lian",
                "Dachao Hao",
                "Ye Tian",
                "Qingyu Shi",
                "Biaolong Chen",
                "Hao Jiang",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Alibaba Group",
                "Peking University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02457.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#audio",
                    "#multimodal",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑƒĞ´Ğ¸Ğ¾ ĞºĞ°Ğº ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾: ĞºÑ€Ğ¾ÑÑĞ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ AVFullDiT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-video Ğ¸ text-to-audio Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ», Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ñ… Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Video Quality through Audio-Video Joint Denoising",
                    "desc": "This paper explores how combining audio and video data can enhance the quality of video generation. The authors introduce a new architecture called AVFullDiT, which uses pre-trained models for both text-to-video and text-to-audio tasks to perform joint denoising. They demonstrate that training with both audio and video leads to better video quality, even when the focus is solely on the video. The study shows that audio can help the model understand the relationships between visual events and their sounds, improving the overall dynamics of the generated video."
                },
                "zh": {
                    "title": "éŸ³é¢‘è§†é¢‘è”åˆå»å™ªæå‡è§†é¢‘ç”Ÿæˆè´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†éŸ³é¢‘å’Œè§†é¢‘è”åˆå»å™ªå¯¹è§†é¢‘ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒéŸ³é¢‘ä½œä¸ºä¼˜å…ˆä¿¡å·å¯ä»¥å¸®åŠ©æ”¹å–„è§†é¢‘åŠ¨æ€ï¼Œä»è€Œæé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œç§°ä¸ºAVFullDiTï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘å’Œæ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å—è¿›è¡Œè”åˆå»å™ªã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒéŸ³é¢‘è§†é¢‘è”åˆå»å™ªä¸ä»…èƒ½æé«˜åŒæ­¥æ€§ï¼Œè¿˜èƒ½åœ¨å¤æ‚åœºæ™¯ä¸­æ˜¾è‘—æ”¹å–„è§†é¢‘è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20344",
            "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2511.20344",
            "abstract": "LLMs can encode and apply high-level relational concepts in analogical reasoning but face limitations, particularly when relational information is missing or when transferring to new entities.  \t\t\t\t\tAI-generated summary \t\t\t\t Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "f1888b70ff8fd84e",
            "authors": [
                "Taewhoo Lee",
                "Minju Song",
                "Chanwoong Yoon",
                "Jungwoo Park",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20344.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#transfer_learning",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞµÑ‚Ğ¸, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ¸ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ğ»Ğ¸Ğ±Ğ¾ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ»Ğ¸Ğ±Ğ¾ Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ LLM Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking Analogical Reasoning in LLMs: Potential and Limitations",
                    "desc": "This paper investigates how large language models (LLMs) handle analogical reasoning, which is crucial for human-like thinking. The authors find that LLMs can encode relationships between entities but struggle when relational information is absent or when applying learned concepts to new situations. They also discover that enhancing certain hidden representations can help LLMs transfer information better. Overall, the study highlights both the potential and limitations of LLMs in mimicking human cognitive processes related to analogies."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹çš„ç±»æ¯”æ¨ç†èƒ½åŠ›ä¸å±€é™æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç±»æ¯”æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å…³ç³»ä¿¡æ¯æˆ–è½¬ç§»åˆ°æ–°å®ä½“æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç±»æ¯”å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œä½†åœ¨ç¼ºä¹å…³ç³»ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸äººç±»ä¸åŒï¼ŒLLMsåœ¨å°†å…³ç³»åº”ç”¨äºæ–°å®ä½“æ—¶ä¹Ÿé¢ä¸´å›°éš¾ã€‚æˆåŠŸçš„ç±»æ¯”æ¨ç†ä¾èµ–äºç±»æ¯”æƒ…å¢ƒä¹‹é—´çš„å¼ºç»“æ„å¯¹é½ï¼Œè€Œå¤±è´¥åˆ™å¸¸å¸¸åæ˜ å‡ºå¯¹é½çš„é€€åŒ–æˆ–é”™è¯¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03046",
            "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
            "url": "https://huggingface.co/papers/2512.03046",
            "abstract": "MagicQuill V2 introduces a layered composition paradigm for generative image editing, combining diffusion models with granular control, enabling clear separation and manipulation of user intentions for content, position, shape, and color.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose MagicQuill V2, a novel system that introduces a layered composition paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "f9db573eb8be0401",
            "authors": [
                "Zichen Liu",
                "Yue Yu",
                "Hao Ouyang",
                "Qiuyu Wang",
                "Shuailei Ma",
                "Ka Leong Cheng",
                "Wen Wang",
                "Qingyan Bai",
                "Yuxuan Zhang",
                "Yanhong Zeng",
                "Yixuan Li",
                "Xing Zhu",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group",
                "CUHK",
                "HKUST",
                "NEU",
                "ZJU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03046.jpg",
            "data": {
                "categories": [
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "MagicQuill V2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ¼Ñ‹ÑĞµĞ» Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸: ÑĞ»Ğ¾Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ (Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ), Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ (Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ‚ÑŒ), ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ (Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°) Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹ (Ğ¿Ğ°Ğ»Ğ¸Ñ‚Ñ€Ğ°). Ğ”Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Creators with Layered Control in Image Generation",
                    "desc": "MagicQuill V2 is a new system for generative image editing that uses a layered composition approach. It combines the strengths of diffusion models, which are good at generating images, with the detailed control found in traditional graphic design tools. This system breaks down user intentions into separate layers for content, position, shape, and color, allowing for more precise editing. Experiments show that this method helps users have better control over their creative process, making it easier to achieve their desired results."
                },
                "zh": {
                    "title": "åˆ†å±‚ç»„åˆï¼Œç›´è§‚æ§åˆ¶ç”Ÿæˆå›¾åƒ",
                    "desc": "MagicQuill V2 æ˜¯ä¸€ä¸ªæ–°ç³»ç»Ÿï¼Œé‡‡ç”¨åˆ†å±‚ç»„åˆèŒƒå¼è¿›è¡Œç”Ÿæˆå›¾åƒç¼–è¾‘ã€‚å®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›å’Œä¼ ç»Ÿå›¾å½¢è½¯ä»¶çš„ç»†ç²’åº¦æ§åˆ¶ã€‚è¯¥æ–¹æ³•å°†åˆ›æ„æ„å›¾åˆ†è§£ä¸ºå¯æ§çš„è§†è§‰çº¿ç´¢ï¼ŒåŒ…æ‹¬å†…å®¹å±‚ã€ç©ºé—´å±‚ã€ç»“æ„å±‚å’Œé¢œè‰²å±‚ã€‚é€šè¿‡è¿™ç§åˆ†å±‚æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥æ›´ç›´è§‚åœ°æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œè§£å†³äº†ç”¨æˆ·æ„å›¾ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02551",
            "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.02551",
            "abstract": "CUDA-L2, a system combining large language models and reinforcement learning, optimizes Half-precision General Matrix Multiply CUDA kernels, achieving significant speedups over existing baselines in both offline and server modes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "8d68dbb36c1ff1b4",
            "authors": [
                "Songqiao Su",
                "Xiaofei Sun",
                "Xiaoya Li",
                "Albert Wang",
                "Jiwei Li",
                "Chris Shum"
            ],
            "affiliations": [
                "deep-reinforce.com"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02551.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GPU ÑĞ´ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CUDA-L2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ CUDA ÑĞ´ĞµÑ€ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° GPU ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· 1000 ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. CUDA-L2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ torch.matmul, cuBLAS Ğ¸ cuBLASLt, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 22% Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ Ğ´Ğ¾ 28.7% Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ´Ñ€Ğ° Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…, Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing CUDA Kernels with LLMs and RL",
                    "desc": "This paper introduces CUDA-L2, a novel system that leverages large language models (LLMs) and reinforcement learning (RL) to enhance the performance of Half-precision General Matrix Multiply (HGEMM) CUDA kernels. By using the execution speed of CUDA as a reward signal, CUDA-L2 explores over 1,000 configurations to optimize these kernels effectively. The results demonstrate significant performance improvements, with speedups of up to 28.7% in server mode compared to existing libraries like torch.matmul and cuBLAS. This work highlights the potential of combining LLMs with RL to automate and optimize complex computational tasks that are typically challenging for human engineers."
                },
                "zh": {
                    "title": "CUDA-L2ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–CUDAå†…æ ¸çš„åˆ›æ–°ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†CUDA-L2ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè‡ªåŠ¨ä¼˜åŒ–åŠç²¾åº¦é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ˆHGEMMï¼‰CUDAå†…æ ¸ã€‚é€šè¿‡å°†CUDAæ‰§è¡Œé€Ÿåº¦ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ï¼ŒCUDA-L2åœ¨1000ç§é…ç½®ä¸­è‡ªåŠ¨ä¼˜åŒ–HGEMMå†…æ ¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCUDA-L2åœ¨ç¦»çº¿æ¨¡å¼å’ŒæœåŠ¡å™¨æ¨¡å¼ä¸‹å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ä¸»è¦çŸ©é˜µä¹˜æ³•åŸºå‡†ã€‚è¯¥ç³»ç»Ÿå±•ç¤ºäº†å³ä½¿æ˜¯æ€§èƒ½è¦æ±‚æé«˜çš„HGEMMå†…æ ¸ï¼Œä¹Ÿå¯ä»¥é€šè¿‡LLMå¼•å¯¼çš„RLè‡ªåŠ¨åŒ–è¿›è¡Œæ”¹è¿›ï¼Œæ¢ç´¢äººç±»éš¾ä»¥å®ç°çš„é…ç½®ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02622",
            "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
            "url": "https://huggingface.co/papers/2512.02622",
            "abstract": "RULER-Bench evaluates video generation models' reasoning abilities through 40 tasks across six categories, revealing gaps in their rule coherence and providing a benchmark for future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "3999e7743da0a3c1",
            "authors": [
                "Xuming He",
                "Zehao Fan",
                "Hengjia Li",
                "Fan Zhuo",
                "Hankun Xu",
                "Senlin Cheng",
                "Di Weng",
                "Haifeng Liu",
                "Can Ye",
                "Boxi Wu"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02622.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RULER-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 40 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ñ 622 Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ text-to-video Ğ¸ image-to-video. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ GPT-o3, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ 85% ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 48.87% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Reasoning in Video Generation Models with RULER-Bench",
                    "desc": "RULER-Bench is a new benchmark designed to assess the reasoning abilities of video generation models through 40 tasks across six categories. It highlights the current limitations in rule coherence among these models, which have primarily been evaluated on visual quality and temporal consistency. By utilizing a checklist of metrics and leveraging GPT-3 for scoring, RULER-Bench provides a structured way to evaluate cognitive reasoning in video generation. The findings indicate that existing models have significant gaps in their reasoning capabilities, suggesting a need for further advancements in this area."
                },
                "zh": {
                    "title": "RULER-Benchï¼šæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "RULER-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–äº†å…­ä¸ªç±»åˆ«çš„40ä¸ªä»»åŠ¡ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨è§„åˆ™ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚å°½ç®¡è§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è§†è§‰æ„ŸçŸ¥å’Œç†è§£ä¸Šï¼Œå¿½è§†äº†åŸºäºè§„åˆ™çš„æ¨ç†èƒ½åŠ›ã€‚RULER-Benché€šè¿‡æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä¸¤ç§åŸºæœ¬èŒƒå¼ï¼Œæä¾›äº†622ä¸ªé«˜è´¨é‡æ ‡æ³¨å®ä¾‹ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åè®®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è§„åˆ™ä¸€è‡´æ€§æŒ‡æ ‡ä¸Šä»…è¾¾åˆ°48.87%ï¼Œè¡¨æ˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01715",
            "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
            "url": "https://huggingface.co/papers/2512.01715",
            "abstract": "DiG-Flow enhances VLA models' robustness by using geometric regularization to align observation and action embeddings, improving performance on complex tasks and with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "00f40bcf8148038d",
            "authors": [
                "Wanpeng Zhang",
                "Ye Wang",
                "Hao Luo",
                "Haoqi Yuan",
                "Yicheng Feng",
                "Sipeng Zheng",
                "Qin Jin",
                "Zongqing Lu"
            ],
            "affiliations": [
                "BeingBeyond",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01715.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#robotics",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "DiG-Flow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ flow matching Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¼ĞµÑ€Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğº ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ flow matching, Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ ÑĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing VLA Robustness with Geometric Regularization",
                    "desc": "DiG-Flow is a framework designed to improve the robustness of Vision-Language-Action (VLA) models by applying geometric regularization. It focuses on aligning observation and action embeddings to enhance performance, especially in complex tasks and when data is limited. The method measures the discrepancy between these embeddings, using it to adjust the representations without altering the flow matching process. Theoretical guarantees support that this approach reduces the training objective, leading to better model performance in challenging scenarios."
                },
                "zh": {
                    "title": "DiG-Flowï¼šæå‡VLAæ¨¡å‹é²æ£’æ€§çš„å‡ ä½•æ­£åˆ™åŒ–",
                    "desc": "DiG-Flow æ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é²æ£’æ€§çš„æ¡†æ¶ï¼Œé‡‡ç”¨å‡ ä½•æ­£åˆ™åŒ–æ¥å¯¹é½è§‚å¯Ÿå’ŒåŠ¨ä½œåµŒå…¥ã€‚é€šè¿‡è®¡ç®—è§‚å¯Ÿå’ŒåŠ¨ä½œåµŒå…¥çš„åˆ†å¸ƒå·®å¼‚ï¼ŒDiG-Flow æä¾›äº†æœ‰æ„ä¹‰çš„å‡ ä½•ä¿¡å·ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡å’Œæœ‰é™æ•°æ®ä¸‹çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨è¡¨ç¤ºå±‚é¢è¿›è¡Œå¹²é¢„ï¼Œè€Œä¸æ”¹å˜æµåŒ¹é…è·¯å¾„æˆ–ç›®æ ‡å‘é‡åœºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiG-Flow èƒ½å¤Ÿæœ‰æ•ˆæå‡ç°æœ‰ VLA æ¶æ„çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01248",
            "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
            "url": "https://huggingface.co/papers/2512.01248",
            "abstract": "TRivia, a self-supervised fine-tuning method, enables pretrained vision-language models to learn table recognition from unlabeled data using a question-answering-based reward mechanism, outperforming existing models on popular benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "bd64d161463ca78e",
            "authors": [
                "Junyuan Zhang",
                "Bin Wang",
                "Qintong Zhang",
                "Fan Wu",
                "Zichen Wen",
                "Jialin Lu",
                "Junjie Shan",
                "Ziqi Zhao",
                "Shuya Yang",
                "Ziling Wang",
                "Ziyang Miao",
                "Huaping Zhong",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Ka-Ho Chow",
                "Conghui He"
            ],
            "affiliations": [
                "Peking University",
                "Sensetime",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01248.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#cv",
                    "#small_models"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "TRivia â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Group Relative Policy Optimization, TRivia Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TRivia-3B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini 2.5 Pro."
                },
                "en": {
                    "title": "Unlocking Table Recognition with Self-Supervised Learning",
                    "desc": "TRivia is a self-supervised fine-tuning method designed for table recognition (TR) using pretrained vision-language models (VLMs). It allows these models to learn from unlabeled data by employing a question-answering-based reward mechanism, which helps in identifying the most informative samples for training. This approach eliminates the need for costly labeled datasets, making it accessible for open-source models that often lack resources. The result is TRivia-3B, a state-of-the-art TR model that outperforms existing proprietary systems on key benchmarks, demonstrating the effectiveness of self-supervised learning in this domain."
                },
                "zh": {
                    "title": "TRiviaï¼šæ— æ ‡æ³¨æ•°æ®ä¸‹çš„è¡¨æ ¼è¯†åˆ«æ–°æ–¹æ³•",
                    "desc": "TRiviaæ˜¯ä¸€ç§è‡ªç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿä½¿é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å­¦ä¹ è¡¨æ ¼è¯†åˆ«ã€‚å®ƒé€šè¿‡åŸºäºé—®ç­”çš„å¥–åŠ±æœºåˆ¶ï¼Œè‡ªåŠ¨è¯†åˆ«æœ€æœ‰æ•ˆçš„æœªæ ‡è®°æ ·æœ¬ï¼Œä»è€Œæ¶ˆé™¤å¯¹äººå·¥æ ‡æ³¨çš„éœ€æ±‚ã€‚TRiviaåˆ©ç”¨æ³¨æ„åŠ›å¼•å¯¼æ¨¡å—ä¸ºæ¯ä¸ªè¡¨æ ¼å›¾åƒç”Ÿæˆå¤šæ ·åŒ–çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡æ­£ç¡®å›ç­”è¿™äº›é—®é¢˜æ¥ä¼˜åŒ–è¡¨æ ¼è¯†åˆ«æ¨¡å‹ã€‚æœ€ç»ˆï¼ŒTRivia-3Bæ¨¡å‹åœ¨å¤šä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨è¡¨æ ¼è¯†åˆ«é¢†åŸŸçš„å…ˆè¿›æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03040",
            "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
            "url": "https://huggingface.co/papers/2512.03040",
            "abstract": "Video4Spatial demonstrates that video diffusion models can perform complex spatial tasks using only visual data, achieving strong spatial understanding and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "a905632bcc245598",
            "authors": [
                "Zeqi Xiao",
                "Yiwei Zhao",
                "Lingxiao Li",
                "Yushi Lan",
                "Yu Ning",
                "Rahul Garg",
                "Roshni Cooper",
                "Mohammad H. Taghavi",
                "Xingang Pan"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Netflix",
                "Netflix Eyeline Studios",
                "University of Oxford"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03040.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#video",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "Video4Spatial â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹, Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ·Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Video Models with Spatial Intelligence",
                    "desc": "Video4Spatial is a framework that shows how video diffusion models can achieve advanced spatial reasoning using only visual data. It focuses on two main tasks: scene navigation, where the model follows camera-pose instructions while adhering to the 3D structure of the scene, and object grounding, which involves locating objects based on semantic understanding and planning. The framework operates solely on video inputs, without relying on additional data like depth or poses, demonstrating effective spatial understanding. Overall, Video4Spatial pushes the boundaries of video generative models towards enhanced visuospatial intelligence."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç©ºé—´æ™ºèƒ½æ–°çªç ´",
                    "desc": "Video4Spatialå±•ç¤ºäº†è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»…ä½¿ç”¨è§†è§‰æ•°æ®æ‰§è¡Œå¤æ‚çš„ç©ºé—´ä»»åŠ¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç©ºé—´ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è§†é¢‘åœºæ™¯ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶åŒ–ï¼ŒéªŒè¯äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªä»»åŠ¡ï¼šåœºæ™¯å¯¼èˆªå’Œç‰©ä½“å®šä½ï¼Œå‡ä»…ä½¿ç”¨è§†é¢‘è¾“å…¥ï¼Œä¸ä¾èµ–æ·±åº¦æˆ–å§¿æ€ç­‰è¾…åŠ©æ¨¡æ€ã€‚ç»“æœè¡¨æ˜ï¼ŒVideo4Spatialåœ¨è§†é¢‘ä¸Šä¸‹æ–‡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§„åˆ’å¯¼èˆªå’Œå®šä½ç›®æ ‡ç‰©ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22586",
            "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
            "url": "https://huggingface.co/papers/2511.22586",
            "abstract": "Investigating different Chain-of-Thought designs in vision-language models reveals that concise grounding steps are most effective for improving generalizable visual reasoning across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "f70a8cf0c03c72d6",
            "authors": [
                "Yifan Du",
                "Kun Zhou",
                "Yingqian Min",
                "Yue Ling",
                "Wayne Xin Zhao",
                "Youbin Wu"
            ],
            "affiliations": [
                "ByteDance",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22586.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞšÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ, Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Â«ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ â€” ÑÑ‚Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ°Â» Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Less is More: Concise CoT Boosts Visual Reasoning",
                    "desc": "This paper explores how different Chain-of-Thought (CoT) designs impact the ability of vision-language models (VLMs) to perform visual reasoning tasks. The authors find that while longer CoT formats can speed up the learning process, they do not necessarily improve overall performance. Instead, concise CoT designs that focus on essential grounding steps lead to better generalization across various tasks. The study emphasizes the importance of simplicity in CoT design for enhancing the effectiveness of visual reasoning in machine learning models."
                },
                "zh": {
                    "title": "ç®€æ´æ€ç»´é“¾ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒçš„æ€ç»´é“¾è®¾è®¡å¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰æ¨ç†èƒ½åŠ›ä¸Šçš„æ³›åŒ–ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€æ´çš„åŸºç¡€æ­¥éª¤æ¯”é•¿çš„æ€ç»´é“¾æ›´æœ‰æ•ˆï¼Œèƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨æ§åˆ¶çš„è¿·å®«æ±‚è§£åŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¯­è¨€æ€ç»´é“¾ã€åŸºç¡€æ€ç»´é“¾å’Œè§†è§‰æ€ç»´é“¾çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œä¿ç•™æœ€å°‘åŸºç¡€æ­¥éª¤çš„æ€ç»´é“¾åœ¨ä¸åŒè¿·å®«å¤§å°ä¸­å…·æœ‰æœ€ä½³çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01989",
            "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
            "url": "https://huggingface.co/papers/2512.01989",
            "abstract": "PAI-Bench evaluates the perception and prediction capabilities of multi-modal large language models and video generative models, revealing limitations in physical coherence and causal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "1fe6c349a49cb3e9",
            "authors": [
                "Fengzhe Zhou",
                "Jiannan Huang",
                "Jialuo Li",
                "Deva Ramanan",
                "Humphrey Shi"
            ],
            "affiliations": [
                "CMU",
                "Georgia Tech"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01989.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° PAI-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2808 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ°Ğ±ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Evaluating the Limits of AI in Understanding Reality",
                    "desc": "PAI-Bench is a benchmark designed to assess how well multi-modal large language models and video generative models can perceive and predict real-world dynamics. The evaluation includes 2,808 real-world scenarios and uses specific metrics to measure physical plausibility and reasoning capabilities. Findings indicate that while video generative models produce visually appealing results, they often fail to maintain physical coherence, and multi-modal language models struggle with forecasting and causal reasoning. This research highlights the current limitations of these models in the context of Physical AI and sets the stage for future improvements."
                },
                "zh": {
                    "title": "è¯„ä¼°ç‰©ç†äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ä¸æŒ‘æˆ˜",
                    "desc": "PAI-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥å’Œé¢„æµ‹èƒ½åŠ›æ–¹é¢çš„åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è§†è§‰ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¿æŒç‰©ç†ä¸€è‡´æ€§å’Œå› æœæ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹å’Œå› æœè§£é‡Šçš„è¡¨ç°ä¹Ÿæœ‰é™ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç‰©ç†äººå·¥æ™ºèƒ½çš„è¯„ä¼°æä¾›äº†åŸºç¡€ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç³»ç»Ÿéœ€è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00903",
            "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
            "url": "https://huggingface.co/papers/2512.00903",
            "abstract": "SwiftVLA enhances compact Vision-Language-Action models with 4D understanding using Fusion Tokens and a mask-and-reconstruct strategy, achieving high performance with reduced computational and memory demands.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "315d311b99ab78f8",
            "authors": [
                "Chaojun Ni",
                "Cheng Chen",
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Wenzhao Zheng",
                "Boyuan Wang",
                "Tianrun Chen",
                "Guosheng Zhao",
                "Haoyun Li",
                "Zhehao Dong",
                "Qiang Zhang",
                "Yun Ye",
                "Yang Wang",
                "Guan Huang",
                "Wenjun Mei"
            ],
            "affiliations": [
                "GigaAI",
                "Moxin (Huzhou) Technology Co., Ltd.",
                "Peking University",
                "Tsinghua University",
                "X-Humanoid"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00903.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#robotics",
                    "#video",
                    "#3d",
                    "#architecture",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 4D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "SwiftVLA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ 4D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğº Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Fusion Tokens. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºÑÑˆĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ 4D Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ mask-and-reconstruct Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 4D Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ 4D Ğ²ĞµÑ‚Ğ²ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² 7 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 18 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ² 12 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "SwiftVLA: Compact 4D Vision-Language-Action for Efficient Performance",
                    "desc": "SwiftVLA is a novel architecture that improves compact Vision-Language-Action (VLA) models by integrating 4D understanding through the use of Fusion Tokens and a mask-and-reconstruct strategy. This approach allows the model to effectively combine 2D images with 4D features, enhancing its ability to understand actions in a spatiotemporal context. By employing a pretrained 4D visual geometry transformer and a future prediction objective, SwiftVLA generates unified representations that maintain high performance while significantly reducing computational and memory requirements. Experimental results demonstrate that SwiftVLA not only outperforms lightweight baselines but also competes with much larger VLA models, making it suitable for deployment on edge devices."
                },
                "zh": {
                    "title": "SwiftVLAï¼šé«˜æ•ˆçš„4Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "SwiftVLAæ˜¯ä¸€ç§å¢å¼ºç´§å‡‘å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ¶æ„ï¼Œå…·å¤‡4Dç†è§£èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼•å…¥èåˆæ ‡è®°å’Œæ©è”½é‡æ„ç­–ç•¥ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„4Dè§†è§‰å‡ ä½•å˜æ¢å™¨ï¼Œä»2Då›¾åƒä¸­æå–4Dç‰¹å¾ï¼Œå¹¶é€šè¿‡å­¦ä¹ æœªæ¥é¢„æµ‹ç›®æ ‡æ¥ç”Ÿæˆç»Ÿä¸€çš„åŠ¨ä½œè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSwiftVLAåœ¨çœŸå®å’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­è¡¨ç°ä¼˜äºè½»é‡çº§åŸºçº¿ï¼Œå¹¶ä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ï¼Œä¸”åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œé€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02492",
            "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
            "url": "https://huggingface.co/papers/2512.02492",
            "abstract": "YingVideo-MV generates high-quality music performance videos with synchronized camera motion using cascaded frameworks, audio semantic analysis, and temporal-aware diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "2fb7f707b682aeef",
            "authors": [
                "Jiahui Chen",
                "Weida Wang",
                "Runhua Shi",
                "Huan Yang",
                "Chaofan Ding",
                "Zihao Chen"
            ],
            "affiliations": [
                "AI Lab, GiantNetwork Shanghai, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02492.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹",
                    "desc": "YingVideo-MV â€” ÑÑ‚Ğ¾ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾ĞºĞ½Ğ° Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ¸Ğ¿Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Synchronized Music Performance Videos with YingVideo-MV",
                    "desc": "YingVideo-MV is a novel framework designed to generate high-quality music performance videos that synchronize camera movements with audio. It utilizes a cascaded approach that combines audio semantic analysis and a shot planning module to create coherent long videos. The framework incorporates temporal-aware diffusion Transformers to ensure consistency across video sequences and introduces a camera adapter module for precise camera motion control. By leveraging a large-scale dataset, YingVideo-MV demonstrates significant advancements in generating expressive music videos with accurate synchronization between music, motion, and camera angles."
                },
                "zh": {
                    "title": "éŸ³ä¹è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "YingVideo-MV æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜è´¨é‡éŸ³ä¹è¡¨æ¼”è§†é¢‘çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ä¸éŸ³é¢‘åŒæ­¥çš„æ‘„åƒæœºè¿åŠ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†éŸ³é¢‘è¯­ä¹‰åˆ†æã€å¯è§£é‡Šçš„é•œå¤´è§„åˆ’æ¨¡å—ï¼ˆMV-Directorï¼‰å’Œæ—¶é—´æ„ŸçŸ¥çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„éŸ³ä¹æ•°æ®é›†ï¼ŒYingVideo-MV èƒ½å¤Ÿè‡ªåŠ¨åˆæˆå¤šæ ·åŒ–çš„éŸ³ä¹è¡¨æ¼”è§†é¢‘ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†æ‘„åƒæœºé€‚é…æ¨¡å—å’Œæ—¶é—´æ„ŸçŸ¥åŠ¨æ€çª—å£ç­–ç•¥ï¼Œä»¥æé«˜é•¿åºåˆ—æ¨ç†ä¸­çš„è¿ç»­æ€§å’Œæ‘„åƒæœºè¿åŠ¨çš„æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22982",
            "title": "Ovis-Image Technical Report",
            "url": "https://huggingface.co/papers/2511.22982",
            "abstract": "Ovis-Image is a 7B text-to-image model optimized for high-quality text rendering under computational constraints, combining a diffusion-based visual decoder with a multimodal backbone and a text-centric training pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ovis-Image, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "4f657aab7983000b",
            "authors": [
                "Guo-Hua Wang",
                "Liangfu Cao",
                "Tianyu Cui",
                "Minghao Fu",
                "Xiaohao Chen",
                "Pengxin Zhan",
                "Jianshan Zhao",
                "Lan Li",
                "Bowen Fu",
                "Jiaqi Liu",
                "Qing-Guo Chen"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22982.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ovis-Image â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ovis 2.5 Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ovis-Image Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚-Ñ‚Ñƒ-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Qwen-Image, Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ñ‚Ğ¸Ğ¿Ğ° Seedream Ğ¸ GPT4o. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¼ GPU Ñ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient High-Quality Text Rendering with Ovis-Image",
                    "desc": "Ovis-Image is a 7 billion parameter text-to-image model that excels in rendering high-quality text while being efficient in its computational use. It utilizes a diffusion-based visual decoder and a robust multimodal backbone, enhancing its ability to generate images from text prompts. The model is trained using a text-centric approach that includes extensive pre-training and specific post-training adjustments. Remarkably, Ovis-Image delivers performance comparable to larger models, making it suitable for deployment on a single high-end GPU, thus bridging the gap between advanced text rendering capabilities and practical application."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ–‡æœ¬æ¸²æŸ“çš„Ovis-Imageæ¨¡å‹",
                    "desc": "Ovis-Imageæ˜¯ä¸€ç§7Bçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äºé«˜è´¨é‡æ–‡æœ¬æ¸²æŸ“ï¼ŒåŒæ—¶åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹é«˜æ•ˆè¿è¡Œã€‚è¯¥æ¨¡å‹ç»“åˆäº†åŸºäºæ‰©æ•£çš„è§†è§‰è§£ç å™¨å’Œå¼ºå¤§çš„Ovis 2.5å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œï¼Œé‡‡ç”¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è®­ç»ƒæµç¨‹ã€‚å°½ç®¡æ¶æ„ç´§å‡‘ï¼ŒOvis-Imageåœ¨æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ä¸Šä¸æ›´å¤§è§„æ¨¡çš„å¼€æ”¾æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³æ¥è¿‘ä¸€äº›å°é—­æºç³»ç»Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„å¤šæ¨¡æ€éª¨å¹²ä¸ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬è®­ç»ƒæ–¹æ¡ˆç»“åˆï¼Œå¯ä»¥å®ç°å¯é çš„åŒè¯­æ–‡æœ¬æ¸²æŸ“ï¼Œè€Œæ— éœ€ä¾èµ–è¿‡å¤§çš„æˆ–ä¸“æœ‰çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22973",
            "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
            "url": "https://huggingface.co/papers/2511.22973",
            "abstract": "BlockVid addresses challenges in block diffusion video generation by employing semantic-aware sparse KV caching, Block Forcing training, and noise scheduling to produce high-quality, coherent minute-long videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "5dc0a78d1b37194f",
            "authors": [
                "Zeyu Zhang",
                "Shuning Chang",
                "Yuanyu He",
                "Yizeng Han",
                "Jiasheng Tang",
                "Fan Wang",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "ZIP Lab, Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22973.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#long_context",
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ»Ğ³Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "BlockVid Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ»ÑƒĞ´Ğ²Ğ¾Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ KV-Ğ¿Ğ°Ñ€, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Block Forcing Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LV-Bench Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "BlockVid: Revolutionizing Minute-Long Video Generation with Block Diffusion",
                    "desc": "BlockVid is a new framework designed to improve the generation of minute-long videos using a block diffusion approach. It tackles issues like long-horizon error accumulation and the need for better evaluation metrics by introducing semantic-aware sparse KV caching and a training method called Block Forcing. Additionally, it features noise scheduling to enhance the coherence and quality of the generated videos. The framework is validated through extensive experiments, showing significant improvements over existing methods in producing high-quality, coherent videos."
                },
                "zh": {
                    "title": "BlockVidï¼šç”Ÿæˆé«˜è´¨é‡è¿è´¯è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "BlockVid æ˜¯ä¸€ç§æ–°é¢–çš„å—æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç– KV ç¼“å­˜ã€Block Forcing è®­ç»ƒç­–ç•¥å’Œå™ªå£°è°ƒåº¦æ¥ç”Ÿæˆé«˜è´¨é‡ä¸”è¿è´¯çš„åˆ†é’Ÿé•¿è§†é¢‘ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†é•¿æ—¶é—´é”™è¯¯ç´¯ç§¯ï¼Œå¹¶æé«˜äº†æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† LV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†é’Ÿé•¿è§†é¢‘çš„ç»†ç²’åº¦åŸºå‡†ï¼Œé…å¤‡äº†æ–°çš„é•¿èŒƒå›´ä¸€è‡´æ€§è¯„ä¼°æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02423",
            "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.02423",
            "abstract": "GUI Exploration Lab enables effective training and evaluation of GUI agents through simulation, leveraging supervised and reinforcement learning to enhance navigation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "0bdbd42045c4a55c",
            "authors": [
                "Haolong Yan",
                "Yeqing Shen",
                "Xin Huang",
                "Jia Wang",
                "Kaijun Tan",
                "Zhixuan Liang",
                "Hongxin Li",
                "Zheng Ge",
                "Osamu Yoshie",
                "Si Li",
                "Xiangyu Zhang",
                "Daxin Jiang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Institute of Automation, Chinese Academy of Sciences",
                "StepFun",
                "Waseda University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· supervised Ğ¸ reinforcement learning",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GUI Exploration Lab â€” ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ supervised fine-tuning Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ reinforcement learning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑĞºÑ€Ğ°Ğ½Ğ°Ğ¼. ĞĞ´Ğ½Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ RL Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering GUI Agents with Advanced Learning Techniques",
                    "desc": "The paper presents GUI Exploration Lab, a simulation environment designed to improve the training and evaluation of GUI agents using supervised and reinforcement learning techniques. It addresses the challenges posed by complex real-world GUI environments, which often lack accessible information for effective agent training. The study shows that supervised fine-tuning helps agents memorize essential knowledge, while reinforcement learning enhances their ability to generalize to new situations. Ultimately, the research demonstrates that multi-turn reinforcement learning fosters better exploration strategies, leading to improved navigation performance in GUI tasks."
                },
                "zh": {
                    "title": "GUIä»£ç†å¯¼èˆªçš„æ¢ç´¢ä¸æå‡",
                    "desc": "GUIæ¢ç´¢å®éªŒå®¤æ˜¯ä¸€ä¸ªç”¨äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†å¯¼èˆªç ”ç©¶çš„æ¨¡æ‹Ÿç¯å¢ƒå¼•æ“ã€‚å®ƒé€šè¿‡çµæ´»å®šä¹‰å’Œç»„åˆå±å¹•ã€å›¾æ ‡å’Œå¯¼èˆªå›¾ï¼Œæä¾›å…¨é¢çš„ç¯å¢ƒä¿¡æ¯ï¼Œä¿ƒè¿›ä»£ç†çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒèƒ½å¤Ÿæœ‰æ•ˆè®°å¿†åŸºç¡€çŸ¥è¯†ï¼Œä¸ºåç»­è®­ç»ƒå¥ å®šåŸºç¡€ï¼Œè€Œå•å›åˆå¼ºåŒ–å­¦ä¹ åˆ™å¢å¼ºäº†å¯¹æœªè§åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤šå›åˆå¼ºåŒ–å­¦ä¹ é€šè¿‡äº’åŠ¨è¯•é”™é¼“åŠ±æ¢ç´¢ç­–ç•¥çš„å‘å±•ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡å±å¹•å¯¼èˆªæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02017",
            "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
            "url": "https://huggingface.co/papers/2512.02017",
            "abstract": "VisualSync aligns unposed, unsynchronized videos using multi-view dynamics and epipolar constraints, outperforming existing methods with millisecond accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "747b85773b618400",
            "authors": [
                "Shaowei Liu",
                "David Yifan Yao",
                "Saurabh Gupta",
                "Shenlong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02017.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿Ğ¸Ğ¿Ğ¾Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "VisualSync â€” ÑÑ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¸Ğ¿Ğ¾Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº 3D Ñ‚Ğ¾Ñ‡ĞºĞ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸. VisualSync Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 50 Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´."
                },
                "en": {
                    "title": "Achieving Millisecond Video Synchronization with VisualSync",
                    "desc": "VisualSync is a novel framework designed to synchronize unposed and unsynchronized videos captured from multiple cameras. It utilizes multi-view dynamics and epipolar constraints to achieve high accuracy in alignment, outperforming traditional methods that often rely on controlled environments or manual adjustments. By leveraging 3D reconstruction, feature matching, and dense tracking, VisualSync effectively extracts tracklets and relative poses to minimize epipolar error. The results demonstrate that it can achieve median synchronization errors below 50 milliseconds across various challenging datasets."
                },
                "zh": {
                    "title": "VisualSyncï¼šæ¯«ç§’çº§è§†é¢‘åŒæ­¥çš„æ–°æ–¹æ³•",
                    "desc": "VisualSyncæ˜¯ä¸€ç§ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å¯¹æœªå®šä½å’Œä¸åŒæ­¥çš„è§†é¢‘è¿›è¡Œå¯¹é½ï¼Œåˆ©ç”¨å¤šè§†è§’åŠ¨æ€å’Œæçº¿çº¦æŸå®ç°æ¯«ç§’çº§çš„ç²¾åº¦ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œä»»ä½•åœ¨ä¸¤ä¸ªæ‘„åƒæœºä¸­å¯è§çš„ç§»åŠ¨3Dç‚¹ï¼Œåœ¨æ­£ç¡®åŒæ­¥åéƒ½éµå¾ªæçº¿çº¦æŸã€‚VisualSyncé€šè¿‡åˆ©ç”¨ç°æˆçš„3Dé‡å»ºã€ç‰¹å¾åŒ¹é…å’Œå¯†é›†è·Ÿè¸ªæ¥æå–è½¨è¿¹ã€ç›¸å¯¹å§¿æ€å’Œè·¨è§†å›¾å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisualSyncåœ¨å››ä¸ªå¤šæ ·åŒ–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°çš„ä¸­ä½åŒæ­¥è¯¯å·®ä½äº50æ¯«ç§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02790",
            "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
            "url": "https://huggingface.co/papers/2512.02790",
            "abstract": "A lightweight data pipeline and benchmark are introduced to improve the quality and scale of image editing datasets and model evaluations, addressing the performance gap between closed-source and open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "ee6f6a045efa5da4",
            "authors": [
                "Keming Ye",
                "Zhipeng Huang",
                "Canmiao Fu",
                "Qingyang Liu",
                "Jiani Cai",
                "Zheqi Lv",
                "Chen Li",
                "Jing Lyu",
                "Zhou Zhao",
                "Shengyu Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "WeChat Vision, Tencent Inc.",
                "Xinjiang University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02790.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen-Verify Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UnicEdit-10M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº UnicBench Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap in Image Editing Models with Scalable Data Solutions",
                    "desc": "This paper presents a new lightweight data pipeline and benchmark aimed at enhancing the quality and scale of image editing datasets and model evaluations. It addresses the growing performance gap between closed-source and open-source models, which is largely due to the lack of large, high-quality training data. The authors introduce a dual-task expert model, Qwen-Verify, to improve quality control and failure detection in the data pipeline. Additionally, they create UnicEdit-10M, a dataset with 10 million samples for various editing tasks, and UnicBench, a benchmark that evaluates models on more complex editing capabilities and reasoning skills."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç¼–è¾‘æ¨¡å‹çš„è´¨é‡ä¸è§„æ¨¡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„æ•°æ®ç®¡é“å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥æé«˜å›¾åƒç¼–è¾‘æ•°æ®é›†å’Œæ¨¡å‹è¯„ä¼°çš„è´¨é‡å’Œè§„æ¨¡ï¼Œè§£å†³äº†é—­æºæ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚éšç€å¤šæ¨¡æ€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§å’Œç¼ºä¹å…¨é¢åŸºå‡†å¯¼è‡´äº†è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æå‡ºçš„è½»é‡çº§æ•°æ®ç®¡é“é€šè¿‡ç«¯åˆ°ç«¯æ¨¡å‹å’Œç»Ÿä¸€çš„åéªŒè¯é˜¶æ®µï¼Œæ›¿ä»£äº†å¤šå·¥å…·é“¾ï¼Œä»è€Œæé«˜äº†æ•°æ®æ„å»ºçš„æ•ˆç‡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†UnicEdit-10Mæ•°æ®é›†å’ŒUnicBenchåŸºå‡†ï¼Œæä¾›äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯Šæ–­æ¨¡å‹çš„ä¸è¶³ä¹‹å¤„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01540",
            "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
            "url": "https://huggingface.co/papers/2512.01540",
            "abstract": "FlashVGGT uses descriptor-based attention to efficiently perform 3D reconstruction from multi-view images, significantly reducing inference time and improving scalability compared to VGGT.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "5598260ead1e9bfa",
            "authors": [
                "Zipeng Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01540.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#cv",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "FlashVGGT â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Visual Geometry Grounding Transformer Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ self-attention Ğ½Ğ°Ğ´ Ğ²ÑĞµĞ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº cross-attention Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ñ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° VGGT Ğ¿Ñ€Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ."
                },
                "en": {
                    "title": "Efficient 3D Reconstruction with Descriptor-Based Attention",
                    "desc": "FlashVGGT introduces a novel descriptor-based attention mechanism for 3D reconstruction from multi-view images, enhancing efficiency and scalability. By compressing spatial information into a smaller set of descriptor tokens, it reduces the computational burden associated with traditional self-attention methods. This approach allows for effective global attention computation while maintaining high reconstruction accuracy. Experimental results demonstrate that FlashVGGT significantly decreases inference time to just 9.3% of the original VGGT model, even for large image sequences."
                },
                "zh": {
                    "title": "FlashVGGTï¼šé«˜æ•ˆçš„3Dé‡å»ºæ–°æ–¹æ³•",
                    "desc": "FlashVGGTæ˜¯ä¸€ç§é«˜æ•ˆçš„3Dé‡å»ºæ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºæè¿°ç¬¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»å¤šè§†å›¾å›¾åƒä¸­è¿›è¡Œé‡å»ºã€‚ä¸ä¼ ç»Ÿçš„VGGTæ¨¡å‹ç›¸æ¯”ï¼ŒFlashVGGTæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œå¹¶æé«˜äº†å¯æ‰©å±•æ€§ã€‚å®ƒé€šè¿‡å°†æ¯å¸§çš„ç©ºé—´ä¿¡æ¯å‹ç¼©ä¸ºä¸€ç»„ç´§å‡‘çš„æè¿°ç¬¦ä»¤ç‰Œï¼Œæ¥è§£å†³è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashVGGTåœ¨é‡å»ºç²¾åº¦ä¸Šä¸VGGTç›¸å½“ï¼Œä½†æ¨ç†æ—¶é—´ä»…ä¸ºVGGTçš„9.3%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22146",
            "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
            "url": "https://huggingface.co/papers/2511.22146",
            "abstract": "A Causal Concept-Guided Diffusion Language Model (C2DLM) improves reasoning capabilities by learning causal relationships between concepts, enhancing performance and training efficiency in downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "c5877ed1da904ebb",
            "authors": [
                "Kairong Han",
                "Nuanqiao Shan",
                "Ziyu Zhao",
                "Zijing Hu",
                "Xinpeng Dong",
                "Junjian Ye",
                "Lujia Pan",
                "Fei Wu",
                "Kun Kuang"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "Noahs Ark Lab, Huawei Technologies"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22146.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#graphs",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ C2DLM â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ»ĞµĞ²Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¾, Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, C2DLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 12% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ COT-OrderPerturb Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 3.2 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ½Ğ° 1.31% Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning with Causal Relationships in Language Models",
                    "desc": "The Causal Concept-Guided Diffusion Language Model (C2DLM) enhances reasoning in language models by incorporating causal relationships between concepts. Unlike traditional autoregressive and diffusion models, which struggle with reasoning due to their token prediction and attention mechanisms, C2DLM utilizes a concept-level causal graph to guide its attention. This approach allows the model to focus on meaningful causal connections, leading to improved performance and training efficiency. As a result, C2DLM shows significant gains in reasoning tasks, achieving a 12% improvement in specific benchmarks and a 1.31% average increase across multiple tasks."
                },
                "zh": {
                    "title": "å› æœå…³ç³»å¼•å¯¼çš„è¯­è¨€æ¨¡å‹æå‡æ¨ç†èƒ½åŠ›",
                    "desc": "Causal Concept-Guided Diffusion Language Model (C2DLM) æ˜¯ä¸€ç§é€šè¿‡å­¦ä¹ æ¦‚å¿µä¹‹é—´çš„å› æœå…³ç³»æ¥æé«˜æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚å®ƒç»“åˆäº†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ä¸‹æ¸¸ä»»åŠ¡ã€‚C2DLM é€šè¿‡ä»æ•™å¸ˆæ¨¡å‹è·å–æ¦‚å¿µçº§å› æœå›¾ï¼Œæ˜ç¡®å¼•å¯¼æ³¨æ„åŠ›å­¦ä¹ å› æœå…³ç³»ï¼Œä»è€Œé¿å…äº†å› æœé€†è½¬å¸¦æ¥çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC2DLM åœ¨ COT-OrderPerturb ä»»åŠ¡ä¸­æé«˜äº† 12%ï¼Œå¹¶åœ¨å…­ä¸ªä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­å¹³å‡æå‡äº† 1.31%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18685",
            "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
            "url": "https://huggingface.co/papers/2511.18685",
            "abstract": "CFG-Bench evaluates multimodal large language models on fine-grained action intelligence and higher-order reasoning in embodied agent tasks, revealing limitations and potential for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "2e31bba11f6604ea",
            "authors": [
                "Dayong Liu",
                "Chao Xu",
                "Weihong Chen",
                "Suyu Zhang",
                "Juncheng Wang",
                "Jiankang Deng",
                "Baigui Sun",
                "Yang Liu"
            ],
            "affiliations": [
                "Imperial College London",
                "The Hong Kong Polytechnic University",
                "Wolf 1069B, Sany Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18685.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ CFG-Bench, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1368 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 19562 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CFG-Bench Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "CFG-Bench: Elevating Action Intelligence in Multimodal Models",
                    "desc": "This paper introduces CFG-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) on their ability to perform fine-grained action intelligence and higher-order reasoning in tasks involving embodied agents. The benchmark includes a dataset of 1,368 videos and 19,562 question-answer pairs that assess four cognitive abilities: Physical Interaction, Temporal-Causal Relation, Intentional Understanding, and Evaluative Judgment. The findings reveal that current MLLMs struggle with generating detailed instructions for physical interactions and have significant limitations in reasoning about intentions and evaluations. Additionally, the paper shows that supervised fine-tuning on CFG-Bench data can lead to improved performance on existing embodied benchmarks, suggesting a path for enhancing the capabilities of embodied agents."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦åŠ¨ä½œæ™ºèƒ½",
                    "desc": "CFG-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦åŠ¨ä½œæ™ºèƒ½å’Œé«˜é˜¶æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ä¾§é‡äºé«˜å±‚æ¬¡çš„è§„åˆ’æˆ–ç©ºé—´æ¨ç†ï¼Œè€Œå¯¹èº«ä½“äº¤äº’æ‰€éœ€çš„ç»†ç²’åº¦åŠ¨ä½œæ™ºèƒ½å…³æ³¨ä¸è¶³ã€‚CFG-BenchåŒ…å«1368ä¸ªç²¾å¿ƒæŒ‘é€‰çš„è§†é¢‘å’Œ19562ä¸ªä¸‰æ¨¡æ€é—®ç­”å¯¹ï¼Œé’ˆå¯¹å››ç§è®¤çŸ¥èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¯¦ç»†çš„ç‰©ç†äº¤äº’æŒ‡ä»¤æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶åœ¨æ„å›¾å’Œè¯„ä¼°çš„é«˜é˜¶æ¨ç†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03013",
            "title": "In-Context Sync-LoRA for Portrait Video Editing",
            "url": "https://huggingface.co/papers/2512.03013",
            "abstract": "Sync-LoRA uses an image-to-video diffusion model trained with in-context LoRA to enable precise frame-accurate edits in portrait videos while maintaining identity consistency and temporal coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "db640efe9facd120",
            "authors": [
                "Sagi Polaczek",
                "Or Patashnik",
                "Ali Mahdavi-Amiri",
                "Daniel Cohen-Or"
            ],
            "affiliations": [
                "Simon Fraser University",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03013.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#open_source",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "Sync-LoRA â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ… Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ²Ğ¸Ğ´Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ„Ğ¾Ğ½), Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Edit Portrait Videos with Precision and Consistency",
                    "desc": "Sync-LoRA is a novel method for editing portrait videos that focuses on maintaining identity consistency and temporal coherence. It utilizes an image-to-video diffusion model, where modifications are initiated from the first frame and seamlessly applied to the entire video sequence. The method employs in-context LoRA, trained on paired videos with similar motion but different appearances, to ensure precise synchronization of edits. This approach allows for high-quality visual changes while preserving the original motion dynamics of the subject across frames."
                },
                "zh": {
                    "title": "ç²¾å‡†ç¼–è¾‘è‚–åƒè§†é¢‘çš„Sync-LoRA",
                    "desc": "Sync-LoRAæ˜¯ä¸€ç§ç”¨äºç¼–è¾‘è‚–åƒè§†é¢‘çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„è§†è§‰ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒå¸§ç²¾ç¡®çš„åŒæ­¥å’Œèº«ä»½ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¿®æ”¹ç¬¬ä¸€å¸§å¹¶å°†ç¼–è¾‘ä¼ æ’­åˆ°æ•´ä¸ªåºåˆ—æ¥å®ç°ç¼–è¾‘ã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„åŒæ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é…å¯¹è§†é¢‘è®­ç»ƒäº†ä¸€ä¸ªä¸Šä¸‹æ–‡LoRAï¼Œè¿™äº›è§†é¢‘å±•ç¤ºäº†ç›¸åŒçš„è¿åŠ¨è½¨è¿¹ä½†å¤–è§‚ä¸åŒã€‚Sync-LoRAåœ¨å¤„ç†æœªè§èº«ä»½å’Œå¤šæ ·åŒ–ç¼–è¾‘æ—¶è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç¨³å¥åœ°åº”å¯¹å§¿åŠ¿å’Œè¡¨æƒ…çš„å˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02942",
            "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
            "url": "https://huggingface.co/papers/2512.02942",
            "abstract": "VideoScience-Bench evaluates video models' scientific reasoning by assessing their ability to generate phenomena consistent with undergraduate-level physics and chemistry concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "45d3233f5ce96ac9",
            "authors": [
                "Lanxiang Hu",
                "Abhilash Shankarampeta",
                "Yixin Huang",
                "Zilin Dai",
                "Haoyang Yu",
                "Yujie Zhao",
                "Haoqiang Kang",
                "Daniel Zhao",
                "Tajana Rosing",
                "Hao Zhang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02942.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾-Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "VideoScience-Bench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºÑƒÑ€ÑĞ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 200 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 14 Ñ‚ĞµĞ¼ Ğ¸ 103 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞµĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Evaluating Scientific Reasoning in Video Models with VideoScience-Bench",
                    "desc": "VideoScience-Bench is a new benchmark that tests video models on their ability to understand and apply undergraduate-level physics and chemistry concepts. It focuses on zero-shot reasoning, which means models must generate accurate physical outcomes without prior examples. The benchmark includes 200 prompts that require reasoning across multiple scientific topics, assessing models on dimensions like consistency and dynamism. This is the first evaluation that measures not just the generation of videos but also the scientific reasoning behind them, using a VLM-as-a-Judge for reliable assessments."
                },
                "zh": {
                    "title": "è§†é¢‘æ¨¡å‹çš„ç§‘å­¦æ¨ç†æ–°åŸºå‡†",
                    "desc": "VideoScience-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œä¸»è¦å…³æ³¨å®ƒä»¬ç”Ÿæˆç¬¦åˆæœ¬ç§‘ç‰©ç†å’ŒåŒ–å­¦æ¦‚å¿µçš„ç°è±¡çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«200ä¸ªç²¾å¿ƒç­–åˆ’çš„æç¤ºï¼Œæ¶µç›–14ä¸ªä¸»é¢˜å’Œ103ä¸ªç‰©ç†ä¸åŒ–å­¦æ¦‚å¿µï¼Œè¦æ±‚æ¨¡å‹ç†è§£å’Œæ¨ç†å¤šä¸ªç§‘å­¦æ¦‚å¿µã€‚æˆ‘ä»¬å¯¹ä¸ƒä¸ªæœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè€ƒå¯Ÿäº†ç”Ÿæˆçš„ä¸€è‡´æ€§ã€ç°è±¡çš„ç¬¦åˆæ€§ã€åŠ¨æ€çš„æ­£ç¡®æ€§ç­‰äº”ä¸ªç»´åº¦ã€‚VideoScience-Bench æ˜¯é¦–ä¸ªä¸ä»…è¯„ä¼°è§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜è¯„ä¼°ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œç¡®ä¿ç”Ÿæˆçš„å†…å®¹ç¬¦åˆé¢„æœŸçš„ç‰©ç†å’ŒåŒ–å­¦ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02817",
            "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
            "url": "https://huggingface.co/papers/2512.02817",
            "abstract": "BOOM is a multimodal multilingual lecture companion that translates audio and slides, producing synchronized outputs across text, images, and speech, enhancing accessibility and preservation of educational content.  \t\t\t\t\tAI-generated summary \t\t\t\t The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present BOOM, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "c0223712269209e2",
            "authors": [
                "Sai Koneru",
                "Fabian Retkowski",
                "Christian Huber",
                "Lukas Hilgert",
                "Seymanur Akti",
                "Enes Yavuz Ugan",
                "Alexander Waibel",
                "Jan Niehues"
            ],
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02817.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#open_source",
                    "#video",
                    "#translation",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ»ĞµĞºÑ†Ğ¸Ğ¹",
                    "desc": "BOOM â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ»ĞµĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ‚Ñ€Ñ‘Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…: Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ»Ğ°Ğ¹Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹."
                },
                "en": {
                    "title": "Transforming Lectures: Multimodal Translation for All!",
                    "desc": "BOOM is a novel multimodal multilingual lecture companion designed to enhance the accessibility of educational content by translating both audio and slides. It processes multiple input modalities, ensuring that translations maintain the integrity of text, images, and speech. This system allows students to engage with lectures in their native language while preserving the original content's meaning and context. Additionally, the approach improves downstream tasks like summarization and question answering by providing slide-aware transcripts."
                },
                "zh": {
                    "title": "BOOMï¼šå¤šæ¨¡æ€å¤šè¯­è¨€è®²åº§åŠ©æ‰‹",
                    "desc": "BOOMæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤šè¯­è¨€çš„è®²åº§åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç¿»è¯‘éŸ³é¢‘å’Œå¹»ç¯ç‰‡ï¼Œç”Ÿæˆæ–‡æœ¬ã€å›¾åƒå’Œè¯­éŸ³çš„åŒæ­¥è¾“å‡ºï¼Œä»è€Œå¢å¼ºæ•™è‚²å†…å®¹çš„å¯åŠæ€§å’Œä¿å­˜æ€§ã€‚éšç€æ•™è‚²å…¨çƒåŒ–å’Œåœ¨çº¿å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œæœ¬åœ°åŒ–æ•™è‚²å†…å®¹æˆä¸ºä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚BOOMé€šè¿‡è”åˆç¿»è¯‘è®²åº§éŸ³é¢‘å’Œå¹»ç¯ç‰‡ï¼Œæä¾›å®Œæ•´çš„å­¦ä¹ ä½“éªŒï¼Œç¡®ä¿ç¿»è¯‘ä¿ç•™æ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰ä¸‰ç§æ¨¡æ€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå…³æ³¨å¹»ç¯ç‰‡çš„è½¬å½•å¯¹åç»­ä»»åŠ¡å¦‚æ‘˜è¦å’Œé—®ç­”ä¹Ÿæœ‰ç§¯æå½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02351",
            "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2512.02351",
            "abstract": "Unified multimodal models suffer from inefficiencies in certain tasks, leading to the proposal of Mixture-of-Experts Adaptation to improve compression and maintain performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at https://github.com/Shwai-He/SparseUnifiedModel{this link}.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "aa141ead4528a3a8",
            "authors": [
                "Shwai He",
                "Chaorui Deng",
                "Ang Li",
                "Shen Yan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02351.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ñ‹Ğ¹, Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mixture-of-Experts Adaptation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BAGEL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Efficiency in Multimodal Models with Mixture-of-Experts",
                    "desc": "This paper addresses the inefficiencies found in unified multimodal models, which combine understanding and generation tasks. The authors analyze these models using training-free pruning techniques to identify which components can be compressed without losing performance. They discover that while the understanding component can be compressed effectively, the generation component is sensitive to such reductions. To improve efficiency, they propose the Mixture-of-Experts Adaptation, which allows for selective activation of model components, resulting in a model that performs comparably to the full version while using only half of its parameters."
                },
                "zh": {
                    "title": "ä¸“å®¶æ··åˆé€‚åº”ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸­çš„ä½æ•ˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸“å®¶æ··åˆé€‚åº”ï¼ˆMixture-of-Experts Adaptationï¼‰æ¥æé«˜å‹ç¼©æ•ˆç‡å’Œä¿æŒæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç†è§£ç»„ä»¶åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„å¯å‹ç¼©æ€§ï¼Œè€Œç”Ÿæˆç»„ä»¶å¯¹å‹ç¼©éå¸¸æ•æ„Ÿï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºå°†ç”Ÿæˆæ¨¡å—åˆ’åˆ†ä¸ºå¤šä¸ªä¸“å®¶ï¼Œå¹¶é€šè¿‡ç¨€ç–æ¿€æ´»æ¥æ¢å¤ç”Ÿæˆè´¨é‡ã€‚ç»è¿‡éªŒè¯ï¼Œé€‚åº”åçš„BAGELæ¨¡å‹åœ¨ä»…æ¿€æ´»çº¦ä¸€åŠå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸å®Œæ•´æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01988",
            "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
            "url": "https://huggingface.co/papers/2512.01988",
            "abstract": "Artemis, a perception-policy learning framework, enhances performance on visual tasks by using structured spatial reasoning with (label, bounding-box) pairs instead of linguistic intermediate reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "01182161eb97cc07",
            "authors": [
                "Wei Tang",
                "Yanpeng Sun",
                "Shan Zhang",
                "Xiaofan Li",
                "Piotr Koniusz",
                "Wei Li",
                "Na Zhao",
                "Zechao Li"
            ],
            "affiliations": [
                "Adelaide AIML",
                "Baidu Inc.",
                "Data61 CSIRO",
                "NJUST IMAG",
                "SUTD IMPL",
                "SenseTime"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01988.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#rl",
                    "#reasoning",
                    "#cv",
                    "#small_models"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ²: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "Artemis â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ’ Artemis ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ° (Ğ¼ĞµÑ‚ĞºĞ°, bounding box), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³Ñ€Ğ°Ğ½Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Visual Perception with Spatial Reasoning",
                    "desc": "Artemis is a new framework designed to improve visual perception tasks by using structured reasoning based on spatial information rather than relying on natural language. It utilizes (label, bounding-box) pairs to represent visual states, allowing for clear tracking and supervision of intermediate reasoning steps. This approach addresses the limitations of traditional linguistic reasoning, which can introduce ambiguity and reduce performance. By aligning reasoning with spatial representations, Artemis demonstrates enhanced capabilities in grounding, detection, and generalization across various visual tasks."
                },
                "zh": {
                    "title": "ç©ºé—´æ¨ç†æå‡è§†è§‰æ„ŸçŸ¥çš„åŠ›é‡",
                    "desc": "Artemisæ˜¯ä¸€ä¸ªæ„ŸçŸ¥-ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨ç»“æ„åŒ–çš„ç©ºé—´æ¨ç†æ¥æå‡è§†è§‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€ä¸­é—´æ¨ç†ä¸åŒï¼ŒArtemisé‡‡ç”¨(label, bounding-box)å¯¹æ¥è¡¨ç¤ºæ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œä»è€Œæ•æ‰å¯éªŒè¯çš„è§†è§‰çŠ¶æ€ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ä¸­é—´çŠ¶æ€çš„è·Ÿè¸ªæ›´åŠ æ˜ç¡®ï¼Œå¹¶ä¸”é¿å…äº†è¯­è¨€æ¨ç†å¸¦æ¥çš„æ­§ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArtemisåœ¨åŸºç¡€å’Œæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨è®¡æ•°å’Œå‡ ä½•æ„ŸçŸ¥ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22184",
            "title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
            "url": "https://huggingface.co/papers/2511.22184",
            "abstract": "A framework for dense foot contact estimation addresses challenges of shoe appearance diversity and ground feature extraction using adversarial training and spatial context-based feature extraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "f450292da503aba3",
            "authors": [
                "Daniel Sungho Jung",
                "Kyoung Mu Lee"
            ],
            "affiliations": [
                "Dept. of ECE & ASRI, Seoul National University",
                "IPAI, Seoul National University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22184.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ‘£",
                "ru": {
                    "title": "ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ¾Ğµä¼°è®¡ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° ÑÑ‚Ğ¾Ğ¿Ñ‹, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑÑ‚Ğ¸Ğ»Ñ Ğ¾Ğ±ÑƒĞ²Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° FECO Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ä¼°è®¡ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° ÑÑ‚Ğ¾Ğ¿Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ·ĞµĞ¼Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑƒĞ²Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ adversarial training, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğº ÑÑ‚Ğ¸Ğ»Ñ Ğ¾Ğ±ÑƒĞ²Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. Ğ”Ğ»Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ·ĞµĞ¼Ğ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Robust Foot Contact Estimation with Style-Invariant Learning",
                    "desc": "This paper presents a framework called FECO for estimating dense foot contact from images, addressing the challenges posed by diverse shoe appearances and the need for effective ground feature extraction. The authors utilize adversarial training to ensure that the model learns shoe style-invariant features, allowing it to generalize across different shoe types. Additionally, they introduce a spatial context-based ground feature extractor to enhance the model's understanding of the ground's properties. Overall, the FECO framework significantly improves the accuracy of foot contact estimation, which is essential for understanding human movement and interaction with the environment."
                },
                "zh": {
                    "title": "å®ç°ç¨³å¥çš„è¶³éƒ¨æ¥è§¦ä¼°è®¡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¯†é›†è¶³éƒ¨æ¥è§¦ä¼°è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‹å­å¤–è§‚å¤šæ ·æ€§å’Œåœ°é¢ç‰¹å¾æå–çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æŠ—è®­ç»ƒå’ŒåŸºäºç©ºé—´ä¸Šä¸‹æ–‡çš„ç‰¹å¾æå–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå­¦ä¹ é‹å­é£æ ¼ä¸å˜çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ¥è§¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¼•å…¥äº†åœ°é¢ç‰¹å¾æå–å™¨ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨åœ°é¢ä¿¡æ¯ï¼Œæ•æ‰åœ°é¢çš„å±æ€§ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒé‹å­å¤–è§‚ä¸‹å®ç°äº†ç¨³å¥çš„è¶³éƒ¨æ¥è§¦ä¼°è®¡ï¼Œå¹¶å°†å‘å¸ƒç›¸å…³ä»£ç ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00097",
            "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
            "url": "https://huggingface.co/papers/2512.00097",
            "abstract": "HAGeo, a heuristic-based method for adding auxiliary constructions in geometric deduction, achieves gold-medal level performance on IMO geometry problems, surpassing neural network-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "69d591881461c973",
            "authors": [
                "Boyan Duan",
                "Xiao Liang",
                "Shuai Lu",
                "Yaoxiang Wang",
                "Yelong Shen",
                "Kai-Wei Chang",
                "Ying Nian Wu",
                "Mao Yang",
                "Weizhu Chen",
                "Yeyun Gong"
            ],
            "affiliations": [
                "ETH Zurich",
                "Microsoft",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00097.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#math",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ…",
                "ru": {
                    "title": "Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğ¼ Ğ¼ĞµĞ´Ğ°Ğ»ÑĞ¼ Ğ² Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ HAGeo â€” ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° CPU Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. HAGeo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ IMO-30, Ñ€ĞµÑˆĞ¸Ğ² 28 Ğ¸Ğ· 30 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ AlphaGeometry. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HAGeo-409 Ğ¸Ğ· 409 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼."
                },
                "en": {
                    "title": "HAGeo: Revolutionizing Geometry Theorem Proving with Heuristics",
                    "desc": "HAGeo is a heuristic-based method designed to enhance geometric deduction by adding auxiliary constructions. It demonstrates exceptional performance on International Mathematical Olympiad (IMO) geometry problems, achieving gold-medal level results. Unlike neural network approaches, HAGeo operates solely on CPUs, making it efficient and accessible. The method outperforms existing models, including AlphaGeometry, and introduces a new benchmark, HAGeo-409, to further assess the challenges in automated theorem proving."
                },
                "zh": {
                    "title": "HAGeoï¼šè¶…è¶Šç¥ç»ç½‘ç»œçš„å‡ ä½•æ¨ç†æ–°æ–¹æ³•",
                    "desc": "HAGeoæ˜¯ä¸€ç§åŸºäºå¯å‘å¼çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å‡ ä½•æ¨ç†ä¸­æ·»åŠ è¾…åŠ©æ„é€ ã€‚è¯¥æ–¹æ³•åœ¨å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰å‡ ä½•é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç®€å•çš„éšæœºç­–ç•¥å¯ä»¥è¾¾åˆ°é“¶ç‰Œæ°´å¹³ï¼Œè€ŒHAGeoåˆ™é€šè¿‡æ·»åŠ è¾…åŠ©ç‚¹ï¼ŒæˆåŠŸè§£å†³äº†30ä¸ªé—®é¢˜ä¸­çš„28ä¸ªï¼Œè¾¾åˆ°äº†é‡‘ç‰Œæ°´å¹³ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†HAGeo-409åŸºå‡†ï¼ŒåŒ…å«409ä¸ªå‡ ä½•é—®é¢˜ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•å’Œç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.21338",
            "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
            "url": "https://huggingface.co/papers/2511.21338",
            "abstract": "Masked Diffusion Language Models (MDLMs) exhibit locality bias and are negatively impacted by appended mask tokens, but a mask-agnostic loss function improves their context comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "49da52ba520a8a6b",
            "authors": [
                "Julianna Piskorz",
                "Cristina Pinneri",
                "Alvaro Correia",
                "Motasem Alfarra",
                "Risheek Garrepalli",
                "Christos Louizos"
            ],
            "affiliations": [
                "Qualcomm AI Research",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.21338.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ (MDLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ: Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, MDLM Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Context Comprehension in Masked Diffusion Language Models",
                    "desc": "Masked Diffusion Language Models (MDLMs) are a new type of language model that aim to improve context understanding compared to traditional models. However, they struggle with locality bias, meaning they perform better with nearby information rather than distant context. Additionally, adding too many mask tokens during training can confuse the model and hurt its performance. To solve these issues, the authors propose a mask-agnostic loss function that helps the model focus better on relevant information, leading to improved context comprehension."
                },
                "zh": {
                    "title": "æå‡æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆMDLMsï¼‰çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¹¶å‘ç°äº†ä¸¤ä¸ªä¸»è¦é™åˆ¶ã€‚é¦–å…ˆï¼Œå°½ç®¡MDLMsé‡‡ç”¨äº†å…¨å±€è®­ç»ƒç›®æ ‡å’ŒåŒå‘æ³¨æ„æœºåˆ¶ï¼Œä½†å®ƒä»¬ä»ç„¶è¡¨ç°å‡ºå¼ºçƒˆçš„å±€éƒ¨åè§ï¼Œå¯¹è¾“å…¥ä¸­ç›¸å…³ä¿¡æ¯çš„ä½ç½®éå¸¸æ•æ„Ÿã€‚å…¶æ¬¡ï¼Œæ·»åŠ å¤§é‡æ©ç ä»¤ç‰Œä¼šæ˜¾è‘—é™ä½ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå› ä¸ºè¿™äº›æ©ç ä¼šå¹²æ‰°æ¨¡å‹å¤„ç†ç›¸å…³ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸æ©ç æ— å…³çš„æŸå¤±å‡½æ•°ï¼Œæ˜¾è‘—æé«˜äº†MDLMsçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19661",
            "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
            "url": "https://huggingface.co/papers/2511.19661",
            "abstract": "CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO), improves faithful tool use and accuracy in visual and multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "9c32e7e0aaae848a",
            "authors": [
                "Xinhai Hou",
                "Shaoyuan Xu",
                "Manan Biyani",
                "Moyan Li",
                "Jia Liu",
                "Todd C. Hollon",
                "Bryan Wang"
            ],
            "affiliations": [
                "Amazon.com",
                "The Ohio State University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19661.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#training",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#rl",
                    "#agents",
                    "#rlhf",
                    "#reasoning",
                    "#cv"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Ğ§ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: ÑƒÑ‡Ğ¸Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾",
                    "desc": "CodeV â€” ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Tool-Aware Policy Optimization (TAPO) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ â€” Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. TAPO â€” ÑÑ‚Ğ¾ framework Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. CodeV Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Trustworthy Visual Reasoning with CodeV",
                    "desc": "This paper introduces CodeV, a visual agent that enhances the accuracy and reliability of tool use in visual reasoning tasks through a method called Tool-Aware Policy Optimization (TAPO). The authors highlight that many existing models may achieve high accuracy but often do so without faithfully using visual tools, which can lead to misleading results. They propose a new evaluation protocol to assess the faithfulness of tool use, revealing that many agents struggle with this aspect despite their performance. By training CodeV with TAPO, which focuses on rewarding correct tool outputs, the authors demonstrate improved accuracy and more reliable tool usage across various visual and multimodal reasoning tasks."
                },
                "zh": {
                    "title": "CodeVï¼šæå‡è§†è§‰æ¨ç†çš„å¯ä¿¡å·¥å…·ä½¿ç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCodeVçš„ä»£ç åŸºç¡€è§†è§‰ä»£ç†ï¼Œå®ƒé€šè¿‡å·¥å…·æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆTAPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æé«˜å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šè§†è§‰ä»£ç†åœ¨æœ€ç»ˆç­”æ¡ˆä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§†è§‰æ¨ç†ä¸­å´å­˜åœ¨ä¸å¯ä¿¡çš„å·¥å…·ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯„ä¼°åè®®ï¼Œæµ‹é‡ä¸­é—´è§†è§‰å·¥å…·è¾“å‡ºæ˜¯å¦åŒ…å«æ‰€æŸ¥è¯¢çš„è¯æ®ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰ä»£ç†åœ¨è§†è§‰æœç´¢åŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ã€‚CodeVé€šè¿‡å°†è§†è§‰å·¥å…·è¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„Pythonä»£ç ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥æä¾›åŸºäºé—®é¢˜å’Œå·¥å…·è¾“å‡ºçš„å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å·¥å…·ä½¿ç”¨çš„å¯ä¿¡åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15948",
            "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
            "url": "https://huggingface.co/papers/2511.15948",
            "abstract": "Click2Graph is an interactive framework for panoptic video scene graph generation that combines user cues with dynamic interaction discovery and semantic classification for precise and controllable scene understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "9c7483da5a8434d4",
            "authors": [
                "Raphael Ruschel",
                "Hardikkumar Prajapati",
                "Awsafur Rahman",
                "B. S. Manjunath"
            ],
            "affiliations": [
                "UC Santa Barbara Electrical & Computer Engineering"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15948.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#graphs"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Click2Graph â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑÑ†ĞµĞ½ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµÑ€Ñ‘Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ²Ğ¾Ğ´ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (ĞºĞ»Ğ¸Ğº Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸Ğº) Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ¾ĞµĞº Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ (ÑÑƒĞ±ÑŠĞµĞºÑ‚, Ğ¾Ğ±ÑŠĞµĞºÑ‚, Ğ¿Ñ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚). Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Empowering Video Understanding with Interactive Scene Graphs",
                    "desc": "Click2Graph is an innovative framework designed for generating scene graphs from videos, allowing users to interactively guide the process. It combines user inputs, like clicks or bounding boxes, with advanced techniques for discovering interactions and classifying objects semantically. This framework autonomously tracks subjects over time and identifies relationships between objects, creating a structured representation of the scene. By integrating dynamic interaction discovery and semantic reasoning, Click2Graph enhances video understanding, making it more controllable and interpretable for users."
                },
                "zh": {
                    "title": "äº¤äº’å¼å…¨æ™¯è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆçš„æœªæ¥",
                    "desc": "Click2Graph æ˜¯ä¸€ä¸ªäº¤äº’å¼æ¡†æ¶ï¼Œç”¨äºå…¨æ™¯è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆï¼Œç»“åˆäº†ç”¨æˆ·æç¤ºã€åŠ¨æ€äº¤äº’å‘ç°å’Œè¯­ä¹‰åˆ†ç±»ï¼Œä»¥å®ç°ç²¾ç¡®å’Œå¯æ§çš„åœºæ™¯ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„é—­ç¯è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆç³»ç»Ÿä¸åŒï¼ŒClick2Graph èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„å•ä¸€æç¤ºï¼ˆå¦‚ç‚¹å‡»æˆ–è¾¹ç•Œæ¡†ï¼‰è¿›è¡Œåˆ†å‰²å’Œè·Ÿè¸ªï¼Œå¹¶è‡ªä¸»å‘ç°äº¤äº’å¯¹è±¡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€äº¤äº’å‘ç°æ¨¡å—å’Œè¯­ä¹‰åˆ†ç±»å¤´ï¼Œèƒ½å¤Ÿè¿›è¡Œè”åˆå®ä½“å’Œè°“è¯æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClick2Graph ä¸ºç”¨æˆ·å¼•å¯¼çš„å…¨æ™¯è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆå¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œå±•ç¤ºäº†äººç±»æç¤ºå¦‚ä½•ä¸å…¨æ™¯åŸºç¡€å’Œå…³ç³»æ¨ç†ç›¸ç»“åˆï¼Œä»¥å®ç°å¯æ§å’Œå¯è§£é‡Šçš„è§†é¢‘åœºæ™¯ç†è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-02.html",
    "link_next": "2025-12-04.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 13,
        "#data": 1,
        "#benchmark": 19,
        "#agents": 10,
        "#cv": 6,
        "#rl": 7,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 7,
        "#3d": 4,
        "#audio": 4,
        "#video": 16,
        "#multimodal": 23,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 13,
        "#healthcare": 0,
        "#training": 15,
        "#robotics": 5,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 6,
        "#reasoning": 17,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 13,
        "#survey": 1,
        "#diffusion": 11,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 5,
        "#science": 2,
        "#low_resource": 0,
        "#translation": 1
    }
}