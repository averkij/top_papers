{
    "date": {
        "ru": "27 мая",
        "en": "May 27",
        "zh": "5月27日"
    },
    "time_utc": "2025-05-27 02:29",
    "weekday": 1,
    "issue_id": 3967,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.18675",
            "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps",
            "url": "https://huggingface.co/papers/2505.18675",
            "abstract": "Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.",
            "score": 4,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "61fe1af51f08d30f",
            "authors": [
                "Sicheng Feng",
                "Song Wang",
                "Shuyi Ouyang",
                "Lingdong Kong",
                "Zikai Song",
                "Jianke Zhu",
                "Huan Wang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "National University of Singapore",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18675.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "ReasonMap: новый взгляд на визуальное мышление языковых моделей",
                    "desc": "ReasonMap - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к пониманию визуальной информации и пространственному мышлению. Он включает в себя карты общественного транспорта из 30 городов и 1008 пар вопросов-ответов. Исследование показало, что среди открытых моделей базовые версии превосходят версии с рассуждениями, а у закрытых моделей наблюдается обратная тенденция. Результаты также указывают на то, что для эффективного решения задач визуального мышления моделям по-прежнему необходимо подлинное визуальное восприятие."
                },
                "en": {
                    "title": "Evaluating Visual Reasoning in Multimodal Models with ReasonMap",
                    "desc": "This paper introduces ReasonMap, a benchmark aimed at evaluating the fine-grained visual understanding and spatial reasoning capabilities of multimodal large language models (MLLMs). The benchmark includes high-resolution transit maps and a set of question-answer pairs to rigorously test the models' reasoning abilities. The study finds that base models often outperform reasoning variants in open-source settings, while closed-source models show the opposite trend. Additionally, the results indicate that masking visual inputs generally leads to decreased performance, highlighting the importance of genuine visual perception in complex reasoning tasks."
                },
                "zh": {
                    "title": "细粒度视觉推理的新基准",
                    "desc": "多模态大型语言模型（MLLMs）在视觉任务上取得了显著进展，但在细粒度视觉理解的推理任务上仍然不足。为了解决这个问题，我们提出了ReasonMap，一个基准测试，旨在评估MLLMs的细粒度视觉理解和空间推理能力。ReasonMap包含来自13个国家30个城市的高分辨率交通地图，并设计了两级评估流程来准确评估答案的正确性和质量。我们的研究揭示了一个反直觉的模式：在开源模型中，基础模型的表现优于推理模型，而在闭源模型中则相反。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18545",
            "title": "B-score: Detecting biases in large language models using response\n  history",
            "url": "https://huggingface.co/papers/2505.18545",
            "abstract": "Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.",
            "score": 3,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "60113080c6881b99",
            "authors": [
                "An Vo",
                "Mohammad Reza Taesiri",
                "Daeyoung Kim",
                "Anh Totti Nguyen"
            ],
            "affiliations": [
                "Auburn University, USA",
                "KAIST, South Korea",
                "University of Alberta, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18545.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#ethics",
                    "#hallucinations",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самокоррекция предвзятости в языковых моделях через многоэтапный диалог",
                    "desc": "Исследование посвящено изучению предвзятости в больших языковых моделях (LLM) и возможности ее уменьшения в многоэтапном диалоге. Авторы разработали набор вопросов по 9 темам, включая субъективные, случайные и объективные типы. Результаты показывают, что LLM способны снижать предвзятость при ответах на вопросы, требующие случайного, непредвзятого ответа. Предложен новый метрический показатель B-score для обнаружения предвзятости, который улучшает точность верификации ответов LLM на различных наборах данных."
                },
                "en": {
                    "title": "De-biasing LLMs Through Multi-Turn Conversations",
                    "desc": "This paper explores how large language models (LLMs) can reduce their biases during multi-turn conversations by referencing their previous answers. The authors categorize questions into three types: Subjective, Random, and Objective, and find that LLMs can effectively 'de-bias' themselves when responding to Random questions. They introduce a new metric called B-score, which helps identify biases in LLM responses across various question types. The results show that using B-score enhances the accuracy of verifying LLM answers compared to traditional methods like confidence scores."
                },
                "zh": {
                    "title": "多轮对话中的去偏见能力",
                    "desc": "大型语言模型（LLMs）常常表现出明显的偏见，例如对女性的偏见或对数字7的偏好。我们研究了在多轮对话中，LLMs是否能够在观察到自己之前的回答后，输出更少偏见的答案。我们提出了一组涵盖9个主题的测试问题，分为主观、随机和客观三种类型，以了解哪些问题更容易引发偏见。结果表明，LLMs在面对寻求随机、不偏见答案的问题时，能够自我“去偏见”，并且我们提出的B-score指标在检测偏见方面表现出色，显著提高了LLM答案的验证准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13426",
            "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.13426",
            "abstract": "VLM-Gym addresses the \"knowing-doing\" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.",
            "score": 3,
            "issue_id": 3967,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "8cfc8c8f1a7e5589",
            "authors": [
                "Liang Chen",
                "Hongcheng Gao",
                "Tianyu Liu",
                "Zhiqi Huang",
                "Flood Sung",
                "Xinyu Zhou",
                "Yuxin Wu",
                "Baobao Chang"
            ],
            "affiliations": [
                "Moonshot AI",
                "Peking University",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13426.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#games",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Преодоление разрыва между знанием и действием в Vision-Language Models с помощью RL",
                    "desc": "Статья представляет VLM-Gym - среду обучения с подкреплением для Vision-Language Models, направленную на преодоление разрыва между знанием и действием. VLM-Gym включает разнообразные визуальные игры с унифицированными интерфейсами и настраиваемой сложностью. Используя эту среду, авторы обучили модели G0 и G1, которые демонстрируют улучшенные способности восприятия и рассуждения. Результаты показывают, что модели G1 превосходят ведущие проприетарные модели в интерактивных играх."
                },
                "en": {
                    "title": "Bridging the Knowing-Doing Gap in Vision-Language Models",
                    "desc": "VLM-Gym is a new training environment designed to improve Vision-Language Models (VLMs) by bridging the gap between their knowledge and practical application in interactive games. Traditional VLMs excel in tasks involving text and images but struggle with decision-making in dynamic environments. By using reinforcement learning (RL) in a diverse set of visual games, VLM-Gym enables models to develop better perception and reasoning skills. The G1 models trained in this environment outperform existing models, demonstrating that enhanced perception and reasoning can support each other during training."
                },
                "zh": {
                    "title": "VLM-Gym：提升视觉语言模型的决策能力",
                    "desc": "VLM-Gym 是一个针对视觉语言模型（VLM）的强化学习环境，旨在解决它们在互动游戏中的决策能力不足的问题。通过在多样化的游戏环境中训练，VLM-Gym 提升了模型的感知和推理能力，使其在简单游戏中表现优于现有模型。我们开发了 G0 和 G1 模型，其中 G1 模型在强化学习微调之前进行了感知增强，以应对游戏多样性带来的挑战。最终，G1 模型在所有游戏中均超越了其教师模型，并且在性能上超过了领先的专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19706",
            "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision",
            "url": "https://huggingface.co/papers/2505.19706",
            "abstract": "PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "e60e6086053e62d0",
            "authors": [
                "Tej Deep Pala",
                "Panshul Sharma",
                "Amir Zadeh",
                "Chuan Li",
                "Soujanya Poria"
            ],
            "affiliations": [
                "Lambda Labs",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19706.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#hallucinations",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Точная навигация в математических рассуждениях с PathFinder-PRM",
                    "desc": "PathFinder-PRM - это новая иерархическая модель вознаграждения процесса, учитывающая ошибки, для улучшения решения математических задач. Она использует детальную классификацию ошибок и оценку правильности каждого шага. PathFinder-PRM достигает наилучших результатов по метрике PRMScore, используя при этом в 3 раза меньше данных для обучения. Модель эффективно направляет языковые модели к построению согласованных решений математических задач."
                },
                "en": {
                    "title": "PathFinder-PRM: Enhancing Math Problem-Solving with Fine-Grained Error Detection",
                    "desc": "PathFinder-PRM is a new model designed to enhance mathematical problem-solving by focusing on detailed error classification and assessing the correctness of each step in the solution process. Unlike traditional Outcome Reward Models that only evaluate final answers, this model uses a hierarchical and error-aware approach to score intermediate steps, which helps guide the generation of coherent solutions. It was trained on a large dataset that includes fine-grained labels for errors, allowing it to achieve a state-of-the-art PRMScore while using significantly less data. The results show that this model not only improves error detection but also enhances overall performance in reward-guided reasoning tasks."
                },
                "zh": {
                    "title": "提升数学推理的错误感知模型",
                    "desc": "PathFinder-PRM是一种层次化且具备错误感知的过程奖励模型，旨在通过细致的错误分类和步骤正确性估计来提升数学问题解决能力。该模型通过对每个步骤的数学错误和一致性错误进行分类，结合这些细致的信号来评估步骤的正确性。通过构建一个包含40万样本的数据集，PathFinder-PRM在PRMBench上达到了67.7的最新状态，使用的数据量比之前减少了三倍。研究结果表明，解耦的错误检测和奖励估计不仅提升了细粒度错误检测的能力，还显著改善了基于奖励的数学推理效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19630",
            "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
            "url": "https://huggingface.co/papers/2505.19630",
            "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "ab312c010a92062f",
            "authors": [
                "Yichun Feng",
                "Jiawei Wang",
                "Lu Zhou",
                "Yixue Li"
            ],
            "affiliations": [
                "Department of EEIS, University of Science and Technology of China",
                "Guangzhou National Laboratory",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19630.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#science",
                    "#healthcare",
                    "#dataset",
                    "#games",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Умный виртуальный доктор: ИИ учится вести диалог с пациентом",
                    "desc": "Статья представляет DoctorAgent-RL - систему на основе обучения с подкреплением для моделирования медицинских консультаций. Она использует мультиагентный подход, где агент-врач оптимизирует стратегию опроса пациента через многоэтапное взаимодействие. Система преодолевает ограничения существующих методов, позволяя динамически адаптировать сбор информации. Эксперименты показывают превосходство DoctorAgent-RL над другими моделями в многоэтапных рассуждениях и точности диагностики."
                },
                "en": {
                    "title": "Revolutionizing Clinical Consultations with Reinforcement Learning",
                    "desc": "This paper introduces DoctorAgent-RL, a novel framework that enhances biomedical question answering by using reinforcement learning (RL) to improve multi-turn medical consultations. Unlike traditional systems that rely on static data, DoctorAgent-RL allows a doctor agent to adaptively optimize its questioning strategy through dynamic interactions with a patient agent. The framework is designed to intelligently extract relevant clinical information, addressing the limitations of vague patient descriptions. Additionally, the authors present MTMedDialog, a new dataset for simulating patient interactions, which supports the framework's effectiveness in real-world clinical settings."
                },
                "zh": {
                    "title": "智能医疗咨询的新突破",
                    "desc": "大型语言模型（LLMs）在生物医学问答领域表现出色，但在实际临床咨询中的应用仍面临核心挑战。现有系统依赖单向信息传递模式，患者必须在一次性描述症状，导致模糊投诉时的诊断建议不够具体。传统的基于监督学习的多轮对话方法受限于静态数据驱动的范式，缺乏泛化能力，难以智能提取关键临床信息。为了解决这些问题，我们提出了DoctorAgent-RL，一个基于强化学习的多智能体协作框架，将医疗咨询建模为不确定性下的动态决策过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19443",
            "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI",
            "url": "https://huggingface.co/papers/2505.19443",
            "abstract": "A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  \t\t\t\t\tAI-generated summary \t\t\t\t This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "04b7019c8d659b76",
            "authors": [
                "Ranjan Sapkota",
                "Konstantinos I. Roumeliotis",
                "Manoj Karkee"
            ],
            "affiliations": [
                "Cornell University, Department of Biological and Environmental Engineering, USA",
                "University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19443.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Вайб vs Агент: Новые горизонты ИИ-ассистированной разработки",
                    "desc": "Эта статья представляет сравнительный анализ двух парадигм в разработке программного обеспечения с помощью ИИ: вайб-кодинга и агентного кодинга. Вайб-кодинг фокусируется на интуитивном взаимодействии человека с ИИ через диалоговые интерфейсы, поддерживая творческий процесс и экспериментирование. Агентный кодинг, напротив, обеспечивает автономную разработку с минимальным вмешательством человека, используя целеориентированных агентов. Авторы предлагают таксономию, охватывающую концептуальные основы, модели выполнения, механизмы обратной связи и стратегии отладки для обеих парадигм."
                },
                "en": {
                    "title": "Harmonizing Vibe and Agentic Coding for AI Development",
                    "desc": "This paper reviews two coding paradigms in AI-assisted software development: vibe coding and agentic coding. Vibe coding focuses on human interaction and creativity, using conversational prompts to aid in ideation and experimentation. In contrast, agentic coding allows for more autonomous development, where AI agents can plan and execute tasks with little human input. The authors propose a taxonomy to compare these paradigms and suggest that the future of AI software engineering will benefit from integrating both approaches for a more effective development process."
                },
                "zh": {
                    "title": "融合情感编码与自主编码的未来软件开发",
                    "desc": "本文对两种新兴的人工智能辅助软件开发范式进行了全面分析：情感编码和自主编码。情感编码强调通过基于提示的对话工作流程实现直观的人机交互，适合于创意探索和实验。而自主编码则通过目标驱动的智能体实现自主软件开发，能够在最小人类干预下进行规划、执行和测试。研究表明，成功的人工智能软件工程将依赖于将这两种范式的优势结合在一个以人为中心的开发生命周期中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18536",
            "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.18536",
            "abstract": "Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "03c065b99c9707fe",
            "authors": [
                "Haoyuan Sun",
                "Jiaqi Wu",
                "Bo Xia",
                "Yifu Luo",
                "Yifei Zhao",
                "Kai Qin",
                "Xufei Lv",
                "Tiantian Zhang",
                "Yongzhe Chang",
                "Xueqian Wang"
            ],
            "affiliations": [
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18536.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#rlhf",
                    "#agi",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RFT: ключ к усилению рассуждений в мультимодальных ИИ-моделях",
                    "desc": "Эта статья посвящена применению метода тонкой настройки с подкреплением (RFT) для улучшения способности мультимодальных больших языковых моделей (MLLM) к рассуждению. Авторы утверждают, что RFT значительно усиливает возможности MLLM в области рассуждений. В работе подробно рассматриваются пять ключевых аспектов улучшения MLLM с помощью RFT, включая разнообразие модальностей и задач, совершенствование алгоритмов обучения и создание новых бенчмарков. Статья также предлагает пять перспективных направлений для будущих исследований в этой области."
                },
                "en": {
                    "title": "Reinforcement Fine-Tuning: Powering Reasoning in Multimodal AI",
                    "desc": "This paper discusses the role of reinforcement fine-tuning (RFT) in improving the reasoning abilities of multimodal large language models (MLLMs). It highlights how RFT has contributed to the development of advanced AI models and emphasizes its importance in enhancing reasoning across various tasks and domains. The authors summarize five key improvements brought by RFT, including better training algorithms and diverse benchmarks. They also suggest future research directions to further explore the potential of RFT in the quest for Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "强化微调：推动多模态模型推理能力的关键",
                    "desc": "在2025年，强化微调（RFT）在提升大型语言模型（LLMs）的推理能力方面展现出显著潜力，并推动了如OpenAI-o1和DeepSeek-R1等前沿AI模型的发展。RFT在多模态大型语言模型（MLLMs）中的高效应用引起了广泛关注。本文详细介绍了研究者应了解的基础知识，并总结了RFT在提升MLLM推理能力方面的五个关键改进点。最后，我们提出了五个未来研究的有前景方向，以期为AGI的进步提供有价值的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16972",
            "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition",
            "url": "https://huggingface.co/papers/2505.16972",
            "abstract": "Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "784a648ae844599f",
            "authors": [
                "Tianduo Wang",
                "Lu Xu",
                "Wei Lu",
                "Shanbo Cheng"
            ],
            "affiliations": [
                "ByteDance Seed",
                "StatNLP Research Group, Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16972.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#audio",
                    "#synthetic"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Синтетическая речь открывает новые горизонты для многоязычного ASR",
                    "desc": "Эта статья представляет метод Speech Back-Translation для улучшения многоязычных систем автоматического распознавания речи (ASR). Метод использует текстовые корпусы и модели text-to-speech (TTS) для генерации синтетической речи в больших объемах. Авторы показывают, что даже небольшое количество реальной транскрибированной речи позволяет обучить TTS модели для генерации качественных синтетических данных. Применение этого метода для дообучения модели Whisper-large-v3 на 500 000 часах синтетической речи на 10 языках привело к снижению ошибок транскрипции в среднем на 30%."
                },
                "en": {
                    "title": "Scaling ASR with Synthetic Speech: Speech Back-Translation",
                    "desc": "This paper presents a method called Speech Back-Translation to enhance Automatic Speech Recognition (ASR) systems for multiple languages, especially those with limited resources. By using text-to-speech (TTS) models, the authors convert large text corpora into synthetic speech, significantly increasing the amount of training data available. They show that even a small amount of real speech can train TTS models to produce high-quality synthetic speech at a much larger scale. The results indicate that this approach can improve ASR performance, achieving over 30% reduction in transcription errors across ten languages."
                },
                "zh": {
                    "title": "语音反向翻译：提升多语言ASR的有效利器",
                    "desc": "这篇论文介绍了一种名为语音反向翻译（Speech Back-Translation）的新方法，旨在改善多语言自动语音识别（ASR）模型。该方法通过将大规模文本语料库转换为合成语音，利用现成的文本到语音（TTS）模型，解决了资源有限语言的覆盖问题。研究表明，仅需数十小时的真实转录语音，就能有效训练TTS模型生成数百倍的合成语音，同时保持高质量。通过这种方法，我们生成了超过50万小时的合成语音，并在多语言ASR系统中实现了超过30%的转录错误率降低。"
                }
            }
        }
    ],
    "link_prev": "2025-05-26.html",
    "link_next": "2025-05-28.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "26.05",
        "en": "05/26",
        "zh": "5月26日"
    },
    "short_date_next": {
        "ru": "28.05",
        "en": "05/28",
        "zh": "5月28日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种名为TabSTAR的模型。它是一种用于表格数据的基础模型，能够处理文本特征。TabSTAR通过迁移学习实现了最先进的分类任务性能，而不需要特定于数据集的参数。它利用预训练的文本编码器，结合目标令牌，学习任务特定的嵌入。TabSTAR在中等和大型数据集的分类任务中表现出色，并展示了预训练阶段的扩展规律。",
        "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
        "pinyin": "这篇文章介绍了一种名为TabSTAR的模型。它是一种用于表格数据的基础模型，能够处理文本特征。TabSTAR通过迁移学习实现了最先进的分类任务性能，而不需要特定于数据集的参数。它利用预训练的文本编码器，结合目标令牌，学习任务特定的嵌入。TabSTAR在中等和大型数据集的分类任务中表现出色，并展示了预训练阶段的扩展规律。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wéi TabSTAR de mó xíng. tā shì yī zhǒng yòng yú biǎo gé shù jù de jī chǔ mó xíng, néng gòu chǔ lǐ wén běn tè zhèng. TabSTAR tōng guò qiān yí xué xí shí xiàn le zuì xiān jìn de fēn lèi rèn wù xíng néng, ér bù xū yào tè dìng yú shù jù de cān shù. tā lì yòng yù xùn liàn de wén běn biān mǎ qì, jié hé mù biāo lìng pái, xué xí rèn wù tè dìng de qiàn rù. TabSTAR zài zhōng děng hé dà xíng shù jù de fēn lèi rèn wù zhōng biǎo xiàn chū sè, bìng zhǎn shì le yù xùn liàn jiē duàn de kuò zhǎn guī lǜ.",
        "vocab": "[\n    {\"word\": \"迁移学习\", \"pinyin\": \"qiān yí xué xí\", \"trans\": \"transfer learning\"},\n    {\"word\": \"分类任务\", \"pinyin\": \"fēn lèi rèn wù\", \"trans\": \"classification task\"},\n    {\"word\": \"参数\", \"pinyin\": \"cān shǔ\", \"trans\": \"parameter\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"文本编码器\", \"pinyin\": \"wén běn biān mǎ qì\", \"trans\": \"text encoder\"},\n    {\"word\": \"目标令牌\", \"pinyin\": \"mù biāo lìng pái\", \"trans\": \"target token\"},\n    {\"word\": \"嵌入\", \"pinyin\": \"qiàn rù\", \"trans\": \"embedding\"},\n    {\"word\": \"表现出色\", \"pinyin\": \"biǎo xiàn chū sè\", \"trans\": \"perform excellently\"},\n    {\"word\": \"扩展规律\", \"pinyin\": \"kuò zhǎn guī lǜ\", \"trans\": \"expansion pattern\"}\n]",
        "trans": "This article introduces a model called TabSTAR. It is a foundational model for tabular data that can handle textual features. TabSTAR achieves state-of-the-art performance in classification tasks through transfer learning, without requiring parameters specific to the dataset. It leverages a pre-trained text encoder combined with target tokens to learn task-specific embeddings. TabSTAR performs exceptionally well in classification tasks on medium and large datasets and demonstrates scalability during the pre-training phase.",
        "update_ts": "2025-05-26 09:39"
    }
}