{
    "date": {
        "ru": "18 ноября",
        "en": "November 18",
        "zh": "11月18日"
    },
    "time_utc": "2024-11-18 18:14",
    "weekday": 0,
    "issue_id": 643,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10440",
            "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
            "url": "https://huggingface.co/papers/2411.10440",
            "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.",
            "score": 55,
            "issue_id": 627,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "a706d5cae9964c9e",
            "authors": [
                "Guowei Xu",
                "Peng Jin",
                "Li Hao",
                "Yibing Song",
                "Lichao Sun",
                "Li Yuan"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "Institute for Interdisciplinary Information Sciences, Tsinghua University",
                "Peng Cheng Laboratory",
                "AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School",
                "Alibaba DAMO Academy",
                "Computer Science and Engineering, Lehigh University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10440.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLaVA-o1: Прорыв в автономном визуальном рассуждении",
                    "desc": "LLaVA-o1 - это новая мультимодальная языковая модель, способная к автономному многоступенчатому рассуждению. В отличие от других подходов, LLaVA-o1 самостоятельно выполняет последовательные этапы обобщения, визуальной интерпретации, логического рассуждения и формулирования выводов. Для обучения модели был создан датасет LLaVA-o1-100k со структурированными аннотациями рассуждений. Несмотря на небольшой объем обучающих данных, LLaVA-o1 превзошла более крупные модели на ряде тестов мультимодальных рассуждений."
                },
                "en": {
                    "title": "LLaVA-o1: Revolutionizing Visual Reasoning with Structured Multistage Processing",
                    "desc": "This paper presents LLaVA-o1, a new Vision-Language Model (VLM) that enhances reasoning capabilities in visual question-answering tasks. Unlike traditional methods that rely on chain-of-thought prompting, LLaVA-o1 autonomously performs multistage reasoning, which includes summarization, visual interpretation, logical reasoning, and conclusion generation. The model is trained on the LLaVA-o1-100k dataset, which contains diverse visual question-answering samples with structured reasoning annotations. With an innovative inference-time stage-level beam search method, LLaVA-o1 achieves significant improvements in precision, outperforming both its base model and larger closed-source models with only 100k training samples."
                },
                "zh": {
                    "title": "LLaVA-o1：自主多阶段推理的视觉语言模型",
                    "desc": "本论文介绍了一种新型的视觉语言模型LLaVA-o1，旨在进行自主的多阶段推理。与传统的链式思维提示不同，LLaVA-o1能够独立进行总结、视觉解读、逻辑推理和结论生成等多个阶段。通过这种结构化的方法，LLaVA-o1在推理密集型任务上显著提高了准确性。我们还构建了LLaVA-o1-100k数据集，并提出了一种有效的推理时间阶段级束搜索方法，以实现推理时间的有效扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06558",
            "title": "Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement",
            "url": "https://huggingface.co/papers/2411.06558",
            "abstract": "In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods.",
            "score": 21,
            "issue_id": 641,
            "pub_date": "2024-11-10",
            "pub_date_card": {
                "ru": "10 ноября",
                "en": "November 10",
                "zh": "11月10日"
            },
            "hash": "f4d24a7a6c0b27a0",
            "authors": [
                "Zhennan Chen",
                "Yajie Li",
                "Haofan Wang",
                "Zhibo Chen",
                "Zhengkai Jiang",
                "Jun Li",
                "Qian Wang",
                "Jian Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "China Mobile",
                "HKUST",
                "InstantX",
                "Liblib AI",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.06558.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#cv",
                    "#multimodal",
                    "#optimization",
                    "#games"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точный контроль над генерацией изображений с помощью региональных описаний",
                    "desc": "В этой статье представлен метод RAG (Regional-Aware Generation) для генерации изображений по текстовому описанию с учетом региональных особенностей. RAG разделяет процесс на два этапа: создание отдельных регионов и общую доработку деталей. Метод позволяет точно контролировать пространственное расположение объектов без дополнительного обучения модели. RAG также дает возможность перерисовывать отдельные неудовлетворительные области изображения, сохраняя остальные неизменными."
                },
                "en": {
                    "title": "RAG: Precision in Image Generation through Regional Awareness",
                    "desc": "This paper introduces RAG, a method for generating images from text that focuses on specific regions of the image for better layout control. It addresses limitations of previous methods by breaking down the generation process into two tasks: creating individual regions accurately and refining the overall image details. RAG allows users to modify specific areas of an image without affecting others, making it user-friendly and flexible. The method is designed to work without additional training and shows improved performance in generating images with clear attributes and relationships compared to earlier techniques."
                },
                "zh": {
                    "title": "区域感知生成，精确布局新方法",
                    "desc": "本文提出了一种名为RAG的区域感知文本到图像生成方法，旨在通过区域描述实现精确的布局组合。该方法通过区域提示和组合生成，提供了细粒度的空间控制，适用于实际应用。与以往方法不同，RAG将多区域生成分解为两个子任务，确保区域提示的有效执行和整体细节的优化。RAG还支持用户在最后生成中修改特定区域，而无需依赖额外的修复模型，展现了优越的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08033",
            "title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation",
            "url": "https://huggingface.co/papers/2411.08033",
            "abstract": "While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.",
            "score": 18,
            "issue_id": 627,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "9c28ee6de37c05b2",
            "authors": [
                "Yushi Lan",
                "Shangchen Zhou",
                "Zhaoyang Lyu",
                "Fangzhou Hong",
                "Shuai Yang",
                "Bo Dai",
                "Xingang Pan",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore",
                "Shanghai Artificial Intelligence Laboratory",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.08033.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Революция в 3D-генерации: от точек к реальности",
                    "desc": "Эта статья представляет новый фреймворк для генерации 3D-контента, который решает существующие проблемы с форматами ввода, дизайном латентного пространства и представлением выходных данных. Предложенный метод, названный GaussianAnything, использует вариационный автоэнкодер (VAE) с многоракурсными RGB-D-N рендерингами в качестве входных данных и уникальным дизайном латентного пространства, сохраняющим информацию о 3D-форме. Фреймворк поддерживает мультимодальную условную 3D-генерацию, позволяя использовать облака точек, текстовые описания и одиночные/многоракурсные изображения в качестве входных данных. Экспериментальные результаты показывают эффективность подхода на нескольких наборах данных, превосходя существующие методы в 3D-генерации, обусловленной текстом и изображениями."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with GaussianAnything",
                    "desc": "This paper presents a new framework for generating 3D content that overcomes limitations in current methods related to input formats and latent space design. It utilizes a Variational Autoencoder (VAE) that takes multi-view RGB-D-Normal renderings as input, creating a structured latent space that maintains essential 3D shape details. The framework also features a cascaded latent diffusion model to separate shape and texture effectively, enhancing the quality of generated 3D models. The proposed method, called GaussianAnything, allows for flexible 3D generation based on various input types, demonstrating superior performance in experiments compared to existing techniques."
                },
                "zh": {
                    "title": "创新3D生成框架：解耦形状与纹理",
                    "desc": "本论文提出了一种新颖的3D生成框架，旨在解决现有方法在输入格式、潜在空间设计和输出表示方面的挑战。该框架采用变分自编码器（VAE），以多视角的RGB-D（深度）-N（法线）渲染作为输入，独特的潜在空间设计能够保留3D形状信息。通过引入级联潜在扩散模型，改进了形状与纹理的解耦。实验结果表明，该方法在多个数据集上表现优异，超越了现有的文本和图像条件下的3D生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10323",
            "title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use",
            "url": "https://huggingface.co/papers/2411.10323",
            "abstract": "The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic, which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.",
            "score": 13,
            "issue_id": 631,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "fc2174371c27ab64",
            "authors": [
                "Siyuan Hu",
                "Mingyu Ouyang",
                "Difei Gao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Shou Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10323.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Claude 3.5: Революция в автоматизации пользовательского интерфейса",
                    "desc": "В статье представлен анализ модели Claude 3.5 Computer Use - первого публично доступного ИИ-агента с графическим интерфейсом. Авторы провели ряд тестов в различных доменах и программах для оценки возможностей модели. Результаты демонстрируют беспрецедентные способности Claude 3.5 в преобразовании естественного языка в действия на рабочем столе. Также предложена готовая к использованию фреймворк для развертывания моделей автоматизации GUI на основе API."
                },
                "en": {
                    "title": "Exploring the Future of GUI Agents with Claude 3.5",
                    "desc": "The paper introduces Claude 3.5 Computer Use, a pioneering AI model that functions as a graphical user interface (GUI) agent in public beta. It presents a case study that evaluates the model's performance across various tasks and software applications, highlighting its ability to translate natural language commands into desktop actions. The study also provides a framework for deploying API-based GUI automation models, making it easier for developers to implement similar systems. Through detailed analyses, the paper discusses the capabilities and limitations of Claude 3.5, raising important questions for future research in the GUI agent field."
                },
                "zh": {
                    "title": "探索Claude 3.5：前沿AI的GUI代理能力",
                    "desc": "Claude 3.5计算机使用模型是首个提供图形用户界面（GUI）代理的前沿人工智能模型。本文通过设计一系列任务，探索其在复杂环境中的表现。研究表明，Claude 3.5在语言到桌面操作的端到端能力上具有前所未有的表现。我们还提供了一个易于实现的API基础的GUI自动化模型框架，以便于部署和测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10332",
            "title": "Number it: Temporal Grounding Videos like Flipping Manga",
            "url": "https://huggingface.co/papers/2411.10332",
            "abstract": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9\\% in mIoU for moment retrieval and 8.5\\% in mAP for highlight detection. The code will be available at https://github.com/yongliang-wu/NumPro.",
            "score": 9,
            "issue_id": 631,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "92d2bcabe629d81c",
            "authors": [
                "Yongliang Wu",
                "Xinting Hu",
                "Yuyang Sun",
                "Yizhou Zhou",
                "Wenbo Zhu",
                "Fengyun Rao",
                "Bernt Schiele",
                "Xu Yang"
            ],
            "affiliations": [
                "WeChat, Tencent Inc.",
                "Max Planck Institute for Informatics",
                "University of California, Berkeley",
                "Southeast University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10332.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#video",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "NumPro: Помогаем ИИ ориентироваться во времени видео",
                    "desc": "Video-LLM достигли значительных успехов в понимании видеоконтента для диалогов вопросов и ответов, но испытывают трудности с точной временной локализацией. Для решения этой проблемы авторы предлагают метод Number-Prompt (NumPro), который добавляет уникальные числовые идентификаторы к каждому кадру видео. Это позволяет Video-LLM 'читать' временные линии событий, точно связывая визуальный контент с соответствующей временной информацией. Эксперименты показывают, что NumPro значительно повышает производительность Video-LLM в задачах временной локализации видео без дополнительных вычислительных затрат."
                },
                "en": {
                    "title": "Bridging Visual Understanding and Temporal Grounding in Videos",
                    "desc": "This paper presents a new method called Number-Prompt (NumPro) to improve Video Temporal Grounding (VTG) in Video Large Language Models (Vid-LLMs). Vid-LLMs have difficulty linking visual content with specific timeframes, which is crucial for tasks like question answering about videos. NumPro addresses this by assigning unique numerical identifiers to each video frame, allowing the model to process video as a sequence of numbered images, similar to reading manga panels. The results show that NumPro enhances the performance of Vid-LLMs in VTG tasks significantly, achieving state-of-the-art results without increasing computational costs."
                },
                "zh": {
                    "title": "数字提示：提升视频时间定位的创新方法",
                    "desc": "视频大型语言模型（Vid-LLMs）在理解视频内容方面取得了显著进展，但在需要精确时间定位的任务（视频时间定位，VTG）中仍然存在困难。为了解决这个问题，我们提出了一种新方法——数字提示（NumPro），通过为每个视频帧添加独特的数字标识符，帮助Vid-LLMs将视觉理解与时间定位结合起来。NumPro将VTG转化为一种直观的过程，类似于按顺序翻阅漫画面板，使Vid-LLMs能够“阅读”事件时间线，准确地将视觉内容与相应的时间信息关联。实验表明，NumPro显著提升了顶级Vid-LLMs在VTG任务中的表现，且没有额外的计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10083",
            "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
            "url": "https://huggingface.co/papers/2411.10083",
            "abstract": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM.",
            "score": 4,
            "issue_id": 641,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "745c93b05bda9f01",
            "authors": [
                "Wang Qun",
                "Liu Yang",
                "Lin Qingquan",
                "Jiang Ling"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.10083.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#small_models",
                    "#low_resource"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Xmodel-1.5: Мультиязычный ИИ для лучшего понимания между культурами",
                    "desc": "Исследователи представили Xmodel-1.5 - новую мультиязычную большую языковую модель с 1 миллиардом параметров, обученную на 2 триллионах токенов. Модель демонстрирует высокую производительность на нескольких языках, особенно на тайском, арабском и французском, наряду с китайским и английским. Авторы также выпустили набор данных для оценки на тайском языке, содержащий сотни вопросов, аннотированных студентами. Модель и код доступны в открытом доступе на GitHub."
                },
                "en": {
                    "title": "Empowering Multilingual AI with Xmodel-1.5",
                    "desc": "Xmodel-1.5 is a large multilingual model with 1 billion parameters, trained on a massive dataset of 2 trillion tokens. It shows impressive performance in multiple languages, especially in Thai, Arabic, and French, while also being effective in Chinese and English. The authors provide a new evaluation dataset for Thai, created with input from students, to aid in further research. This work aims to enhance multilingual AI capabilities and foster better understanding across different languages in natural language processing tasks."
                },
                "zh": {
                    "title": "推动多语言AI研究的前沿",
                    "desc": "我们介绍了Xmodel-1.5，这是一个新型的十亿参数多语言大模型，预训练于大约2万亿个标记上。该模型在多种语言上表现出色，尤其在泰语、阿拉伯语和法语方面的结果尤为显著，同时在中文和英文中也表现良好。此外，我们还为研究社区贡献了一个泰语评估数据集，包含由朱拉隆功大学综合创新学院的学生标注的数百个问题。尽管结果令人鼓舞，但我们承认仍有改进的空间，希望这项工作能推动多语言人工智能研究的进展，并促进各种自然语言处理任务中的跨语言理解。"
                }
            }
        }
    ],
    "link_prev": "2024-11-15.html",
    "link_next": "2024-11-19.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "15.11",
        "en": "11/15",
        "zh": "11月15日"
    },
    "short_date_next": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11月19日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de shì jué yǔ yán mó xìng (VLM)，chēng wéi LLaVA-o1，zhǐ yǐn jìn xíng zì zhǔ duō jiē duàn tuí lǐ. yǔ liàn shì sī wéi tí shì bù tóng，LLaVA-o1 dú lì jìn xíng zǒng jiě，shì jué jiě shì，luó ji tuí lǐ hé jié lùn shēng chéng. zhè zhǒng jiē gòu huà fǎ shǐ qí zài tuí lǐ mì jī xíng rèn wù shàng qǔ dé xiǎn zhù jīng dù tí shēng. yán jiū tuán duì biān zhì le LLaVA-o1-100k shù jù jí，bìng tí chū le yī zhǒng tuí lǐ shí jiē duàn jí shù sōu suǒ fǎ，yǐ shí xiàn yán jiū shí kuò zhǎn. jié guǒ xiǎn shì，LLaVA-o1 zài duō mó shuài tuí lǐ jī zhǔn cè shì zhōng biǎo xiàn chū sè，chāo yuè le duō gè dà xíng hé fēng bì yuán mó xìng.",
        "vocab": "[\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shìjué yǔyán móxíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomous\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiēduàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"链式\", \"pinyin\": \"liànshì\", \"trans\": \"chain-like\"},\n    {\"word\": \"提示\", \"pinyin\": \"tíshì\", \"trans\": \"prompt\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"总结\", \"pinyin\": \"zǒngjié\", \"trans\": \"summary\"},\n    {\"word\": \"视觉解释\", \"pinyin\": \"shìjué jiěshì\", \"trans\": \"visual explanation\"},\n    {\"word\": \"逻辑推理\", \"pinyin\": \"luóji tuīlǐ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"结论生成\", \"pinyin\": \"jiélùn shēngchéng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"结构化\", \"pinyin\": \"jiégòuhuà\", \"trans\": \"structured\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"accuracy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improvement\"},\n    {\"word\": \"研究团队\", \"pinyin\": \"yánjiū tuánduì\", \"trans\": \"research team\"},\n    {\"word\": \"编制\", \"pinyin\": \"biānzhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"推理时\", \"pinyin\": \"tuīlǐ shí\", \"trans\": \"during reasoning\"},\n    {\"word\": \"阶段级\", \"pinyin\": \"jiēduàn jí\", \"trans\": \"stage-level\"},\n    {\"word\": \"束搜索\", \"pinyin\": \"shù sōusuǒ\", \"trans\": \"beam search\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"expansion\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jīzhǔn cèshì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēngbì yuán\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}