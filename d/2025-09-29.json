{
    "date": {
        "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 29",
        "zh": "9æœˆ29æ—¥"
    },
    "time_utc": "2025-09-29 02:22",
    "weekday": 0,
    "issue_id": 6129,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.21760",
            "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
            "url": "https://huggingface.co/papers/2509.21760",
            "abstract": "A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
            "score": 4,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "e74140613312626b",
            "authors": [
                "Lan Chen",
                "Yuchao Gu",
                "Qi Mao"
            ],
            "affiliations": [
                "Communication University of China",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21760.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ UniVid - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞºĞ°Ğº ÑĞ°Ğ¼Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. UniVid Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ÑÑÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "UniVid: A Unified Framework for Diverse Vision Tasks",
                    "desc": "The paper introduces UniVid, a pre-trained video diffusion transformer that can be fine-tuned for various vision tasks without needing specific modifications for each task. It leverages the concept of visual sentences, where tasks are represented in a sequence that guides the model's output. UniVid demonstrates strong generalization capabilities across different modalities, such as images and videos, and can adapt to tasks from various sources without requiring extensive pre-training. This approach shows that pre-trained video generation models can provide a scalable and unified framework for handling diverse vision challenges."
                },
                "zh": {
                    "title": "UniVidï¼šç»Ÿä¸€è§†è§‰ä»»åŠ¡çš„å¼ºå¤§å·¥å…·",
                    "desc": "UniVidæ˜¯ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œç‰¹å®šä»»åŠ¡ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œé€‚åº”å¤šç§è§†è§‰ä»»åŠ¡ã€‚å®ƒé€šè¿‡å°†ä»»åŠ¡è¡¨ç¤ºä¸ºè§†è§‰å¥å­ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡åºåˆ—æ¥å®šä¹‰ä»»åŠ¡å’ŒæœŸæœ›çš„è¾“å‡ºå½¢å¼ã€‚UniVidåœ¨è·¨æ¨¡æ€æ¨ç†å’Œè·¨æºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡ä»…åœ¨è‡ªç„¶è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¯ä»¥ä½œä¸ºè§†è§‰å»ºæ¨¡çš„ç»Ÿä¸€å’Œå¯æ‰©å±•åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22496",
            "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation",
            "url": "https://huggingface.co/papers/2509.22496",
            "abstract": "EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
            "score": 2,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ad53ca1f5bbc2302",
            "authors": [
                "Ruoyu Chen",
                "Xiaoqing Guo",
                "Kangwei Liu",
                "Siyuan Liang",
                "Shiming Liu",
                "Qunli Zhang",
                "Hua Zhang",
                "Xiaochun Cao"
            ],
            "affiliations": [
                "Department of Computer Science, Hong Kong Baptist University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "RAMS Lab, Huawei Technologies Co., Ltd.",
                "RAMS Lab, Munich Research Center, Huawei Technologies DÃ¼sseldorf GmbH",
                "School of Computing, NUS",
                "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22496.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#inference",
                    "#multimodal",
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "ĞĞ±ÑŠÑÑĞ½ÑĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½: ĞºĞ°Ğº MLLM Ğ²Ğ¸Ğ´ÑÑ‚ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚",
                    "desc": "EAGLE - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ ÑĞ²ÑĞ·Ğ°Ğ½ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAGLE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "EAGLE: Unraveling Token Generation in Multimodal Models",
                    "desc": "EAGLE is a new framework designed to explain how multimodal large language models (MLLMs) generate tokens by linking them to specific visual areas. It quantifies the influence of both language and visual information on token generation, enhancing the interpretability of these models. The framework uses an objective function to measure how essential and sufficient different visual regions are for generating specific tokens, optimizing this through a greedy search method. EAGLE has been shown to outperform existing methods in terms of accuracy and efficiency while using less computational resources, making it a practical tool for understanding MLLM behavior."
                },
                "zh": {
                    "title": "EAGLEï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§çš„è½»é‡çº§æ¡†æ¶",
                    "desc": "EAGLEæ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œç”¨äºè§£é‡Šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä»¤ç‰Œç”Ÿæˆã€‚å®ƒé€šè¿‡å°†ä»¤ç‰Œå½’å› äºè§†è§‰åŒºåŸŸï¼Œå¹¶é‡åŒ–è¯­è¨€å’Œæ„ŸçŸ¥è¯æ®çš„å½±å“ï¼Œæ¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œç»Ÿä¸€äº†å……åˆ†æ€§å’Œå¿…è¦æ€§è¯„åˆ†ï¼Œå¹¶é€šè¿‡è´ªå©ªæœç´¢ä¼˜åŒ–ç¨€ç–å›¾åƒåŒºåŸŸã€‚EAGLEåœ¨å¤šä¸ªå¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨å¯ä¿¡åº¦ã€å®šä½å’Œå¹»è§‰è¯Šæ–­æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†GPUå†…å­˜çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22186",
            "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
            "url": "https://huggingface.co/papers/2509.22186",
            "abstract": "MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
            "score": 2,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "136108b6a4721246",
            "authors": [
                "Junbo Niu",
                "Zheng Liu",
                "Zhuangcheng Gu",
                "Bin Wang",
                "Linke Ouyang",
                "Zhiyuan Zhao",
                "Tao Chu",
                "Tianyao He",
                "Fan Wu",
                "Qintong Zhang",
                "Zhenjiang Jin",
                "Guang Liang",
                "Rui Zhang",
                "Wenzheng Zhang",
                "Yuan Qu",
                "Zhifei Ren",
                "Yuefeng Sun",
                "Yuanhong Zheng",
                "Dongsheng Ma",
                "Zirui Tang",
                "Boyu Niu",
                "Ziyang Miao",
                "Hejun Dong",
                "Siyi Qian",
                "Junyuan Zhang",
                "Jingzhou Chen",
                "Fangdong Wang",
                "Xiaomeng Zhao",
                "Liqun Wei",
                "Wei Li",
                "Shasha Wang",
                "Ruiliang Xu",
                "Yuanyuan Cao",
                "Lu Chen",
                "Qianqian Wu",
                "Huaiyu Gu",
                "Lindong Lu",
                "Keming Wang",
                "Dechen Lin",
                "Guanlin Shen",
                "Xuanhe Zhou",
                "Linfeng Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Jiaqi Wang",
                "Bo Zhang",
                "Lei Bai",
                "Pei Chu",
                "Weijia Li",
                "Jiang Wu",
                "Lijun Wu",
                "Zhenxiang Li",
                "Guangyu Wang",
                "Zhongying Tu",
                "Chao Xu",
                "Kai Chen",
                "Yu Qiao",
                "Bowen Zhou",
                "Dahua Lin",
                "Wentao Zhang",
                "Conghui He"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22186.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MinerU2.5 - vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. MinerU2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Efficient Document Parsing with MinerU2.5",
                    "desc": "MinerU2.5 is a vision-language model designed for document parsing, featuring 1.2 billion parameters. It utilizes a coarse-to-fine parsing strategy that separates the analysis of document layout from the recognition of content, enhancing efficiency. The model first conducts layout analysis on downsampled images to identify structural elements, then focuses on detailed content recognition using high-resolution crops. This innovative approach allows MinerU2.5 to achieve top performance on various benchmarks while reducing computational costs compared to other models."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ–‡æ¡£è§£æçš„æ–°æ ‡æ†",
                    "desc": "MinerU2.5æ˜¯ä¸€ç§å…·æœ‰12äº¿å‚æ•°çš„æ–‡æ¡£è§£æè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ç²—åˆ°ç»†çš„è§£æç­–ç•¥ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«å‡†ç¡®ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¨¡å‹çš„ç¬¬ä¸€é˜¶æ®µåœ¨é™é‡‡æ ·å›¾åƒä¸Šè¿›è¡Œé«˜æ•ˆçš„å¸ƒå±€åˆ†æï¼Œä»¥è¯†åˆ«ç»“æ„å…ƒç´ ï¼Œä»è€Œé¿å…å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥çš„è®¡ç®—å¼€é”€ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åœ¨åŸå§‹å›¾åƒä¸­æå–çš„åŸç”Ÿåˆ†è¾¨ç‡è£å‰ªå›¾ä¸Šè¿›è¡Œç›®æ ‡å†…å®¹è¯†åˆ«ï¼Œä¿ç•™äº†å¯†é›†æ–‡æœ¬ã€å¤æ‚å…¬å¼å’Œè¡¨æ ¼ä¸­çš„ç»†èŠ‚ã€‚é€šè¿‡å¼€å‘å…¨é¢çš„æ•°æ®å¼•æ“ï¼ŒMinerU2.5èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“ï¼Œæ”¯æŒé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæœ€ç»ˆåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21574",
            "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
            "url": "https://huggingface.co/papers/2509.21574",
            "abstract": "X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
            "score": 1,
            "issue_id": 6129,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "addda28b1d3ecc10",
            "authors": [
                "You Xie",
                "Tianpei Gu",
                "Zenan Li",
                "Chenxu Zhang",
                "Guoxian Song",
                "Xiaochen Zhao",
                "Chao Liang",
                "Jianwen Jiang",
                "Hongyi Xu",
                "Linjie Luo"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21574.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#architecture",
                    "#multimodal",
                    "#agi",
                    "#interpretability",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ X-Streamer â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ dual-transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Thinker-Actor, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Thinker Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ° Actor Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ chunk-wise autoregressive diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… GPU A100, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "X-Streamer: Real-Time Multimodal Interactions Redefined",
                    "desc": "X-Streamer is a cutting-edge framework that combines text, speech, and video interactions using a dual-transformer architecture. It allows for real-time communication by transforming a static image into a dynamic digital human capable of engaging in endless conversations. The framework consists of two main components: the Thinker, which processes and understands user inputs, and the Actor, which generates synchronized multimodal outputs. By utilizing advanced models and attention mechanisms, X-Streamer ensures smooth and coherent interactions, making it a significant advancement in creating interactive digital agents."
                },
                "zh": {
                    "title": "X-Streamerï¼šå®æ—¶å¤šæ¨¡æ€äº’åŠ¨çš„æ–°çºªå…ƒ",
                    "desc": "X-Streameræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé‡‡ç”¨åŒå˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘ä¹‹é—´çš„å®æ—¶äº’åŠ¨ã€‚å®ƒé€šè¿‡æµå¼å¤šæ¨¡æ€è¾“å…¥ï¼Œå°†é™æ€è‚–åƒè½¬å˜ä¸ºæŒä¹…çš„æ™ºèƒ½è§†å¬äº’åŠ¨ã€‚æ¡†æ¶çš„æ ¸å¿ƒæ˜¯Thinker-ActoråŒå˜æ¢å™¨ï¼ŒThinkeræ¨¡å—è´Ÿè´£æ„ŸçŸ¥å’Œæ¨ç†ç”¨æˆ·è¾“å…¥ï¼Œè€ŒActoræ¨¡å—åˆ™å°†è¿™äº›ä¿¡æ¯å®æ—¶è½¬æ¢ä¸ºåŒæ­¥çš„å¤šæ¨¡æ€è¾“å‡ºã€‚X-Streameråœ¨ä¸¤å—A100 GPUä¸Šå®æ—¶è¿è¡Œï¼Œèƒ½å¤Ÿæ”¯æŒæ•°å°æ—¶çš„æŒç»­è§†é¢‘èŠå¤©ï¼Œæ¨åŠ¨äº’åŠ¨æ•°å­—äººç±»çš„ç»Ÿä¸€ä¸–ç•Œå»ºæ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22244",
            "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.22244",
            "abstract": "FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150times speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ea3d5abcdd7f6370",
            "authors": [
                "Junyi Wu",
                "Zhiteng Li",
                "Haotong Qin",
                "Xiaohong Liu",
                "Linghe Kong",
                "Yulun Zhang",
                "Xiaokang Yang"
            ],
            "affiliations": [
                "ETH Zurich",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22244.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "FlashEdit â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (OSIE), Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ñ„Ğ¾Ğ½Ğ° (BG-Shield) Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SSCA). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° 0.2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ² 150 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². FlashEdit ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Real-Time Image Editing Revolutionized with FlashEdit",
                    "desc": "FlashEdit is a cutting-edge framework that allows for real-time image editing using diffusion models, significantly improving the editing speed and quality. It introduces a One-Step Inversion-and-Editing (OSIE) pipeline that eliminates the need for slow iterative processes, enabling quick modifications. The Background Shield (BG-Shield) technique ensures that the background remains intact while only the desired features are altered, enhancing the overall visual coherence. Additionally, the Sparsified Spatial Cross-Attention (SSCA) mechanism allows for precise edits by minimizing unwanted changes to the background, achieving edits in under 0.2 seconds, which is over 150 times faster than previous methods."
                },
                "zh": {
                    "title": "FlashEditï¼šå®æ—¶é«˜ä¿çœŸå›¾åƒç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "FlashEdit æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶ã€é«˜ä¿çœŸçš„å›¾åƒç¼–è¾‘ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°æé«˜äº†æ•ˆç‡ï¼šé¦–å…ˆï¼Œé‡‡ç”¨äº†ä¸€ç§ä¸€æ­¥åæ¼”ä¸ç¼–è¾‘ï¼ˆOSIEï¼‰æµç¨‹ï¼Œé¿å…äº†è€—æ—¶çš„è¿­ä»£è¿‡ç¨‹ï¼›å…¶æ¬¡ï¼ŒèƒŒæ™¯ä¿æŠ¤æŠ€æœ¯ï¼ˆBG-Shieldï¼‰ç¡®ä¿äº†èƒŒæ™¯çš„ä¸€è‡´æ€§ï¼Œä»…åœ¨ç¼–è¾‘åŒºåŸŸå†…è¿›è¡Œç‰¹å¾ä¿®æ”¹ï¼›æœ€åï¼Œç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ç¡®ä¿äº†ç²¾ç¡®çš„å±€éƒ¨ç¼–è¾‘ï¼Œé˜²æ­¢è¯­ä¹‰ä¿¡æ¯æ³„æ¼åˆ°èƒŒæ™¯ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashEdit åœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘é€Ÿåº¦è¶…è¿‡0.2ç§’ï¼Œæ¯”ä¹‹å‰çš„å¤šæ­¥éª¤æ–¹æ³•å¿«150å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19768",
            "title": "CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
            "url": "https://huggingface.co/papers/2509.19768",
            "abstract": "CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate text recognition for historical documents can greatly advance the study and preservation of cultural heritage. Existing vision-language models (VLMs), however, are designed for modern, standardized texts and are not equipped to read the diverse languages and scripts, irregular layouts, and frequent degradation found in historical materials.   This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for historical text recognition. The model is trained on CHURRO-DS, the largest historical text recognition dataset to date. CHURRO-DS unifies 155 historical corpora comprising 99,491 pages, spanning 22 centuries of textual heritage across 46 language clusters, including historical variants and dead languages.   We evaluate several open-weight and closed VLMs and optical character recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and 70.1% (handwritten) normalized Levenshtein similarity, surpassing the second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being 15.5 times more cost-effective.   By releasing the model and dataset, we aim to enable community-driven research to improve the readability of historical texts and accelerate scholarship.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "8759baf8bb974573",
            "authors": [
                "Sina J. Semnani",
                "Han Zhang",
                "Xinyan He",
                "Merve TekgÃ¼rler",
                "Monica S. Lam"
            ],
            "affiliations": [
                "Stanford University, Stanford, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19768.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#science",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ“œ",
                "ru": {
                    "title": "CHURRO: AI Ğ´Ğ»Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¸Ñ",
                    "desc": "CHURRO - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ CHURRO-DS, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ 99,491 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 46 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ñ… Ğ·Ğ° 22 Ğ²ĞµĞºĞ°. CHURRO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ VLM Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ (82.3%) Ğ¸ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾ (70.1%) Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°Ñ Gemini 2.5 Pro Ğ½Ğ° 1.4% Ğ¸ 6.5% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² 15.5 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unlocking Historical Texts with CHURRO",
                    "desc": "The paper introduces CHURRO, a vision-language model specifically designed for recognizing historical texts. It is trained on CHURRO-DS, the largest dataset for this purpose, which includes a wide variety of historical documents across multiple languages and scripts. CHURRO outperforms existing models in accuracy and cost-effectiveness, achieving high similarity scores in recognizing both printed and handwritten texts. By making CHURRO and its dataset publicly available, the authors aim to foster research that enhances the understanding and preservation of cultural heritage."
                },
                "zh": {
                    "title": "CHURROï¼šå†å²æ–‡æœ¬è¯†åˆ«çš„æ–°çªç ´",
                    "desc": "CHURROæ˜¯ä¸€ç§å…·æœ‰30äº¿å‚æ•°çš„å¼€æ”¾æƒé‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå†å²æ–‡æœ¬è¯†åˆ«ã€‚å®ƒåœ¨CHURRO-DSæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¯¥æ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å†å²æ–‡æœ¬è¯†åˆ«æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª22ä¸ªä¸–çºªçš„155ä¸ªå†å²è¯­æ–™åº“ã€‚CHURROåœ¨è¯†åˆ«å°åˆ·å’Œæ‰‹å†™æ–‡æœ¬æ–¹é¢çš„è¡¨ç°ä¼˜äºæ‰€æœ‰ç°æœ‰æ¨¡å‹ï¼Œä¸”æˆæœ¬æ•ˆç›Šæ›´é«˜ã€‚é€šè¿‡å‘å¸ƒè¯¥æ¨¡å‹å’Œæ•°æ®é›†ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿ƒè¿›ç¤¾åŒºé©±åŠ¨çš„ç ”ç©¶ï¼Œæå‡å†å²æ–‡æœ¬çš„å¯è¯»æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-26.html",
    "link_next": "2025-09-30.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "26.09",
        "en": "09/26",
        "zh": "9æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}