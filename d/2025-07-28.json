{
    "date": {
        "ru": "28 Ğ¸ÑĞ»Ñ",
        "en": "July 28",
        "zh": "7æœˆ28æ—¥"
    },
    "time_utc": "2025-07-28 07:19",
    "weekday": 0,
    "issue_id": 5040,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.10510",
            "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
            "url": "https://huggingface.co/papers/2507.10510",
            "abstract": "Artic addresses latency issues in AI Video Chat by optimizing video streaming and frame rate adaptation to enhance MLLM accuracy and reduce bitrate.  \t\t\t\t\tAI-generated summary \t\t\t\t AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat.",
            "score": 1,
            "issue_id": 5037,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 Ğ¸ÑĞ»Ñ",
                "en": "July 14",
                "zh": "7æœˆ14æ—¥"
            },
            "hash": "d931480372d88084",
            "authors": [
                "Jiangkai Wu",
                "Zhiyuan Ren",
                "Liming Liu",
                "Xinggong Zhang"
            ],
            "affiliations": [
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10510.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#survey",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Artic: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğµ Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Artic - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğ° Ñ Ğ˜Ğ˜. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼, Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DeViBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing AI Video Chat for Real-Time Interaction",
                    "desc": "The paper presents Artic, a framework designed to improve latency in AI Video Chat by optimizing video streaming and frame rate adaptation. It focuses on enhancing the accuracy of Multimodal Large Language Models (MLLMs) while minimizing the bitrate required for video transmission. By implementing Context-Aware Video Streaming, Artic prioritizes important video regions for chat, ensuring efficient bitrate allocation. Additionally, the Loss-Resilient Adaptive Frame Rate technique helps maintain video quality by using previous frames to compensate for lost or delayed ones, thus addressing the challenges posed by network instability."
                },
                "zh": {
                    "title": "ä¼˜åŒ–AIè§†é¢‘èŠå¤©ï¼Œæå‡å®æ—¶æ²Ÿé€šä½“éªŒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºArticçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³AIè§†é¢‘èŠå¤©ä¸­çš„å»¶è¿Ÿé—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–è§†é¢‘æµå’Œå¸§ç‡é€‚åº”ï¼ŒArticèƒ½å¤Ÿæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘æ¯”ç‰¹ç‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†é¢‘æµæŠ€æœ¯ï¼Œä¼˜å…ˆåˆ†é…æ¯”ç‰¹ç‡ç»™å¯¹èŠå¤©é‡è¦çš„è§†é¢‘åŒºåŸŸï¼Œä»è€Œæ˜¾è‘—é™ä½æ¯”ç‰¹ç‡ã€‚ä¸ºäº†é¿å…æ•°æ®åŒ…é‡ä¼ ï¼ŒArticè¿˜å¼•å…¥äº†æŠ—ä¸¢åŒ…è‡ªé€‚åº”å¸§ç‡æŠ€æœ¯ï¼Œåˆ©ç”¨ä¹‹å‰çš„å¸§æ¥æ›¿ä»£ä¸¢å¤±æˆ–å»¶è¿Ÿçš„å¸§ï¼Œè¿›ä¸€æ­¥æå‡äº†è§†é¢‘èŠå¤©çš„æµç•…æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17596",
            "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2507.17596",
            "abstract": "PRIX, an end-to-end driving architecture using only camera data, achieves state-of-the-art performance with a Context-aware Recalibration Transformer, outperforming larger multimodal planners in efficiency and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix.",
            "score": 0,
            "issue_id": 5040,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ»Ñ",
                "en": "July 23",
                "zh": "7æœˆ23æ—¥"
            },
            "hash": "13d4f0dd6ee82302",
            "authors": [
                "Maciej K. Wozniak",
                "Lianhang Liu",
                "Yixi Cai",
                "Patric Jensfelt"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Sweden",
                "Scania CV AB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17596.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· LiDAR",
                    "desc": "PRIX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ğ¼ĞµÑ€ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LiDAR Ğ¸ BEV-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Context-aware Recalibration Transformer (CaRT), ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. PRIX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… NavSim Ğ¸ nuScenes, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Driving Innovation with Camera-Only Solutions",
                    "desc": "PRIX is an innovative end-to-end driving architecture that utilizes only camera data to enhance autonomous driving capabilities. It introduces the Context-aware Recalibration Transformer (CaRT), which improves the processing of visual features for better trajectory planning. By eliminating the need for expensive LiDAR sensors and complex BEV representations, PRIX offers a more efficient and scalable solution for mass-market vehicles. Comprehensive experiments show that PRIX achieves state-of-the-art performance while being smaller and faster than traditional multimodal planners."
                },
                "zh": {
                    "title": "PRIXï¼šä»…ç”¨æ‘„åƒå¤´æ•°æ®çš„é«˜æ•ˆè‡ªåŠ¨é©¾é©¶è§£å†³æ–¹æ¡ˆ",
                    "desc": "PRIXæ˜¯ä¸€ç§ä»…ä½¿ç”¨æ‘„åƒå¤´æ•°æ®çš„ç«¯åˆ°ç«¯é©¾é©¶æ¶æ„ï¼Œé‡‡ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ ¡å‡†å˜æ¢å™¨ï¼ˆCaRTï¼‰ï¼Œåœ¨æ•ˆç‡å’Œå¯æ‰©å±•æ€§ä¸Šè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„å¤šæ¨¡æ€è§„åˆ’å™¨ã€‚è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¾èµ–æ˜‚è´µçš„æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨å’Œè®¡ç®—å¯†é›†å‹BEVç‰¹å¾è¡¨ç¤ºçš„é—®é¢˜ï¼Œä½¿å…¶æ›´é€‚åˆå¤§ä¼—å¸‚åœºçš„è½¦è¾†ã€‚PRIXé€šè¿‡è§†è§‰ç‰¹å¾æå–å™¨å’Œç”Ÿæˆè§„åˆ’å¤´ï¼Œç›´æ¥ä»åŸå§‹åƒç´ è¾“å…¥é¢„æµ‹å®‰å…¨è½¨è¿¹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPRIXåœ¨NavSimå’ŒnuScenesåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡ä¸å¤§å‹å¤šæ¨¡æ€æ‰©æ•£è§„åˆ’å™¨ç›¸å½“çš„èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹å¤§å°ä¸Šæ˜¾è‘—æ›´é«˜æ•ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-25.html",
    "link_next": "2025-07-29.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "25.07",
        "en": "07/25",
        "zh": "7æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "29.07",
        "en": "07/29",
        "zh": "7æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}