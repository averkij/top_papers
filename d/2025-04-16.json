{
    "date": {
        "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 16",
        "zh": "4æœˆ16æ—¥"
    },
    "time_utc": "2025-04-16 05:11",
    "weekday": 2,
    "issue_id": 3261,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10481",
            "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
            "url": "https://huggingface.co/papers/2504.10481",
            "abstract": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.",
            "score": 18,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "72678fc1ff453072",
            "authors": [
                "Ding Chen",
                "Qingchen Yu",
                "Pengyuan Wang",
                "Wentao Zhang",
                "Bo Tang",
                "Feiyu Xiong",
                "Xinchi Li",
                "Minchuan Yang",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Center for Data Science, Peking University",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Research Institute of China Telecom, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10481.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "xVerify: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ xVerify - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. xVerify ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ xVerify Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VAR, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ xVerify Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ F1-Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑˆĞµ 95% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "xVerify: Elevating Reasoning Model Evaluation with Precision",
                    "desc": "This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods."
                },
                "zh": {
                    "title": "xVerifyï¼šæ¨ç†æ¨¡å‹è¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "éšç€OpenAIå‘å¸ƒo1æ¨¡å‹ï¼Œé‡‡ç”¨æ…¢æ€ç»´ç­–ç•¥çš„æ¨ç†æ¨¡å‹é€æ¸å‡ºç°ã€‚è¿™äº›æ¨¡å‹ç”Ÿæˆçš„å“åº”é€šå¸¸åŒ…å«å¤æ‚çš„æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªæˆ‘åæ€ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ¤æ–­LLMè¾“å‡ºæ˜¯å¦çœŸæ­£ç­‰åŒäºå‚è€ƒç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†xVerifyï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œç”¨äºæ¨ç†æ¨¡å‹çš„è¯„ä¼°ã€‚xVerifyåœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ¤æ–­æ¨ç†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆç­‰ä»·ï¼Œå¹¶åœ¨å¤šä¸ªå®¢è§‚é—®é¢˜ç±»å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10337",
            "title": "Heimdall: test-time scaling on the generative verification",
            "url": "https://huggingface.co/papers/2504.10337",
            "abstract": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.",
            "score": 16,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "6da5db970a101d21",
            "authors": [
                "Wenlei Shi",
                "Xing Jin"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10337.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#math"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Heimdall: Ğ˜Ğ˜-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Heimdall - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ° Ñ 62.5% Ğ´Ğ¾ 94.5% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑÑĞ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Heimdall Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning",
                    "desc": "This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies."
                },
                "zh": {
                    "title": "æå‡AIçŸ¥è¯†éªŒè¯èƒ½åŠ›çš„Heimdallæ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHeimdallçš„é•¿é“¾æ€ç»´éªŒè¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜è§£å†³ç«äº‰æ€§æ•°å­¦é—®é¢˜çš„è§£ç­”å‡†ç¡®æ€§ã€‚é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ ï¼ŒHeimdallçš„éªŒè¯å‡†ç¡®ç‡ä»62.5%æå‡è‡³94.5%ï¼Œå¹¶é€šè¿‡é‡å¤é‡‡æ ·è¿›ä¸€æ­¥æé«˜è‡³97.5%ã€‚æ­¤å¤–ï¼ŒHeimdallè¿˜å±•ç¤ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ£€æµ‹å‡ºè®­ç»ƒä¸­æœªåŒ…å«çš„å¤æ‚æ•°å­¦è¯æ˜ä¸­çš„å¤§å¤šæ•°é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ‚²è§‚éªŒè¯æ–¹æ³•ï¼Œä»¥æ‰©å±•Heimdallçš„åŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11442",
            "title": "TextArena",
            "url": "https://huggingface.co/papers/2504.11442",
            "abstract": "TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
            "score": 12,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "7c9ae6533757828a",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "TextArena: ĞÑ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "TextArena - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 57 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¸Ğ³Ñ€Ñ‹ Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ TrueSkill Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. TextArena Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ğ¼Ğ°Ğ½. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Train LLMs in Competitive Text Games!",
                    "desc": "TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes."
                },
                "zh": {
                    "title": "TextArenaï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æŠ€èƒ½è®­ç»ƒå¹³å°",
                    "desc": "TextArenaæ˜¯ä¸€ä¸ªå¼€æºçš„ç«äº‰æ€§æ–‡æœ¬æ¸¸æˆé›†åˆï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç†è¡Œä¸ºã€‚å®ƒåŒ…å«57ä¸ªä»¥ä¸Šç‹¬ç‰¹çš„ç¯å¢ƒï¼Œæ”¯æŒå•äººã€åŒäººå’Œå¤šäººè®¾ç½®ï¼Œå¹¶é€šè¿‡åœ¨çº¿æ¸¸æˆç³»ç»Ÿè½»æ¾è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚ä¼ ç»ŸåŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•è¯„ä¼°åŠ¨æ€ç¤¾äº¤æŠ€èƒ½ï¼Œå¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ï¼Œè€ŒTextArenaæ­£å¥½å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚è¯¥å¹³å°å¼ºè°ƒæ˜“äºæ·»åŠ æ–°æ¸¸æˆã€é€‚åº”æ¡†æ¶ã€æµ‹è¯•æ¨¡å‹å’Œè®­ç»ƒæ¨¡å‹ï¼Œé€‚åˆç ”ç©¶å’Œç¤¾åŒºä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10766",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
            "url": "https://huggingface.co/papers/2504.10766",
            "abstract": "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.",
            "score": 12,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "b95bec819bad5307",
            "authors": [
                "Ming Li",
                "Yanhong Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Chicago",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10766.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº IFD, InsTag, Difficulty Ğ¸ Reward, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ ÑĞ´ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ."
                },
                "en": {
                    "title": "Unlocking the Secrets of Data Quality in LLM Fine-Tuning",
                    "desc": "This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks."
                },
                "zh": {
                    "title": "æ•°æ®è´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§çš„ç»Ÿä¸€è§†è§’",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åè®­ç»ƒé˜¶æ®µä¸­ï¼Œä¸åŒæ•°æ®å¯¹å¾®è°ƒåŠ¨æ€çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä½è´¨é‡å’Œé«˜è´¨é‡æŒ‡ä»¤åŠæ¨ç†æ•°æ®çš„å±‚çº§æ¢¯åº¦è¿›è¡Œè°±åˆ†æï¼Œå‘ç°å¸¸ç”¨çš„æ•°æ®è¯„ä¼°æŒ‡æ ‡å¯ä»¥é€šè¿‡æ¢¯åº¦çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è°±ç‰¹æ€§æ¥è§£é‡Šå’Œç»Ÿä¸€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡æ•°æ®é€šå¸¸ä¸è¾ƒä½çš„æ ¸èŒƒæ•°å’Œè¾ƒé«˜çš„æœ‰æ•ˆç§©ç›¸å…³ï¼Œä¸”æœ‰æ•ˆç§©åœ¨æ•æ‰ç»†å¾®è´¨é‡å·®å¼‚æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§å’Œåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼ŒåŒä¸€å®¶æ—çš„æ¨¡å‹åœ¨æ¢¯åº¦æ¨¡å¼ä¸Šç›¸ä¼¼ï¼Œè€Œä¸åŒæ¨¡å‹å®¶æ—ä¹‹é—´åˆ™å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11346",
            "title": "Seedream 3.0 Technical Report",
            "url": "https://huggingface.co/papers/2504.11346",
            "abstract": "We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.",
            "score": 6,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "3a4b797a3a9516d2",
            "authors": [
                "Yu Gao",
                "Lixue Gong",
                "Qiushan Guo",
                "Xiaoxia Hou",
                "Zhichao Lai",
                "Fanshi Li",
                "Liang Li",
                "Xiaochen Lian",
                "Chao Liao",
                "Liyang Liu",
                "Wei Liu",
                "Yichun Shi",
                "Shiqi Sun",
                "Yu Tian",
                "Zhi Tian",
                "Peng Wang",
                "Rui Wang",
                "Xuanda Wang",
                "Xun Wang",
                "Ye Wang",
                "Guofeng Wu",
                "Jie Wu",
                "Xin Xia",
                "Xuefeng Xiao",
                "Zhonghua Zhai",
                "Xinyu Zhang",
                "Qi Zhang",
                "Yuwei Zhang",
                "Shijia Zhao",
                "Jianchao Yang",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.11346.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Seedream 3.0 Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ",
                    "desc": "Seedream 3.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM. Seedream 3.0 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² 4-8 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Bilingual Image Generation with Seedream 3.0",
                    "desc": "Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications."
                },
                "zh": {
                    "title": "Seedream 3.0ï¼šé«˜æ•ˆçš„ä¸­è‹±æ–‡å›¾åƒç”Ÿæˆæ–°çºªå…ƒ",
                    "desc": "Seedream 3.0 æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ä¸­è‹±æ–‡åŒè¯­å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡å¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼Œè§£å†³äº† Seedream 2.0 ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚æç¤ºçš„å¯¹é½ã€ç»†è‡´çš„æ’ç‰ˆç”Ÿæˆå’Œå›¾åƒåˆ†è¾¨ç‡é™åˆ¶ã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®æ„å»ºå’Œæ¨¡å‹éƒ¨ç½²çš„æ•´ä¸ªæµç¨‹ä¸­è¿›è¡Œäº†æ”¹è¿›ï¼Œé‡‡ç”¨äº†ç¼ºé™·æ„ŸçŸ¥è®­ç»ƒå’ŒåŒè½´åä½œæ•°æ®é‡‡æ ·ç­‰æ–¹æ³•ã€‚Seedream 3.0 è¿˜å¼•å…¥äº†æ–°çš„åŠ é€ŸèŒƒå¼ï¼Œå®ç°äº†å›¾åƒè´¨é‡ä¸ç”Ÿæˆé€Ÿåº¦çš„æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ±‰å­—çš„æ–‡æœ¬æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10559",
            "title": "Efficient Process Reward Model Training via Active Learning",
            "url": "https://huggingface.co/papers/2504.10559",
            "abstract": "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.",
            "score": 6,
            "issue_id": 3259,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "f718e5da41cde633",
            "authors": [
                "Keyu Duan",
                "Zichen Liu",
                "Xin Mao",
                "Tianyu Pang",
                "Changyu Chen",
                "Qiguang Chen",
                "Michael Qizhe Shieh",
                "Longxu Dou"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab",
                "Singapore Management University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10559.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ActPRM - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (PRM). ActPRM Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PRM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ActPRM ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training",
                    "desc": "This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks."
                },
                "zh": {
                    "title": "ä¸»åŠ¨å­¦ä¹ æå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ActPRMï¼Œç”¨äºæé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼ŒActPRMæ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œä»…ä¿ç•™é«˜åº¦ä¸ç¡®å®šçš„æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActPRMåœ¨å‡å°‘50%æ ‡æ³¨çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11427",
            "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.11427",
            "abstract": "Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.",
            "score": 3,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "f1c298d14d78b468",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#cv",
                    "#long_context",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "NormalCrafter: Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NormalCrafter - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (SFR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹."
                },
                "en": {
                    "title": "NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights",
                    "desc": "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."
                },
                "zh": {
                    "title": "è§†é¢‘æ³•çº¿ä¼°è®¡çš„æ–°æ–¹æ³•ï¼šNormalCrafter",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•NormalCrafterï¼Œç”¨äºè§†é¢‘ä¸­çš„æ³•çº¿ä¼°è®¡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å…ˆéªŒï¼Œç¡®ä¿æ³•çº¿ä¼°è®¡åœ¨æ—¶é—´ä¸Šçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰ç‰¹å¾æ­£åˆ™åŒ–ï¼ˆSFRï¼‰ï¼Œé€šè¿‡å¯¹é½æ‰©æ•£ç‰¹å¾å’Œè¯­ä¹‰çº¿ç´¢ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨åœºæ™¯çš„å†…åœ¨è¯­ä¹‰ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒåè®®ï¼Œæˆ‘ä»¬åœ¨ä¿æŒç©ºé—´ç²¾åº¦çš„åŒæ—¶ï¼Œå¢å¼ºäº†å¯¹é•¿æ—¶é—´ä¸Šä¸‹æ–‡çš„å­¦ä¹ ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç»†èŠ‚ä¸°å¯Œä¸”æ—¶é—´ä¸€è‡´çš„æ³•çº¿åºåˆ—æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10465",
            "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
            "url": "https://huggingface.co/papers/2504.10465",
            "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.",
            "score": 3,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "6c90d31f3f941694",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pixel-SAIL - ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰ĞµĞ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PerBench Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Simplifying Multimodal Learning with Pixel-SAIL",
                    "desc": "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."
                },
                "zh": {
                    "title": "ç®€åŒ–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼šPixel-SAIL",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»†ç²’åº¦åƒç´ çº§ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä¾èµ–äºé¢å¤–çš„ç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨å’Œåˆ†å‰²ä¸“å®¶ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§é«˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„MLLMï¼Œåä¸ºPixel-SAILï¼Œæ—¨åœ¨ä¸å¼•å…¥é¢å¤–ç»„ä»¶çš„æƒ…å†µä¸‹è¿›è¡Œåƒç´ çº§ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—ã€è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥å’Œè§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥ï¼Œæå‡äº†å•ä¸€å˜æ¢å™¨çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixel-SAILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰æ›´ç®€å•çš„å¤„ç†æµç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11447",
            "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2504.11447",
            "abstract": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.",
            "score": 2,
            "issue_id": 3258,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "cda8d39111df28d8",
            "authors": [
                "An Zhaol",
                "Shengyuan Zhang",
                "Ling Yang",
                "Zejian Li",
                "Jiale Wu",
                "Haoran Xu",
                "AnYang Wei",
                "Perry Pengyun GU Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11447.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#rlhf",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Distillation-DPO",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Distillation-DPO Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (DPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ†ĞµĞ½ LiDAR Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Distillation-DPO Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with Distillation-DPO",
                    "desc": "This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications."
                },
                "zh": {
                    "title": "æå‡LiDARåœºæ™¯è¡¥å…¨é€Ÿåº¦ä¸è´¨é‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºDistillation-DPOï¼Œç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åå¥½å¯¹é½æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œé¦–å…ˆç”Ÿæˆä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹è¡¥å…¨åœºæ™¯ã€‚ç„¶åï¼Œåˆ©ç”¨LiDARåœºæ™¯è¯„ä¼°æŒ‡æ ‡æ„å»ºèƒœè´Ÿæ ·æœ¬å¯¹ï¼Œä»¥æ­¤æ¥ä¼˜åŒ–æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDistillation-DPOåœ¨åœºæ™¯è¡¥å…¨è´¨é‡å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œè¡¥å…¨é€Ÿåº¦æé«˜äº†5å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11343",
            "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
            "url": "https://huggingface.co/papers/2504.11343",
            "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.",
            "score": 2,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "72714d765a5a497f",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² RAFT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº GRPO. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Reinforce-Rej, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ RAFT ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Simplifying Reinforcement Learning for Better Language Model Training",
                    "desc": "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’ï¼šæ‹’ç»é‡‡æ ·çš„åŠ›é‡",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†GRPOç®—æ³•ï¼Œå‘ç°ä¸€ä¸ªç®€å•çš„æ‹’ç»é‡‡æ ·åŸºçº¿RAFTåœ¨è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³ä¸GRPOå’ŒPPOç›¸å½“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGRPOçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºä¸¢å¼ƒå®Œå…¨é”™è¯¯çš„æç¤ºï¼Œè€Œä¸æ˜¯å…¶å¥–åŠ±å½’ä¸€åŒ–ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Reinforce-Rejï¼Œè¿™æ˜¯ä¸€ç§è¿‡æ»¤å®Œå…¨é”™è¯¯å’Œå®Œå…¨æ­£ç¡®æ ·æœ¬çš„ç­–ç•¥æ¢¯åº¦æ‰©å±•ï¼Œèƒ½å¤Ÿæé«˜KLæ•ˆç‡å’Œç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10462",
            "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
            "url": "https://huggingface.co/papers/2504.10462",
            "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.",
            "score": 2,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "1f70a22447fd1fc5",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agi",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SAIL: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "SAIL - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, SAIL Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ vision transformer, Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. SAIL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SAIL: A Unified Approach to Multimodal Learning",
                    "desc": "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."
                },
                "zh": {
                    "title": "SAILï¼šç®€çº¦æ¶æ„ä¸‹çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SAILï¼Œè¿™æ˜¯ä¸€ç§å•ä¸€å˜æ¢å™¨ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªæ¶æ„ä¸­æ•´åˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç ã€‚ä¸ç°æœ‰çš„æ¨¡å—åŒ–MLLMä¸åŒï¼ŒSAILä¸éœ€è¦å•ç‹¬çš„è§†è§‰ç¼–ç å™¨ï¼Œå‘ˆç°å‡ºæ›´ç®€çº¦çš„æ¶æ„è®¾è®¡ã€‚SAILé‡‡ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼ŒSAILåœ¨æ€§èƒ½ä¸Šä¸æ¨¡å—åŒ–MLLMç›¸å½“ï¼ŒåŒæ—¶åœ¨è§†è§‰è¡¨ç¤ºèƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10188",
            "title": "Efficient Generative Model Training via Embedded Representation Warmup",
            "url": "https://huggingface.co/papers/2504.10188",
            "abstract": "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.",
            "score": 2,
            "issue_id": 3261,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "280d2a5386c25fa2",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Embedded Representation Warmup (ERW). ERW Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ERW ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 40 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with Embedded Representation Warmup",
                    "desc": "This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods."
                },
                "zh": {
                    "title": "åŠ é€Ÿæ‰©æ•£æ¨¡å‹è®­ç»ƒçš„åµŒå…¥è¡¨ç¤ºé¢„çƒ­",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ç»´æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®­ç»ƒæ•ˆç‡å’Œè¡¨ç¤ºè´¨é‡ä¸Šä¸å¦‚è‡ªç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªå……åˆ†åˆ©ç”¨é«˜è´¨é‡ã€è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åµŒå…¥è¡¨ç¤ºé¢„çƒ­ï¼ˆERWï¼‰æ¡†æ¶ï¼Œé€šè¿‡åœ¨åˆå§‹é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„é«˜è´¨é‡è¡¨ç¤ºæ¥åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹çš„æ—©æœŸå±‚ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼ŒERWçš„æœ‰æ•ˆæ€§ä¾èµ–äºå…¶åœ¨ç‰¹å®šç¥ç»ç½‘ç»œå±‚çš„ç²¾ç¡®é›†æˆï¼Œè¿™äº›å±‚ä¸»è¦å¤„ç†å’Œè½¬æ¢ç‰¹å¾è¡¨ç¤ºä»¥ä¾¿åç»­ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06949",
            "title": "Adaptive Computation Pruning for the Forgetting Transformer",
            "url": "https://huggingface.co/papers/2504.06949",
            "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.",
            "score": 2,
            "issue_id": 3259,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "bda352daa194f6f8",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ¾ Ğ½Ğµ Ñ…ÑƒĞ¶Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Adaptive Computation Pruning (ACP) - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Forgetting Transformer (FoX). ACP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ° 70% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 10-35%, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Boosting Efficiency with Adaptive Computation Pruning in FoX",
                    "desc": "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è®¡ç®—å‰ªææå‡FoXæ•ˆç‡",
                    "desc": "æœ€è¿‘æå‡ºçš„é—å¿˜å˜æ¢å™¨ï¼ˆFoXï¼‰åœ¨è½¯æœ€å¤§æ³¨æ„åŠ›ä¸­å¼•å…¥äº†é—å¿˜é—¨ï¼Œä¸æ ‡å‡†çš„RoPEå˜æ¢å™¨ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚FoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å¿«é€Ÿé—å¿˜ï¼Œä½¿å¾—å®ƒä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸»è¦ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€å‰ªé™¤ä¸è¾“å…¥è¾“å‡ºä¾èµ–å…³ç³»å¼ºçƒˆè¡°å‡çš„è®¡ç®—ã€‚é€šè¿‡åœ¨FoXçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­åº”ç”¨ACPï¼Œæˆ‘ä»¬å®ç°äº†çº¦70%çš„è®¡ç®—é‡å‡å°‘ï¼ŒåŒæ—¶è®­ç»ƒååé‡æé«˜äº†10%åˆ°35%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11326",
            "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
            "url": "https://huggingface.co/papers/2504.11326",
            "abstract": "This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.",
            "score": 0,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "abbfe1ceb2f9688a",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 4-Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ° PVUW Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… CVPR 2025. ĞšĞ¾Ğ½ĞºÑƒÑ€Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ´Ğ²Ğ° Ñ‚Ñ€ĞµĞºĞ°: MOSE Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ MeViS Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸ÑÑ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Advancing Video Segmentation: Insights from the PVUW Challenge",
                    "desc": "This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤æ‚è§†é¢‘åˆ†å‰²çš„å‰æ²¿æŒ‘æˆ˜",
                    "desc": "æœ¬æŠ¥å‘Šå…¨é¢æ¦‚è¿°äº†2025å¹´CVPRä¼šè®®æœŸé—´ä¸¾è¡Œçš„ç¬¬å››å±Šåƒç´ çº§è§†é¢‘ç†è§£æŒ‘æˆ˜èµ›ï¼ˆPVUWï¼‰ã€‚æŒ‘æˆ˜èµ›åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼šMOSEä¸“æ³¨äºå¤æ‚åœºæ™¯çš„è§†é¢‘ç‰©ä½“åˆ†å‰²ï¼Œè€ŒMeViSåˆ™é’ˆå¯¹åŸºäºè¿åŠ¨å¼•å¯¼å’Œè¯­è¨€çš„è§†é¢‘åˆ†å‰²ã€‚ä¸¤ä¸ªèµ›é“éƒ½å¼•å…¥äº†æ–°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°åæ˜ ç°å®ä¸–ç•Œçš„åœºæ™¯ã€‚é€šè¿‡è¯¦ç»†çš„è¯„ä¼°å’Œåˆ†æï¼Œè¯¥æŒ‘æˆ˜èµ›ä¸ºå¤æ‚è§†é¢‘åˆ†å‰²çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€å’Œæ–°å…´è¶‹åŠ¿æä¾›äº†å®è´µçš„è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-15.html",
    "link_next": "2025-04-17.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "15.04",
        "en": "04/15",
        "zh": "4æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
        "pinyin": "WÇ’men jiÃ¨shÃ o le InternVL3, zhÃ¨ shÃ¬ InternVL xÃ¬liÃ¨ de zhÃ²ngdÃ  jÃ¬nbÃ¹, cÇiyÃ²ng le bÄ›ndÃ¬ duÅmÃ³shÄ« yÃ¹xÃ¹nliÃ n fÃ nshÃ¬. YÇ” qÃ­tÄ fÄngfÇ bÃ¹tÃ³ng, InternVL3 zÃ i dÄn gÃ¨ yÃ¹xÃ¹nliÃ n jiÄ“duÃ n nÃ¨i, cÃ³ng duÅmÃ³shÄ« shÃ¹jÃ¹ hÃ© chÃºn wÃ©nbÄ›n shÃ¹jÃ¹ zhÅng tÃ³ngshÃ­ huÃ²dÃ© duÅmÃ³shÄ« hÃ© yÇ”yÃ¡n nÃ©nglÃ¬. ZhÃ¨ zhÇ’ng tÇ’ngyÄ« de xÃ¹nliÃ n fÃ nshÃ¬ yÇ’uxiÃ o jiÄ›juÃ© le chuÃ¡ntÇ’ng xÃ¹nliÃ n liÃºchÃ©ng zhÅng de fÃ¹zÃ¡xÃ¬ng hÃ© duÃ¬qÇ tiÇozhÃ n. WÃ¨ile jÃ¬nfÅ« tÄ«gÄo xÃ­ngnÃ©ng hÃ© kÄ›kuÃ²zhÇn xÃ¬ng, InternVL3 shÇyÃ²ng le kÄ›biÃ n shÃ¬juÃ© wÃ¨izhÃ¬ biÄnmÇ hÃ© xiÄnjÃ¬n de hÃ²uxÃ¹nliÃ n jÃ¬shÃ¹. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, InternVL3 zÃ i duÅzhÇ’ng duÅmÃ³shÄ« rÃ¨nwÃ¹ zhÅng biÇoxiÃ n yÅuyÃ¹, tÃ¨biÃ© shÃ¬ InternVL3-78B zÃ i MMMU jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng huÃ²dÃ© le 72.2 de fÄ“nshÃ¹, chuÃ ng xiÃ  xÄ«n jÃ¬lÃ¹. WÇ’men jiÄng gÅngkÄi fÄbÃ¹ xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© mÃ³xÃ­ng quÃ¡nzhÃ²ng, yÇ cÃ¹jÃ¬n xiÃ  yÄ«dÃ i MLLMs de yÃ¡njiÅ« hÃ© kÄifÄ.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'},\n{'word': 'é‡å¤§', 'pinyin': 'zhÃ²ng dÃ ', 'trans': 'major'},\n{'word': 'è¿›æ­¥', 'pinyin': 'jÃ¬n bÃ¹', 'trans': 'progress'},\n{'word': 'é‡‡ç”¨', 'pinyin': 'cÇi yÃ²ng', 'trans': 'adopt'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'},\n{'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'},\n{'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},\n{'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'},\n{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},\n{'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'å¯å˜', 'pinyin': 'kÄ› biÃ n', 'trans': 'variable'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'ä½ç½®', 'pinyin': 'wÃ¨i zhÃ¬', 'trans': 'position'},\n{'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encoding'},\n{'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'},\n{'word': 'åè®­ç»ƒ', 'pinyin': 'hÃ²u xÃ¹n liÃ n', 'trans': 'post-training'},\n{'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},\n{'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨ biÃ©', 'trans': 'especially'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'},\n{'word': 'åˆ†æ•°', 'pinyin': 'fÄ“n shÃ¹', 'trans': 'score'},\n{'word': 'çºªå½•', 'pinyin': 'jÃ¬ lÃ¹', 'trans': 'record'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},\n{'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'},\n{'word': 'æƒé‡', 'pinyin': 'quÃ¡n zhÃ²ng', 'trans': 'weights'},\n{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'},\n{'word': 'ä¸‹ä¸€ä»£', 'pinyin': 'xiÃ  yÄ« dÃ i', 'trans': 'next generation'},\n{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'},\n{'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'development'}]",
        "trans": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.",
        "update_ts": "2025-04-15 09:12"
    }
}