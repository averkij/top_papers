{
    "date": {
        "ru": "16 апреля",
        "en": "April 16",
        "zh": "4月16日"
    },
    "time_utc": "2025-04-16 05:11",
    "weekday": 2,
    "issue_id": 3261,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10481",
            "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
            "url": "https://huggingface.co/papers/2504.10481",
            "abstract": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.",
            "score": 18,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "72678fc1ff453072",
            "authors": [
                "Ding Chen",
                "Qingchen Yu",
                "Pengyuan Wang",
                "Wentao Zhang",
                "Bo Tang",
                "Feiyu Xiong",
                "Xinchi Li",
                "Minchuan Yang",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Center for Data Science, Peking University",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Research Institute of China Telecom, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10481.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "xVerify: точная верификация ответов моделей рассуждения",
                    "desc": "Статья представляет xVerify - эффективный верификатор ответов для оценки моделей рассуждения. xVerify способен определять эквивалентность ответов, генерируемых моделями рассуждения, эталонным ответам для различных типов объективных вопросов. Для обучения и оценки xVerify был создан набор данных VAR, содержащий пары вопросов-ответов от нескольких языковых моделей. Эксперименты показали, что модели xVerify достигают F1-меры и точности выше 95% на тестовом и обобщающем наборах данных."
                },
                "en": {
                    "title": "xVerify: Elevating Reasoning Model Evaluation with Precision",
                    "desc": "This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods."
                },
                "zh": {
                    "title": "xVerify：推理模型评估的新标准",
                    "desc": "随着OpenAI发布o1模型，采用慢思维策略的推理模型逐渐出现。这些模型生成的响应通常包含复杂的推理、中间步骤和自我反思，现有的评估方法往往无法有效判断LLM输出是否真正等同于参考答案。为了解决这个问题，我们提出了xVerify，一个高效的答案验证器，用于推理模型的评估。xVerify在等价判断方面表现出色，能够有效判断推理模型生成的答案是否与参考答案等价，并在多个客观问题类型中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10337",
            "title": "Heimdall: test-time scaling on the generative verification",
            "url": "https://huggingface.co/papers/2504.10337",
            "abstract": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.",
            "score": 16,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "6da5db970a101d21",
            "authors": [
                "Wenlei Shi",
                "Xing Jin"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10337.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#math"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Heimdall: ИИ-верификатор для повышения надежности рассуждений языковых моделей",
                    "desc": "Статья представляет Heimdall - модель верификации для длинных цепочек рассуждений (Chain-of-Thought). С помощью обучения с подкреплением точность верификации была повышена с 62.5% до 94.5% на сложных математических задачах. Предложен метод пессимистической верификации для масштабирования решения задач. Heimdall демонстрирует высокую способность к обобщению, обнаруживая ошибки даже в сложных математических доказательствах."
                },
                "en": {
                    "title": "Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning",
                    "desc": "This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies."
                },
                "zh": {
                    "title": "提升AI知识验证能力的Heimdall模型",
                    "desc": "本文提出了一种名为Heimdall的长链思维验证大语言模型（LLM），旨在提高解决竞争性数学问题的解答准确性。通过纯强化学习，Heimdall的验证准确率从62.5%提升至94.5%，并通过重复采样进一步提高至97.5%。此外，Heimdall还展示了出色的泛化能力，能够检测出训练中未包含的复杂数学证明中的大多数问题。我们还提出了悲观验证方法，以扩展Heimdall的功能，显著提高了解决方案的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11442",
            "title": "TextArena",
            "url": "https://huggingface.co/papers/2504.11442",
            "abstract": "TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
            "score": 12,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "7c9ae6533757828a",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "TextArena: Арена для оттачивания социального интеллекта языковых моделей",
                    "desc": "TextArena - это открытый набор соревновательных текстовых игр для обучения и оценки агентного поведения больших языковых моделей (LLM). Он включает более 57 уникальных сред, позволяющих оценивать возможности моделей через систему онлайн-игры с рейтингом TrueSkill в реальном времени. TextArena восполняет пробел в оценке динамических социальных навыков, таких как ведение переговоров, теория разума и обман. Платформа разработана с учетом исследовательских целей, возможностей сообщества и расширяемости, облегчая добавление новых игр и тестирование моделей."
                },
                "en": {
                    "title": "Train LLMs in Competitive Text Games!",
                    "desc": "TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes."
                },
                "zh": {
                    "title": "TextArena：提升语言模型的社交技能训练平台",
                    "desc": "TextArena是一个开源的竞争性文本游戏集合，旨在训练和评估大型语言模型（LLMs）的代理行为。它包含57个以上独特的环境，支持单人、双人和多人设置，并通过在线游戏系统轻松评估模型能力。传统基准测试通常无法评估动态社交技能，如谈判、心智理论和欺骗，而TextArena正好填补了这一空白。该平台强调易于添加新游戏、适应框架、测试模型和训练模型，适合研究和社区使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10766",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
            "url": "https://huggingface.co/papers/2504.10766",
            "abstract": "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.",
            "score": 12,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "b95bec819bad5307",
            "authors": [
                "Ming Li",
                "Yanhong Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Chicago",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10766.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Спектральный анализ раскрывает секреты качества данных в обучении языковых моделей",
                    "desc": "Эта статья представляет спектральный анализ послойных градиентов, вызванных данными разного качества при дообучении больших языковых моделей (LLM). Исследование показывает, что метрики оценки данных, такие как IFD, InsTag, Difficulty и Reward, можно объяснить и объединить с помощью спектральных свойств, вычисленных из сингулярного разложения градиентов. Авторы обнаружили, что данные более высокого качества обычно связаны с более низкими ядерными нормами и более высокими эффективными рангами. Эксперименты также показывают, что модели одного семейства имеют схожие паттерны градиентов независимо от их размера, в то время как разные семейства моделей значительно различаются."
                },
                "en": {
                    "title": "Unlocking the Secrets of Data Quality in LLM Fine-Tuning",
                    "desc": "This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks."
                },
                "zh": {
                    "title": "数据质量与训练稳定性的统一视角",
                    "desc": "本文探讨了大语言模型（LLM）在后训练阶段中，不同数据对微调动态的影响。我们通过对低质量和高质量指令及推理数据的层级梯度进行谱分析，发现常用的数据评估指标可以通过梯度的奇异值分解（SVD）谱特性来解释和统一。研究表明，高质量数据通常与较低的核范数和较高的有效秩相关，且有效秩在捕捉细微质量差异方面表现出更好的鲁棒性和分辨率。我们的实验还表明，同一家族的模型在梯度模式上相似，而不同模型家族之间则存在显著差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11346",
            "title": "Seedream 3.0 Technical Report",
            "url": "https://huggingface.co/papers/2504.11346",
            "abstract": "We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.",
            "score": 6,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "3a4b797a3a9516d2",
            "authors": [
                "Yu Gao",
                "Lixue Gong",
                "Qiushan Guo",
                "Xiaoxia Hou",
                "Zhichao Lai",
                "Fanshi Li",
                "Liang Li",
                "Xiaochen Lian",
                "Chao Liao",
                "Liyang Liu",
                "Wei Liu",
                "Yichun Shi",
                "Shiqi Sun",
                "Yu Tian",
                "Zhi Tian",
                "Peng Wang",
                "Rui Wang",
                "Xuanda Wang",
                "Xun Wang",
                "Ye Wang",
                "Guofeng Wu",
                "Jie Wu",
                "Xin Xia",
                "Xuefeng Xiao",
                "Zhonghua Zhai",
                "Xinyu Zhang",
                "Qi Zhang",
                "Yuwei Zhang",
                "Shijia Zhao",
                "Jianchao Yang",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.11346.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в генерации изображений: Seedream 3.0 поднимает планку",
                    "desc": "Seedream 3.0 - это двуязычная модель генерации изображений, улучшающая предыдущую версию. Модель использует новые техники обработки данных, включая обучение на смешанных разрешениях и выравнивание представлений. Применяются усовершенствованные методы постобработки, такие как эстетические подписи и модель вознаграждения на основе VLM. Seedream 3.0 также вводит новую парадигму ускорения, позволяющую увеличить скорость в 4-8 раз без потери качества."
                },
                "en": {
                    "title": "Revolutionizing Bilingual Image Generation with Seedream 3.0",
                    "desc": "Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications."
                },
                "zh": {
                    "title": "Seedream 3.0：高效的中英文图像生成新纪元",
                    "desc": "Seedream 3.0 是一个高性能的中英文双语图像生成基础模型。它通过多项技术改进，解决了 Seedream 2.0 中存在的挑战，如复杂提示的对齐、细致的排版生成和图像分辨率限制。该模型在数据构建和模型部署的整个流程中进行了改进，采用了缺陷感知训练和双轴协作数据采样等方法。Seedream 3.0 还引入了新的加速范式，实现了图像质量与生成速度的显著提升，特别是在复杂汉字的文本渲染方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10559",
            "title": "Efficient Process Reward Model Training via Active Learning",
            "url": "https://huggingface.co/papers/2504.10559",
            "abstract": "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.",
            "score": 6,
            "issue_id": 3259,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "f718e5da41cde633",
            "authors": [
                "Keyu Duan",
                "Zichen Liu",
                "Xin Mao",
                "Tianyu Pang",
                "Changyu Chen",
                "Qiguang Chen",
                "Michael Qizhe Shieh",
                "Longxu Dou"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab",
                "Singapore Management University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10559.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#math",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Активное обучение для эффективных моделей вознаграждения процессов",
                    "desc": "Статья представляет ActPRM - подход активного обучения для моделей вознаграждения процессов (PRM). ActPRM выбирает наиболее неопределенные образцы для обучения, что значительно снижает затраты на разметку данных. Метод использует PRM для оценки неопределенности после прямого прохода, сохраняя только высоко неопределенные данные. Эксперименты показывают, что ActPRM сокращает аннотацию на 50% при сохранении или улучшении производительности по сравнению с обычной тонкой настройкой."
                },
                "en": {
                    "title": "Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training",
                    "desc": "This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks."
                },
                "zh": {
                    "title": "主动学习提升模型训练效率",
                    "desc": "本文提出了一种主动学习方法ActPRM，用于提高大语言模型（LLMs）的训练效率。通过选择最不确定的样本进行训练，ActPRM显著降低了标注成本。训练过程中，使用过程奖励模型（PRM）来估计不确定性，仅保留高度不确定的数据进行标注。实验结果表明，ActPRM在减少50%标注的同时，性能与传统微调方法相当甚至更好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11427",
            "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.11427",
            "abstract": "Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.",
            "score": 3,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "f1c298d14d78b468",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#cv",
                    "#long_context",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "NormalCrafter: Временная согласованность нормалей в видео с помощью диффузионных моделей",
                    "desc": "Эта статья представляет NormalCrafter - новый метод для оценки поверхностных нормалей в видео с использованием диффузионных моделей. Авторы предлагают технику семантической регуляризации признаков (SFR) для улучшения согласованности оценок во времени. Также описывается двухэтапный протокол обучения, сочетающий обучение в латентном и пиксельном пространствах. Эксперименты показывают превосходство метода в генерации темпорально согласованных последовательностей нормалей с сохранением деталей."
                },
                "en": {
                    "title": "NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights",
                    "desc": "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."
                },
                "zh": {
                    "title": "视频法线估计的新方法：NormalCrafter",
                    "desc": "本论文提出了一种新的方法NormalCrafter，用于视频中的法线估计。与传统方法不同，我们利用视频扩散模型的时间先验，确保法线估计在时间上的一致性。我们引入了语义特征正则化（SFR），通过对齐扩散特征和语义线索，帮助模型关注场景的内在语义。通过两阶段的训练协议，我们在保持空间精度的同时，增强了对长时间上下文的学习，实验结果表明该方法在生成细节丰富且时间一致的法线序列方面表现优越。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10465",
            "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
            "url": "https://huggingface.co/papers/2504.10465",
            "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.",
            "score": 3,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "6c90d31f3f941694",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Единый трансформер для точного анализа изображений",
                    "desc": "Статья представляет Pixel-SAIL - единую трансформерную модель для задач мультимодального машинного обучения на уровне пикселей. Авторы предлагают три ключевых улучшения: модуль повышающей дискретизации, стратегию внедрения визуальных подсказок и дистилляцию экспертных знаний. Модель показывает сопоставимые или лучшие результаты по сравнению с более сложными системами на нескольких эталонных наборах данных. Исследователи также представляют новый набор данных PerBench для комплексной оценки понимания изображений на уровне пикселей."
                },
                "en": {
                    "title": "Simplifying Multimodal Learning with Pixel-SAIL",
                    "desc": "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."
                },
                "zh": {
                    "title": "简化的多模态语言模型：Pixel-SAIL",
                    "desc": "多模态大型语言模型（MLLMs）在细粒度像素级理解任务中表现出色，但大多数工作依赖于额外的组件，如视觉编码器和分割专家，导致系统复杂性高，限制了模型的扩展性。本文提出了一种简化的MLLM，名为Pixel-SAIL，旨在不引入额外组件的情况下进行像素级任务。我们通过设计可学习的上采样模块、视觉提示注入策略和视觉专家蒸馏策略，提升了单一变换器的特征提取能力。实验结果表明，Pixel-SAIL在多个基准测试中表现出色，且具有更简单的处理流程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11447",
            "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2504.11447",
            "abstract": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.",
            "score": 2,
            "issue_id": 3258,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "cda8d39111df28d8",
            "authors": [
                "An Zhaol",
                "Shengyuan Zhang",
                "Ling Yang",
                "Zejian Li",
                "Jiale Wu",
                "Haoran Xu",
                "AnYang Wei",
                "Perry Pengyun GU Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11447.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#rlhf",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Ускоренное и улучшенное заполнение сцен LiDAR с помощью Distillation-DPO",
                    "desc": "Эта статья представляет новый метод под названием Distillation-DPO для ускорения и улучшения качества заполнения сцен LiDAR с использованием диффузионных моделей. Метод сочетает дистилляцию знаний с оптимизацией прямой политики (DPO), используя метрики оценки сцен LiDAR в качестве предпочтений. Distillation-DPO оптимизирует модель ученика, используя разницу в функциях оценки между учителем и учеником на парных сценах завершения. Эксперименты показывают, что метод достигает более высокого качества заполнения сцены при ускорении процесса более чем в 5 раз по сравнению с современными моделями."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with Distillation-DPO",
                    "desc": "This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications."
                },
                "zh": {
                    "title": "提升LiDAR场景补全速度与质量的创新方法",
                    "desc": "本论文提出了一种新的扩散蒸馏框架，称为Distillation-DPO，用于3D LiDAR场景补全。该方法通过偏好对齐来优化学生模型，首先生成不同初始噪声的配对补全场景。然后，利用LiDAR场景评估指标构建胜负样本对，以此来优化模型。实验表明，Distillation-DPO在场景补全质量和速度上均优于现有的最先进模型，补全速度提高了5倍以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11343",
            "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
            "url": "https://huggingface.co/papers/2504.11343",
            "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.",
            "score": 2,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "72714d765a5a497f",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Простота и эффективность в обучении языковых моделей с подкреплением",
                    "desc": "Это исследование анализирует методы обучения с подкреплением (RL) для улучшения больших языковых моделей (LLM) в задачах рассуждения. Авторы обнаружили, что простой метод отбора положительных примеров RAFT показывает результаты, сравнимые с более сложными алгоритмами, такими как GRPO. На основе этого наблюдения предложен новый алгоритм Reinforce-Rej, который фильтрует как полностью неправильные, так и полностью правильные образцы. Исследование предлагает использовать RAFT как надежный базовый метод и рекомендует сосредоточиться на более обоснованном включении отрицательных примеров в будущих разработках."
                },
                "en": {
                    "title": "Simplifying Reinforcement Learning for Better Language Model Training",
                    "desc": "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."
                },
                "zh": {
                    "title": "强化学习的新视角：拒绝采样的力量",
                    "desc": "强化学习（RL）在复杂推理任务中对大型语言模型（LLM）的微调中变得越来越重要。本文重新审视了GRPO算法，发现一个简单的拒绝采样基线RAFT在训练中表现出色，甚至与GRPO和PPO相当。研究表明，GRPO的主要优势在于丢弃完全错误的提示，而不是其奖励归一化。基于这一发现，我们提出了Reinforce-Rej，这是一种过滤完全错误和完全正确样本的策略梯度扩展，能够提高KL效率和稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10462",
            "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
            "url": "https://huggingface.co/papers/2504.10462",
            "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.",
            "score": 2,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "1f70a22447fd1fc5",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agi",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAIL: единая архитектура для мультимодального машинного обучения",
                    "desc": "SAIL - это унифицированная мультимодальная большая языковая модель, объединяющая обработку изображений и текста в единой архитектуре трансформера. В отличие от модульных моделей, SAIL не использует предобученный vision transformer, а напрямую кодирует пиксели изображений. Модель адаптирует механизмы смешанного внимания и мультимодальные позиционные кодировки для лучшего согласования визуальной и текстовой модальностей. SAIL демонстрирует сопоставимую производительность с модульными моделями и сильные возможности визуального представления."
                },
                "en": {
                    "title": "SAIL: A Unified Approach to Multimodal Learning",
                    "desc": "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."
                },
                "zh": {
                    "title": "SAIL：简约架构下的多模态语言模型",
                    "desc": "本文介绍了SAIL，这是一种单一变换器统一多模态大语言模型（MLLM），它在一个架构中整合了原始像素编码和语言解码。与现有的模块化MLLM不同，SAIL不需要单独的视觉编码器，呈现出更简约的架构设计。SAIL采用混合注意力机制和多模态位置编码，以更好地适应视觉和文本模态的独特特征。通过扩大训练数据和模型规模，SAIL在性能上与模块化MLLM相当，同时在视觉表示能力上也表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10188",
            "title": "Efficient Generative Model Training via Embedded Representation Warmup",
            "url": "https://huggingface.co/papers/2504.10188",
            "abstract": "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.",
            "score": 2,
            "issue_id": 3261,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "280d2a5386c25fa2",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения диффузионных моделей с помощью предобученных представлений",
                    "desc": "Статья представляет новый метод обучения диффузионных моделей под названием Embedded Representation Warmup (ERW). ERW использует предобученные высококачественные представления для инициализации ранних слоев модели, что значительно ускоряет сходимость и улучшает качество генерации. Авторы идентифицируют ключевую область обработки представлений в нейронной сети, где происходит обучение семантическим и структурным паттернам. Теоретический и эмпирический анализ показывает, что ERW ускоряет обучение в 40 раз по сравнению с современными методами."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with Embedded Representation Warmup",
                    "desc": "This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods."
                },
                "zh": {
                    "title": "加速扩散模型训练的嵌入表示预热",
                    "desc": "扩散模型在生成高维数据方面表现出色，但在训练效率和表示质量上不如自监督方法。我们发现一个关键瓶颈：在训练过程中未充分利用高质量、语义丰富的表示，显著减缓了收敛速度。为了解决这个问题，我们提出了嵌入表示预热（ERW）框架，通过在初始阶段使用预训练的高质量表示来初始化扩散模型的早期层，从而加速收敛并提高性能。我们的理论分析表明，ERW的有效性依赖于其在特定神经网络层的精确集成，这些层主要处理和转换特征表示以便后续生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06949",
            "title": "Adaptive Computation Pruning for the Forgetting Transformer",
            "url": "https://huggingface.co/papers/2504.06949",
            "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.",
            "score": 2,
            "issue_id": 3259,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "bda352daa194f6f8",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умная обрезка вычислений: быстрее, но не хуже",
                    "desc": "Статья представляет Adaptive Computation Pruning (ACP) - метод динамической обрезки вычислений для модели Forgetting Transformer (FoX). ACP использует порог обрезки, чтобы исключить незначительные веса внимания, что позволяет сократить количество операций с плавающей запятой на 70% без потери производительности. Это приводит к увеличению скорости обучения на 10-35%, особенно для длинных контекстов. Анализ показывает эффективность метода для различных размеров моделей и длин контекста."
                },
                "en": {
                    "title": "Boosting Efficiency with Adaptive Computation Pruning in FoX",
                    "desc": "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."
                },
                "zh": {
                    "title": "自适应计算剪枝提升FoX效率",
                    "desc": "最近提出的遗忘变换器（FoX）在软最大注意力中引入了遗忘门，与标准的RoPE变换器相比，表现出更好的性能。FoX中的许多注意力头快速遗忘，使得它们在每个时间步的输出主要依赖于局部上下文。基于这一观察，我们提出了自适应计算剪枝（ACP）方法，动态剪除与输入输出依赖关系强烈衰减的计算。通过在FoX的语言模型预训练中应用ACP，我们实现了约70%的计算量减少，同时训练吞吐量提高了10%到35%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11326",
            "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
            "url": "https://huggingface.co/papers/2504.11326",
            "abstract": "This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.",
            "score": 0,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "abbfe1ceb2f9688a",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Прорыв в сегментации видео: новые горизонты понимания сложных сцен",
                    "desc": "Статья описывает результаты 4-го конкурса PVUW по пониманию видео на уровне пикселей, проведенного в рамках CVPR 2025. Конкурс включал два трека: MOSE для сегментации объектов в сложных сценах и MeViS для сегментации на основе движения и языка. Были представлены новые, более сложные наборы данных, лучше отражающие реальные сценарии. Анализ результатов дает ценную информацию о современном состоянии и тенденциях в области сложной сегментации видео."
                },
                "en": {
                    "title": "Advancing Video Segmentation: Insights from the PVUW Challenge",
                    "desc": "This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation."
                },
                "zh": {
                    "title": "推动复杂视频分割的前沿挑战",
                    "desc": "本报告全面概述了2025年CVPR会议期间举行的第四届像素级视频理解挑战赛（PVUW）。挑战赛包括两个赛道：MOSE专注于复杂场景的视频物体分割，而MeViS则针对基于运动引导和语言的视频分割。两个赛道都引入了新的、更具挑战性的数据集，以更好地反映现实世界的场景。通过详细的评估和分析，该挑战赛为复杂视频分割的最新技术状态和新兴趋势提供了宝贵的见解。"
                }
            }
        }
    ],
    "link_prev": "2025-04-15.html",
    "link_next": "2025-04-17.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "15.04",
        "en": "04/15",
        "zh": "4月15日"
    },
    "short_date_next": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4月17日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
        "pinyin": "Wǒmen jièshào le InternVL3, zhè shì InternVL xìliè de zhòngdà jìnbù, cǎiyòng le běndì duōmóshī yùxùnliàn fànshì. Yǔ qítā fāngfǎ bùtóng, InternVL3 zài dān gè yùxùnliàn jiēduàn nèi, cóng duōmóshī shùjù hé chún wénběn shùjù zhōng tóngshí huòdé duōmóshī hé yǔyán nénglì. Zhè zhǒng tǒngyī de xùnliàn fànshì yǒuxiào jiějué le chuántǒng xùnliàn liúchéng zhōng de fùzáxìng hé duìqǐ tiǎozhàn. Wèile jìnfū tīgāo xíngnéng hé kěkuòzhǎn xìng, InternVL3 shǐyòng le kěbiàn shìjué wèizhì biānmǎ hé xiānjìn de hòuxùnliàn jìshù. Shíyàn jiéguǒ xiǎnshì, InternVL3 zài duōzhǒng duōmóshī rènwù zhōng biǎoxiàn yōuyù, tèbié shì InternVL3-78B zài MMMU jīzhǔn cèshì zhōng huòdé le 72.2 de fēnshù, chuàng xià xīn jìlù. Wǒmen jiāng gōngkāi fābù xùnliàn shùjù hé móxíng quánzhòng, yǐ cùjìn xià yīdài MLLMs de yánjiū hé kāifā.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},\n{'word': '重大', 'pinyin': 'zhòng dà', 'trans': 'major'},\n{'word': '进步', 'pinyin': 'jìn bù', 'trans': 'progress'},\n{'word': '采用', 'pinyin': 'cǎi yòng', 'trans': 'adopt'},\n{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'},\n{'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'},\n{'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},\n{'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '可变', 'pinyin': 'kě biàn', 'trans': 'variable'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '位置', 'pinyin': 'wèi zhì', 'trans': 'position'},\n{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encoding'},\n{'word': '先进', 'pinyin': 'xiān jìn', 'trans': 'advanced'},\n{'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'},\n{'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'},\n{'word': '特别', 'pinyin': 'tè bié', 'trans': 'especially'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'},\n{'word': '分数', 'pinyin': 'fēn shù', 'trans': 'score'},\n{'word': '纪录', 'pinyin': 'jì lù', 'trans': 'record'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'},\n{'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'},\n{'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'},\n{'word': '下一代', 'pinyin': 'xià yī dài', 'trans': 'next generation'},\n{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'development'}]",
        "trans": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.",
        "update_ts": "2025-04-15 09:12"
    }
}