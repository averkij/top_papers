{
    "date": {
        "ru": "16 апреля",
        "en": "April 16",
        "zh": "4月16日"
    },
    "time_utc": "2025-04-16 22:10",
    "weekday": 2,
    "issue_id": 3278,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10481",
            "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
            "url": "https://huggingface.co/papers/2504.10481",
            "abstract": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.",
            "score": 62,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "72678fc1ff453072",
            "authors": [
                "Ding Chen",
                "Qingchen Yu",
                "Pengyuan Wang",
                "Wentao Zhang",
                "Bo Tang",
                "Feiyu Xiong",
                "Xinchi Li",
                "Minchuan Yang",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Center for Data Science, Peking University",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Research Institute of China Telecom, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10481.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "xVerify: точная верификация ответов моделей рассуждения",
                    "desc": "Статья представляет xVerify - эффективный верификатор ответов для оценки моделей рассуждения. xVerify способен определять эквивалентность ответов, генерируемых моделями рассуждения, эталонным ответам для различных типов объективных вопросов. Для обучения и оценки xVerify был создан набор данных VAR, содержащий пары вопросов-ответов от нескольких языковых моделей. Эксперименты показали, что модели xVerify достигают F1-меры и точности выше 95% на тестовом и обобщающем наборах данных."
                },
                "en": {
                    "title": "xVerify: Elevating Reasoning Model Evaluation with Precision",
                    "desc": "This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods."
                },
                "zh": {
                    "title": "xVerify：推理模型评估的新标准",
                    "desc": "随着OpenAI发布o1模型，采用慢思维策略的推理模型逐渐出现。这些模型生成的响应通常包含复杂的推理、中间步骤和自我反思，现有的评估方法往往无法有效判断LLM输出是否真正等同于参考答案。为了解决这个问题，我们提出了xVerify，一个高效的答案验证器，用于推理模型的评估。xVerify在等价判断方面表现出色，能够有效判断推理模型生成的答案是否与参考答案等价，并在多个客观问题类型中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08672",
            "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
            "url": "https://huggingface.co/papers/2504.08672",
            "abstract": "Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.",
            "score": 41,
            "issue_id": 3264,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 апреля",
                "en": "April 11",
                "zh": "4月11日"
            },
            "hash": "1de57b79eb1b4e84",
            "authors": [
                "Fangzhi Xu",
                "Hang Yan",
                "Chang Ma",
                "Haiteng Zhao",
                "Qiushi Sun",
                "Kanzhi Cheng",
                "Junxian He",
                "Jun Liu",
                "Zhiyong Wu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Peking University",
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08672.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосовершенствование ИИ: рассуждение без учителя",
                    "desc": "Эта статья представляет новый метод под названием Genius для улучшения навыков рассуждения языковых моделей без использования внешнего надзора. Метод основан на самообучении и использует стратегию пошагового предвидения для исследования потенциальных шагов рассуждения. Для решения проблемы шума и неопределенности в неконтролируемой среде авторы предлагают функцию потерь с калибровкой преимущества. Genius открывает новые возможности для масштабирования способностей языковых моделей к рассуждению, используя только общие запросы без дополнительной разметки."
                },
                "en": {
                    "title": "Genius: Unsupervised Self-Training for Enhanced LLM Reasoning",
                    "desc": "This paper presents Genius, a novel unsupervised self-training framework designed to enhance the reasoning skills of large language models (LLMs) without relying on external supervisory signals. Genius optimizes LLMs by seeking the best response sequences through a stepwise approach, utilizing a foresight re-sampling strategy to simulate and evaluate potential future outcomes. To address the challenges of noise and uncertainty in unsupervised settings, the authors introduce an advantage-calibrated optimization (ACO) loss function that improves the robustness of the optimization process. Overall, Genius aims to advance LLM reasoning capabilities efficiently, leveraging the abundance of general queries available."
                },
                "zh": {
                    "title": "无监督自我提升LLM推理能力的革命性进展",
                    "desc": "本文提出了一种名为Genius的无监督自我训练框架，旨在提升大型语言模型（LLM）的推理能力，而无需依赖外部监督信号。Genius通过逐步寻找最佳响应序列来优化LLM，并引入了一种逐步前瞻重采样策略，以模拟未来结果并评估步骤价值。为了应对无监督设置中固有的噪声和不确定性，本文还提出了一种优势校准优化（ACO）损失函数，以减轻估计不一致性。通过结合这些技术，Genius为无监督条件下的LLM推理自我提升提供了一个先进的初步步骤，推动了推理扩展法则的变革。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10337",
            "title": "Heimdall: test-time scaling on the generative verification",
            "url": "https://huggingface.co/papers/2504.10337",
            "abstract": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.",
            "score": 28,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "6da5db970a101d21",
            "authors": [
                "Wenlei Shi",
                "Xing Jin"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10337.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#math"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Heimdall: ИИ-верификатор для повышения надежности рассуждений языковых моделей",
                    "desc": "Статья представляет Heimdall - модель верификации для длинных цепочек рассуждений (Chain-of-Thought). С помощью обучения с подкреплением точность верификации была повышена с 62.5% до 94.5% на сложных математических задачах. Предложен метод пессимистической верификации для масштабирования решения задач. Heimdall демонстрирует высокую способность к обобщению, обнаруживая ошибки даже в сложных математических доказательствах."
                },
                "en": {
                    "title": "Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning",
                    "desc": "This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies."
                },
                "zh": {
                    "title": "提升AI知识验证能力的Heimdall模型",
                    "desc": "本文提出了一种名为Heimdall的长链思维验证大语言模型（LLM），旨在提高解决竞争性数学问题的解答准确性。通过纯强化学习，Heimdall的验证准确率从62.5%提升至94.5%，并通过重复采样进一步提高至97.5%。此外，Heimdall还展示了出色的泛化能力，能够检测出训练中未包含的复杂数学证明中的大多数问题。我们还提出了悲观验证方法，以扩展Heimdall的功能，显著提高了解决方案的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11346",
            "title": "Seedream 3.0 Technical Report",
            "url": "https://huggingface.co/papers/2504.11346",
            "abstract": "We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.",
            "score": 25,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "3a4b797a3a9516d2",
            "authors": [
                "Yu Gao",
                "Lixue Gong",
                "Qiushan Guo",
                "Xiaoxia Hou",
                "Zhichao Lai",
                "Fanshi Li",
                "Liang Li",
                "Xiaochen Lian",
                "Chao Liao",
                "Liyang Liu",
                "Wei Liu",
                "Yichun Shi",
                "Shiqi Sun",
                "Yu Tian",
                "Zhi Tian",
                "Peng Wang",
                "Rui Wang",
                "Xuanda Wang",
                "Xun Wang",
                "Ye Wang",
                "Guofeng Wu",
                "Jie Wu",
                "Xin Xia",
                "Xuefeng Xiao",
                "Zhonghua Zhai",
                "Xinyu Zhang",
                "Qi Zhang",
                "Yuwei Zhang",
                "Shijia Zhao",
                "Jianchao Yang",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.11346.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в генерации изображений: Seedream 3.0 поднимает планку",
                    "desc": "Seedream 3.0 - это двуязычная модель генерации изображений, улучшающая предыдущую версию. Модель использует новые техники обработки данных, включая обучение на смешанных разрешениях и выравнивание представлений. Применяются усовершенствованные методы постобработки, такие как эстетические подписи и модель вознаграждения на основе VLM. Seedream 3.0 также вводит новую парадигму ускорения, позволяющую увеличить скорость в 4-8 раз без потери качества."
                },
                "en": {
                    "title": "Revolutionizing Bilingual Image Generation with Seedream 3.0",
                    "desc": "Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications."
                },
                "zh": {
                    "title": "Seedream 3.0：高效的中英文图像生成新纪元",
                    "desc": "Seedream 3.0 是一个高性能的中英文双语图像生成基础模型。它通过多项技术改进，解决了 Seedream 2.0 中存在的挑战，如复杂提示的对齐、细致的排版生成和图像分辨率限制。该模型在数据构建和模型部署的整个流程中进行了改进，采用了缺陷感知训练和双轴协作数据采样等方法。Seedream 3.0 还引入了新的加速范式，实现了图像质量与生成速度的显著提升，特别是在复杂汉字的文本渲染方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10766",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
            "url": "https://huggingface.co/papers/2504.10766",
            "abstract": "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.",
            "score": 25,
            "issue_id": 3258,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "b95bec819bad5307",
            "authors": [
                "Ming Li",
                "Yanhong Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Chicago",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10766.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Спектральный анализ раскрывает секреты качества данных в обучении языковых моделей",
                    "desc": "Эта статья представляет спектральный анализ послойных градиентов, вызванных данными разного качества при дообучении больших языковых моделей (LLM). Исследование показывает, что метрики оценки данных, такие как IFD, InsTag, Difficulty и Reward, можно объяснить и объединить с помощью спектральных свойств, вычисленных из сингулярного разложения градиентов. Авторы обнаружили, что данные более высокого качества обычно связаны с более низкими ядерными нормами и более высокими эффективными рангами. Эксперименты также показывают, что модели одного семейства имеют схожие паттерны градиентов независимо от их размера, в то время как разные семейства моделей значительно различаются."
                },
                "en": {
                    "title": "Unlocking the Secrets of Data Quality in LLM Fine-Tuning",
                    "desc": "This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks."
                },
                "zh": {
                    "title": "数据质量与训练稳定性的统一视角",
                    "desc": "本文探讨了大语言模型（LLM）在后训练阶段中，不同数据对微调动态的影响。我们通过对低质量和高质量指令及推理数据的层级梯度进行谱分析，发现常用的数据评估指标可以通过梯度的奇异值分解（SVD）谱特性来解释和统一。研究表明，高质量数据通常与较低的核范数和较高的有效秩相关，且有效秩在捕捉细微质量差异方面表现出更好的鲁棒性和分辨率。我们的实验还表明，同一家族的模型在梯度模式上相似，而不同模型家族之间则存在显著差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10465",
            "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
            "url": "https://huggingface.co/papers/2504.10465",
            "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.",
            "score": 22,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "6c90d31f3f941694",
            "authors": [
                "Tao Zhang",
                "Xiangtai Li",
                "Zilong Huang",
                "Yanwei Li",
                "Weixian Lei",
                "Xueqing Deng",
                "Shihao Chen",
                "Shunping Ji",
                "Jiashi Feng"
            ],
            "affiliations": [
                "Bytedance Seed",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10465.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Единый трансформер для точного анализа изображений",
                    "desc": "Статья представляет Pixel-SAIL - единую трансформерную модель для задач мультимодального машинного обучения на уровне пикселей. Авторы предлагают три ключевых улучшения: модуль повышающей дискретизации, стратегию внедрения визуальных подсказок и дистилляцию экспертных знаний. Модель показывает сопоставимые или лучшие результаты по сравнению с более сложными системами на нескольких эталонных наборах данных. Исследователи также представляют новый набор данных PerBench для комплексной оценки понимания изображений на уровне пикселей."
                },
                "en": {
                    "title": "Simplifying Multimodal Learning with Pixel-SAIL",
                    "desc": "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."
                },
                "zh": {
                    "title": "简化的多模态语言模型：Pixel-SAIL",
                    "desc": "多模态大型语言模型（MLLMs）在细粒度像素级理解任务中表现出色，但大多数工作依赖于额外的组件，如视觉编码器和分割专家，导致系统复杂性高，限制了模型的扩展性。本文提出了一种简化的MLLM，名为Pixel-SAIL，旨在不引入额外组件的情况下进行像素级任务。我们通过设计可学习的上采样模块、视觉提示注入策略和视觉专家蒸馏策略，提升了单一变换器的特征提取能力。实验结果表明，Pixel-SAIL在多个基准测试中表现出色，且具有更简单的处理流程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11442",
            "title": "TextArena",
            "url": "https://huggingface.co/papers/2504.11442",
            "abstract": "TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
            "score": 20,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "7c9ae6533757828a",
            "authors": [
                "Leon Guertler",
                "Bobby Cheng",
                "Simon Yu",
                "Bo Liu",
                "Leshem Choshen",
                "Cheston Tan"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR",
                "MIT, MIT-IBM Watson AI Lab",
                "National University of Singapore",
                "Northeastern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11442.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "TextArena: Арена для оттачивания социального интеллекта языковых моделей",
                    "desc": "TextArena - это открытый набор соревновательных текстовых игр для обучения и оценки агентного поведения больших языковых моделей (LLM). Он включает более 57 уникальных сред, позволяющих оценивать возможности моделей через систему онлайн-игры с рейтингом TrueSkill в реальном времени. TextArena восполняет пробел в оценке динамических социальных навыков, таких как ведение переговоров, теория разума и обман. Платформа разработана с учетом исследовательских целей, возможностей сообщества и расширяемости, облегчая добавление новых игр и тестирование моделей."
                },
                "en": {
                    "title": "Train LLMs in Competitive Text Games!",
                    "desc": "TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes."
                },
                "zh": {
                    "title": "TextArena：提升语言模型的社交技能训练平台",
                    "desc": "TextArena是一个开源的竞争性文本游戏集合，旨在训练和评估大型语言模型（LLMs）的代理行为。它包含57个以上独特的环境，支持单人、双人和多人设置，并通过在线游戏系统轻松评估模型能力。传统基准测试通常无法评估动态社交技能，如谈判、心智理论和欺骗，而TextArena正好填补了这一空白。该平台强调易于添加新游戏、适应框架、测试模型和训练模型，适合研究和社区使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10462",
            "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
            "url": "https://huggingface.co/papers/2504.10462",
            "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.",
            "score": 12,
            "issue_id": 3260,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "1f70a22447fd1fc5",
            "authors": [
                "Weixian Lei",
                "Jiacong Wang",
                "Haochen Wang",
                "Xiangtai Li",
                "Jun Hao Liew",
                "Jiashi Feng",
                "Zilong Huang"
            ],
            "affiliations": [
                "Bytedance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10462.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agi",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAIL: единая архитектура для мультимодального машинного обучения",
                    "desc": "SAIL - это унифицированная мультимодальная большая языковая модель, объединяющая обработку изображений и текста в единой архитектуре трансформера. В отличие от модульных моделей, SAIL не использует предобученный vision transformer, а напрямую кодирует пиксели изображений. Модель адаптирует механизмы смешанного внимания и мультимодальные позиционные кодировки для лучшего согласования визуальной и текстовой модальностей. SAIL демонстрирует сопоставимую производительность с модульными моделями и сильные возможности визуального представления."
                },
                "en": {
                    "title": "SAIL: A Unified Approach to Multimodal Learning",
                    "desc": "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."
                },
                "zh": {
                    "title": "SAIL：简约架构下的多模态语言模型",
                    "desc": "本文介绍了SAIL，这是一种单一变换器统一多模态大语言模型（MLLM），它在一个架构中整合了原始像素编码和语言解码。与现有的模块化MLLM不同，SAIL不需要单独的视觉编码器，呈现出更简约的架构设计。SAIL采用混合注意力机制和多模态位置编码，以更好地适应视觉和文本模态的独特特征。通过扩大训练数据和模型规模，SAIL在性能上与模块化MLLM相当，同时在视觉表示能力上也表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10903",
            "title": "Efficient Reasoning Models: A Survey",
            "url": "https://huggingface.co/papers/2504.10903",
            "abstract": "Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this \"slow-thinking\" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.",
            "score": 10,
            "issue_id": 3269,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "8a7af29eb70394e2",
            "authors": [
                "Sicheng Feng",
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Nankai University, Tianjin, China",
                "National University of Singapore, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10903.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#reasoning",
                    "#survey",
                    "#optimization",
                    "#data",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация рассуждений: короче, меньше, быстрее",
                    "desc": "Эта статья представляет обзор последних достижений в области эффективных рассуждений для моделей машинного обучения. Авторы рассматривают три основных направления: сокращение цепочек рассуждений, разработка компактных языковых моделей и ускорение процесса вывода. В работе обсуждаются методы сжатия длинных цепочек рассуждений, техники уменьшения размера моделей и стратегии эффективного декодирования. Статья предоставляет комплексный анализ существующих подходов к оптимизации рассуждений в современных языковых моделях."
                },
                "en": {
                    "title": "Accelerating Reasoning: Shorter, Smaller, and Faster!",
                    "desc": "This paper discusses the advancements in reasoning models that generate Chain-of-Thoughts (CoTs) to solve complex tasks. However, the traditional approach can be slow and computationally expensive due to the lengthy sequences of tokens. The authors propose three strategies to enhance efficiency: creating shorter CoTs, developing smaller models with strong reasoning abilities, and implementing faster decoding methods. The survey also includes a collection of relevant research papers to support these advancements."
                },
                "zh": {
                    "title": "高效推理的未来：加速思维链",
                    "desc": "推理模型在解决复杂和逻辑密集型任务方面取得了显著进展，通过生成扩展的思维链（CoTs）来得出最终答案。然而，这种“慢思考”范式的出现，导致了大量序列生成的计算开销。为此，迫切需要有效的加速方法。本文综述了高效推理的最新进展，并将现有工作分为三个主要方向：压缩长思维链、开发紧凑的语言模型以及设计高效的解码策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10559",
            "title": "Efficient Process Reward Model Training via Active Learning",
            "url": "https://huggingface.co/papers/2504.10559",
            "abstract": "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.",
            "score": 10,
            "issue_id": 3259,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "f718e5da41cde633",
            "authors": [
                "Keyu Duan",
                "Zichen Liu",
                "Xin Mao",
                "Tianyu Pang",
                "Changyu Chen",
                "Qiguang Chen",
                "Michael Qizhe Shieh",
                "Longxu Dou"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab",
                "Singapore Management University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10559.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#math",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Активное обучение для эффективных моделей вознаграждения процессов",
                    "desc": "Статья представляет ActPRM - подход активного обучения для моделей вознаграждения процессов (PRM). ActPRM выбирает наиболее неопределенные образцы для обучения, что значительно снижает затраты на разметку данных. Метод использует PRM для оценки неопределенности после прямого прохода, сохраняя только высоко неопределенные данные. Эксперименты показывают, что ActPRM сокращает аннотацию на 50% при сохранении или улучшении производительности по сравнению с обычной тонкой настройкой."
                },
                "en": {
                    "title": "Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training",
                    "desc": "This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks."
                },
                "zh": {
                    "title": "主动学习提升模型训练效率",
                    "desc": "本文提出了一种主动学习方法ActPRM，用于提高大语言模型（LLMs）的训练效率。通过选择最不确定的样本进行训练，ActPRM显著降低了标注成本。训练过程中，使用过程奖励模型（PRM）来估计不确定性，仅保留高度不确定的数据进行标注。实验结果表明，ActPRM在减少50%标注的同时，性能与传统微调方法相当甚至更好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11427",
            "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.11427",
            "abstract": "Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.",
            "score": 8,
            "issue_id": 3259,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "f1c298d14d78b468",
            "authors": [
                "Yanrui Bin",
                "Wenbo Hu",
                "Haoyuan Wang",
                "Xinya Chen",
                "Bing Wang"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong",
                "Huazhong University of Science and Technology",
                "Spatial Intelligence Group, The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11427.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#long_context",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "NormalCrafter: Временная согласованность нормалей в видео с помощью диффузионных моделей",
                    "desc": "Эта статья представляет NormalCrafter - новый метод для оценки поверхностных нормалей в видео с использованием диффузионных моделей. Авторы предлагают технику семантической регуляризации признаков (SFR) для улучшения согласованности оценок во времени. Также описывается двухэтапный протокол обучения, сочетающий обучение в латентном и пиксельном пространствах. Эксперименты показывают превосходство метода в генерации темпорально согласованных последовательностей нормалей с сохранением деталей."
                },
                "en": {
                    "title": "NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights",
                    "desc": "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."
                },
                "zh": {
                    "title": "视频法线估计的新方法：NormalCrafter",
                    "desc": "本论文提出了一种新的方法NormalCrafter，用于视频中的法线估计。与传统方法不同，我们利用视频扩散模型的时间先验，确保法线估计在时间上的一致性。我们引入了语义特征正则化（SFR），通过对齐扩散特征和语义线索，帮助模型关注场景的内在语义。通过两阶段的训练协议，我们在保持空间精度的同时，增强了对长时间上下文的学习，实验结果表明该方法在生成细节丰富且时间一致的法线序列方面表现优越。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11001",
            "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
            "url": "https://huggingface.co/papers/2504.11001",
            "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.",
            "score": 7,
            "issue_id": 3263,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "9e93b5032c2a9d9c",
            "authors": [
                "Alan Dao",
                "Thinh Le"
            ],
            "affiliations": [
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11001.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#rag"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Настойчивость окупается: ReZero повышает эффективность поиска информации с помощью LLM",
                    "desc": "В статье представлен новый подход ReZero для улучшения поиска информации с помощью больших языковых моделей (LLM). Метод использует обучение с подкреплением, чтобы поощрять модель делать повторные попытки поиска после неудачного запроса. Это позволяет LLM исследовать альтернативные формулировки запросов, вместо того чтобы преждевременно останавливаться. ReZero показывает значительное улучшение точности с 25% до 46.88% по сравнению с базовым методом."
                },
                "en": {
                    "title": "Persistence Pays Off: Enhancing LLMs with ReZero",
                    "desc": "This paper presents ReZero, a new reinforcement learning framework designed to improve the performance of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). Unlike traditional methods that focus on refining search queries or analyzing results, ReZero encourages LLMs to persist and retry after an unsuccessful search. By rewarding the act of retrying, it promotes exploration of alternative queries, leading to better outcomes. The results show a significant accuracy increase, demonstrating that persistence can enhance LLM effectiveness in challenging knowledge-intensive tasks."
                },
                "zh": {
                    "title": "重试搜索，提升模型鲁棒性",
                    "desc": "Retrieval-Augmented Generation（RAG）通过增强大型语言模型（LLM）在知识密集型任务上的表现，但其效果依赖于初始搜索查询的质量。现有方法通常使用强化学习（RL），主要关注查询的制定或结果的推理，而没有明确鼓励在搜索失败后继续尝试。我们提出了ReZero（Retry-Zero），一种新的强化学习框架，直接奖励在初次搜索失败后重试查询的行为。这种方法显著提高了LLM的鲁棒性，在复杂的信息检索场景中表现出更好的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10342",
            "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
            "url": "https://huggingface.co/papers/2504.10342",
            "abstract": "Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with \"thinking\" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.",
            "score": 7,
            "issue_id": 3269,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "86516767518e49b1",
            "authors": [
                "Yueqi Song",
                "Tianyue Ou",
                "Yibo Kong",
                "Zecheng Li",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10342.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisualPuzzles: новый взгляд на оценку визуального мышления ИИ",
                    "desc": "В статье представлен новый бенчмарк VisualPuzzles для оценки визуального рассуждения мультимодальных моделей машинного обучения. В отличие от существующих тестов, VisualPuzzles минимизирует зависимость от специализированных знаний, фокусируясь на общих навыках рассуждения. Эксперименты показали, что современные мультимодальные языковые модели значительно отстают от людей в решении таких задач. Исследование также выявило, что улучшение вычислительных возможностей и увеличение размера модели не всегда приводят к повышению производительности в задачах рассуждения."
                },
                "en": {
                    "title": "VisualPuzzles: Evaluating Pure Reasoning in Multimodal AI",
                    "desc": "The paper introduces VisualPuzzles, a new benchmark designed to assess visual reasoning without relying heavily on specialized knowledge. It includes a variety of reasoning types such as algorithmic, analogical, deductive, inductive, and spatial reasoning. The authors demonstrate that existing multimodal large language models struggle with VisualPuzzles, highlighting that success in knowledge-heavy tasks does not guarantee performance in reasoning-focused scenarios. This benchmark aims to provide a clearer evaluation of reasoning abilities, emphasizing the importance of reasoning over mere factual recall."
                },
                "zh": {
                    "title": "VisualPuzzles：评估推理能力的新基准",
                    "desc": "当前的多模态基准测试常常将推理与特定领域知识混淆，这使得在非专业环境中评估一般推理能力变得困难。为了解决这个问题，我们引入了VisualPuzzles，这是一个专注于视觉推理的基准，故意减少对专业知识的依赖。VisualPuzzles包含五类多样化的问题：算法推理、类比推理、演绎推理、归纳推理和空间推理。实验表明，VisualPuzzles需要的领域特定知识显著减少，同时推理的复杂性更高，从而更好地评估真正的多模态推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10277",
            "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
            "url": "https://huggingface.co/papers/2504.10277",
            "abstract": "Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.",
            "score": 7,
            "issue_id": 3274,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "996eec7ad28b00d4",
            "authors": [
                "Pierre Le Jeune",
                "Jiaen Liu",
                "Luca Rossi",
                "Matteo Dora"
            ],
            "affiliations": [
                "Giskard AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10277.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#ethics",
                    "#data",
                    "#security"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "RealHarm: реальные угрозы ИИ-систем под микроскопом",
                    "desc": "Статья представляет датасет RealHarm, содержащий аннотированные проблемные взаимодействия с ИИ-агентами, основанный на анализе публично сообщенных инцидентов. Исследование фокусируется на рисках и опасностях с точки зрения компаний, внедряющих языковые модели. Авторы обнаружили, что репутационный ущерб является преобладающим организационным вредом, а дезинформация - наиболее распространенной категорией опасности. Эмпирическая оценка современных систем защиты и модерации контента выявила значительные пробелы в защите ИИ-приложений."
                },
                "en": {
                    "title": "Understanding Real-World Risks of AI Deployments",
                    "desc": "This paper addresses the risks associated with deploying machine learning models in consumer applications, focusing on real-world failures. It introduces RealHarm, a dataset that captures problematic interactions with AI agents based on a review of reported incidents. The study highlights that reputational damage is the main harm organizations face, while misinformation is the most frequent hazard. Additionally, it evaluates existing safety measures and finds that current guardrails and content moderation systems are insufficient to prevent these incidents."
                },
                "zh": {
                    "title": "揭示AI应用中的潜在风险与危害",
                    "desc": "本文探讨了消费者应用中语言模型部署所带来的风险。我们引入了RealHarm数据集，记录了与AI代理的有问题互动，基于对公开报告事件的系统性审查。研究发现，声誉损害是主要的组织性危害，而错误信息则是最常见的危险类别。通过评估现有的防护措施和内容审核系统，我们发现这些系统在保护AI应用方面存在显著的不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10188",
            "title": "Efficient Generative Model Training via Embedded Representation Warmup",
            "url": "https://huggingface.co/papers/2504.10188",
            "abstract": "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.",
            "score": 7,
            "issue_id": 3261,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "280d2a5386c25fa2",
            "authors": [
                "Deyuan Liu",
                "Peng Sun",
                "Xufeng Li",
                "Tao Lin"
            ],
            "affiliations": [
                "Nanjing University",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10188.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения диффузионных моделей с помощью предобученных представлений",
                    "desc": "Статья представляет новый метод обучения диффузионных моделей под названием Embedded Representation Warmup (ERW). ERW использует предобученные высококачественные представления для инициализации ранних слоев модели, что значительно ускоряет сходимость и улучшает качество генерации. Авторы идентифицируют ключевую область обработки представлений в нейронной сети, где происходит обучение семантическим и структурным паттернам. Теоретический и эмпирический анализ показывает, что ERW ускоряет обучение в 40 раз по сравнению с современными методами."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with Embedded Representation Warmup",
                    "desc": "This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods."
                },
                "zh": {
                    "title": "加速扩散模型训练的嵌入表示预热",
                    "desc": "扩散模型在生成高维数据方面表现出色，但在训练效率和表示质量上不如自监督方法。我们发现一个关键瓶颈：在训练过程中未充分利用高质量、语义丰富的表示，显著减缓了收敛速度。为了解决这个问题，我们提出了嵌入表示预热（ERW）框架，通过在初始阶段使用预训练的高质量表示来初始化扩散模型的早期层，从而加速收敛并提高性能。我们的理论分析表明，ERW的有效性依赖于其在特定神经网络层的精确集成，这些层主要处理和转换特征表示以便后续生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11456",
            "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
            "url": "https://huggingface.co/papers/2504.11456",
            "abstract": "The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath.",
            "score": 6,
            "issue_id": 3266,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "3af8b373358fa97c",
            "authors": [
                "Zhiwei He",
                "Tian Liang",
                "Jiahao Xu",
                "Qiuzhi Liu",
                "Xingyu Chen",
                "Yue Wang",
                "Linfeng Song",
                "Dian Yu",
                "Zhenwen Liang",
                "Wenxuan Wang",
                "Zhuosheng Zhang",
                "Rui Wang",
                "Zhaopeng Tu",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11456.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#math",
                    "#dataset",
                    "#data",
                    "#open_source",
                    "#rl"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "DeepMath-103K: Прорыв в обучении ИИ математическим рассуждениям",
                    "desc": "DeepMath-103K - это новый крупномасштабный набор данных, содержащий около 103 тысяч математических задач, разработанный для обучения моделей продвинутым рассуждениям с помощью обучения с подкреплением. Набор данных отличается высокой сложностью задач, охватывает широкий спектр математических тем и включает проверяемые окончательные ответы. Каждая задача содержит три различных решения, сгенерированных моделью R1, что позволяет использовать различные парадигмы обучения. Модели, обученные на DeepMath-103K, демонстрируют значительные улучшения на сложных математических тестах."
                },
                "en": {
                    "title": "Empowering AI with DeepMath-103K: A Leap in Mathematical Reasoning",
                    "desc": "This paper presents DeepMath-103K, a new dataset designed to enhance the training of AI models in complex mathematical reasoning. It addresses the challenges faced by reinforcement learning (RL) in large language models (LLMs) due to the lack of high-quality, difficult training data. The dataset includes around 103,000 carefully curated mathematical problems with verifiable answers, allowing for effective rule-based RL training. By providing diverse solutions and covering a wide range of topics, DeepMath-103K aims to improve the generalization capabilities of AI reasoning systems."
                },
                "zh": {
                    "title": "DeepMath-103K：推动AI数学推理的突破性数据集",
                    "desc": "本文介绍了DeepMath-103K，这是一个新的大规模数据集，包含约103,000个数学问题，旨在通过强化学习（RL）训练高级推理模型。该数据集经过严格的筛选和去污染处理，确保问题具有高难度，并且每个问题都提供可验证的最终答案，适合规则基础的RL。DeepMath-103K涵盖广泛的数学主题，促进了可推广推理的发展。实验表明，基于DeepMath-103K训练的模型在挑战性数学基准测试中取得了显著的改进，验证了其有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11393",
            "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
            "url": "https://huggingface.co/papers/2504.11393",
            "abstract": "Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.",
            "score": 6,
            "issue_id": 3275,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "c08bb6187070e5e2",
            "authors": [
                "Ian Magnusson",
                "Nguyen Tai",
                "Ben Bogin",
                "David Heineman",
                "Jena D. Hwang",
                "Luca Soldaini",
                "Akshita Bhagia",
                "Jiacheng Liu",
                "Dirk Groeneveld",
                "Oyvind Tafjord",
                "Noah A. Smith",
                "Pang Wei Koh",
                "Jesse Dodge"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11393.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективный выбор данных для обучения больших языковых моделей",
                    "desc": "Исследование посвящено оптимизации выбора данных для предобучения больших языковых моделей. Авторы представляют DataDecide - набор моделей, данных и оценок для изучения влияния различных наборов данных на качество моделей разного масштаба. Эксперименты показали, что ранжирование небольших моделей (150M параметров) хорошо предсказывает лучшие модели большего масштаба (1B параметров). Использование метрик правдоподобия в небольших экспериментах позволяет предсказать производительность на ряде бенчмарков с точностью >80% при использовании всего 0.01% вычислительных ресурсов."
                },
                "en": {
                    "title": "Optimize Dataset Selection for Large Language Models with DataDecide",
                    "desc": "This paper addresses the challenge of efficiently selecting datasets for pretraining large language models by utilizing smaller-scale experiments. It introduces DataDecide, a comprehensive suite that allows researchers to evaluate the impact of different datasets and model sizes on performance. The authors conduct extensive experiments across various corpora and find that performance rankings at smaller model sizes can effectively predict outcomes at larger scales. Additionally, they demonstrate that using continuous likelihood metrics can significantly enhance the predictability of benchmarks with minimal computational resources."
                },
                "zh": {
                    "title": "小规模实验助力大模型数据选择",
                    "desc": "本论文探讨了如何通过小规模实验来选择数据集，以降低大型语言模型的预训练成本。我们发布了DataDecide，这是一个开放的模型、数据和评估套件，旨在帮助研究者探索最佳数据集选择。通过在25个不同来源的语料库上进行控制预训练实验，我们发现小规模模型的排名可以有效预测大规模模型的表现。使用连续似然度指标作为小规模实验的代理，可以在目标规模下实现超过80%的可预测性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11343",
            "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
            "url": "https://huggingface.co/papers/2504.11343",
            "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.",
            "score": 6,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "72714d765a5a497f",
            "authors": [
                "Wei Xiong",
                "Jiarui Yao",
                "Yuhui Xu",
                "Bo Pang",
                "Lei Wang",
                "Doyen Sahoo",
                "Junnan Li",
                "Nan Jiang",
                "Tong Zhang",
                "Caiming Xiong",
                "Hanze Dong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11343.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Простота и эффективность в обучении языковых моделей с подкреплением",
                    "desc": "Это исследование анализирует методы обучения с подкреплением (RL) для улучшения больших языковых моделей (LLM) в задачах рассуждения. Авторы обнаружили, что простой метод отбора положительных примеров RAFT показывает результаты, сравнимые с более сложными алгоритмами, такими как GRPO. На основе этого наблюдения предложен новый алгоритм Reinforce-Rej, который фильтрует как полностью неправильные, так и полностью правильные образцы. Исследование предлагает использовать RAFT как надежный базовый метод и рекомендует сосредоточиться на более обоснованном включении отрицательных примеров в будущих разработках."
                },
                "en": {
                    "title": "Simplifying Reinforcement Learning for Better Language Model Training",
                    "desc": "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."
                },
                "zh": {
                    "title": "强化学习的新视角：拒绝采样的力量",
                    "desc": "强化学习（RL）在复杂推理任务中对大型语言模型（LLM）的微调中变得越来越重要。本文重新审视了GRPO算法，发现一个简单的拒绝采样基线RAFT在训练中表现出色，甚至与GRPO和PPO相当。研究表明，GRPO的主要优势在于丢弃完全错误的提示，而不是其奖励归一化。基于这一发现，我们提出了Reinforce-Rej，这是一种过滤完全错误和完全正确样本的策略梯度扩展，能够提高KL效率和稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11455",
            "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL",
            "url": "https://huggingface.co/papers/2504.11455",
            "abstract": "This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR.",
            "score": 4,
            "issue_id": 3269,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "cec2567a7d0af48a",
            "authors": [
                "Junke Wang",
                "Zhi Tian",
                "Xun Wang",
                "Xinyu Zhang",
                "Weilin Huang",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11455.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#open_source",
                    "#benchmark",
                    "#optimization",
                    "#small_models",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Простота и эффективность в генерации изображений",
                    "desc": "SimpleAR - это автореггрессивная модель для генерации изображений, использующая простую архитектуру без сложных модификаций. Несмотря на небольшой размер (0.5 млрд параметров), модель способна генерировать качественные изображения размером 1024x1024 и показывает конкурентоспособные результаты на сложных бенчмарках для преобразования текста в изображение. Авторы демонстрируют, что как супервизорная дообучение (SFT), так и оптимизация групповой относительной политики (GRPO) значительно улучшают эстетику генерации и соответствие промпту. С помощью техник ускорения вывода, таких как vLLM, время генерации изображения 1024x1024 сокращается до 14 секунд."
                },
                "en": {
                    "title": "Unlocking High-Quality Image Generation with SimpleAR",
                    "desc": "This paper introduces SimpleAR, a straightforward autoregressive framework for generating high-quality images without the need for complex architecture changes. The model, with only 0.5 billion parameters, can produce 1024x1024 resolution images and performs competitively on text-to-image benchmarks. It highlights the effectiveness of supervised fine-tuning and Group Relative Policy Optimization in enhancing image aesthetics and prompt alignment. Additionally, by employing inference acceleration techniques, the generation time for images is significantly reduced, showcasing the efficiency of the SimpleAR model."
                },
                "zh": {
                    "title": "SimpleAR：高效自回归视觉生成的潜力",
                    "desc": "本文介绍了一种名为SimpleAR的自回归视觉生成框架，该框架没有复杂的架构修改。我们通过对训练和推理优化的深入探索，展示了该模型在仅有5亿参数的情况下，能够生成高保真度的1024x1024分辨率图像，并在具有挑战性的文本到图像基准测试中取得竞争性结果。我们发现，监督微调（SFT）和组相对策略优化（GRPO）训练都能显著提高生成图像的美学和提示对齐效果。此外，使用推理加速技术如vLLM后，SimpleAR生成1024x1024图像的时间可缩短至约14秒。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11447",
            "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2504.11447",
            "abstract": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.",
            "score": 4,
            "issue_id": 3258,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "cda8d39111df28d8",
            "authors": [
                "An Zhaol",
                "Shengyuan Zhang",
                "Ling Yang",
                "Zejian Li",
                "Jiale Wu",
                "Haoran Xu",
                "AnYang Wei",
                "Perry Pengyun GU Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11447.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#rlhf",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Ускоренное и улучшенное заполнение сцен LiDAR с помощью Distillation-DPO",
                    "desc": "Эта статья представляет новый метод под названием Distillation-DPO для ускорения и улучшения качества заполнения сцен LiDAR с использованием диффузионных моделей. Метод сочетает дистилляцию знаний с оптимизацией прямой политики (DPO), используя метрики оценки сцен LiDAR в качестве предпочтений. Distillation-DPO оптимизирует модель ученика, используя разницу в функциях оценки между учителем и учеником на парных сценах завершения. Эксперименты показывают, что метод достигает более высокого качества заполнения сцены при ускорении процесса более чем в 5 раз по сравнению с современными моделями."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with Distillation-DPO",
                    "desc": "This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications."
                },
                "zh": {
                    "title": "提升LiDAR场景补全速度与质量的创新方法",
                    "desc": "本论文提出了一种新的扩散蒸馏框架，称为Distillation-DPO，用于3D LiDAR场景补全。该方法通过偏好对齐来优化学生模型，首先生成不同初始噪声的配对补全场景。然后，利用LiDAR场景评估指标构建胜负样本对，以此来优化模型。实验表明，Distillation-DPO在场景补全质量和速度上均优于现有的最先进模型，补全速度提高了5倍以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11326",
            "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
            "url": "https://huggingface.co/papers/2504.11326",
            "abstract": "This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.",
            "score": 4,
            "issue_id": 3260,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "abbfe1ceb2f9688a",
            "authors": [
                "Henghui Ding",
                "Chang Liu",
                "Nikhila Ravi",
                "Shuting He",
                "Yunchao Wei",
                "Song Bai",
                "Philip Torr",
                "Kehuan Song",
                "Xinglin Xie",
                "Kexin Zhang",
                "Licheng Jiao",
                "Lingling Li",
                "Shuyuan Yang",
                "Xuqiang Cao",
                "Linnan Zhao",
                "Jiaxuan Zhao",
                "Fang Liu",
                "Mengjiao Wang",
                "Junpei Zhang",
                "Xu Liu",
                "Yuting Yang",
                "Mengru Ma",
                "Hao Fang",
                "Runmin Cong",
                "Xiankai Lu",
                "Zhiyang Che",
                "Wei Zhan",
                "Tianming Liang",
                "Haichao Jiang",
                "Wei-Shi Zheng",
                "Jian-Fang Hu",
                "Haobo Yuan",
                "Xiangtai Li",
                "Tao Zhang",
                "Lu Qi",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "the Institute of Big Data, Fudan University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11326.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Прорыв в сегментации видео: новые горизонты понимания сложных сцен",
                    "desc": "Статья описывает результаты 4-го конкурса PVUW по пониманию видео на уровне пикселей, проведенного в рамках CVPR 2025. Конкурс включал два трека: MOSE для сегментации объектов в сложных сценах и MeViS для сегментации на основе движения и языка. Были представлены новые, более сложные наборы данных, лучше отражающие реальные сценарии. Анализ результатов дает ценную информацию о современном состоянии и тенденциях в области сложной сегментации видео."
                },
                "en": {
                    "title": "Advancing Video Segmentation: Insights from the PVUW Challenge",
                    "desc": "This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation."
                },
                "zh": {
                    "title": "推动复杂视频分割的前沿挑战",
                    "desc": "本报告全面概述了2025年CVPR会议期间举行的第四届像素级视频理解挑战赛（PVUW）。挑战赛包括两个赛道：MOSE专注于复杂场景的视频物体分割，而MeViS则针对基于运动引导和语言的视频分割。两个赛道都引入了新的、更具挑战性的数据集，以更好地反映现实世界的场景。通过详细的评估和分析，该挑战赛为复杂视频分割的最新技术状态和新兴趋势提供了宝贵的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08846",
            "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
            "url": "https://huggingface.co/papers/2504.08846",
            "abstract": "We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.",
            "score": 4,
            "issue_id": 3265,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 апреля",
                "en": "April 11",
                "zh": "4月11日"
            },
            "hash": "0d4f794fba95381f",
            "authors": [
                "Mostafa Faghih Shojaei",
                "Rahul Gulati",
                "Benjamin A. Jasperson",
                "Shangshang Wang",
                "Simone Cimolato",
                "Dangli Cao",
                "Willie Neiswanger",
                "Krishna Garikipati"
            ],
            "affiliations": [
                "Department of Aerospace and Mechanical Engineering, University of Southern California",
                "Department of Astronautical Engineering, University of Southern California",
                "Department of Computer Science, University of Southern California",
                "Department of Electrical and Computer Engineering, University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08846.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multimodal",
                    "#training",
                    "#open_source",
                    "#science",
                    "#rag"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "AI-U: персонализированное обучение с помощью адаптивных языковых моделей",
                    "desc": "Статья представляет AI University (AI-U) - гибкую систему для адаптивной подачи учебного контента с помощью искусственного интеллекта. В основе AI-U лежит большая языковая модель (LLM), дообученная с использованием метода retrieval-augmented generation (RAG) на основе видеолекций, заметок и учебников. Авторы разработали масштабируемый конвейер для систематического создания обучающих данных, дообучения open-source LLM с помощью Low-Rank Adaptation (LoRA) и оптимизации ответов через RAG-синтез. Оценка системы, включающая косинусное сходство, оценку на основе LLM и экспертный анализ, показала сильное соответствие учебным материалам."
                },
                "en": {
                    "title": "AI-U: Tailoring AI Learning to Teaching Styles",
                    "desc": "AI University (AI-U) is a framework designed to enhance AI-driven course content delivery by adapting to different teaching styles. It utilizes a large language model (LLM) that is fine-tuned with retrieval-augmented generation (RAG) to create responses that align closely with lecture materials. The framework was tested using a finite-element-method (FEM) course, demonstrating effective training data construction and optimization of AI responses. Evaluation results show that AI-U's model significantly outperforms the base model, indicating its potential for scalable AI-assisted education in higher learning environments."
                },
                "zh": {
                    "title": "AI大学：灵活的AI辅助教育框架",
                    "desc": "AI大学（AI-U）是一个灵活的框架，旨在根据教师的教学风格调整AI驱动的课程内容交付。它的核心是通过检索增强生成（RAG）技术对大型语言模型（LLM）进行微调，以生成与讲座视频、笔记和教科书一致的教师响应。以研究生级别的有限元方法（FEM）课程为案例，我们展示了一个可扩展的管道，系统地构建训练数据，并通过低秩适应（LoRA）微调开源LLM。我们的评估表明，AI-U在教育中提供了一种可扩展的方法，促进了高等教育中AI辅助学习的更广泛应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11409",
            "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
            "url": "https://huggingface.co/papers/2504.11409",
            "abstract": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.",
            "score": 3,
            "issue_id": 3275,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "0a29da0f494ed358",
            "authors": [
                "Ali Taghibakhshi",
                "Sharath Turuvekere Sreenivas",
                "Saurav Muralidharan",
                "Marcin Chochowski",
                "Yashaswi Karnati",
                "Raviraj Joshi",
                "Ameya Sunil Mahabaleshwarkar",
                "Zijia Chen",
                "Yoshi Suhara",
                "Oluwatobi Olabiyi",
                "Daniel Korzekwa",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Jan Kautz",
                "Bryan Catanzaro",
                "Ashwath Aithal",
                "Nima Tajbakhsh",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11409.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#small_models",
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Эффективное сжатие гибридных нейросетей без потери качества",
                    "desc": "В этой статье представлен новый подход к сжатию гибридных архитектур нейронных сетей, сочетающих механизмы внимания и модели пространства состояний (SSM). Авторы предлагают метод групповой обрезки, сохраняющий структурную целостность блоков SSM. Применяя комбинацию различных техник сжатия и дистилляции знаний, им удалось уменьшить модель Nemotron-H 8B до 4 миллиардов параметров, сохранив высокую точность. Полученная модель превосходит аналоги по точности и в 2 раза быстрее при выводе, что значительно улучшает соотношение производительности и размера модели."
                },
                "en": {
                    "title": "Efficient Hybrid Models: Pruning for Performance and Accuracy",
                    "desc": "This paper discusses a new method for improving Hybrid LLM architectures that use both Attention and State Space Models (SSMs). The authors propose a group-aware pruning strategy that maintains the effectiveness of SSMs while reducing the model size. They show that this pruning, combined with knowledge distillation, leads to smaller models that are not only more accurate but also faster in making predictions. The results indicate that their approach can significantly enhance model performance while using fewer training resources."
                },
                "zh": {
                    "title": "压缩混合模型，提升性能与效率",
                    "desc": "本研究探讨了混合大语言模型（LLM）架构的压缩效果，这些架构结合了注意力机制和状态空间模型（SSM）。我们提出了一种新颖的群体感知剪枝策略，能够保持SSM模块的结构完整性和序列建模能力。通过这种压缩方法，我们将Nemotron-H 8B混合模型压缩到4B参数，并减少了训练所需的标记数量。最终得到的模型在准确性和推理速度上均优于同等规模的模型，显著推动了模型性能的边界。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09454",
            "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
            "url": "https://huggingface.co/papers/2504.09454",
            "abstract": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D^2iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT.",
            "score": 3,
            "issue_id": 3265,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "4e07861591445707",
            "authors": [
                "Weinan Jia",
                "Mengqi Huang",
                "Nan Chen",
                "Lei Zhang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China",
                "University of Science and Technology of China, Hefei, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09454.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#open_source",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Динамическое сжатие и диффузия для улучшенной генерации изображений",
                    "desc": "Статья представляет новый двухэтапный подход к генерации изображений с использованием диффузионных моделей. Первый этап, Dynamic VAE, применяет иерархический энкодер для сжатия различных участков изображения с разной степенью в зависимости от их информационной плотности. Второй этап, Dynamic Diffusion Transformer (D^2iT), генерирует изображения, предсказывая шум разной зернистости для разных областей. Этот метод позволяет достичь баланса между глобальной согласованностью и локальным реализмом в генерируемых изображениях."
                },
                "en": {
                    "title": "Dynamic Compression for Enhanced Image Generation",
                    "desc": "This paper introduces a new approach to improve image generation using diffusion models by dynamically compressing different regions of an image based on their information density. The proposed two-stage framework consists of a Dynamic VAE that encodes image regions at varying downsampling rates, allowing for more accurate latent representations. The second stage, Dynamic Diffusion Transformer (D^2iT), generates images by predicting noise at multiple granularities, balancing the need for detail in complex areas and simplicity in smoother regions. This method enhances both local realism and global consistency in generated images, as demonstrated through extensive experiments."
                },
                "zh": {
                    "title": "动态压缩，提升图像生成质量",
                    "desc": "扩散模型因其生成高质量图像的能力而受到广泛认可。尽管扩散变换器（DiT）架构表现出色，但在扩散过程中对不同图像区域应用固定压缩，忽视了这些区域信息密度的自然变化。我们提出通过动态压缩不同图像区域来解决这一问题，采用两阶段框架来提高图像生成的有效性和效率。第一阶段的动态变分自编码器（DVAE）和第二阶段的动态扩散变换器（D^2iT）结合了粗细噪声预测，成功实现了全局一致性与局部真实感的统一。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06949",
            "title": "Adaptive Computation Pruning for the Forgetting Transformer",
            "url": "https://huggingface.co/papers/2504.06949",
            "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.",
            "score": 3,
            "issue_id": 3259,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "bda352daa194f6f8",
            "authors": [
                "Zhixuan Lin",
                "Johan Obando-Ceron",
                "Xu Owen He",
                "Aaron Courville"
            ],
            "affiliations": [
                "MakerMaker AI",
                "Mila & Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06949.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умная обрезка вычислений: быстрее, но не хуже",
                    "desc": "Статья представляет Adaptive Computation Pruning (ACP) - метод динамической обрезки вычислений для модели Forgetting Transformer (FoX). ACP использует порог обрезки, чтобы исключить незначительные веса внимания, что позволяет сократить количество операций с плавающей запятой на 70% без потери производительности. Это приводит к увеличению скорости обучения на 10-35%, особенно для длинных контекстов. Анализ показывает эффективность метода для различных размеров моделей и длин контекста."
                },
                "en": {
                    "title": "Boosting Efficiency with Adaptive Computation Pruning in FoX",
                    "desc": "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."
                },
                "zh": {
                    "title": "自适应计算剪枝提升FoX效率",
                    "desc": "最近提出的遗忘变换器（FoX）在软最大注意力中引入了遗忘门，与标准的RoPE变换器相比，表现出更好的性能。FoX中的许多注意力头快速遗忘，使得它们在每个时间步的输出主要依赖于局部上下文。基于这一观察，我们提出了自适应计算剪枝（ACP）方法，动态剪除与输入输出依赖关系强烈衰减的计算。通过在FoX的语言模型预训练中应用ACP，我们实现了约70%的计算量减少，同时训练吞吐量提高了10%到35%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11042",
            "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
            "url": "https://huggingface.co/papers/2504.11042",
            "abstract": "Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)",
            "score": 2,
            "issue_id": 3268,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "d40c91fff861e80f",
            "authors": [
                "Sukannya Purkayastha",
                "Zhuang Li",
                "Anne Lauscher",
                "Lizhen Qu",
                "Iryna Gurevych"
            ],
            "affiliations": [
                "Data Science Group, University of Hamburg",
                "Department of Data Science & AI, Monash University, Australia",
                "School of Computing Technologies, Royal Melbourne Institute of Technology, Australia",
                "Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11042.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#science",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Борьба с ленивым мышлением в рецензировании",
                    "desc": "В статье рассматривается проблема \"ленивого мышления\" в процессе рецензирования научных работ, что снижает качество рецензий. Авторы представляют новый датасет LazyReview, содержащий предложения рецензий с аннотациями по категориям ленивого мышления. Исследование показывает, что LLM плохо справляются с обнаружением таких случаев без предварительной настройки, но обучение на новом датасете значительно улучшает результаты. Также экспериментально доказано, что рецензии, исправленные с учётом ленивого мышления, становятся более полными и полезными."
                },
                "en": {
                    "title": "Enhancing Peer Review Quality with LazyReview Dataset",
                    "desc": "This paper addresses the problem of lazy thinking in peer review, which can degrade the quality of scientific evaluations. It introduces LazyReview, a new dataset containing peer-review sentences annotated with specific categories of lazy thinking. The study shows that Large Language Models (LLMs) have difficulty identifying these lazy thinking instances without prior training. However, by fine-tuning LLMs on the LazyReview dataset, their performance improves significantly, demonstrating the value of high-quality annotated data for enhancing peer review processes."
                },
                "zh": {
                    "title": "提升同行评审质量，打击懒惰思维！",
                    "desc": "同行评审是科学出版质量控制的重要环节。随着工作量的增加，评审中出现了使用快速启发式方法的现象，这被称为懒惰思维，影响了评审质量。本文介绍了LazyReview数据集，该数据集包含标注了细粒度懒惰思维类别的同行评审句子。我们的研究表明，使用大型语言模型在零样本设置下难以检测这些实例，但在我们的数据集上进行基于指令的微调可以显著提高性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10443",
            "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
            "url": "https://huggingface.co/papers/2504.10443",
            "abstract": "Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video.",
            "score": 2,
            "issue_id": 3268,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "a6b528cbce95d1ef",
            "authors": [
                "Haoran Hao",
                "Jiaming Han",
                "Yiyuan Zhang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, The Chinese University of Hong Kong",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10443.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "Динамическое кодирование для эффективного понимания длинных видео",
                    "desc": "Статья представляет новый метод для понимания длинных видео с использованием больших языковых моделей (LLM). Авторы предлагают динамическое кодирование видео, названное Temporal Dynamic Context (TDC), которое учитывает временные отношения между кадрами. Метод включает сегментацию видео, кодирование кадров с помощью визуально-аудио энкодеров и сжатие контекста с использованием трансформера на основе запросов. Для экстремально длинных видео предлагается стратегия цепочки рассуждений, которая постепенно извлекает ответы из нескольких сегментов видео."
                },
                "en": {
                    "title": "Enhancing Long Video Understanding with Temporal Dynamic Context",
                    "desc": "This paper introduces a new method called Temporal Dynamic Context (TDC) for improving the understanding of long videos using Large Language Models (LLMs). The approach segments videos into meaningful scenes and encodes each frame with visual and audio information, addressing the limitations of existing models that struggle with long contexts. A novel temporal context compressor is used to reduce the number of tokens while preserving essential information, allowing for better integration of video, audio, and text data. Additionally, a training-free chain-of-thought strategy is proposed to extract answers progressively from different video segments, enhancing the reasoning process for video understanding tasks."
                },
                "zh": {
                    "title": "动态长视频编码，提升视频理解能力",
                    "desc": "本研究提出了一种动态长视频编码方法，称为时间动态上下文（TDC），旨在解决现有大语言模型在处理长视频时面临的挑战。我们通过基于帧间相似性将视频分割为语义一致的场景，并使用视觉-音频编码器将每帧编码为标记。接着，我们引入了一种新颖的时间上下文压缩器，利用基于查询的Transformer将视频、音频和指令文本标记聚合为有限的时间上下文标记。最后，我们将静态帧标记和时间上下文标记输入到大语言模型中，以实现视频理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10049",
            "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
            "url": "https://huggingface.co/papers/2504.10049",
            "abstract": "Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations using VLMs with various representations as input. From these experiments, we suggest cost-effective strategies for generating summaries from text-heavy multimodal documents under different input-length budgets using VLMs. We show that slides extracted from the video stream can be beneficially used as input against the raw video, and that a structured representation from interleaved slides and transcript provides the best performance. Finally, we reflect and comment on the nature of cross-modal interactions in multimodal presentations and share suggestions to improve the capabilities of VLMs to understand documents of this nature.",
            "score": 2,
            "issue_id": 3264,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "49c72fe8f6e82ce0",
            "authors": [
                "Théo Gigant",
                "Camille Guinaudeau",
                "Frédéric Dufaux"
            ],
            "affiliations": [
                "Université Paris-Saclay, CNRS, CentraleSupelec, Laboratoire des signaux et systemes, France",
                "Université Paris-Saclay, CNRS, LISN, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10049.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Оптимизация суммирования мультимодальных презентаций с помощью VLM",
                    "desc": "Статья посвящена анализу использования моделей машинного обучения, работающих с визуальной и текстовой информацией (VLM), для автоматического суммирования мультимодальных презентаций. Авторы проводят детальный анализ различных стратегий ввода данных в VLM для генерации резюме. Исследование показывает, что использование слайдов, извлеченных из видеопотока, может быть более эффективным, чем использование сырого видео. Наилучшие результаты достигаются при использовании структурированного представления, сочетающего слайды и транскрипт."
                },
                "en": {
                    "title": "Enhancing Summarization of Multimodal Presentations with VLMs",
                    "desc": "This paper explores how Vision-Language Models (VLMs) can effectively summarize multimodal presentations that include text and images. The authors analyze different input formats and their impact on summarization quality, revealing that using slides from videos can enhance performance compared to raw video input. They propose strategies for optimizing summary generation based on varying input lengths, emphasizing the importance of structured representations. Additionally, the paper discusses the interactions between visual and textual data in these presentations and offers insights for improving VLM capabilities."
                },
                "zh": {
                    "title": "提升多模态文档摘要的智能策略",
                    "desc": "本文研究了视觉-语言模型（VLMs）在处理多模态信息（如文本、图像和视频）时的自动摘要能力。我们进行了细致的定量和定性分析，探讨了如何在不同输入长度预算下，从文本密集的多模态文档中生成摘要。实验结果表明，从视频流中提取的幻灯片作为输入比原始视频更有效，而交错的幻灯片和转录文本的结构化表示则提供了最佳性能。最后，我们讨论了多模态演示中的跨模态交互特性，并提出了改进VLMs理解此类文档能力的建议。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11457",
            "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception",
            "url": "https://huggingface.co/papers/2504.11457",
            "abstract": "With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at https://github.com/ziqipang/ADDP.",
            "score": 0,
            "issue_id": 3272,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "3aac006cdbd11ccd",
            "authors": [
                "Ziqi Pang",
                "Xin Xu",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11457.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#alignment",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация диффузионных моделей для точного восприятия",
                    "desc": "Статья посвящена улучшению применения генеративных диффузионных моделей для задач восприятия. Авторы анализируют процесс шумоподавления и предлагают адаптированные целевые функции обучения, учитывающие вклад разных временных шагов. Они также вводят специальную аугментацию данных для решения проблемы расхождения распределений при обучении и шумоподавлении. Исследователи демонстрируют, как генеративные модели могут служить интерактивным пользовательским интерфейсом, адаптируемым к корректирующим подсказкам."
                },
                "en": {
                    "title": "Bridging Generative and Discriminative Learning for Enhanced Perception",
                    "desc": "This paper explores how generative diffusion models, which are typically used for creating images, can be adapted for tasks that require precise classification and segmentation. The authors identify that while generative models can handle some errors during the image creation process, discriminative tasks need consistent accuracy at every step. They propose new learning objectives that focus on the importance of early denoising steps and address issues that arise in later steps due to shifts in training data. Their findings lead to significant improvements in performance for various perception tasks without changing the underlying model architecture."
                },
                "zh": {
                    "title": "提升生成模型在判别任务中的感知能力",
                    "desc": "这篇论文探讨了生成扩散模型在判别任务中的应用，尤其是在图像生成成功后。研究发现，直接将生成去噪过程用于判别目标时存在重要的差距，尤其是在多模态任务中。论文分析了生成扩散过程与感知任务之间的对齐，并提出了针对不同时间步贡献的学习目标。通过改进数据增强和学习策略，显著提升了基于扩散的感知模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11080",
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "url": "https://huggingface.co/papers/2504.11080",
            "abstract": "Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.",
            "score": 0,
            "issue_id": 3268,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "abf55fd00ae45494",
            "authors": [
                "Elman Ghazaei",
                "Erchan Aptoula"
            ],
            "affiliations": [
                "Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11080.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективное обнаружение изменений с помощью State Space Models",
                    "desc": "Эта статья представляет новую модель Change State Space Model для обнаружения изменений в данных дистанционного зондирования. Модель фокусируется на релевантных изменениях между разновременными изображениями, эффективно фильтруя несущественную информацию. Благодаря концентрации только на измененных признаках, количество параметров сети уменьшается, что значительно повышает вычислительную эффективность при сохранении высокой производительности обнаружения. Предложенная модель превзошла сверточные нейронные сети, Vision Transformers и модели на основе Mamba на трех эталонных наборах данных, при этом имея меньшую вычислительную сложность."
                },
                "en": {
                    "title": "Efficient Change Detection with the Change State Space Model",
                    "desc": "This paper introduces the Change State Space Model (CSSM), a new architecture designed specifically for change detection in bi-temporal images. Unlike traditional ConvNets and Vision Transformers, which struggle with long-range dependencies and computational efficiency, CSSM effectively filters out irrelevant information to focus on significant changes. By reducing the number of network parameters, CSSM enhances computational efficiency while maintaining high detection performance and robustness against input degradation. The model has been tested on three benchmark datasets, outperforming existing methods with lower computational complexity."
                },
                "zh": {
                    "title": "高效变化检测的新方法",
                    "desc": "本文介绍了一种新的变化状态空间模型（Change State Space Model），专门用于图像变化检测。该模型通过关注双时相图像之间的相关变化，有效过滤掉无关信息，从而提高了计算效率。与传统的卷积神经网络（ConvNets）和视觉变换器（ViTs）相比，该模型在保持高检测性能的同时，显著减少了网络参数数量。实验结果表明，该模型在三个基准数据集上表现优于现有的模型，且计算复杂度更低。"
                }
            }
        }
    ],
    "link_prev": "2025-04-15.html",
    "link_next": "2025-04-17.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "15.04",
        "en": "04/15",
        "zh": "4月15日"
    },
    "short_date_next": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4月17日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 8,
        "#benchmark": 12,
        "#agents": 1,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 8,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 21,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 11,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 18,
        "#survey": 2,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 4,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。目前的方法依赖监督信号，存在可扩展性和高标注成本问题。作者提出了一种无监督的自训练框架，名为Genius。Genius通过步进式预测重采样策略和优势校准优化（ACO）损失函数，实现了无需外部监督的LLM推理能力提升。代码将在https://github.com/xufangzhi/Genius发布。",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "pinyin": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。\nZhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLM) tuīlǐ nénglì de xìngqù.\n\n目前的方法依赖监督信号，存在可扩展性和高标注成本问题。\nMùqián de fāngfǎ yīlài jiàndū xìnhà",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improve\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"兴趣\", \"pinyin\": \"xìng qù\", \"trans\": \"interest\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"监督\", \"pinyin\": \"jiàn dū\", \"trans\": \"supervised\"},\n    {\"word\": \"信号\", \"pinyin\": \"xìn hào\", \"trans\": \"signal\"},\n    {\"word\": \"可扩展性\", \"pinyin\": \"kě kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"无监督\", \"pinyin\": \"wú jiàn dū\", \"trans\": \"unsupervised\"},\n    {\"word\": \"自训练\", \"pinyin\": \"zì xùn liàn\", \"trans\": \"self-training\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"步进式\", \"pinyin\": \"bù jìn shì\", \"trans\": \"stepwise\"},\n    {\"word\": \"预测\", \"pinyin\": \"yù cè\", \"trans\": \"prediction\"},\n    {\"word\": \"重采样\", \"pinyin\": \"chóng cǎi yàng\", \"trans\": \"resampling\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"校准\", \"pinyin\": \"jiào zhǔn\", \"trans\": \"calibration\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"损失函数\", \"pinyin\": \"sǔn shī hán shù\", \"trans\": \"loss function\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"外部\", \"pinyin\": \"wài bù\", \"trans\": \"external\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"}\n]",
        "trans": "This article discusses the interest in enhancing the reasoning capabilities of large language models (LLMs). Current methods rely on supervised signals, which present issues with scalability and high annotation costs. The authors",
        "update_ts": "2025-04-16 09:12"
    }
}