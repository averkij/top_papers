{
    "date": {
        "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 8",
        "zh": "1æœˆ8æ—¥"
    },
    "time_utc": "2025-01-08 04:12",
    "weekday": 2,
    "issue_id": 1550,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.03895",
            "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
            "url": "https://huggingface.co/papers/2501.03895",
            "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.",
            "score": 5,
            "issue_id": 1550,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "925d2f81d6fcbb0b",
            "authors": [
                "Shaolei Zhang",
                "Qingkai Fang",
                "Zhe Yang",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03895.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#video",
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLaVA-Mini - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLaVA-Mini Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaVA-Mini Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LLaVA-v1.5, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 576, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Maximizing Efficiency with Minimal Vision Tokens in LMMs",
                    "desc": "This paper presents LLaVA-Mini, an efficient large multimodal model (LMM) designed to reduce the number of vision tokens while maintaining visual information integrity. The authors identify that most vision tokens are primarily important in the early layers of the language model, where they integrate visual data with text. By implementing a technique called modality pre-fusion, LLaVA-Mini compresses the input from 576 vision tokens to just one, significantly enhancing efficiency. Experimental results show that LLaVA-Mini not only outperforms its predecessor but also achieves a 77% reduction in computational load and rapid processing times for high-resolution images and videos."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€æ¨¡å‹LLaVA-Miniçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹LLaVA-Miniï¼Œè¯¥æ¨¡å‹é€šè¿‡å‡å°‘è§†è§‰æ ‡è®°çš„æ•°é‡æ¥æé«˜æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°è§†è§‰æ ‡è®°åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—©æœŸå±‚ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› æ­¤å¯ä»¥åœ¨æ­¤ä¹‹å‰å°†è§†è§‰ä¿¡æ¯ä¸æ–‡æœ¬æ ‡è®°èåˆã€‚LLaVA-Minié‡‡ç”¨äº†æ¨¡æ€é¢„èåˆçš„æ–¹æ³•ï¼Œå°†è§†è§‰ä¿¡æ¯æå‰èåˆï¼Œä»è€Œå°†è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹çš„è§†è§‰æ ‡è®°å‹ç¼©ä¸ºä¸€ä¸ªæ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-Miniåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„æ¨¡å‹ï¼Œä¸”æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œå»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02260",
            "title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control",
            "url": "https://huggingface.co/papers/2501.02260",
            "abstract": "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.",
            "score": 2,
            "issue_id": 1550,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "9eeeb5b132839793",
            "authors": [
                "Mengting Wei",
                "Tuomas Varanka",
                "Xingxun Jiang",
                "Huai-Qian Khor",
                "Guoying Zhao"
            ],
            "affiliations": [
                "Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland",
                "Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02260.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ»Ğ¸Ñ†Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ MagicFace. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ†Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµĞ³Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ñƒ Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (AU) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ID-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ°. MagicFace Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MagicFace: Fine-Grained Facial Expression Editing with Consistent Identity",
                    "desc": "This paper presents a method for editing facial expressions while maintaining the identity and other attributes of the person. The proposed model, named MagicFace, utilizes a diffusion model that is conditioned on facial action unit (AU) variations, allowing for fine-grained control over expressions. It incorporates a pretrained Stable-Diffusion model and an ID encoder to ensure high consistency in facial details. Additionally, an Attribute Controller is introduced to maintain background and pose consistency during the editing process, resulting in high-fidelity expression animations."
                },
                "zh": {
                    "title": "é­”æ³•é¢å­”ï¼šé«˜ä¿çœŸé¢éƒ¨è¡¨æƒ…ç¼–è¾‘çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢éƒ¨è¡¨æƒ…ç¼–è¾‘çš„æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶åŒä¸€äººçš„é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰çš„ç›¸å¯¹å˜åŒ–æ¥å®ç°ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç»†è‡´ã€è¿ç»­ä¸”å¯è§£é‡Šåœ°ç¼–è¾‘ç‰¹å®šäººçš„è¡¨æƒ…ï¼ŒåŒæ—¶ä¿æŒä»–ä»¬çš„èº«ä»½ã€å§¿åŠ¿ã€èƒŒæ™¯å’Œé¢éƒ¨ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç§°ä¸ºMagicFaceï¼Œæ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºAUå˜åŒ–çš„æ‰©æ•£æ¨¡å‹å’Œä¸€ä¸ªIDç¼–ç å™¨ï¼Œä»¥ä¿æŒé¢éƒ¨ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å°†AUå˜åŒ–æ³¨å…¥å»å™ªUNetï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»¥é«˜ä¿çœŸåº¦ç¼–è¾‘é¢éƒ¨è¡¨æƒ…ï¼Œæ•ˆæœä¼˜äºå…¶ä»–ç›¸å…³å·¥ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03931",
            "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2501.03931",
            "abstract": "We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/",
            "score": 2,
            "issue_id": 1550,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "1c9696a99b57f781",
            "authors": [
                "Yuechen Zhang",
                "Yaoyang Liu",
                "Bin Xia",
                "Bohao Peng",
                "Zexin Yan",
                "Eric Lo",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CMU",
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03931.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#multimodal",
                    "#open_source",
                    "#synthetic",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸª",
                "ru": {
                    "title": "ĞœĞ°Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·ĞµÑ€ĞºĞ°Ğ»Ğ¾: Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Magic Mirror - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ»ĞµĞ³ĞºĞ¸Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼. Magic Mirror Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ."
                },
                "en": {
                    "title": "Magic Mirror: Identity-Preserved Video Generation with Cinematic Quality",
                    "desc": "Magic Mirror is a new framework designed to create high-quality videos that maintain the identity of individuals while showcasing dynamic motion. It addresses the challenges faced by previous video generation methods, which often struggled to keep a consistent identity or required extensive fine-tuning for specific individuals. The framework utilizes Video Diffusion Transformers and introduces innovative components like a dual-branch facial feature extractor and a cross-modal adapter to enhance identity integration. Through a two-stage training approach, Magic Mirror achieves a remarkable balance between identity preservation and natural motion, outperforming existing techniques with fewer additional parameters."
                },
                "zh": {
                    "title": "Magic Mirrorï¼šä¿æŒèº«ä»½ä¸€è‡´çš„åŠ¨æ€è§†é¢‘ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Magic Mirrorï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆä¿æŒèº«ä»½ä¸€è‡´çš„è§†é¢‘æ¡†æ¶ï¼Œå…·æœ‰ç”µå½±çº§è´¨é‡å’ŒåŠ¨æ€è¿åŠ¨ã€‚å°½ç®¡æœ€è¿‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆè‡ªç„¶è¿åŠ¨çš„åŒæ—¶ä¿æŒä¸€è‡´çš„èº«ä»½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼Œæå‡ºäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œä»¥æœ‰æ•ˆæ•´åˆèº«ä»½ä¿¡æ¯å¹¶ä¿æŒè¿åŠ¨å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagic Mirroråœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å¢åŠ çš„å‚æ•°æå°‘ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-07.html",
    "link_next": "2025-01-09.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚",
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i jiÄ›juÃ© fÃ¹zÃ¡ xÃ¹exuÃ© wÃ¨ntÃ­ shÃ­ de biÇoxiÃ n. MÃ³xÃ­ng shÇyÃ²ng fÄ“n Ã©r zhÃ¬zhÄ« de fÄngfÇ hÃ© shÃ ngxiÃ wÃ©n xuÃ©xÃ­ (ICL) shÃ¬lÃ¬. RÃ¡n'Ã©r, ICL shÃ¬lÃ¬ cÃºnzÃ i liÇng gÃ¨ guÇnjiÃ n wÃ¨ntÃ­: lÃ¬dÃ¹ bÃ¹ pÇpÄ›i hÃ© fÃ¹miÃ n xiÃ oyÃ¬ng zÃ oshÄ“ng wÃ¨ntÃ­. ZhÃ¨xiÄ“ wÃ¨ntÃ­ huÃ¬ yÇngxiÇng mÃ³xÃ­ng de tuÄ«lÇ zhÃ¬liÃ ng. ZuÃ²zhÄ› tÃ­chÅ« le BoostStep fÄngfÇ, tÅngguÃ² bÃ¹zhÃ²u lÃ¬dÃ¹ de duÃ¬qÃ­ hÃ© â€œshÇ’ucÃ¬ chÃ¡ngshÃ¬â€ cÃ¨lÃ¼Ã¨ tÃ­gÅng xiÄngguÄn de ICL shÃ¬lÃ¬, tÃ­gÄo tuÄ«lÇ zhÃ¬liÃ ng. BoostStep hÃ¡i kÄ›yÇ yÇ” mÃ©ngtÃ¨kÇluÃ³ shÃ¹ sÅusuÇ’ fÄngfÇ (MCTS) jiÃ©hÃ© shÇyÃ²ng, tÃ­shÄ“ng mÃ³xÃ­ng xÃ¬ngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, BoostStep xiÇnzhÃ¹ tÃ­gÄo le GPT-4o hÃ© Qwen2.5-Math-72B de biÇoxiÃ n.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'æ•°å­¦', 'pinyin': 'shÃ¹ xuÃ©', 'trans': 'mathematics'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'åˆ†è€Œæ²»ä¹‹', 'pinyin': 'fÄ“n Ã©r zhÃ¬ zhÄ«', 'trans': 'divide and conquer'}, {'word': 'ä¸Šä¸‹æ–‡å­¦ä¹ ', 'pinyin': 'shÃ ng xiÃ  wÃ©n xuÃ© xÃ­', 'trans': 'in-context learning'}, {'word': 'ç¤ºä¾‹', 'pinyin': 'shÃ¬ lÃ¬', 'trans': 'example'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'ç²’åº¦', 'pinyin': 'lÃ¬ dÃ¹', 'trans': 'granularity'}, {'word': 'è´Ÿé¢æ•ˆåº”', 'pinyin': 'fÃ¹ miÃ n xiÃ o yÃ¬ng', 'trans': 'negative effect'}, {'word': 'å™ªå£°', 'pinyin': 'zÃ o shÄ“ng', 'trans': 'noise'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'align'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'relevant'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•', 'pinyin': 'mÃ©ng tÃ¨ kÇ luÃ³ shÃ¹ sÅu suÇ’ fÄng fÇ', 'trans': 'Monte Carlo Tree Search method'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ© hÃ©', 'trans': 'combine'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'enhance'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]",
        "trans": "This article discusses the performance of large language models (LLMs) in solving complex mathematical problems. The models use a divide-and-conquer approach and in-context learning (ICL) examples. However, ICL examples face two key issues: granularity mismatch and negative impact noise problems. These issues affect the quality of the model's reasoning. The authors propose the BoostStep method, which aligns step granularity and provides relevant ICL examples through a \"first attempt\" strategy, thereby improving reasoning quality. BoostStep can also be combined with the Monte Carlo Tree Search (MCTS) method to enhance model performance. Experimental results show that BoostStep significantly improves the performance of GPT-4o and Qwen2.5-Math-72B.",
        "update_ts": "2025-01-07 09:11"
    }
}