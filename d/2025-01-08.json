{
    "date": {
        "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 8",
        "zh": "1æœˆ8æ—¥"
    },
    "time_utc": "2025-01-08 14:09",
    "weekday": 2,
    "issue_id": 1560,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.03262",
            "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
            "url": "https://huggingface.co/papers/2501.03262",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical approach for aligning large language models with human preferences, witnessing rapid algorithmic evolution through methods such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We present REINFORCE++, an enhanced variant of the classical REINFORCE algorithm that incorporates key optimization techniques from PPO while eliminating the need for a critic network. REINFORCE++ achieves three primary objectives: (1) simplicity (2) enhanced training stability, and (3) reduced computational overhead. Through extensive empirical evaluation, we demonstrate that REINFORCE++ exhibits superior stability compared to GRPO and achieves greater computational efficiency than PPO while maintaining comparable performance. The implementation is available at https://github.com/OpenRLHF/OpenRLHF.",
            "score": 32,
            "issue_id": 1553,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "a05acf5aab0c07dd",
            "authors": [
                "Jian Hu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.03262.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "REINFORCE++: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ RLHF",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ REINFORCE++, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° REINFORCE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF). REINFORCE++ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· PPO, Ğ½Ğ¾ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ REINFORCE++ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ PPO, Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "REINFORCE++: Simplifying Reinforcement Learning with Human Feedback",
                    "desc": "This paper introduces REINFORCE++, a new version of the REINFORCE algorithm designed to improve the training of reinforcement learning models using human feedback. It combines the strengths of Proximal Policy Optimization (PPO) while removing the need for a critic network, making it simpler and more efficient. The authors highlight that REINFORCE++ offers better training stability and lower computational costs compared to existing methods like GRPO and PPO. Their experiments show that REINFORCE++ performs well while being easier to use and faster to train."
                },
                "zh": {
                    "title": "REINFORCE++ï¼šç®€åŒ–ä¸é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–°é€‰æ‹©",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§é‡è¦çš„æ–¹æ³•ï¼Œç”¨äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ›´ç¬¦åˆäººç±»çš„åå¥½ã€‚æœ¬æ–‡æå‡ºäº†REINFORCE++ï¼Œè¿™æ˜¯ç»å…¸REINFORCEç®—æ³•çš„å¢å¼ºç‰ˆæœ¬ï¼Œç»“åˆäº†PPOçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶ä¸”ä¸å†éœ€è¦è¯„è®ºç½‘ç»œã€‚REINFORCE++çš„ä¸»è¦ç›®æ ‡æ˜¯å®ç°ç®€å•æ€§ã€æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå‡å°‘è®¡ç®—å¼€é”€ã€‚é€šè¿‡å¤§é‡å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†REINFORCE++åœ¨ç¨³å®šæ€§ä¸Šä¼˜äºGRPOï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¶…è¿‡PPOï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02955",
            "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
            "url": "https://huggingface.co/papers/2501.02955",
            "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io .",
            "score": 26,
            "issue_id": 1551,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "a7051c2d239484b4",
            "authors": [
                "Wenyi Hong",
                "Yean Cheng",
                "Zhuoyi Yang",
                "Weihan Wang",
                "Lefan Wang",
                "Xiaotao Gu",
                "Shiyu Huang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02955.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MotionBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MotionBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Through-Encoder Fusion, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Fine-Grained Motion Comprehension",
                    "desc": "This paper introduces MotionBench, a new benchmark for evaluating how well vision language models (VLMs) understand fine-grained motion in videos. It identifies a gap in current models' abilities to comprehend detailed motion, which is crucial for accurate video analysis. The benchmark includes various motion-oriented question types and diverse video data to ensure comprehensive testing. The authors also propose a Through-Encoder Fusion method to improve VLM performance, highlighting the need for further advancements in fine-grained motion comprehension."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£çš„ç»†ç²’åº¦è¿åŠ¨èƒ½åŠ›",
                    "desc": "è¿‘å¹´æ¥ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç»†ç²’åº¦è¿åŠ¨ç†è§£è¿™ä¸€å…³é”®èƒ½åŠ›åœ¨å½“å‰åŸºå‡†æµ‹è¯•ä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘ç†è§£æ¨¡å‹çš„ç»†ç²’åº¦è¿åŠ¨ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„VLMåœ¨ç†è§£ç»†ç²’åº¦è¿åŠ¨æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„Through-Encoderï¼ˆTEï¼‰èåˆæ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„è¿åŠ¨ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03895",
            "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
            "url": "https://huggingface.co/papers/2501.03895",
            "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.",
            "score": 17,
            "issue_id": 1550,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "925d2f81d6fcbb0b",
            "authors": [
                "Shaolei Zhang",
                "Qingkai Fang",
                "Zhe Yang",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03895.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#video",
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLaVA-Mini - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLaVA-Mini Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaVA-Mini Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LLaVA-v1.5, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 576, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Maximizing Efficiency with Minimal Vision Tokens in LMMs",
                    "desc": "This paper presents LLaVA-Mini, an efficient large multimodal model (LMM) designed to reduce the number of vision tokens while maintaining visual information integrity. The authors identify that most vision tokens are primarily important in the early layers of the language model, where they integrate visual data with text. By implementing a technique called modality pre-fusion, LLaVA-Mini compresses the input from 576 vision tokens to just one, significantly enhancing efficiency. Experimental results show that LLaVA-Mini not only outperforms its predecessor but also achieves a 77% reduction in computational load and rapid processing times for high-resolution images and videos."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€æ¨¡å‹LLaVA-Miniçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹LLaVA-Miniï¼Œè¯¥æ¨¡å‹é€šè¿‡å‡å°‘è§†è§‰æ ‡è®°çš„æ•°é‡æ¥æé«˜æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°è§†è§‰æ ‡è®°åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—©æœŸå±‚ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› æ­¤å¯ä»¥åœ¨æ­¤ä¹‹å‰å°†è§†è§‰ä¿¡æ¯ä¸æ–‡æœ¬æ ‡è®°èåˆã€‚LLaVA-Minié‡‡ç”¨äº†æ¨¡æ€é¢„èåˆçš„æ–¹æ³•ï¼Œå°†è§†è§‰ä¿¡æ¯æå‰èåˆï¼Œä»è€Œå°†è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹çš„è§†è§‰æ ‡è®°å‹ç¼©ä¸ºä¸€ä¸ªæ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-Miniåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„æ¨¡å‹ï¼Œä¸”æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œå»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04001",
            "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos",
            "url": "https://huggingface.co/papers/2501.04001",
            "abstract": "This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications.",
            "score": 14,
            "issue_id": 1555,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "d079946bf74858cd",
            "authors": [
                "Haobo Yuan",
                "Xiangtai Li",
                "Tao Zhang",
                "Zilong Huang",
                "Shilin Xu",
                "Shunping Ji",
                "Yunhai Tong",
                "Lu Qi",
                "Jiashi Feng",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Bytedance Seed",
                "Peking University",
                "UC Merced",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04001.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Sa2VA: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Sa2VA - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ SAM-2 (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾) Ñ LLaVA (Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°) Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Sa2VA Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ SAM-2 Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Sa2VA: Unifying Image and Video Understanding for Enhanced Multi-Modal Tasks",
                    "desc": "Sa2VA is a groundbreaking model that integrates image and video understanding into a single framework. It combines the strengths of SAM-2 for video segmentation and LLaVA for vision-language tasks, allowing it to handle various multi-modal tasks with minimal tuning. By creating a shared token space for text, images, and videos, Sa2VA can generate specific instruction tokens that help in accurately segmenting objects in both images and videos. The introduction of the Ref-SAV dataset further enhances its capabilities, enabling it to achieve top performance in complex visual environments."
                },
                "zh": {
                    "title": "Sa2VAï¼šå›¾åƒä¸è§†é¢‘çš„ç»Ÿä¸€ç†è§£æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†Sa2VAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹å›¾åƒå’Œè§†é¢‘è¿›è¡Œå¯†é›†çš„åŸºç¡€ç†è§£ã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒSa2VAæ”¯æŒå¤šç§å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼•ç”¨åˆ†å‰²å’Œå¯¹è¯ï¼Œä¸”åªéœ€æœ€å°‘çš„ä¸€æ¬¡æ€§æŒ‡ä»¤è°ƒä¼˜ã€‚Sa2VAç»“åˆäº†åŸºç¡€è§†é¢‘åˆ†å‰²æ¨¡å‹SAM-2å’Œå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹LLaVAï¼Œå°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„LLMä»¤ç‰Œç©ºé—´ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒSa2VAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ç°å®åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03575",
            "title": "Cosmos World Foundation Model Platform for Physical AI",
            "url": "https://huggingface.co/papers/2501.03575",
            "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via https://github.com/NVIDIA/Cosmos.",
            "score": 14,
            "issue_id": 1552,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "f4b2044cbc1076a8",
            "authors": [
                "NVIDIA",
                ":",
                "Niket Agarwal",
                "Arslan Ali",
                "Maciej Bala",
                "Yogesh Balaji",
                "Erik Barker",
                "Tiffany Cai",
                "Prithvijit Chattopadhyay",
                "Yongxin Chen",
                "Yin Cui",
                "Yifan Ding",
                "Daniel Dworakowski",
                "Jiaojiao Fan",
                "Michele Fenzi",
                "Francesco Ferroni",
                "Sanja Fidler",
                "Dieter Fox",
                "Songwei Ge",
                "Yunhao Ge",
                "Jinwei Gu",
                "Siddharth Gururani",
                "Ethan He",
                "Jiahui Huang",
                "Jacob Huffman",
                "Pooya Jannaty",
                "Jingyi Jin",
                "Seung Wook Kim",
                "Gergely KlÃ¡r",
                "Grace Lam",
                "Shiyi Lan",
                "Laura Leal-Taixe",
                "Anqi Li",
                "Zhaoshuo Li",
                "Chen-Hsuan Lin",
                "Tsung-Yi Lin",
                "Huan Ling",
                "Ming-Yu Liu",
                "Xian Liu",
                "Alice Luo",
                "Qianli Ma",
                "Hanzi Mao",
                "Kaichun Mo",
                "Arsalan Mousavian",
                "Seungjun Nah",
                "Sriharsha Niverty",
                "David Page",
                "Despoina Paschalidou",
                "Zeeshan Patel",
                "Lindsey Pavao",
                "Morteza Ramezanali",
                "Fitsum Reda",
                "Xiaowei Ren",
                "Vasanth Rao Naik Sabavat",
                "Ed Schmerling",
                "Stella Shi",
                "Bartosz Stefaniak",
                "Shitao Tang",
                "Lyne Tchapmi",
                "Przemek Tredak",
                "Wei-Cheng Tseng",
                "Jibin Varghese",
                "Hao Wang",
                "Haoxiang Wang",
                "Heng Wang",
                "Ting-Chun Wang",
                "Fangyin Wei",
                "Xinyue Wei",
                "Jay Zhangjie Wu",
                "Jiashu Xu",
                "Wei Yang",
                "Lin Yen-Chen",
                "Xiaohui Zeng",
                "Yu Zeng",
                "Jing Zhang",
                "Qinsheng Zhang",
                "Yuxuan Zhang",
                "Qingqing Zhao",
                "Artur Zolkowski"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03575.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#benchmark",
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸Ğº Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Cosmos World Foundation Model Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° GitHub Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Empowering Physical AI with Customizable World Models",
                    "desc": "This paper introduces the Cosmos World Foundation Model Platform, designed to assist developers in creating tailored world models for Physical AI systems. It emphasizes the necessity of having a digital twin of both the AI and its environment to enable effective training. The platform includes a comprehensive video curation pipeline, pre-trained models, and tools for fine-tuning these models for specific applications. By making the platform and models open-source, the authors aim to empower developers to address significant societal challenges using Physical AI."
                },
                "zh": {
                    "title": "æ„å»ºç‰©ç†AIçš„æ•°å­—åŒèƒèƒä¸ä¸–ç•Œæ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ç‰©ç†äººå·¥æ™ºèƒ½ï¼ˆPhysical AIï¼‰åœ¨æ•°å­—è®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œéœ€è¦æ„å»ºä¸€ä¸ªæ•°å­—åŒèƒèƒï¼ˆdigital twinï¼‰å’Œä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼ˆworld modelï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†Cosmosä¸–ç•ŒåŸºç¡€æ¨¡å‹å¹³å°ï¼Œå¸®åŠ©å¼€å‘è€…ä¸ºç‰©ç†äººå·¥æ™ºèƒ½å®šåˆ¶ä¸–ç•Œæ¨¡å‹ã€‚è¯¥å¹³å°æä¾›äº†è§†é¢‘ç­–åˆ’ç®¡é“ã€é¢„è®­ç»ƒçš„ä¸–ç•ŒåŸºç¡€æ¨¡å‹ä»¥åŠåè®­ç»ƒç¤ºä¾‹ï¼Œæ—¨åœ¨è§£å†³ç¤¾ä¼šä¸­çš„å…³é”®é—®é¢˜ï¼Œå¹¶ä¸”æ˜¯å¼€æºçš„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03847",
            "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control",
            "url": "https://huggingface.co/papers/2501.03847",
            "abstract": "Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.",
            "score": 8,
            "issue_id": 1552,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "975d5fa9d59bde28",
            "authors": [
                "Zekai Gu",
                "Rui Yan",
                "Jiahao Lu",
                "Peng Li",
                "Zhiyang Dou",
                "Chenyang Si",
                "Zhen Dong",
                "Qifeng Liu",
                "Cheng Lin",
                "Ziwei Liu",
                "Wenping Wang",
                "Yuan Liu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology, China",
                "Nanyang Technological University, Singapore",
                "Texas A&M University, U.S.A",
                "The University of Hong Kong, China",
                "Wuhan University, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03847.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DaS: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 3D-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diffusion as Shader (DaS) Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, DaS Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 3D-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼. DaS Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Video Generation with 3D Control Signals",
                    "desc": "This paper presents Diffusion as Shader (DaS), a new method for generating videos that allows for precise control over various aspects of video creation. Unlike previous models that only used 2D control signals, DaS utilizes 3D tracking videos, which helps in managing the dynamic nature of video content. This approach enables users to manipulate video elements like camera angles and object movements more effectively. The results show that DaS can maintain high-quality video generation while ensuring temporal consistency across frames, even with limited training data."
                },
                "zh": {
                    "title": "å¤šæ ·åŒ–è§†é¢‘æ§åˆ¶çš„æ–°æ–¹æ³•ï¼šæ‰©æ•£ä½œä¸ºç€è‰²å™¨",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨ä»æ–‡æœ¬æç¤ºæˆ–å›¾åƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ï¼Œå¦‚ç›¸æœºæ“ä½œæˆ–å†…å®¹ç¼–è¾‘ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å—æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•é€šå¸¸ä»…é™äºå•ä¸€æ§åˆ¶ç±»å‹ï¼Œç¼ºä¹å¤„ç†å¤šæ ·åŒ–æ§åˆ¶éœ€æ±‚çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æ‰©æ•£ä½œä¸ºç€è‰²å™¨ï¼ˆDaSï¼‰ï¼Œå®ƒåœ¨ç»Ÿä¸€æ¶æ„ä¸­æ”¯æŒå¤šç§è§†é¢‘æ§åˆ¶ä»»åŠ¡ï¼Œåˆ©ç”¨3Dæ§åˆ¶ä¿¡å·æ¥å®ç°æ›´çµæ´»çš„è§†é¢‘æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03936",
            "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
            "url": "https://huggingface.co/papers/2501.03936",
            "abstract": "Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, often overlooking visual design and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to understand their structural patterns and content schemas, then drafts outlines and generates slides through code actions to ensure consistency and alignment. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Experiments show that PPTAgent significantly outperforms traditional automatic presentation generation methods across all three dimensions. The code and data are available at https://github.com/icip-cas/PPTAgent.",
            "score": 5,
            "issue_id": 1557,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "57bb4703056c9e20",
            "authors": [
                "Hao Zheng",
                "Xinyan Guan",
                "Hao Kong",
                "Jia Zheng",
                "Hongyu Lin",
                "Yaojie Lu",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "Shanghai Jiexin Technology",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03936.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "PPTAgent: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PPTAgent - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², PPTAgent ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PPTEval Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "PPTAgent: Elevating Presentation Generation with Content, Design, and Coherence",
                    "desc": "This paper presents PPTAgent, a novel approach for automatically generating presentations from documents. Unlike existing methods that focus solely on content quality, PPTAgent enhances the overall presentation by considering visual design and structural coherence as well. It employs a two-stage, edit-based process that first analyzes reference presentations to extract patterns and then generates slides through code actions. Additionally, the authors introduce PPTEval, a framework for evaluating presentations based on content, design, and coherence, demonstrating that PPTAgent outperforms traditional methods in all areas."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç”Ÿæˆé«˜è´¨é‡æ¼”ç¤ºæ–‡ç¨¿çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPPTAgentçš„è‡ªåŠ¨ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µçš„ç¼–è¾‘å¼æµç¨‹ï¼Œç»¼åˆè€ƒè™‘å†…å®¹è´¨é‡ã€è§†è§‰è®¾è®¡å’Œç»“æ„ä¸€è‡´æ€§ã€‚PPTAgenté¦–å…ˆåˆ†æå‚è€ƒæ¼”ç¤ºæ–‡ç¨¿ï¼Œä»¥ç†è§£å…¶ç»“æ„æ¨¡å¼å’Œå†…å®¹æ¡†æ¶ï¼Œç„¶åé€šè¿‡ä»£ç æ“ä½œè‰æ‹Ÿå¤§çº²å¹¶ç”Ÿæˆå¹»ç¯ç‰‡ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿çš„è´¨é‡ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†PPTEvalè¯„ä¼°æ¡†æ¶ï¼Œä»å†…å®¹ã€è®¾è®¡å’Œä¸€è‡´æ€§ä¸‰ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03714",
            "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2501.03714",
            "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.",
            "score": 5,
            "issue_id": 1556,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "c6cfa761edc047da",
            "authors": [
                "Sangwoon Kwak",
                "Joonsoo Kim",
                "Jun Young Jeong",
                "Won-Sik Cheong",
                "Jihyong Oh",
                "Munchurl Kim"
            ],
            "affiliations": [
                "Chung-Ang University",
                "Electronics and Telecommunications Research Institute",
                "Korea Advanced Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03714.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…",
                    "desc": "MoDecGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Gaussian Splatting. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ GlobaltoLocal Motion Decomposition (GLMD) Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Global Canonical Scaffolds Ğ¸ Local Canonical Scaffolds. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Global Anchor Deformation (GAD) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Local Gaussian Deformation (LGD) Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. MoDecGS Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Dynamic Scene Rendering with MoDecGS",
                    "desc": "The paper presents MoDecGS, a new framework for 3D Gaussian Splatting that efficiently handles dynamic scenes in neural rendering. It introduces GlobaltoLocal Motion Decomposition (GLMD) to capture complex motions using both Global and Local Canonical Scaffolds. The method employs Global Anchor Deformation (GAD) for global dynamics and Local Gaussian Deformation (LGD) for fine-tuning local motions. MoDecGS significantly reduces model size by 70% compared to existing methods while enhancing rendering quality, making it suitable for real-world dynamic video reconstruction."
                },
                "zh": {
                    "title": "é«˜æ•ˆåŠ¨æ€åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•",
                    "desc": "3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨åœºæ™¯è¡¨ç¤ºå’Œç¥ç»æ¸²æŸ“æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶ä»é¢ä¸´å­˜å‚¨éœ€æ±‚å’Œå¤æ‚è¿åŠ¨è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoDecGSï¼Œä¸€ä¸ªå†…å­˜é«˜æ•ˆçš„é«˜æ–¯ç‚¹äº‘æ¡†æ¶ï¼Œæ—¨åœ¨é‡å»ºå…·æœ‰å¤æ‚è¿åŠ¨çš„æ–°è§†è§’ã€‚æˆ‘ä»¬å¼•å…¥äº†å…¨å±€åˆ°å±€éƒ¨è¿åŠ¨åˆ†è§£ï¼ˆGLMDï¼‰ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼æœ‰æ•ˆæ•æ‰åŠ¨æ€è¿åŠ¨ï¼Œå¹¶æ‰©å±•äº†é™æ€æ”¯æ¶è¡¨ç¤ºä»¥é€‚åº”åŠ¨æ€è§†é¢‘é‡å»ºã€‚é€šè¿‡å…¨å±€é”šç‚¹å˜å½¢ï¼ˆGADï¼‰å’Œå±€éƒ¨é«˜æ–¯å˜å½¢ï¼ˆLGDï¼‰ï¼ŒMoDecGSåœ¨ä¿æŒæˆ–æé«˜æ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘äº†70%çš„æ¨¡å‹å¤§å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03916",
            "title": "Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback",
            "url": "https://huggingface.co/papers/2501.03916",
            "abstract": "The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose Dolphin, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. Dolphin can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, Dolphin first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that Dolphin can generate novel ideas continuously and complete the experiment in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 2D image classification and 3D point classification.",
            "score": 3,
            "issue_id": 1555,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "9a18a60e788b7840",
            "authors": [
                "Jiakang Yuan",
                "Xiangchao Yan",
                "Botian Shi",
                "Tao Chen",
                "Wanli Ouyang",
                "Bo Zhang",
                "Lei Bai",
                "Yu Qiao",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03916.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#science",
                    "#3d",
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Dolphin: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dolphin - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Dolphin Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ´ĞµĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ†Ğ¸ĞºĞ»Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Dolphin Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dolphin: Automating Scientific Research with AI",
                    "desc": "This paper introduces Dolphin, an innovative framework designed to automate the scientific research process. Dolphin operates in a closed-loop system, generating research ideas, conducting experiments, and analyzing results to refine future ideas. It utilizes AI to rank relevant literature and automatically generate and debug code, enhancing research efficiency. The framework has been tested on various benchmark datasets, demonstrating its ability to produce novel ideas and achieve results comparable to leading methods in tasks like image classification."
                },
                "zh": {
                    "title": "Dolphinï¼šè‡ªåŠ¨åŒ–ç§‘å­¦ç ”ç©¶çš„æ–°çºªå…ƒ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºDolphinçš„é—­ç¯å¼€æ”¾å¼è‡ªåŠ¨ç ”ç©¶æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç§‘å­¦ç ”ç©¶çš„æ•ˆç‡ã€‚Dolphinèƒ½å¤Ÿç”Ÿæˆç ”ç©¶æƒ³æ³•ã€è¿›è¡Œå®éªŒï¼Œå¹¶æ ¹æ®å®éªŒç»“æœåé¦ˆç”Ÿæˆæ›´é«˜è´¨é‡çš„æƒ³æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒDolphiné¦–å…ˆæ ¹æ®ç›¸å…³è®ºæ–‡ç”Ÿæˆæ–°æƒ³æ³•ï¼Œç„¶åè‡ªåŠ¨ç”Ÿæˆå’Œè°ƒè¯•ä»£ç ï¼Œæœ€ååˆ†ææ¯ä¸ªæƒ³æ³•çš„ç»“æœå¹¶åé¦ˆåˆ°ä¸‹ä¸€è½®ç”Ÿæˆä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDolphinèƒ½å¤ŸæŒç»­ç”Ÿæˆæ–°æƒ³æ³•ï¼Œå¹¶åœ¨å¾ªç¯ä¸­å®Œæˆå®éªŒï¼Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02260",
            "title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control",
            "url": "https://huggingface.co/papers/2501.02260",
            "abstract": "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.",
            "score": 3,
            "issue_id": 1550,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "9eeeb5b132839793",
            "authors": [
                "Mengting Wei",
                "Tuomas Varanka",
                "Xingxun Jiang",
                "Huai-Qian Khor",
                "Guoying Zhao"
            ],
            "affiliations": [
                "Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland",
                "Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02260.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ»Ğ¸Ñ†Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ MagicFace. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ†Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµĞ³Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ñƒ Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (AU) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ID-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ°. MagicFace Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MagicFace: Fine-Grained Facial Expression Editing with Consistent Identity",
                    "desc": "This paper presents a method for editing facial expressions while maintaining the identity and other attributes of the person. The proposed model, named MagicFace, utilizes a diffusion model that is conditioned on facial action unit (AU) variations, allowing for fine-grained control over expressions. It incorporates a pretrained Stable-Diffusion model and an ID encoder to ensure high consistency in facial details. Additionally, an Attribute Controller is introduced to maintain background and pose consistency during the editing process, resulting in high-fidelity expression animations."
                },
                "zh": {
                    "title": "é­”æ³•é¢å­”ï¼šé«˜ä¿çœŸé¢éƒ¨è¡¨æƒ…ç¼–è¾‘çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢éƒ¨è¡¨æƒ…ç¼–è¾‘çš„æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶åŒä¸€äººçš„é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰çš„ç›¸å¯¹å˜åŒ–æ¥å®ç°ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç»†è‡´ã€è¿ç»­ä¸”å¯è§£é‡Šåœ°ç¼–è¾‘ç‰¹å®šäººçš„è¡¨æƒ…ï¼ŒåŒæ—¶ä¿æŒä»–ä»¬çš„èº«ä»½ã€å§¿åŠ¿ã€èƒŒæ™¯å’Œé¢éƒ¨ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç§°ä¸ºMagicFaceï¼Œæ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºAUå˜åŒ–çš„æ‰©æ•£æ¨¡å‹å’Œä¸€ä¸ªIDç¼–ç å™¨ï¼Œä»¥ä¿æŒé¢éƒ¨ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å°†AUå˜åŒ–æ³¨å…¥å»å™ªUNetï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»¥é«˜ä¿çœŸåº¦ç¼–è¾‘é¢éƒ¨è¡¨æƒ…ï¼Œæ•ˆæœä¼˜äºå…¶ä»–ç›¸å…³å·¥ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03931",
            "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2501.03931",
            "abstract": "We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/",
            "score": 3,
            "issue_id": 1550,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "1c9696a99b57f781",
            "authors": [
                "Yuechen Zhang",
                "Yaoyang Liu",
                "Bin Xia",
                "Bohao Peng",
                "Zexin Yan",
                "Eric Lo",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CMU",
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03931.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#multimodal",
                    "#open_source",
                    "#synthetic",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸª",
                "ru": {
                    "title": "ĞœĞ°Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·ĞµÑ€ĞºĞ°Ğ»Ğ¾: Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Magic Mirror - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ»ĞµĞ³ĞºĞ¸Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼. Magic Mirror Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ."
                },
                "en": {
                    "title": "Magic Mirror: Identity-Preserved Video Generation with Cinematic Quality",
                    "desc": "Magic Mirror is a new framework designed to create high-quality videos that maintain the identity of individuals while showcasing dynamic motion. It addresses the challenges faced by previous video generation methods, which often struggled to keep a consistent identity or required extensive fine-tuning for specific individuals. The framework utilizes Video Diffusion Transformers and introduces innovative components like a dual-branch facial feature extractor and a cross-modal adapter to enhance identity integration. Through a two-stage training approach, Magic Mirror achieves a remarkable balance between identity preservation and natural motion, outperforming existing techniques with fewer additional parameters."
                },
                "zh": {
                    "title": "Magic Mirrorï¼šä¿æŒèº«ä»½ä¸€è‡´çš„åŠ¨æ€è§†é¢‘ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Magic Mirrorï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆä¿æŒèº«ä»½ä¸€è‡´çš„è§†é¢‘æ¡†æ¶ï¼Œå…·æœ‰ç”µå½±çº§è´¨é‡å’ŒåŠ¨æ€è¿åŠ¨ã€‚å°½ç®¡æœ€è¿‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆè‡ªç„¶è¿åŠ¨çš„åŒæ—¶ä¿æŒä¸€è‡´çš„èº«ä»½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼Œæå‡ºäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œä»¥æœ‰æ•ˆæ•´åˆèº«ä»½ä¿¡æ¯å¹¶ä¿æŒè¿åŠ¨å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagic Mirroråœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å¢åŠ çš„å‚æ•°æå°‘ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-07.html",
    "link_next": "2025-01-09.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå«åš REINFORCE++ã€‚å®ƒæ”¹è¿›äº†ç»å…¸çš„ REINFORCE ç®—æ³•ï¼Œç»“åˆäº† PPO çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†ä¸éœ€è¦è¯„è®ºç½‘ç»œã€‚REINFORCE++ æœ‰ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šç®€å•ã€æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREINFORCE++ æ¯” GRPO æ›´ç¨³å®šï¼Œæ¯” PPO æ›´é«˜æ•ˆï¼Œæ€§èƒ½ä¹Ÿç›¸å½“ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/OpenRLHF/OpenRLHF æ‰¾åˆ°ã€‚",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå«åš REINFORCE++ã€‚å®ƒæ”¹è¿›äº†ç»å…¸çš„ REINFORCE ç®—æ³•ï¼Œç»“åˆäº† PPO çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†ä¸éœ€è¦è¯„è®ºç½‘ç»œã€‚REINFORCE++ æœ‰ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šç®€å•ã€æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREINFORCE++ æ¯” GRPO æ›´ç¨³å®šï¼Œæ¯” PPO æ›´é«˜æ•ˆï¼Œæ€§èƒ½ä¹Ÿç›¸å½“ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/OpenRLHF/OpenRLHF æ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de qiÃ¡ng huÃ  xuÃ©xÃ­ fÄngfÇ, jiÃ ozuÃ² REINFORCE++. TÄ gÇijÃ¬n le jÄ«ngdiÇn de REINFORCE suÃ nfÇ, jiÃ©hÃ© le PPO de yÅuhuÃ  jÃ¬shÃ¹, dÃ n bÃ¹ xÅ«yÃ o pÃ­nglÃ¹n wÇngluÃ². REINFORCE++ yÇ’u sÄn gÃ¨ zhÇ”yÃ o mÃ¹biÄo: jiÇndÄn, tÃ­gÄo xÃ¹nliÃ n wÄ›ndÃ¬ngxÃ¬ng hÃ© jiÇnshÇo jÃ¬suÃ n kÄixiÄo. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, REINFORCE++ bÇ GRPO gÃ¨ng wÄ›ndÃ¬ng, bÇ PPO gÃ¨ng gÄoxiÃ o, xÃ¬ngnÃ©ng yÄ› xiÄngdÄng. DÃ imÇ kÄ›yÇ zÃ i https://github.com/OpenRLHF/OpenRLHF zhÇo dÃ o.",
        "vocab": "[{'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'}, {'word': 'ç»å…¸', 'pinyin': 'jÄ«ng diÇn', 'trans': 'classic'}, {'word': 'ç®—æ³•', 'pinyin': 'suÃ n fÇ', 'trans': 'algorithm'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ© hÃ©', 'trans': 'combine'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimize'}, {'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'}, {'word': 'è¯„è®º', 'pinyin': 'pÃ­ng lÃ¹n', 'trans': 'comment'}, {'word': 'ç½‘ç»œ', 'pinyin': 'wÇng luÃ²', 'trans': 'network'}, {'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'goal'}, {'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›n dÃ¬ng xÃ¬ng', 'trans': 'stability'}, {'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'calculate'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'show'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'æ‰¾åˆ°', 'pinyin': 'zhÇo dÃ o', 'trans': 'find'}]",
        "trans": "This article introduces a new reinforcement learning method called REINFORCE++. It improves upon the classic REINFORCE algorithm by incorporating optimization techniques from PPO, without the need for a critic network. REINFORCE++ has three main objectives: simplicity, enhanced training stability, and reduced computational overhead. Experimental results show that REINFORCE++ is more stable than GRPO and more efficient than PPO, with comparable performance. The code can be found at https://github.com/OpenRLHF/OpenRLHF.",
        "update_ts": "2025-01-08 09:10"
    }
}