
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. October 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 октября</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-06.html">⬅️ <span id="prev-date">06.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-08.html">➡️ <span id="next-date">08.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'};
        let feedDateNext = {'ru': '08.10', 'en': '10/08', 'zh': '10月8日'};
        let feedDatePrev = {'ru': '06.10', 'en': '10/06', 'zh': '10月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.05096', 'title': 'Paper2Video: Automatic Video Generation from Scientific Papers', 'url': 'https://huggingface.co/papers/2510.05096', 'abstract': "PaperTalker is a multi-agent framework that automates academic presentation video generation by integrating slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.", 'score': 19, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '8a4b07b93a7b0b67', 'authors': ['Zeyu Zhu', 'Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.05096.jpg', 'data': {'categories': ['#benchmark', '#science', '#dataset', '#multimodal', '#open_source', '#agents'], 'emoji': '🎓', 'ru': {'title': 'Автоматическая генерация академических презентаций с помощью мультиагентной системы', 'desc': 'PaperTalker — это первый мультиагентный фреймворк для автоматического создания презентационных видео из научных статей. Система решает сложную задачу координации множества каналов: генерирует слайды с оптимизированной вёрсткой, добавляет субтитры, синтезирует речь и создаёт говорящую голову докладчика. Авторы представили бенчмарк из 101 научной статьи с соответствующими видео и разработали специальные метрики для оценки качества передачи информации аудитории. Эксперименты показали, что PaperTalker создаёт более точные и информативные презентации по сравнению с существующими методами, делая шаг к практической автоматизации академических видео.'}, 'en': {'title': 'Automating Academic Presentations with PaperTalker', 'desc': "PaperTalker is a multi-agent framework designed to automate the generation of academic presentation videos, addressing the labor-intensive nature of this task. It integrates various components such as slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering to create cohesive and informative videos. The framework is evaluated using a new benchmark of 101 research papers and tailored metrics to ensure the videos effectively convey the original paper's information. Experiments show that PaperTalker outperforms existing methods, making it a significant advancement in automated academic video production."}, 'zh': {'title': 'PaperTalker：学术演示视频自动生成的未来', 'desc': 'PaperTalker是一个多智能体框架，旨在自动生成学术演示视频。它通过整合幻灯片生成、布局优化、字幕、语音合成和人像渲染，显著提高了视频生成的效率和质量。该框架解决了学术演示视频生成中的多模态信息协调和输入来源复杂性等挑战。实验结果表明，PaperTalker生成的视频比现有方法更具信息性和准确性，推动了学术视频自动化生成的进程。'}}}, {'id': 'https://huggingface.co/papers/2510.05034', 'title': 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models', 'url': 'https://huggingface.co/papers/2510.05034', 'abstract': 'This survey examines post-training methodologies for Video-LMMs, focusing on supervised fine-tuning, reinforcement learning, and test-time scaling, while addressing challenges in video understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://github.com/yunlong10/Awesome-Video-LMM-Post-Training', 'score': 15, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '9a175b353597fd7f', 'authors': ['Yunlong Tang', 'Jing Bi', 'Pinxin Liu', 'Zhenyu Pan', 'Zhangyun Tan', 'Qianxiang Shen', 'Jiani Liu', 'Hang Hua', 'Junjia Guo', 'Yunzhong Xiao', 'Chao Huang', 'Zhiyuan Wang', 'Susan Liang', 'Xinyi Liu', 'Yizhi Song', 'Yuhe Nie', 'Jia-Xing Zhong', 'Bozheng Li', 'Daiqing Qi', 'Ziyun Zeng', 'Ali Vosoughi', 'Luchuan Song', 'Zeliang Zhang', 'Daiki Shimada', 'Han Liu', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['Brown University', 'CMU', 'NYU', 'Northwestern University', 'Purdue University', 'Sony Group Corporation', 'UCSB', 'University of Oxford', 'University of Rochester', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2510.05034.jpg', 'data': {'categories': ['#benchmark', '#training', '#survey', '#reasoning', '#multimodal', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Пост-тренировка Video-LMMs: ключ к пониманию видео', 'desc': 'Эта статья рассматривает методы пост-тренировки для Video-LMMs, включая супервизионное дообучение, обучение с подкреплением и масштабирование на этапе тестирования. Video-LMMs объединяют визуальные энкодеры с мощными языковыми моделями, что позволяет лучше понимать видео. Авторы предлагают таксономию, которая объясняет роли и взаимосвязи этих методов, а также адаптации для видео. Они также выделяют ключевые принципы дизайна и открытые проблемы, такие как проектирование вознаграждений и оптимизация производительности.'}, 'en': {'title': 'Advancing Video Understanding with Post-Training Techniques', 'desc': 'This paper surveys post-training methods for Video-Large Multimodal Models (Video-LMMs), which are advanced systems that combine visual and language processing for better video understanding. It focuses on three main techniques: supervised fine-tuning, reinforcement learning, and test-time scaling, each addressing specific challenges in video analysis. The authors present a structured taxonomy to clarify how these methods interconnect and adapt to video-specific tasks, such as handling long videos and integrating different types of data. By analyzing existing approaches, the paper highlights key design principles and identifies ongoing challenges, providing a framework for future research in enhancing Video-LMM capabilities.'}, 'zh': {'title': '推动视频理解的后训练方法研究', 'desc': '本调查研究了视频大规模多模态模型（Video-LMMs）的后训练方法，重点关注监督微调、强化学习和测试时扩展等技术。视频理解是计算机视觉中最具挑战性的领域，需要模型处理复杂的时空关系和多模态证据。我们提供了一个结构化的分类法，阐明了这些技术的角色和相互关系，并解决了视频特有的挑战。通过系统分析代表性方法，我们总结了关键设计原则和评估协议，以推动Video-LMM的能力提升。'}}}, {'id': 'https://huggingface.co/papers/2510.03632', 'title': 'MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual\n  Information', 'url': 'https://huggingface.co/papers/2510.03632', 'abstract': 'Mutual Information Tree Search (MITS) uses information-theoretic principles to guide and evaluate reasoning paths in large language models, improving performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.', 'score': 11, 'issue_id': 6277, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '1a4a298f833dcbd5', 'authors': ['Jiaxi Li', 'Yucheng Shi', 'Jin Lu', 'Ninghao Liu'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Georgia'], 'pdf_title_img': 'assets/pdf/title_img/2510.03632.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#reasoning', '#training', '#architecture'], 'emoji': '🌳', 'ru': {'title': 'Информационная теория направляет рассуждения LLM через умный древовидный поиск', 'desc': 'Статья представляет MITS — новый метод древовидного поиска для улучшения рассуждений в LLM, основанный на информационно-теоретических принципах. Авторы используют pointwise mutual information (PMI) для пошаговой оценки качества путей рассуждений и эффективного расширения дерева поиска через beam search, избегая дорогостоящих симуляций. Метод дополнен динамической стратегией сэмплирования на основе энтропии, которая адаптивно распределяет вычислительные ресурсы на наиболее неопределённые шаги рассуждений. Эксперименты показывают, что MITS превосходит базовые методы на различных бенчмарках, обеспечивая эффективное и принципиальное решение для reasoning задач в LLM.'}, 'en': {'title': 'Enhancing LLM Reasoning with Mutual Information', 'desc': 'Mutual Information Tree Search (MITS) enhances reasoning in large language models by applying information-theoretic principles. It introduces a scoring function based on pointwise mutual information (PMI) to evaluate reasoning paths effectively, allowing for efficient tree search without costly simulations. MITS also uses an entropy-based dynamic sampling strategy to focus computational resources on the most uncertain steps, improving exploration. Overall, MITS demonstrates superior performance in reasoning tasks compared to traditional methods, making it a robust framework for LLMs.'}, 'zh': {'title': '互信息树搜索：高效推理的新方法', 'desc': '互信息树搜索（MITS）利用信息论原理来指导和评估大型语言模型中的推理路径，从而提高性能和效率。该方法引入了一种基于点对点互信息（PMI）的有效评分函数，使得推理路径的逐步评估和搜索树的扩展变得更加高效。MITS还采用了一种基于熵的动态采样策略，能够自适应地分配计算资源到不确定的推理步骤上，以实现更有利的探索。通过在多种推理基准上的全面实验，MITS始终超越基线方法，建立了一个原则性和高效的LLM推理框架。'}}}, {'id': 'https://huggingface.co/papers/2510.00263', 'title': 'Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions', 'url': 'https://huggingface.co/papers/2510.00263', 'abstract': "A framework for calibrating probabilistic autoraters to preference distributions using supervised fine-tuning and reinforcement learning improves alignment with human values and reduces bias.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.", 'score': 11, 'issue_id': 6275, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '96cee62eae60ad82', 'authors': ['Zhuohang Li', 'Xiaowei Li', 'Chengyu Huang', 'Guowang Li', 'Katayoon Goshvadi', 'Bo Dai', 'Dale Schuurmans', 'Paul Zhou', 'Hamid Palangi', 'Yiwen Song', 'Palash Goyal', 'Murat Kantarcioglu', 'Bradley A. Malin', 'Yuan Xue'], 'affiliations': ['Cornell University', 'Google', 'Google DeepMind', 'Scale AI', 'University of Alberta', 'Vanderbilt University', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2510.00263.jpg', 'data': {'categories': ['#alignment', '#training', '#ethics', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Автооценщики, настроенные на распределение предпочтений людей', 'desc': 'Статья предлагает framework для калибровки вероятностных автооценщиков (autoraters) - LLM, которые автоматически оценивают ответы других моделей. Вместо обучения на дискретных метках авторы учат модели предсказывать полное распределение предпочтений целевой аудитории людей. Для этого используются два подхода: supervised fine-tuning для плотных вероятностных меток и reinforcement learning для разреженных бинарных меток. Результаты показывают улучшенную калибровку, снижение позиционного bias и лучшее alignment с человеческими ценностями при сохранении качества на объективных задачах.'}, 'en': {'title': 'Aligning Autoraters with Human Preferences through Advanced Calibration', 'desc': 'This paper presents a framework for improving the accuracy of automated judges, known as autoraters, which evaluate preferences in a way that aligns better with human values. The authors highlight the limitations of traditional autoraters that rely on fixed preference labels, which can oversimplify complex human judgments. They propose two methods for training these autoraters: one using supervised fine-tuning for detailed preference data and another using reinforcement learning for simpler binary data. The results demonstrate that their approach enhances the alignment of predictions with actual human preferences, reduces bias, and maintains performance on objective tasks.'}, 'zh': {'title': '校准自动评分器以对齐人类价值观', 'desc': '本文提出了一种框架，用于通过监督微调和强化学习来校准概率自动评分器，以更好地与人类价值观对齐并减少偏见。我们认为，可靠的自动评分器必须学习建模目标人群定义的完整偏好分布，而不是仅依赖于离散的偏好标签。我们提出了两种学习方法，分别适用于不同的数据条件：一种是针对密集概率标签的直接监督微调，另一种是针对稀疏二元标签的强化学习方法。实验证明，使用分布匹配目标微调自动评分器可以提高其预测的概率与目标偏好分布的对齐程度，同时降低位置偏见。'}}}, {'id': 'https://huggingface.co/papers/2510.05091', 'title': 'Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals', 'url': 'https://huggingface.co/papers/2510.05091', 'abstract': 'A comprehensive investigation into generating and editing structured visuals using a unified model integrating a VLM with FLUX Kontext, achieving strong performance and introducing a new benchmark and evaluation metric.  \t\t\t\t\tAI-generated summary \t\t\t\t While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q\\&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.', 'score': 8, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '9350bd6b875c71b5', 'authors': ['Le Zhuo', 'Songhao Han', 'Yuandong Pu', 'Boxiang Qiu', 'Sayak Paul', 'Yue Liao', 'Yihao Liu', 'Jie Shao', 'Xi Chen', 'Si Liu', 'Hongsheng Li'], 'affiliations': ['Beihang University', 'ByteDance', 'CUHK MMLab', 'Hugging Face', 'Krea AI', 'National University of Singapore', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.05091.jpg', 'data': {'categories': ['#benchmark', '#training', '#survey', '#games', '#reasoning', '#dataset', '#data', '#multimodal', '#open_source', '#optimization', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'Единая модель для генерации и редактирования структурированных изображений', 'desc': 'Современные модели генерации изображений хорошо справляются с естественными картинками, но испытывают трудности со структурированными визуальными элементами вроде графиков, диаграмм и математических фигур. Исследователи создали датасет из 1.3 миллиона пар изображений с разметкой chain-of-thought рассуждений и обучили унифицированную модель, интегрирующую VLM с FLUX.1 Kontext через лёгкий коннектор. Для оценки качества представлен новый бенчмарк StructBench с 1700 сложными примерами и метрика StructScore, использующая многораундовый протокол вопросов-ответов для проверки фактической точности. Результаты показывают, что даже лучшие closed-source системы далеки от идеала, а добавление рассуждений на этапе инференса стабильно улучшает качество для разных архитектур.'}, 'en': {'title': 'Advancing Structured Visuals with Unified Multimodal Models', 'desc': 'This paper explores the generation and editing of structured visuals, such as charts and diagrams, using a unified model that combines a Visual Language Model (VLM) with FLUX Kontext. The authors created a large dataset of 1.3 million structured image pairs to train their model, which incorporates a three-stage training process for better feature alignment and reasoning capabilities. They also introduce StructBench, a new benchmark with over 1,700 instances and a unique evaluation metric called StructScore to measure factual accuracy. The results show that their model outperforms existing systems in editing structured visuals, highlighting the need for improved multimodal understanding in this area.'}, 'zh': {'title': '统一模型推动结构化视觉生成与编辑的突破', 'desc': '本论文全面研究了生成和编辑结构化视觉内容的方法，提出了一种将视觉语言模型（VLM）与FLUX Kontext结合的统一模型。我们构建了一个包含130万对高质量结构图像的数据集，并通过链式思维注释进行增强。通过三阶段的训练课程，我们实现了特征对齐和知识注入，提升了多模态理解能力。最后，我们推出了StructBench基准和StructScore评估指标，以评估生成和编辑的准确性，推动结构化视觉内容的研究进展。'}}}, {'id': 'https://huggingface.co/papers/2510.04800', 'title': 'Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights', 'url': 'https://huggingface.co/papers/2510.04800', 'abstract': 'A comprehensive evaluation of hybrid language models combining self-attention with structured state space models, analyzing inter-layer and intra-layer fusion strategies, and providing design recommendations.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.', 'score': 7, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '4c682fe0f2e8d908', 'authors': ['Sangmin Bae', 'Bilge Acun', 'Haroun Habeeb', 'Seungyeon Kim', 'Chien-Yu Lin', 'Liang Luo', 'Junjie Wang', 'Carole-Jean Wu'], 'affiliations': ['FAIR at Meta', 'KAIST', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2510.04800.jpg', 'data': {'categories': ['#architecture', '#long_context', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Оптимальный рецепт гибридных архитектур: как правильно смешивать attention и Mamba', 'desc': 'Исследователи провели системное сравнение гибридных архитектур языковых моделей, которые комбинируют механизм self-attention с моделями структурированного пространства состояний (Mamba). Они проанализировали два подхода к гибридизации: последовательное объединение слоёв (inter-layer) и параллельное внутри слоя (intra-layer). Оценка проводилась по множеству критериев: качество языкового моделирования, работа с длинным контекстом, масштабируемость и эффективность обучения. На основе анализа авторы выявили ключевые факторы успеха каждой стратегии и предложили оптимальные рецепты дизайна гибридных LLM.'}, 'en': {'title': 'Optimizing Hybrid Language Models for Efficiency and Performance', 'desc': 'This paper evaluates hybrid language models that combine self-attention mechanisms with structured state space models to improve performance on long-context tasks. It analyzes different fusion strategies, both inter-layer and intra-layer, to understand their impact on language modeling quality and computational efficiency. The authors provide a systematic comparison of these hybridization strategies and identify key factors that contribute to their effectiveness. Additionally, they offer design recommendations to optimize the architecture of hybrid models for better performance and efficiency.'}, 'zh': {'title': '优化混合语言模型的设计策略', 'desc': '本论文全面评估了结合自注意力机制和结构状态空间模型的混合语言模型，分析了层间和层内融合策略。研究表明，这些混合架构在建模质量和计算效率之间取得了良好的平衡，尤其适用于长上下文任务。我们从语言建模性能、长上下文能力、扩展分析以及训练和推理效率等多个角度评估这些设计。通过研究其计算原语的核心特征，我们识别出每种混合策略的关键要素，并提出了优化设计建议。'}}}, {'id': 'https://huggingface.co/papers/2510.04996', 'title': 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training', 'url': 'https://huggingface.co/papers/2510.04996', 'abstract': 'Reinforce-Ada is an adaptive sampling framework for online reinforcement learning post-training of large language models, which accelerates convergence and improves performance by dynamically reallocating sampling effort based on prompt uncertainty.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.', 'score': 6, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'ce0c9b530b9743c2', 'authors': ['Wei Xiong', 'Chenlu Ye', 'Baohao Liao', 'Hanze Dong', 'Xinxing Xu', 'Christof Monz', 'Jiang Bian', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['Microsoft Research', 'University of Amsterdam', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.04996.jpg', 'data': {'categories': ['#rlhf', '#training', '#rl', '#reasoning', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Умное распределение ресурсов: адаптивный сэмплирование для RL-обучения LLM', 'desc': 'Reinforce-Ada — это адаптивный метод для post-training больших языковых моделей с помощью reinforcement learning, который динамически перераспределяет вычислительные ресурсы между промптами в зависимости от их неопределённости. В отличие от традиционных подходов с фиксированной выборкой, метод чередует оценку и сэмплирование в онлайн-режиме, автоматически останавливая выборку для промпта после получения достаточного сигнала. Для стабилизации обучения используются группы фиксированного размера с разнообразными наградами и глобальная статистика для вычисления advantage baseline. Эксперименты показывают ускоренную сходимость и улучшенную производительность по сравнению с GRPO на задачах reasoning для LLM различных архитектур.'}, 'en': {'title': 'Adaptive Sampling for Faster Learning in Language Models', 'desc': 'Reinforce-Ada is a new framework designed to improve online reinforcement learning for large language models (LLMs) by adapting how sampling is done based on the uncertainty of prompts. It addresses the problem of unstable gradient estimates that arise from fixed sampling methods by reallocating resources to prompts that need more attention. This framework uses an online process to continuously estimate and sample, stopping when enough information is gathered, which helps stabilize learning. The results show that Reinforce-Ada not only speeds up the learning process but also enhances the overall performance of LLMs on reasoning tasks compared to previous methods.'}, 'zh': {'title': '自适应采样，提升强化学习效率', 'desc': 'Reinforce-Ada 是一种自适应采样框架，旨在加速大型语言模型的在线强化学习后训练。它通过根据提示的不确定性动态重新分配采样工作量，从而提高收敛速度和性能。与传统的两阶段分配方法不同，Reinforce-Ada 在在线逐步消除过程中交替进行估计和采样，并在收集到足够信号后自动停止对某个提示的采样。实验证明，Reinforce-Ada 在多个模型架构和推理基准上表现出色，尤其是在使用平衡采样变体时。'}}}, {'id': 'https://huggingface.co/papers/2510.03264', 'title': 'Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data', 'url': 'https://huggingface.co/papers/2510.03264', 'abstract': 'Introducing reasoning data during pretraining significantly enhances LLM performance compared to post-training, with pretraining benefiting more from diverse data patterns while SFT benefits more from high-quality data.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.', 'score': 3, 'issue_id': 6275, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '4ab12dcfe1afbbf7', 'authors': ['Syeda Nahida Akter', 'Shrimai Prabhumoye', 'Eric Nyberg', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Yejin Choi', 'Bryan Catanzaro'], 'affiliations': ['Boston University', 'Carnegie Mellon University', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2510.03264.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Учить рассуждать нужно с самого начала', 'desc': 'Исследование показывает, что добавление данных для обучения рассуждениям на этапе pretraining значительно эффективнее (прирост 19%), чем только на этапе post-training, создавая фундаментальные способности, которые невозможно полностью восстановить последующим fine-tuning. Обнаружен асимметричный принцип: pretraining больше выигрывает от разнообразия паттернов рассуждений (прирост 11%), тогда как supervised fine-tuning более чувствителен к качеству данных (прирост 15%). Высококачественные данные на этапе pretraining имеют латентный эффект, активирующийся только после SFT, а избыточное масштабирование SFT-данных может быть вредным. Результаты бросают вызов традиционному разделению языкового моделирования и обучения рассуждениям, предлагая стратегический подход к распределению данных на всех этапах обучения LLM.'}, 'en': {'title': 'Front-Load Reasoning for Stronger LLMs!', 'desc': 'This paper investigates the impact of introducing reasoning data during the pretraining phase of large language models (LLMs) compared to post-training. The authors find that incorporating diverse reasoning data early in pretraining leads to significant performance improvements, establishing foundational reasoning capabilities that are not fully recoverable through later fine-tuning. They highlight that pretraining benefits from a variety of reasoning patterns, while fine-tuning is more effective with high-quality data. The study challenges traditional views on language modeling and reasoning, offering insights on optimal data allocation throughout the training process.'}, 'zh': {'title': '提前引入推理数据，提升模型性能！', 'desc': '本研究探讨了在预训练阶段引入推理数据对大型语言模型（LLM）性能的影响。研究发现，提前在预训练中加入推理数据可以显著提高模型性能，平均提升19%。此外，预训练阶段更依赖于推理模式的多样性，而微调阶段则更注重数据的质量。我们的结果挑战了语言建模与推理的传统分离，为数据在整个训练过程中的合理分配提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2510.05094', 'title': 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation', 'url': 'https://huggingface.co/papers/2510.05094', 'abstract': 'VChain enhances video generation by integrating visual reasoning from multimodal models to guide sparse tuning of a pre-trained video generator.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos.', 'score': 2, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'ba8508ce929b551f', 'authors': ['Ziqi Huang', 'Ning Yu', 'Gordon Chen', 'Haonan Qiu', 'Paul Debevec', 'Ziwei Liu'], 'affiliations': ['Eyeline Labs', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05094.jpg', 'data': {'categories': ['#games', '#inference', '#multimodal', '#video', '#optimization'], 'emoji': '🔗', 'ru': {'title': 'Визуальное мышление для улучшения видеогенерации', 'desc': 'VChain — это новый подход, который использует мультимодальные модели (например, GPT-4o) для улучшения генерации видео. Мультимодальные LLM генерируют ключевые кадры, которые служат «снимками» важных моментов в видео. Эти ключевые кадры затем используются для точечной настройки предобученного видеогенератора только в критических точках временной последовательности. Метод эффективен с точки зрения вычислительных затрат и значительно улучшает качество сгенерированных видео в сложных многошаговых сценариях.'}, 'en': {'title': 'Enhancing Video Generation with Visual Reasoning', 'desc': 'VChain is a new framework that improves video generation by using visual reasoning from multimodal models. It addresses the challenge of creating coherent video sequences by focusing on key moments in the video. By generating important keyframes, VChain guides the tuning of a pre-trained video generator, making the process more efficient. This method enhances the quality of videos while minimizing the need for extensive supervision and computational resources.'}, 'zh': {'title': 'VChain：提升视频生成的新方法', 'desc': 'VChain是一种新颖的视频生成框架，通过整合多模态模型的视觉推理来指导预训练视频生成器的稀疏调优。传统的视频生成模型在合成复杂动态时常常面临挑战，而VChain利用大型多模态模型的视觉状态推理能力来改善这一问题。该方法通过生成关键帧快照，帮助在特定时刻进行稀疏推理调优，从而提高生成视频的质量。实验结果表明，VChain在复杂的多步骤场景中显著提升了生成视频的效果。'}}}, {'id': 'https://huggingface.co/papers/2510.05069', 'title': 'SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs', 'url': 'https://huggingface.co/papers/2510.05069', 'abstract': 'SwiReasoning, a training-free framework for LLMs, dynamically switches between explicit and latent reasoning to improve accuracy and token efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.', 'score': 2, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'd568b729fb721dc9', 'authors': ['Dachuan Shi', 'Abedelkadir Asi', 'Keying Li', 'Xiangchi Yuan', 'Leyan Pan', 'Wenke Lee', 'Wen Xiao'], 'affiliations': ['Georgia Tech', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.05069.jpg', 'data': {'categories': ['#benchmark', '#training', '#reasoning', '#math', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Умное переключение между явным и скрытым рассуждением для эффективности LLM', 'desc': 'SwiReasoning — это framework без обучения для LLM, который динамически переключается между явным рассуждением (chain-of-thought) и скрытым рассуждением в латентном пространстве. Переключение управляется оценкой уверенности на основе энтропии распределений следующих токенов, что помогает балансировать исследование и эксплуатацию. Ограничение максимального числа переключений предотвращает «overthinking» и повышает эффективность использования токенов. На математических и STEM бенчмарках метод улучшает точность на 1.5-2.8% и повышает эффективность токенов на 56-79% при ограниченных бюджетах.'}, 'en': {'title': 'SwiReasoning: Smart Switching for Efficient LLM Reasoning', 'desc': 'SwiReasoning is a novel framework designed for large language models (LLMs) that enhances reasoning capabilities without the need for training. It intelligently alternates between explicit reasoning, which uses clear steps, and latent reasoning, which operates in a more abstract space, to optimize both accuracy and token usage. The framework addresses challenges such as the dilution of probability mass in latent reasoning and the inefficiencies caused by excessive reasoning steps. By implementing a dynamic switching mechanism and limiting the number of reasoning transitions, SwiReasoning significantly boosts performance on mathematical and STEM tasks while improving token efficiency.'}, 'zh': {'title': 'SwiReasoning：动态推理，提升效率与准确性', 'desc': 'SwiReasoning 是一个无需训练的框架，旨在提高大型语言模型（LLMs）的推理能力。它通过动态切换显性推理和潜在推理，来平衡探索与利用，从而提高准确性和令牌效率。该框架解决了潜在推理中的两个主要挑战：过多的隐式路径导致的准确性下降和过度思考造成的令牌浪费。实验结果表明，SwiReasoning 在数学和STEM基准测试中，平均准确率提高了1.5%-2.8%，并在预算受限的情况下，令牌效率提高了56%-79%。'}}}, {'id': 'https://huggingface.co/papers/2510.04673', 'title': 'Watch and Learn: Learning to Use Computers from Online Videos', 'url': 'https://huggingface.co/papers/2510.04673', 'abstract': "Watch & Learn converts web demonstration videos into UI trajectories to enhance computer use agents, improving both in-context demonstrations and supervised training.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, we introduce Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, we cast the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, we develop an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment.", 'score': 2, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '73bca360494694a1', 'authors': ['Chan Hee Song', 'Yiwen Song', 'Palash Goyal', 'Yu Su', 'Oriana Riva', 'Hamid Palangi', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2510.04673.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#dataset', '#data', '#open_source', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Обучение AI-агентов управлению компьютером через просмотр видео из интернета', 'desc': 'Статья представляет фреймворк Watch & Learn, который преобразует видео с демонстрациями работы в интернете в исполняемые UI-траектории для обучения агентов управления компьютером. Вместо прямой генерации траекторий используется подход inverse dynamics: предсказание действий пользователя по последовательным состояниям экрана. На основе этого метода создано более 53 тысяч высококачественных траекторий из веб-видео, которые улучшают работу агентов как in-context примеры и как данные для supervised обучения. Результаты на бенчмарке OSWorld показывают значительное улучшение производительности, особенно для open-source моделей.'}, 'en': {'title': 'Transforming Web Videos into Actionable UI Trajectories for Smart Agents', 'desc': 'The paper presents Watch & Learn (W&L), a framework that transforms web demonstration videos into usable UI trajectories for computer use agents (CUAs). This approach addresses the challenge of limited high-quality training data by leveraging readily available online videos, which are converted into executable actions through an inverse dynamics objective. By predicting user actions from screen states, W&L simplifies the learning process and enhances the generalization of CUAs across various applications. The framework has successfully generated over 53,000 high-quality trajectories, significantly improving the performance of CUAs in both in-context demonstrations and supervised training scenarios.'}, 'zh': {'title': '利用网络视频提升计算机使用代理的学习能力', 'desc': 'Watch & Learn（W&L）是一个将网络演示视频转换为可执行用户界面（UI）轨迹的框架，旨在提升计算机使用代理（CUA）的学习效果。该方法通过逆动力学目标来预测用户在连续屏幕状态下的动作，从而减少了手动工程的需求，并提高了学习的效率和泛化能力。W&L生成了超过53,000条高质量的UI轨迹，这些轨迹在上下文演示和监督训练中均显著提升了CUA的表现。研究结果表明，网络规模的人类演示视频为CUA的实际应用提供了一个可行且可扩展的基础。'}}}, {'id': 'https://huggingface.co/papers/2510.03561', 'title': 'Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models', 'url': 'https://huggingface.co/papers/2510.03561', 'abstract': 'The Reactive Transformer (RxT) addresses the limitations of stateless Transformers in conversational AI by using an event-driven paradigm with a fixed-size Short-Term Memory (STM) system, achieving linear scaling and low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.', 'score': 2, 'issue_id': 6275, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'b213f271f5c52cec', 'authors': ['Adam Filipek'], 'affiliations': ['Reactive AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.03561.jpg', 'data': {'categories': ['#long_context', '#training', '#architecture', '#synthetic'], 'emoji': '🔄', 'ru': {'title': 'Реактивный Transformer: постоянная память для экономичных диалогов', 'desc': 'Авторы предлагают архитектуру Reactive Transformer (RxT), которая решает проблему обработки длинных диалогов в conversational AI. В отличие от обычных Transformer моделей, которые обрабатывают всю историю разговора заново на каждом шаге, RxT использует event-driven подход с фиксированной кратковременной памятью (STM). Это снижает вычислительную сложность с квадратичной O(N²·T) до линейной O(N·T) относительно числа взаимодействий, обеспечивая низкую задержку и экономичность. Архитектура разделяет генерацию ответа и асинхронное обновление памяти, что позволяет вести долгие диалоги в реальном времени с постоянными затратами на каждый шаг.'}, 'en': {'title': 'Revolutionizing Conversational AI with Reactive Transformers', 'desc': 'The Reactive Transformer (RxT) is a new architecture designed to improve conversational AI by addressing the limitations of traditional stateless Transformers. It uses an event-driven approach combined with a fixed-size Short-Term Memory (STM) system, which allows for linear scaling and reduced latency during interactions. By processing each conversational turn as a discrete event, RxT maintains context efficiently and updates memory asynchronously, leading to faster response times. Experimental results show that RxT outperforms stateless models in terms of performance and inference speed, making it suitable for real-time, long-form conversations.'}, 'zh': {'title': '反应式变换器：实现实时对话的创新架构', 'desc': '反应式变换器（RxT）通过使用事件驱动的范式和固定大小的短期记忆（STM）系统，解决了无状态变换器在对话AI中的局限性。与传统模型相比，RxT能够以线性方式扩展，并显著降低延迟。该架构将每个对话轮次视为实时的离散事件，保持上下文的同时，优化了内存更新过程。通过将响应生成与内存更新解耦，RxT实现了真正的实时对话，适用于长时间的交互。'}}}, {'id': 'https://huggingface.co/papers/2509.24613', 'title': 'HiKE: Hierarchical Evaluation Framework for Korean-English\n  Code-Switching Speech Recognition', 'url': 'https://huggingface.co/papers/2509.24613', 'abstract': "A hierarchical benchmark for Korean-English code-switching in ASR evaluates model performance and demonstrates improvement through fine-tuning with code-switched data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that while most multilingual ASR models initially struggle with CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE will be available at https://github.com/ThetaOne-AI/HiKE.", 'score': 2, 'issue_id': 6276, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '1983451336b95e80', 'authors': ['Gio Paik', 'Yongbeom Kim', 'Soungmin Lee', 'Sangmin Ahn', 'Chanwoo Kim'], 'affiliations': ['Georgia Institute of Technology', 'Seoul National University', 'Theta One AI', 'Williams College'], 'pdf_title_img': 'assets/pdf/title_img/2509.24613.jpg', 'data': {'categories': ['#benchmark', '#training', '#low_resource', '#dataset', '#audio', '#machine_translation', '#multilingual'], 'emoji': '🔀', 'ru': {'title': 'HiKE: иерархический бенчмарк для code-switching в корейско-английской речи', 'desc': 'Исследователи представили HiKE — первый публично доступный бенчмарк для оценки систем автоматического распознавания речи (ASR) на корейско-английском code-switching (переключении языков внутри высказывания). Бенчмарк включает высококачественные естественные данные с переключением языков на разных уровнях: слово, фраза и предложение, а также метки заимствованных слов. Эксперименты показали, что большинство мультиязычных ASR-моделей изначально плохо справляются с распознаванием code-switching, но их производительность значительно улучшается после fine-tuning на специализированных данных. Бенчмарк предоставляет систематический инструмент для оценки способности моделей обрабатывать переключение языков на различных уровнях сложности.'}, 'en': {'title': 'Unlocking Code-Switching: HiKE for Enhanced ASR Performance', 'desc': 'This paper presents HiKE, a new benchmark for evaluating Korean-English code-switching in automatic speech recognition (ASR). Code-switching, where speakers mix languages in conversation, poses significant challenges for ASR systems. The HiKE framework includes high-quality code-switched data and a detailed labeling system to assess model performance at different levels of code-switching. The study shows that fine-tuning ASR models with code-switched data can significantly improve their performance in handling this complex linguistic phenomenon.'}, 'zh': {'title': '提升多语言ASR模型的代码切换能力', 'desc': '这篇论文介绍了一个名为HiKE的层次化基准，用于评估韩英代码切换的自动语音识别（ASR）模型性能。代码切换是指在日常交流中混合使用多种语言的现象，然而在多语言ASR领域，这一挑战仍然未被充分研究。HiKE提供了高质量的自然代码切换数据，并采用了细致的借用词标签和层次化的代码切换标注方案，以便系统地评估模型处理不同层次代码切换的能力。通过对多种多语言ASR模型的评估和微调实验，论文表明，尽管大多数模型在初始阶段对代码切换的识别能力较弱，但通过使用代码切换数据进行微调，可以显著提升其性能。'}}}, {'id': 'https://huggingface.co/papers/2510.04618', 'title': 'Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models', 'url': 'https://huggingface.co/papers/2510.04618', 'abstract': 'ACE, a framework for adaptive context engineering, enhances LLM applications by preserving detailed knowledge through structured updates, outperforming baselines in agent and domain-specific tasks with reduced adaptation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.', 'score': 1, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'ad90fc3ef9ebce55', 'authors': ['Qizheng Zhang', 'Changran Hu', 'Shubhangi Upasani', 'Boyuan Ma', 'Fenglu Hong', 'Vamsidhar Kamanuru', 'Jay Rainton', 'Chen Wu', 'Mengmeng Ji', 'Hanchen Li', 'Urmish Thakker', 'James Zou', 'Kunle Olukotun'], 'affiliations': ['SambaNova Systems, Inc.', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2510.04618.jpg', 'data': {'categories': ['#training', '#multimodal', '#open_source', '#agents', '#optimization', '#long_context'], 'emoji': '📚', 'ru': {'title': 'Контекст как живой учебник: адаптивное обучение LLM без обновления весов', 'desc': 'ACE (Agentic Context Engineering) — это фреймворк для адаптации LLM через модификацию контекста вместо обновления весов модели. Метод решает проблему «схлопывания контекста», когда при итеративной переписке теряются важные детали, используя структурированные инкрементальные обновления. ACE работает как эволюционирующий «учебник стратегий», который накапливает, уточняет и организует знания через генерацию, рефлексию и курирование. Фреймворк показывает прирост +10.6% на агентских задачах и +8.6% на финансовых задачах, при этом значительно снижая затраты на адаптацию и работая даже без размеченных данных.'}, 'en': {'title': 'ACE: Evolving Contexts for Enhanced LLM Performance', 'desc': 'The paper introduces ACE, a framework designed for adaptive context engineering in large language model (LLM) applications. ACE enhances the performance of agents and domain-specific tasks by maintaining detailed knowledge through structured updates, avoiding issues like brevity bias and context collapse. It utilizes a modular process of generation, reflection, and curation to treat contexts as evolving playbooks, which allows for efficient scaling with long-context models. The results demonstrate that ACE significantly outperforms existing methods while reducing adaptation costs and latency, proving its effectiveness in real-world applications without the need for labeled supervision.'}, 'zh': {'title': 'ACE：自适应上下文工程的创新框架', 'desc': 'ACE是一个自适应上下文工程框架，旨在增强大型语言模型（LLM）应用的性能。它通过结构化更新来保留详细知识，避免了以往方法中常见的简洁偏见和上下文崩溃问题。ACE将上下文视为不断演变的剧本，通过生成、反思和策划的模块化过程来积累和组织策略。实验结果表明，ACE在代理和特定领域任务中表现优异，显著提高了适应性和效率。'}}}, {'id': 'https://huggingface.co/papers/2510.04399', 'title': 'Utility-Learning Tension in Self-Modifying Agents', 'url': 'https://huggingface.co/papers/2510.04399', 'abstract': 'Self-improving systems face a utility-learning tension that can degrade their ability to learn and generalize, requiring capacity bounds to ensure safe self-modification.  \t\t\t\t\tAI-generated summary \t\t\t\t As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.', 'score': 1, 'issue_id': 6275, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '9fa188fee82ece5c', 'authors': ['Charles L. Wang', 'Keir Dorchen', 'Peter Jin'], 'affiliations': ['Carnegie Mellon University', 'DeepMind', 'ETH Zurich', 'Google Brain', 'IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale)', 'Max Planck Institute for Intelligent Systems', 'New York University', 'SingularityNET', 'Stanford University', 'Technische Universität München', 'University of Amsterdam', 'University of Bath', 'University of California, Berkeley', 'University of Cambridge', 'University of Edinburgh', 'University of Freiburg', 'University of Montreal (MILA)', 'University of Oxford', 'University of Toronto', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.04399.jpg', 'data': {'categories': ['#agents', '#alignment', '#agi', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Парадокс самосовершенствования: как AI может разучиться учиться', 'desc': 'Исследователи формализовали проблему самомодифицирующихся AI-систем, стремящихся к сверхинтеллекту. Они обнаружили фундаментальное противоречие: изменения, улучшающие текущую производительность системы, могут разрушить её способность к обучению и генерализации в будущем. Математически доказано, что безопасная самомодификация возможна только при ограничении ёмкости (capacity) модели - без таких ограничений система может сделать обучаемые задачи необучаемыми. Авторы предложили политики с двойным контролем, которые сохраняют способность к обучению при самомодификации системы.'}, 'en': {'title': 'Balancing Improvement and Learning in Self-Modifying AI Systems', 'desc': "This paper discusses the challenges faced by self-improving AI systems, particularly the conflict between improving performance (utility) and maintaining the ability to learn effectively. It introduces a framework that separates different aspects of self-modification, allowing for a clearer analysis of how changes can impact learning. The authors identify a critical tension where beneficial changes can lead to a decline in the system's ability to generalize from data. They propose that to ensure safe self-modification, the system's capacity for change must be limited, and they validate their findings through numerical experiments comparing different modification strategies."}, 'zh': {'title': '自我改进系统的效用与学习的平衡', 'desc': '自我改进系统面临效用学习的紧张关系，这可能会降低其学习和泛化能力。本文通过五个维度的分解和决策层的形式化，分析了激励与学习行为的分离。我们的主要结果揭示了效用与学习之间的结构性冲突，表明效用驱动的变化可能会破坏可靠学习和泛化的统计前提。研究表明，当模型的容量无限增长时，效用理性的自我变化可能使可学习的任务变得不可学习。'}}}, {'id': 'https://huggingface.co/papers/2510.04016', 'title': 'Thai Semantic End-of-Turn Detection for Real-Time Voice Agents', 'url': 'https://huggingface.co/papers/2510.04016', 'abstract': 'Real-time Thai text-only end-of-turn detection using zero-shot and few-shot prompting of compact LLMs and lightweight transformers achieves near-instant accuracy suitable for on-device agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.', 'score': 1, 'issue_id': 6277, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': 'dd84047ca08dcb3a', 'authors': ['Thanapol Popit', 'Natthapath Rungseesiripak', 'Monthol Charattrakool', 'Saksorn Ruangtanusak'], 'affiliations': ['Department of Computer Engineering KMUTT Bangkok, Thailand', 'Innovation Lab SCBX Bangkok, Thailand', 'R&D SCBX Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2510.04016.jpg', 'data': {'categories': ['#low_resource', '#audio', '#small_models', '#agents', '#dataset', '#training'], 'emoji': '🇹🇭', 'ru': {'title': 'Мгновенное определение конца реплики для тайского языка', 'desc': 'Исследователи разработали систему для определения момента, когда пользователь закончил говорить, специально для тайского языка в голосовых ассистентах реального времени. Традиционные методы, основанные на паузах в аудио, добавляют задержки в сотни миллисекунд и плохо работают с особенностями языка. Авторы сравнили zero-shot и few-shot промптинг компактных LLM с файн-тюнингом лёгких трансформеров на текстовых данных из корпуса субтитров YODAS. Небольшие дообученные модели показали точность, достаточную для работы на пользовательских устройствах с минимальной задержкой.'}, 'en': {'title': 'Real-Time Thai Speech End Detection with Compact Models', 'desc': "This paper presents a novel approach for detecting the end of a user's speech in Thai using text-only methods. It explores zero-shot and few-shot prompting techniques with compact language models (LLMs) and compares them to traditional supervised fine-tuning of lightweight transformers. The study utilizes the YODAS corpus and incorporates Thai linguistic features to improve accuracy in real-time applications. The findings highlight a balance between accuracy and latency, establishing a baseline for Thai end-of-turn detection suitable for on-device use."}, 'zh': {'title': '实时泰语结束检测的创新研究', 'desc': '本文研究了泰语文本的实时结束检测，旨在提高语音交互的流畅性。我们比较了零样本和少样本提示的紧凑型大语言模型（LLMs）与轻量级变换器的监督微调效果。通过使用YODAS语料库的转录字幕和泰语特有的语言线索，我们将结束检测问题转化为在标记边界上的二元决策。研究结果表明，小型微调模型能够实现近乎即时的结束检测，适合在设备上使用。'}}}, {'id': 'https://huggingface.co/papers/2510.01586', 'title': 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.01586', 'abstract': 'AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework, enhances safety and utility in LLM-based multi-agent systems by internally optimizing task agents against evolving attacks without additional overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.', 'score': 1, 'issue_id': 6277, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '9099d373579a2b51', 'authors': ['Zhenyu Pan', 'Yiting Zhang', 'Zhuo Liu', 'Yolo Yunlong Tang', 'Zeliang Zhang', 'Haozheng Luo', 'Yuwei Han', 'Jianshu Zhang', 'Dennis Wu', 'Hong-Yu Chen', 'Haoran Lu', 'Haoyang Fang', 'Manling Li', 'Chenliang Xu', 'Philip S. Yu', 'Han Liu'], 'affiliations': ['Carnegie Mellon University', 'Northwestern University', 'University of Illinois at Chicago', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2510.01586.jpg', 'data': {'categories': ['#agents', '#security', '#reasoning', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Встроенная безопасность через коэволюцию агентов без дополнительных затрат', 'desc': 'AdvEvo-MARL — это фреймворк для обучения мультиагентных систем на основе LLM, который повышает их безопасность через совместную эволюцию атакующих и защищающихся агентов. Вместо использования внешних модулей-защитников, система интегрирует безопасность непосредственно в задачных агентов через adversarial reinforcement learning. Метод использует общий baseline для оценки advantage внутри функциональных групп агентов, что стабилизирует обучение и улучшает координацию. В результате достигается снижение успешности атак до 20% (против 38% у базовых методов) при сохранении или даже улучшении точности выполнения задач.'}, 'en': {'title': 'Enhancing Safety and Utility in Multi-Agent Systems with AdvEvo-MARL', 'desc': 'AdvEvo-MARL is a co-evolutionary multi-agent reinforcement learning framework designed to enhance safety and utility in large language model (LLM)-based multi-agent systems. It addresses vulnerabilities such as jailbreak and prompt-injection attacks by optimizing both attackers and defenders within the same learning environment, allowing agents to learn to resist evolving threats. Unlike traditional methods that rely on external guards or self-verification, AdvEvo-MARL integrates safety directly into the task agents, reducing system overhead and complexity. The framework demonstrates a significant reduction in attack success rates while maintaining or improving task performance, showcasing a balanced approach to safety and utility in multi-agent systems.'}, 'zh': {'title': '共进化强化学习，提升安全与效用', 'desc': 'AdvEvo-MARL是一种共进化的多智能体强化学习框架，旨在提高基于大型语言模型的多智能体系统的安全性和效用。该框架通过内部优化任务代理，抵御不断演变的攻击，而无需额外的系统开销。与传统的自我验证和外部保护模块不同，AdvEvo-MARL在对抗学习环境中共同优化攻击者和防御者，从而实现更高效的安全防护。实验结果表明，AdvEvo-MARL在多种攻击场景下的攻击成功率低于20%，同时保持或提高了任务的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.00507', 'title': 'Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs', 'url': 'https://huggingface.co/papers/2510.00507', 'abstract': "Graph2Eval, a knowledge graph-based framework, generates multimodal and interactive tasks to comprehensively evaluate agents' reasoning, collaboration, and web interaction capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.", 'score': 1, 'issue_id': 6277, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'a0379f746af10737', 'authors': ['Yurun Chen', 'Xavier Hu', 'Yuhan Liu', 'Ziqi Wang', 'Zeyi Liao', 'Lin Chen', 'Feng Wei', 'Yuxi Qian', 'Bo Zheng', 'Keting Yin', 'Shengyu Zhang'], 'affiliations': ['Ant Group', 'The Ohio State University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00507.jpg', 'data': {'categories': ['#games', '#multimodal', '#synthetic', '#agents', '#dataset', '#benchmark', '#reasoning'], 'emoji': '🕸️', 'ru': {'title': 'Граф знаний для автоматической генерации задач оценки AI-агентов', 'desc': 'Graph2Eval — это фреймворк на основе графов знаний, который автоматически генерирует мультимодальные и интерактивные задачи для комплексной оценки AI-агентов. В отличие от статических датасетов, система создаёт задачи для понимания документов и веб-взаимодействия, используя семплирование подграфов, шаблоны и мета-пути из внешних источников данных. Многоступенчатая фильтрация с помощью LLM обеспечивает качество и выполнимость сгенерированных задач. Эксперименты на Graph2Eval-Bench с 1319 задачами показывают эффективность метода в выявлении различий в способностях к reasoning, коллаборации и веб-взаимодействию у разных типов агентов.'}, 'en': {'title': 'Revolutionizing Agent Evaluation with Graph2Eval', 'desc': "Graph2Eval is a framework that uses knowledge graphs to create diverse tasks for evaluating the reasoning and interaction skills of AI agents. It addresses the limitations of traditional evaluation methods that rely on static datasets, which do not reflect the dynamic nature of real-world tasks. By generating multimodal tasks that involve both document comprehension and web interactions, Graph2Eval allows for a more comprehensive assessment of agents' capabilities. The framework includes a filtering process to ensure the quality of tasks and supports evaluations across different types of agents, revealing insights into their performance in various scenarios."}, 'zh': {'title': 'Graph2Eval：全面评估智能体能力的新框架', 'desc': 'Graph2Eval是一个基于知识图谱的框架，旨在生成多模态和互动任务，以全面评估智能体的推理、协作和网络交互能力。随着多模态大语言模型驱动的智能体在自主性和泛化能力上的不断进步，基于静态数据集的评估方法已无法充分反映其在动态环境和多样任务中的真实能力。Graph2Eval通过构建多源外部数据的知识图谱，将语义关系转化为结构化的多模态任务，并应用多阶段过滤管道确保生成任务的质量和可执行性。该框架支持对多种智能体类型的端到端评估，揭示了不同设置下推理、协作和网络交互能力的差距，为智能体评估提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2510.04434', 'title': 'Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?', 'url': 'https://huggingface.co/papers/2510.04434', 'abstract': 'The study reveals that ACL authors are more likely to address social good concerns in non-ACL venues, and most NLP4SG publications are from non-ACL authors.  \t\t\t\t\tAI-generated summary \t\t\t\t The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.', 'score': 0, 'issue_id': 6276, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '6c05bb00a98e925f', 'authors': ['Grace LeFevre', 'Qingcheng Zeng', 'Adam Leif', 'Jason Jewell', 'Denis Peskoff', 'Rob Voigt'], 'affiliations': ['Northwestern University', 'University of California, Davis', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.04434.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': '🌍', 'ru': {'title': 'NLP для социального блага живёт за пределами ACL', 'desc': 'Исследование анализирует публикации по NLP для социального блага (NLP4SG), связанные с целями устойчивого развития ООН. Оказалось, что авторы из ACL-сообщества чаще публикуют работы по социальным проблемам в других конференциях, а не в самой ACL. Более того, большинство исследований, применяющих NLP для решения социальных задач, выполняются авторами вне ACL-сообщества. Эти находки поднимают важные вопросы о том, как ACL-сообщество формирует повестку в области социально значимых NLP-исследований.'}, 'en': {'title': 'NLP for Social Good: A Call for ACL Engagement', 'desc': 'This study investigates the involvement of authors in the field of Natural Language Processing (NLP) with respect to social good initiatives. It finds that authors affiliated with the Association for Computational Linguistics (ACL) are more inclined to publish work on social good in non-ACL venues. Additionally, a significant portion of NLP for Social Good (NLP4SG) research is conducted by authors who are not part of the ACL community. These findings suggest a need for the ACL to reconsider its role and agenda in promoting social good through NLP.'}, 'zh': {'title': '关注社会公益：超越ACL的自然语言处理', 'desc': '这项研究揭示了ACL作者在非ACL场合更倾向于关注社会公益问题。研究表明，几乎20%的ACL文集中的论文涉及与联合国可持续发展目标相关的社会公益主题。通过分析作者和发表场合，我们发现ACL作者在非ACL场合发表社会公益相关工作的可能性显著更高。大多数使用自然语言处理技术解决社会公益问题的论文来自非ACL作者，这对ACL社区在社会公益议题上的关注具有重要意义。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (7)', '#agi (1)', '#alignment (2)', '#architecture (3)', '#audio (2)', '#benchmark (8)', '#cv', '#data (3)', '#dataset (6)', '#diffusion', '#ethics (2)', '#games (3)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (3)', '#low_resource (2)', '#machine_translation (1)', '#math (1)', '#multilingual (1)', '#multimodal (6)', '#open_source (4)', '#optimization (9)', '#plp', '#rag', '#reasoning (8)', '#rl (3)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (3)', '#synthetic (3)', '#training (12)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-07 04:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-07 04:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-07 04:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    