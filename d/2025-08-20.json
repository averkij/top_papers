{
    "date": {
        "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 20",
        "zh": "8æœˆ20æ—¥"
    },
    "time_utc": "2025-08-20 05:12",
    "weekday": 2,
    "issue_id": 5442,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.13167",
            "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
            "url": "https://huggingface.co/papers/2508.13167",
            "abstract": "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.",
            "score": 25,
            "issue_id": 5440,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "f7b2eb384a807652",
            "authors": [
                "Weizhen Li",
                "Jianbo Lin",
                "Zhuosong Jiang",
                "Jingyi Cao",
                "Xinpeng Liu",
                "Jiayu Zhang",
                "Zhenqiang Huang",
                "Qianben Chen",
                "Weichen Sun",
                "Qiexiang Wang",
                "Hongxuan Lu",
                "Tianrui Qin",
                "Chenghao Zhu",
                "Yi Yao",
                "Shuying Fan",
                "Xiaowan Li",
                "Tiannan Wang",
                "Pai Liu",
                "King Zhu",
                "He Zhu",
                "Dingfeng Shi",
                "Piaohong Wang",
                "Yeyi Guan",
                "Xiangru Tang",
                "Minghao Liu",
                "Yuchen Eleanor Jiang",
                "Jian Yang",
                "Jiaheng Liu",
                "Ge Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.13167.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#agi",
                    "#architecture",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Chain-of-Agents: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Chain-of-Agents (CoA). Ğ­Ñ‚Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾ÑĞ½Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (Agent Foundation Models, AFM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AFM ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering LLMs with Chain-of-Agents for Complex Problem Solving",
                    "desc": "The Chain-of-Agents (CoA) paradigm enhances large language models (LLMs) by enabling them to solve complex problems through dynamic activation of multiple agents. This approach allows for end-to-end problem-solving without the need for manual prompt engineering, making it more efficient and capable. By employing multi-agent distillation and agentic reinforcement learning, the model learns to simulate collaboration among agents effectively. The resulting models, termed Agent Foundation Models (AFMs), achieve state-of-the-art performance across various benchmarks, demonstrating significant advancements in multi-agent systems."
                },
                "zh": {
                    "title": "Chain-of-Agentsï¼šç«¯åˆ°ç«¯å¤æ‚é—®é¢˜è§£å†³çš„æ–°èŒƒå¼",
                    "desc": "Chain-of-Agentsï¼ˆCoAï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„LLMæ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿå®ç°ç«¯åˆ°ç«¯çš„å¤æ‚é—®é¢˜è§£å†³ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€æ¿€æ´»ä¸åŒçš„å·¥å…·ä»£ç†å’Œè§’è‰²æ‰®æ¼”ä»£ç†ï¼Œæ¨¡æ‹Ÿå¤šä»£ç†åä½œï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šä»£ç†è’¸é¦æ¡†æ¶ï¼Œå°†å…ˆè¿›çš„å¤šä»£ç†ç³»ç»Ÿè½¬åŒ–ä¸ºChain-of-Agentsè½¨è¿¹ï¼Œä»¥è¿›è¡Œç›‘ç£å¾®è°ƒã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨å¯éªŒè¯çš„ä»£ç†ä»»åŠ¡ä¸Šåº”ç”¨ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨Chain-of-Agentsé—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14041",
            "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
            "url": "https://huggingface.co/papers/2508.14041",
            "abstract": "LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  \t\t\t\t\tAI-generated summary \t\t\t\t LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/",
            "score": 22,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "87ad1cfb038a78fa",
            "authors": [
                "Chin-Yang Lin",
                "Cheng Sun",
                "Fu-En Yang",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA Research",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14041.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#cv",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LongSplat",
                    "desc": "LongSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾ĞºÑ‚Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ°. LongSplat Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing View Synthesis with LongSplat!",
                    "desc": "LongSplat is a new method for creating novel views from long videos that have irregular camera movements. It tackles problems like pose drift and memory limitations by optimizing camera positions and 3D representations together. The method uses a robust pose estimation technique that relies on learned 3D information and an efficient way to organize data into anchors. Experiments show that LongSplat significantly enhances the quality of rendered images and the accuracy of camera poses compared to existing methods."
                },
                "zh": {
                    "title": "LongSplatï¼šæå‡é•¿è§†é¢‘è§†å›¾åˆæˆçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "LongSplatæ˜¯ä¸€ç§æ–°é¢–çš„è§†å›¾åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨å¤„ç†é•¿è§†é¢‘ä¸­ä¸è§„åˆ™è¿åŠ¨å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡è”åˆä¼˜åŒ–ã€ç¨³å¥çš„å§¿æ€ä¼°è®¡å’Œé«˜æ•ˆçš„é”šç‚¹å½¢æˆæ¥æé«˜åˆæˆè´¨é‡ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–ç›¸æœºå§¿æ€å’Œ3Dé«˜æ–¯æ¨¡å‹ï¼Œé¿å…å±€éƒ¨æœ€å°å€¼å¹¶ç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongSplatåœ¨æ¸²æŸ“è´¨é‡ã€å§¿æ€å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13948",
            "title": "Prompt Orchestration Markup Language",
            "url": "https://huggingface.co/papers/2508.13948",
            "abstract": "POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.",
            "score": 12,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "9520c0e7396f1933",
            "authors": [
                "Yuge Zhang",
                "Nan Chen",
                "Jiahang Xu",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13948.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸ—‚ï¸",
                "ru": {
                    "title": "POML: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ POML - ÑĞ·Ñ‹Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. POML Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞ³Ğ¸ Ğ¸ CSS-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑÑ‚Ğ¸Ğ»ĞµĞ¹. POML Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Streamlining Prompts for Better AI Performance with POML",
                    "desc": "POML, or Prompt Orchestration Markup Language, is designed to enhance the prompting process for Large Language Models (LLMs) by introducing a structured and integrated approach. It addresses the challenges of organizing complex prompts that involve various data types, such as documents and images, while also managing different presentation formats. By utilizing a component-based markup system and specialized tags, POML allows for better data integration and reduces sensitivity to formatting issues. The framework also includes templating features and a developer toolkit to facilitate collaboration and improve version control in real-world applications."
                },
                "zh": {
                    "title": "POMLï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºçš„åˆ©å™¨",
                    "desc": "POMLï¼ˆæç¤ºç¼–æ’æ ‡è®°è¯­è¨€ï¼‰æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æç¤ºè¿‡ç¨‹ä¸­é¢ä¸´çš„ç»“æ„ã€æ•°æ®é›†æˆå’Œæ ¼å¼æ•æ„Ÿæ€§ç­‰æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡ç»„ä»¶åŒ–çš„æ ‡è®°è¯­è¨€æä¾›é€»è¾‘ç»“æ„ï¼Œä½¿ç”¨ä¸“é—¨çš„æ ‡ç­¾å®ç°æ•°æ®çš„æ— ç¼é›†æˆï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼CSSçš„æ ·å¼ç³»ç»Ÿæ¥å‡å°‘æ ¼å¼æ•æ„Ÿæ€§ã€‚POMLè¿˜åŒ…æ‹¬åŠ¨æ€æç¤ºçš„æ¨¡æ¿åŠŸèƒ½å’Œå…¨é¢çš„å¼€å‘è€…å·¥å…·åŒ…ï¼Œä»¥æé«˜ç‰ˆæœ¬æ§åˆ¶å’Œåä½œæ•ˆç‡ã€‚é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼ŒéªŒè¯äº†POMLåœ¨å¤æ‚åº”ç”¨é›†æˆå’Œå‡†ç¡®æ€§è¡¨ç°æ–¹é¢çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09131",
            "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2508.09131",
            "abstract": "ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
            "score": 8,
            "issue_id": 5440,
            "pub_date": "2025-08-12",
            "pub_date_card": {
                "ru": "12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 12",
                "zh": "8æœˆ12æ—¥"
            },
            "hash": "77b10751b41fef03",
            "authors": [
                "Zixin Yin",
                "Xili Dai",
                "Ling-Hao Chen",
                "Deyu Zhou",
                "Jianan Wang",
                "Duomin Wang",
                "Gang Yu",
                "Lionel M. Ni",
                "Lei Zhang",
                "Heung-Yeung Shum"
            ],
            "affiliations": [
                "Astribot",
                "International Digital Economy Academy",
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09131.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "ColorCtrl - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (MM-DiT). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†Ğ²ĞµÑ‚ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ². ColorCtrl Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Precision Color Editing with Zero Training",
                    "desc": "ColorCtrl is a novel method for color editing in images and videos that does not require prior training. It utilizes Multi-Modal Diffusion Transformers to achieve precise control over color attributes while maintaining the physical consistency of the scene. By manipulating attention maps and value tokens, ColorCtrl allows users to edit specific regions of an image based on textual prompts, ensuring that unrelated areas remain unchanged. Extensive testing shows that ColorCtrl outperforms existing methods in both quality and consistency, making it a significant advancement in the field of text-guided color editing."
                },
                "zh": {
                    "title": "ColorCtrlï¼šç²¾å‡†ä¸€è‡´çš„é¢œè‰²ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "ColorCtrlæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„é¢œè‰²ç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨å®ç°å›¾åƒå’Œè§†é¢‘çš„ç²¾ç¡®é¢œè‰²ç¼–è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›®æ ‡åŒ–çš„æ³¨æ„åŠ›å›¾å’Œæ•°å€¼æ ‡è®°çš„æ“æ§ï¼Œè§£è€¦ç»“æ„ä¸é¢œè‰²ï¼Œä»è€Œå®ç°ä¸€è‡´çš„é¢œè‰²ç¼–è¾‘å’ŒåŸºäºè¯æ±‡çš„å±æ€§å¼ºåº¦æ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒColorCtrlåœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨æŒ‡å®šåŒºåŸŸè¿›è¡Œä¿®æ”¹è€Œä¸å½±å“å…¶ä»–åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒColorCtrlåœ¨è§†é¢‘æ¨¡å‹ä¸­ä¹Ÿå±•ç°å‡ºæ›´å¤§çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§å’Œç¼–è¾‘ç¨³å®šæ€§æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06905",
            "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
            "url": "https://huggingface.co/papers/2508.06905",
            "abstract": "Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.",
            "score": 5,
            "issue_id": 5442,
            "pub_date": "2025-08-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 9",
                "zh": "8æœˆ9æ—¥"
            },
            "hash": "388c69f1cd50095c",
            "authors": [
                "Ruoxi Chen",
                "Dongping Chen",
                "Siyuan Wu",
                "Sinan Wang",
                "Shiyun Lang",
                "Petr Sushko",
                "Gaoyang Jiang",
                "Yao Wan",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for AI Seattle, USA",
                "Huazhong University of Science and Technology Wuhan, China",
                "University of Washington Seattle, USA",
                "Zhejiang Wanli University Ningbo, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06905.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MultiRef-bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OmniGen, ACE Ğ¸ Show-o, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Image Generation with Multiple Visual References",
                    "desc": "This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations."
                },
                "zh": {
                    "title": "å¤šå‚è€ƒå›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒè¿›è¡Œå¯æ§å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†MultiRef-benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«990ä¸ªåˆæˆæ ·æœ¬å’Œ1000ä¸ªçœŸå®æ ·æœ¬çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•å¤šå‚è€ƒå›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šå‚è€ƒæ¡ä»¶ä¸‹ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬ä¸Šçš„è¡¨ç°ä»…ä¸º66.6%ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´çµæ´»çš„äººæ€§åŒ–åˆ›ä½œå·¥å…·æä¾›äº†é‡è¦æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13632",
            "title": "OmniTry: Virtual Try-On Anything without Masks",
            "url": "https://huggingface.co/papers/2508.13632",
            "abstract": "OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/.",
            "score": 4,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "4d47f2b8665d95cb",
            "authors": [
                "Yutong Feng",
                "Linlin Zhang",
                "Hengyuan Cao",
                "Yiming Chen",
                "Xiaoduan Feng",
                "Jian Cao",
                "Yuxiong Wu",
                "Bin Wang"
            ],
            "affiliations": [
                "Kunbyte AI",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13632.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ•¶ï¸",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ»ÑĞ±Ñ‹Ñ… Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº",
                    "desc": "OmniTry - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ (VTON) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞºÑ€Ğ°ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ĞºÑĞµÑÑÑƒĞ°Ñ€Ñ‹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¼Ğ°ÑĞ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. OmniTry Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "OmniTry: Expanding Virtual Try-ON to All Wearables!",
                    "desc": "OmniTry is a new framework that enhances the Virtual Try-ON (VTON) technology to include various wearable items like jewelry and accessories, not just clothing. It uses a two-stage training process that first employs unpaired images to improve the model's ability to locate objects without needing masks. In the second stage, the model is fine-tuned with paired images to ensure that the appearance of the objects remains consistent. The results show that OmniTry outperforms existing methods in both object localization and identity preservation across a wide range of wearable objects."
                },
                "zh": {
                    "title": "OmniTryï¼šæ‰©å±•è™šæ‹Ÿè¯•ç©¿åˆ°æ‰€æœ‰å¯ç©¿æˆ´ç‰©å“",
                    "desc": "OmniTryæ˜¯ä¸€ä¸ªæ‰©å±•è™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰åˆ°å„ç§å¯ç©¿æˆ´ç‰©å“çš„ç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…æ‹¬ç å®å’Œé…é¥°ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µçš„ç®¡é“ï¼Œé¦–å…ˆåˆ©ç”¨å¤§è§„æ¨¡çš„æ— é…å¯¹å›¾åƒè¿›è¡Œæ— æ©è†œå®šä½è®­ç»ƒï¼Œç„¶åé€šè¿‡é…å¯¹å›¾åƒè¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒï¼Œä»¥æé«˜ç‰©ä½“å¤–è§‚çš„ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„æ¨¡å‹å³ä½¿åœ¨é…å¯¹æ ·æœ¬è¾ƒå°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å¿«é€Ÿæ”¶æ•›ã€‚OmniTryåœ¨åŒ…å«12ç±»å¸¸è§å¯ç©¿æˆ´ç‰©å“çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç‰©ä½“å®šä½å’ŒIDä¿æŒæ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12669",
            "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
            "url": "https://huggingface.co/papers/2508.12669",
            "abstract": "LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the \"Misery Game Show\", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
            "score": 2,
            "issue_id": 5440,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "967909e78c87e1ce",
            "authors": [
                "Bishanka Seal",
                "Rahul Seetharaman",
                "Aman Bansal",
                "Abhilash Nandy"
            ],
            "affiliations": [
                "Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, IN",
                "UMass Amherst, Amherst, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12669.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ˜¢",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑĞ¼Ğ¿Ğ°Ñ‚Ñ‹: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ½ĞµÑÑ‡Ğ°ÑÑ‚ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ zero-shot Ğ¸ few-shot Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ few-shot Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ zero-shot Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ¹Ğ¼Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 'Misery Game Show' Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Emotional Insights: LLMs in Predicting Misery Scores",
                    "desc": "This paper explores how Large Language Models (LLMs) can predict misery scores from text descriptions. The authors treat this as a regression task, where the model assigns a score between 0 and 100 based on the emotional content of the text. They compare different prompting strategies, finding that few-shot learning methods are more effective than zero-shot methods, indicating that context helps improve predictions. Additionally, they introduce a gamified evaluation framework called the 'Misery Game Show' to assess LLMs' adaptability in emotional reasoning tasks, demonstrating their potential in dynamic scenarios."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹ç—›è‹¦åˆ†æ•°çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­é¢„æµ‹äººç±»æ„ŸçŸ¥çš„ç—›è‹¦åˆ†æ•°ã€‚è¯¥ä»»åŠ¡è¢«è§†ä¸ºå›å½’é—®é¢˜ï¼Œæ¨¡å‹ä¸ºæ¯ä¸ªè¾“å…¥è¯­å¥åˆ†é…ä¸€ä¸ªä»0åˆ°100çš„æ ‡é‡å€¼ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å›ºå®šä¸Šä¸‹æ–‡çš„å°‘æ ·æœ¬å’ŒåŸºäºæ£€ç´¢çš„æç¤ºï¼Œç»“æœæ˜¾ç¤ºå°‘æ ·æœ¬æ–¹æ³•åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ã€‚ä¸ºäº†è¶…è¶Šé™æ€è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œç—›è‹¦æ¸¸æˆç§€â€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¸¸æˆåŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•LLMsåœ¨åŠ¨æ€æƒ…æ„Ÿæ¨ç†ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13992",
            "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
            "url": "https://huggingface.co/papers/2508.13992",
            "abstract": "MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild\" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems' progression toward audio general intelligence. The benchmark and code is available at https://sonalkum.github.io/mmau-pro.",
            "score": 1,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "7815d4c4cb2d67a6",
            "authors": [
                "Sonal Kumar",
                "Å imon SedlÃ¡Äek",
                "Vaibhavi Lokegaonkar",
                "Fernando LÃ³pez",
                "Wenyi Yu",
                "Nishit Anand",
                "Hyeonggon Ryu",
                "Lichang Chen",
                "Maxim PliÄka",
                "Miroslav HlavÃ¡Äek",
                "William Fineas Ellingwood",
                "Sathvik Udupa",
                "Siyuan Hou",
                "Allison Ferner",
                "Sara Barahona",
                "Cecilia BolaÃ±os",
                "Satish Rahi",
                "Laura Herrera-AlarcÃ³n",
                "Satvik Dixit",
                "Siddhi Patil",
                "Soham Deshmukh",
                "Lasha Koroshinadze",
                "Yao Liu",
                "Leibny Paola Garcia Perera",
                "Eleni Zanou",
                "Themos Stafylakis",
                "Joon Son Chung",
                "David Harwath",
                "Chao Zhang",
                "Dinesh Manocha",
                "Alicia Lozano-Diez",
                "Santosh Kesiraju",
                "Sreyan Ghosh",
                "Ramani Duraiswami"
            ],
            "affiliations": [
                "Athens University of Economics and Business",
                "Brno University of Technology, Czech Republic",
                "Carnegie Mellon University, USA",
                "Indian Institute of Technology, Bombay",
                "Johns Hopkins University, USA",
                "KAIST, Daejeon",
                "Microsoft",
                "Middlebury College, USA",
                "Phonexia",
                "Shanghai Artificial Intelligence Laboratory",
                "Telefonica",
                "Tsinghua University",
                "Tufts University",
                "Universidad Autonoma de Madrid",
                "Universidad de Buenos Aires",
                "Universiti Sains Malaysia",
                "University of Maryland, College Park, USA",
                "University of Texas, Austin, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13992.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#agi",
                    "#open_source"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "MMAU-Pro: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜",
                    "desc": "MMAU-Pro - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 49 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞ°, Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 5305 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Gemini 2.5 Flash Ğ¸ Audio Flamingo 3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 59.2% Ğ¸ 51.7% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. MMAU-Pro Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "MMAU-Pro: Elevating Audio Intelligence Evaluation in AI",
                    "desc": "MMAU-Pro is a new benchmark designed to evaluate audio intelligence in AI systems, focusing on 49 distinct skills related to speech, sound, and music. It includes 5,305 audio instances paired with expert-generated questions that require complex reasoning to answer. The benchmark reveals that even advanced AI models struggle with audio comprehension, achieving only moderate accuracy. This comprehensive evaluation aims to identify weaknesses in current models and guide improvements towards achieving human-level audio understanding."
                },
                "zh": {
                    "title": "éŸ³é¢‘æ™ºèƒ½è¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "MMAU-Proæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„éŸ³é¢‘æ™ºèƒ½ï¼Œæ¶µç›–äº†49ç§ç‹¬ç‰¹çš„æŠ€èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€å£°éŸ³ã€éŸ³ä¹åŠå…¶ç»„åˆã€‚è¯¥åŸºå‡†åŒ…å«5305ä¸ªå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹é…æœ‰ä¸“å®¶ç”Ÿæˆçš„é—®é¢˜å’Œç­”æ¡ˆï¼Œæ—¨åœ¨æµ‹è¯•AIåœ¨éŸ³é¢‘ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹22ä¸ªé¢†å…ˆçš„å¤šæ¨¡æ€AIæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤šä¸ªç±»åˆ«ä¸­çš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º59.2%å’Œ51.7%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å±€é™æ€§ã€‚MMAU-Proä¸ºæœªæ¥AIç³»ç»Ÿçš„éŸ³é¢‘é€šç”¨æ™ºèƒ½å‘å±•æä¾›äº†é‡è¦çš„è§è§£å’Œæ”¹è¿›æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13139",
            "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
            "url": "https://huggingface.co/papers/2508.13139",
            "abstract": "Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  \t\t\t\t\tAI-generated summary \t\t\t\t This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion.",
            "score": 0,
            "issue_id": 5442,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "799a57dcfacaa430",
            "authors": [
                "Ling-Hao Chen",
                "Yuhong Zhang",
                "Zixin Yin",
                "Zhiyang Dou",
                "Xin Chen",
                "Jingbo Wang",
                "Taku Komura",
                "Lei Zhang"
            ],
            "affiliations": [
                "ByteDance, United States of America",
                "International Digital Economy Academy, China",
                "Shanghai Artificial Intelligence Laboratory, China",
                "The Hong Kong University of Science and Technology, China",
                "The University of Hong Kong, China",
                "Tsinghua University, China",
                "Tsinghua University, International Digital Economy Academy, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13139.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¦´",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Motion2Motion - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞºĞµĞ»ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞºĞµĞ»ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Effortless Animation Transfer Across Diverse Skeletons",
                    "desc": "Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry."
                },
                "zh": {
                    "title": "é«˜æ•ˆåŠ¨ç”»è½¬ç§»ï¼Œè·¨è¶Šéª¨éª¼æ‹“æ‰‘çš„ç•Œé™",
                    "desc": "Motion2Motionæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä¸åŒéª¨éª¼æ‹“æ‰‘çš„è§’è‰²ä¹‹é—´è½¬ç§»åŠ¨ç”»ã€‚è¯¥æ–¹æ³•è§£å†³äº†æºéª¨éª¼å’Œç›®æ ‡éª¨éª¼ä¹‹é—´çš„æ‹“æ‰‘ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œåˆ©ç”¨ç¨€ç–çš„éª¨éª¼å¯¹åº”å…³ç³»è¿›è¡ŒåŠ¨ç”»è½¬ç§»ã€‚å°½ç®¡ç°æœ‰çš„é‡å®šå‘æŠ€æœ¯å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨å¤šæ ·åŒ–æ‹“æ‰‘ç»“æ„ä¹‹é—´çš„è¿åŠ¨è½¬ç§»ä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†Motion2Motionåœ¨ç›¸ä¼¼éª¨éª¼å’Œè·¨ç‰©ç§éª¨éª¼è½¬ç§»åœºæ™¯ä¸­éƒ½èƒ½å®ç°é«˜æ•ˆå¯é çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-19.html",
    "link_next": "2025-08-21.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "19.08",
        "en": "08/19",
        "zh": "8æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.08",
        "en": "08/21",
        "zh": "8æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}