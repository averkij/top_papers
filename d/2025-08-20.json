{
    "date": {
        "ru": "20 августа",
        "en": "August 20",
        "zh": "8月20日"
    },
    "time_utc": "2025-08-20 05:12",
    "weekday": 2,
    "issue_id": 5442,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.13167",
            "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
            "url": "https://huggingface.co/papers/2508.13167",
            "abstract": "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.",
            "score": 25,
            "issue_id": 5440,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "f7b2eb384a807652",
            "authors": [
                "Weizhen Li",
                "Jianbo Lin",
                "Zhuosong Jiang",
                "Jingyi Cao",
                "Xinpeng Liu",
                "Jiayu Zhang",
                "Zhenqiang Huang",
                "Qianben Chen",
                "Weichen Sun",
                "Qiexiang Wang",
                "Hongxuan Lu",
                "Tianrui Qin",
                "Chenghao Zhu",
                "Yi Yao",
                "Shuying Fan",
                "Xiaowan Li",
                "Tiannan Wang",
                "Pai Liu",
                "King Zhu",
                "He Zhu",
                "Dingfeng Shi",
                "Piaohong Wang",
                "Yeyi Guan",
                "Xiangru Tang",
                "Minghao Liu",
                "Yuchen Eleanor Jiang",
                "Jian Yang",
                "Jiaheng Liu",
                "Ge Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.13167.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#agi",
                    "#architecture",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Chain-of-Agents: Новый подход к решению сложных задач в языковых моделях",
                    "desc": "Статья представляет новую парадигму рассуждений для больших языковых моделей (LLM) под названием Chain-of-Agents (CoA). Эта парадигма позволяет моделям динамически активировать различных агентов для решения сложных задач в рамках одной модели. Авторы вводят концепцию моделей-основ агентов (Agent Foundation Models, AFM), которые обучаются с помощью мультиагентной дистилляции и агентного обучения с подкреплением. Эмпирические исследования показывают, что AFM устанавливает новый уровень производительности в различных бенчмарках для веб-агентов и агентов-программистов."
                },
                "en": {
                    "title": "Empowering LLMs with Chain-of-Agents for Complex Problem Solving",
                    "desc": "The Chain-of-Agents (CoA) paradigm enhances large language models (LLMs) by enabling them to solve complex problems through dynamic activation of multiple agents. This approach allows for end-to-end problem-solving without the need for manual prompt engineering, making it more efficient and capable. By employing multi-agent distillation and agentic reinforcement learning, the model learns to simulate collaboration among agents effectively. The resulting models, termed Agent Foundation Models (AFMs), achieve state-of-the-art performance across various benchmarks, demonstrating significant advancements in multi-agent systems."
                },
                "zh": {
                    "title": "Chain-of-Agents：端到端复杂问题解决的新范式",
                    "desc": "Chain-of-Agents（CoA）是一种新颖的LLM推理范式，能够实现端到端的复杂问题解决。该方法通过动态激活不同的工具代理和角色扮演代理，模拟多代理协作，从而提高模型的性能。我们引入了多代理蒸馏框架，将先进的多代理系统转化为Chain-of-Agents轨迹，以进行监督微调。此外，通过在可验证的代理任务上应用代理强化学习，进一步提升了模型在Chain-of-Agents问题解决中的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14041",
            "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
            "url": "https://huggingface.co/papers/2508.14041",
            "abstract": "LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  \t\t\t\t\tAI-generated summary \t\t\t\t LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/",
            "score": 22,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "87ad1cfb038a78fa",
            "authors": [
                "Chin-Yang Lin",
                "Cheng Sun",
                "Fu-En Yang",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA Research",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14041.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#cv",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение синтеза новых ракурсов для длинных видео с помощью LongSplat",
                    "desc": "LongSplat - это новый метод синтеза новых ракурсов из длинных видео с нерегулярным движением камеры. Он использует совместную оптимизацию, надежную оценку положения камеры и эффективное формирование опорных точек на основе октодерева. LongSplat решает проблемы дрейфа положения камеры, неточной инициализации геометрии и ограничений памяти. Эксперименты показывают, что метод достигает лучших результатов по качеству рендеринга, точности положения камеры и вычислительной эффективности по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Revolutionizing View Synthesis with LongSplat!",
                    "desc": "LongSplat is a new method for creating novel views from long videos that have irregular camera movements. It tackles problems like pose drift and memory limitations by optimizing camera positions and 3D representations together. The method uses a robust pose estimation technique that relies on learned 3D information and an efficient way to organize data into anchors. Experiments show that LongSplat significantly enhances the quality of rendered images and the accuracy of camera poses compared to existing methods."
                },
                "zh": {
                    "title": "LongSplat：提升长视频视图合成的创新解决方案",
                    "desc": "LongSplat是一种新颖的视图合成方法，旨在处理长视频中不规则运动带来的挑战。它通过联合优化、稳健的姿态估计和高效的锚点形成来提高合成质量。该方法能够同时优化相机姿态和3D高斯模型，避免局部最小值并确保全局一致性。实验结果表明，LongSplat在渲染质量、姿态准确性和计算效率上均优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13948",
            "title": "Prompt Orchestration Markup Language",
            "url": "https://huggingface.co/papers/2508.13948",
            "abstract": "POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.",
            "score": 12,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "9520c0e7396f1933",
            "authors": [
                "Yuge Zhang",
                "Nan Chen",
                "Jiahang Xu",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13948.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🗂️",
                "ru": {
                    "title": "POML: структурированный подход к промптингу языковых моделей",
                    "desc": "Статья представляет POML - язык разметки для структурированного промптинга больших языковых моделей. POML решает проблемы интеграции данных, чувствительности к форматированию и организации сложных промптов. Он использует компонентную разметку, специальные теги и CSS-подобную систему стилей. POML включает шаблонизацию и инструменты разработчика для улучшения контроля версий и совместной работы."
                },
                "en": {
                    "title": "Streamlining Prompts for Better AI Performance with POML",
                    "desc": "POML, or Prompt Orchestration Markup Language, is designed to enhance the prompting process for Large Language Models (LLMs) by introducing a structured and integrated approach. It addresses the challenges of organizing complex prompts that involve various data types, such as documents and images, while also managing different presentation formats. By utilizing a component-based markup system and specialized tags, POML allows for better data integration and reduces sensitivity to formatting issues. The framework also includes templating features and a developer toolkit to facilitate collaboration and improve version control in real-world applications."
                },
                "zh": {
                    "title": "POML：提升大型语言模型提示的利器",
                    "desc": "POML（提示编排标记语言）旨在解决大型语言模型（LLMs）在提示过程中面临的结构、数据集成和格式敏感性等挑战。它通过组件化的标记语言提供逻辑结构，使用专门的标签实现数据的无缝集成，并采用类似CSS的样式系统来减少格式敏感性。POML还包括动态提示的模板功能和全面的开发者工具包，以提高版本控制和协作效率。通过两个案例研究，验证了POML在复杂应用集成和准确性表现方面的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09131",
            "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2508.09131",
            "abstract": "ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
            "score": 8,
            "issue_id": 5440,
            "pub_date": "2025-08-12",
            "pub_date_card": {
                "ru": "12 августа",
                "en": "August 12",
                "zh": "8月12日"
            },
            "hash": "77b10751b41fef03",
            "authors": [
                "Zixin Yin",
                "Xili Dai",
                "Ling-Hao Chen",
                "Deyu Zhou",
                "Jianan Wang",
                "Duomin Wang",
                "Gang Yu",
                "Lionel M. Ni",
                "Lei Zhang",
                "Heung-Yeung Shum"
            ],
            "affiliations": [
                "Astribot",
                "International Digital Economy Academy",
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09131.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точное редактирование цвета без обучения с помощью диффузионных трансформеров",
                    "desc": "ColorCtrl - это метод редактирования цвета в изображениях и видео без дополнительного обучения, использующий мультимодальные диффузионные трансформеры (MM-DiT). Он позволяет точно и согласованно редактировать цвет с контролем на уровне отдельных слов. ColorCtrl превосходит существующие подходы по качеству редактирования и согласованности результатов. Метод работает путем целенаправленного изменения карт внимания и токенов значений, что позволяет изменять только указанные в запросе области изображения."
                },
                "en": {
                    "title": "Precision Color Editing with Zero Training",
                    "desc": "ColorCtrl is a novel method for color editing in images and videos that does not require prior training. It utilizes Multi-Modal Diffusion Transformers to achieve precise control over color attributes while maintaining the physical consistency of the scene. By manipulating attention maps and value tokens, ColorCtrl allows users to edit specific regions of an image based on textual prompts, ensuring that unrelated areas remain unchanged. Extensive testing shows that ColorCtrl outperforms existing methods in both quality and consistency, making it a significant advancement in the field of text-guided color editing."
                },
                "zh": {
                    "title": "ColorCtrl：精准一致的颜色编辑新方法",
                    "desc": "ColorCtrl是一种无需训练的颜色编辑方法，利用多模态扩散变换器实现图像和视频的精确颜色编辑。该方法通过目标化的注意力图和数值标记的操控，解耦结构与颜色，从而实现一致的颜色编辑和基于词汇的属性强度控制。与现有方法相比，ColorCtrl在编辑质量和一致性方面表现出色，能够在指定区域进行修改而不影响其他区域。实验结果表明，ColorCtrl在视频模型中也展现出更大的优势，特别是在保持时间一致性和编辑稳定性方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06905",
            "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
            "url": "https://huggingface.co/papers/2508.06905",
            "abstract": "Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.",
            "score": 5,
            "issue_id": 5442,
            "pub_date": "2025-08-09",
            "pub_date_card": {
                "ru": "9 августа",
                "en": "August 9",
                "zh": "8月9日"
            },
            "hash": "388c69f1cd50095c",
            "authors": [
                "Ruoxi Chen",
                "Dongping Chen",
                "Siyuan Wu",
                "Sinan Wang",
                "Shiyun Lang",
                "Petr Sushko",
                "Gaoyang Jiang",
                "Yao Wan",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for AI Seattle, USA",
                "Huazhong University of Science and Technology Wuhan, China",
                "University of Washington Seattle, USA",
                "Zhejiang Wanli University Ningbo, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06905.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Генеративные модели нуждаются в улучшении работы с множественными визуальными ссылками",
                    "desc": "Исследование показывает, что современные системы генерации изображений испытывают трудности при работе с несколькими визуальными ссылками. Авторы представляют MultiRef-bench - набор данных для оценки способности моделей комбинировать элементы из нескольких изображений. Эксперименты с различными моделями, включая OmniGen, ACE и Show-o, демонстрируют ограниченную эффективность даже передовых систем в этой задаче. Результаты указывают на необходимость разработки более гибких инструментов генеративного ИИ, способных интегрировать множественные источники визуального вдохновения."
                },
                "en": {
                    "title": "Enhancing Image Generation with Multiple Visual References",
                    "desc": "This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations."
                },
                "zh": {
                    "title": "多参考图像生成的挑战与机遇",
                    "desc": "本论文探讨了使用多个视觉参考进行可控图像生成的任务。我们提出了MultiRef-bench，这是一个包含990个合成样本和1000个真实样本的评估框架，旨在测试多参考图像的生成能力。实验结果显示，即使是最先进的图像生成模型在多参考条件下也面临挑战，最佳模型OmniGen在合成样本上的表现仅为66.6%。这些发现为开发更灵活的人性化创作工具提供了重要方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13632",
            "title": "OmniTry: Virtual Try-On Anything without Masks",
            "url": "https://huggingface.co/papers/2508.13632",
            "abstract": "OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/.",
            "score": 4,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "4d47f2b8665d95cb",
            "authors": [
                "Yutong Feng",
                "Linlin Zhang",
                "Hengyuan Cao",
                "Yiming Chen",
                "Xiaoduan Feng",
                "Jian Cao",
                "Yuxiong Wu",
                "Bin Wang"
            ],
            "affiliations": [
                "Kunbyte AI",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13632.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🕶️",
                "ru": {
                    "title": "Виртуальная примерка любых носимых объектов без масок",
                    "desc": "OmniTry - это унифицированная система, расширяющая возможности виртуальной примерки (VTON) на различные носимые объекты, включая украшения и аксессуары. Она использует двухэтапный конвейер, сочетающий обучение на непарных и парных изображениях для улучшения локализации объектов и согласованности их внешнего вида. Первый этап использует большой набор непарных изображений для обучения модели безмасочной локализации, а второй этап дообучает модель на парных изображениях для переноса согласованности внешнего вида объекта. OmniTry показывает лучшие результаты по локализации объектов и сохранению их идентичности по сравнению с существующими методами."
                },
                "en": {
                    "title": "OmniTry: Expanding Virtual Try-ON to All Wearables!",
                    "desc": "OmniTry is a new framework that enhances the Virtual Try-ON (VTON) technology to include various wearable items like jewelry and accessories, not just clothing. It uses a two-stage training process that first employs unpaired images to improve the model's ability to locate objects without needing masks. In the second stage, the model is fine-tuned with paired images to ensure that the appearance of the objects remains consistent. The results show that OmniTry outperforms existing methods in both object localization and identity preservation across a wide range of wearable objects."
                },
                "zh": {
                    "title": "OmniTry：扩展虚拟试穿到所有可穿戴物品",
                    "desc": "OmniTry是一个扩展虚拟试穿（VTON）到各种可穿戴物品的统一框架，包括珠宝和配饰。该方法采用两阶段的管道，首先利用大规模的无配对图像进行无掩膜定位训练，然后通过配对图像进行进一步微调，以提高物体外观的一致性。研究表明，经过第一阶段训练的模型即使在配对样本较少的情况下也能快速收敛。OmniTry在包含12类常见可穿戴物品的基准测试中表现优于现有方法，尤其在物体定位和ID保持方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12669",
            "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
            "url": "https://huggingface.co/papers/2508.12669",
            "abstract": "LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the \"Misery Game Show\", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
            "score": 2,
            "issue_id": 5440,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 августа",
                "en": "August 18",
                "zh": "8月18日"
            },
            "hash": "967909e78c87e1ce",
            "authors": [
                "Bishanka Seal",
                "Rahul Seetharaman",
                "Aman Bansal",
                "Abhilash Nandy"
            ],
            "affiliations": [
                "Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, IN",
                "UMass Amherst, Amherst, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12669.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#training"
                ],
                "emoji": "😢",
                "ru": {
                    "title": "LLM как эмпаты: оценка способности AI понимать человеческие страдания",
                    "desc": "Это исследование оценивает способность больших языковых моделей (LLM) предсказывать уровень несчастья человека на основе текстовых описаний реальных ситуаций. Авторы сравнивают различные стратегии промптинга, включая zero-shot и few-shot подходы, а также применение ретривальных методов. Результаты показывают, что few-shot методы превосходят zero-shot базовые линии в задаче аффективного прогнозирования. Исследователи также представляют новую геймифицированную структуру оценки 'Misery Game Show' для тестирования адаптивности LLM в динамических задачах эмоционального рассуждения."
                },
                "en": {
                    "title": "Unlocking Emotional Insights: LLMs in Predicting Misery Scores",
                    "desc": "This paper explores how Large Language Models (LLMs) can predict misery scores from text descriptions. The authors treat this as a regression task, where the model assigns a score between 0 and 100 based on the emotional content of the text. They compare different prompting strategies, finding that few-shot learning methods are more effective than zero-shot methods, indicating that context helps improve predictions. Additionally, they introduce a gamified evaluation framework called the 'Misery Game Show' to assess LLMs' adaptability in emotional reasoning tasks, demonstrating their potential in dynamic scenarios."
                },
                "zh": {
                    "title": "利用大型语言模型预测痛苦分数的创新方法",
                    "desc": "本研究探讨了大型语言模型（LLMs）如何从自然语言描述中预测人类感知的痛苦分数。该任务被视为回归问题，模型为每个输入语句分配一个从0到100的标量值。我们评估了多种提示策略，包括零样本、固定上下文的少样本和基于检索的提示，结果显示少样本方法在情感预测中优于零样本基线。为了超越静态评估，我们引入了“痛苦游戏秀”，这是一个新颖的游戏化框架，旨在测试LLMs在动态情感推理任务中的适应能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13992",
            "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
            "url": "https://huggingface.co/papers/2508.13992",
            "abstract": "MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild\" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems' progression toward audio general intelligence. The benchmark and code is available at https://sonalkum.github.io/mmau-pro.",
            "score": 1,
            "issue_id": 5440,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "7815d4c4cb2d67a6",
            "authors": [
                "Sonal Kumar",
                "Šimon Sedláček",
                "Vaibhavi Lokegaonkar",
                "Fernando López",
                "Wenyi Yu",
                "Nishit Anand",
                "Hyeonggon Ryu",
                "Lichang Chen",
                "Maxim Plička",
                "Miroslav Hlaváček",
                "William Fineas Ellingwood",
                "Sathvik Udupa",
                "Siyuan Hou",
                "Allison Ferner",
                "Sara Barahona",
                "Cecilia Bolaños",
                "Satish Rahi",
                "Laura Herrera-Alarcón",
                "Satvik Dixit",
                "Siddhi Patil",
                "Soham Deshmukh",
                "Lasha Koroshinadze",
                "Yao Liu",
                "Leibny Paola Garcia Perera",
                "Eleni Zanou",
                "Themos Stafylakis",
                "Joon Son Chung",
                "David Harwath",
                "Chao Zhang",
                "Dinesh Manocha",
                "Alicia Lozano-Diez",
                "Santosh Kesiraju",
                "Sreyan Ghosh",
                "Ramani Duraiswami"
            ],
            "affiliations": [
                "Athens University of Economics and Business",
                "Brno University of Technology, Czech Republic",
                "Carnegie Mellon University, USA",
                "Indian Institute of Technology, Bombay",
                "Johns Hopkins University, USA",
                "KAIST, Daejeon",
                "Microsoft",
                "Middlebury College, USA",
                "Phonexia",
                "Shanghai Artificial Intelligence Laboratory",
                "Telefonica",
                "Tsinghua University",
                "Tufts University",
                "Universidad Autonoma de Madrid",
                "Universidad de Buenos Aires",
                "Universiti Sains Malaysia",
                "University of Maryland, College Park, USA",
                "University of Texas, Austin, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13992.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#agi",
                    "#open_source"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "MMAU-Pro: новый стандарт оценки аудиоинтеллекта ИИ",
                    "desc": "MMAU-Pro - это комплексный бенчмарк для оценки аудиоинтеллекта в системах искусственного интеллекта. Он оценивает 49 уникальных навыков в области речи, звука, музыки и их комбинаций, используя 5305 примеров с вопросами и ответами экспертов. Бенчмарк выявил значительные ограничения даже у современных моделей, таких как Gemini 2.5 Flash и Audio Flamingo 3, которые достигли точности только 59.2% и 51.7% соответственно. MMAU-Pro предоставляет ценные insights для развития систем ИИ в направлении общего аудиоинтеллекта."
                },
                "en": {
                    "title": "MMAU-Pro: Elevating Audio Intelligence Evaluation in AI",
                    "desc": "MMAU-Pro is a new benchmark designed to evaluate audio intelligence in AI systems, focusing on 49 distinct skills related to speech, sound, and music. It includes 5,305 audio instances paired with expert-generated questions that require complex reasoning to answer. The benchmark reveals that even advanced AI models struggle with audio comprehension, achieving only moderate accuracy. This comprehensive evaluation aims to identify weaknesses in current models and guide improvements towards achieving human-level audio understanding."
                },
                "zh": {
                    "title": "音频智能评估的新标准",
                    "desc": "MMAU-Pro是一个全面的基准，用于评估人工智能系统的音频智能，涵盖了49种独特的技能，包括语音、声音、音乐及其组合。该基准包含5305个实例，每个实例配有专家生成的问题和答案，旨在测试AI在音频理解方面的能力。通过对22个领先的多模态AI模型进行评估，发现即使是最先进的模型在多个类别中的准确率也仅为59.2%和51.7%，显示出显著的局限性。MMAU-Pro为未来AI系统的音频通用智能发展提供了重要的见解和改进方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13139",
            "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
            "url": "https://huggingface.co/papers/2508.13139",
            "abstract": "Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  \t\t\t\t\tAI-generated summary \t\t\t\t This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion.",
            "score": 0,
            "issue_id": 5442,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 августа",
                "en": "August 18",
                "zh": "8月18日"
            },
            "hash": "799a57dcfacaa430",
            "authors": [
                "Ling-Hao Chen",
                "Yuhong Zhang",
                "Zixin Yin",
                "Zhiyang Dou",
                "Xin Chen",
                "Jingbo Wang",
                "Taku Komura",
                "Lei Zhang"
            ],
            "affiliations": [
                "ByteDance, United States of America",
                "International Digital Economy Academy, China",
                "Shanghai Artificial Intelligence Laboratory, China",
                "The Hong Kong University of Science and Technology, China",
                "The University of Hong Kong, China",
                "Tsinghua University, China",
                "Tsinghua University, International Digital Economy Academy, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13139.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#transfer_learning"
                ],
                "emoji": "🦴",
                "ru": {
                    "title": "Универсальный перенос анимации без обучения",
                    "desc": "Motion2Motion - это система для переноса анимации между персонажами с разной скелетной топологией, не требующая обучения. Она использует разреженные соответствия между костями для эффективного переноса движений даже между сильно различающимися скелетами. Система работает с небольшим количеством примеров движений целевого скелета. Эксперименты показали надежную работу как для похожих скелетов, так и для переноса между разными видами."
                },
                "en": {
                    "title": "Effortless Animation Transfer Across Diverse Skeletons",
                    "desc": "Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry."
                },
                "zh": {
                    "title": "高效动画转移，跨越骨骼拓扑的界限",
                    "desc": "Motion2Motion是一个无需训练的框架，能够高效地在不同骨骼拓扑的角色之间转移动画。该方法解决了源骨骼和目标骨骼之间的拓扑不一致性问题，利用稀疏的骨骼对应关系进行动画转移。尽管现有的重定向技术已经取得了一定进展，但在多样化拓扑结构之间的运动转移仍然较少被探索。通过定性和定量评估，我们证明了Motion2Motion在相似骨骼和跨物种骨骼转移场景中都能实现高效可靠的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-08-19.html",
    "link_next": "2025-08-21.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "19.08",
        "en": "08/19",
        "zh": "8月19日"
    },
    "short_date_next": {
        "ru": "21.08",
        "en": "08/21",
        "zh": "8月21日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}