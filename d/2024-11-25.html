
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. November 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 ноября</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-22.html">⬅️ <span id="prev-date">22.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-26.html">➡️ <span id="next-date">26.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'};
        let feedDateNext = {'ru': '26.11', 'en': '11/26', 'zh': '11月26日'};
        let feedDatePrev = {'ru': '22.11', 'en': '11/22', 'zh': '11月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.14793', 'title': 'Style-Friendly SNR Sampler for Style-Driven Generation', 'url': 'https://huggingface.co/papers/2411.14793', 'abstract': 'Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new "style templates", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.', 'score': 21, 'issue_id': 752, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '03859b57f29683ab', 'authors': ['Jooyoung Choi', 'Chaehun Shin', 'Yeongtak Oh', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14793.jpg', 'data': {'categories': ['#synthetic', '#3d', '#multimodal', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях', 'desc': 'Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом.'}, 'en': {'title': 'Unlocking Unique Artistic Styles with Style-friendly SNR Sampler', 'desc': 'This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation.'}, 'zh': {'title': '提升个性化艺术风格生成的信噪比方法', 'desc': '最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。'}}}, {'id': 'https://huggingface.co/papers/2411.15124', 'title': 'TÜLU 3: Pushing Frontiers in Open Language Model Post-Training', 'url': 'https://huggingface.co/papers/2411.15124', 'abstract': 'Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\"ULU 3 approach to more domains.', 'score': 18, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '44809ea81d71ef97', 'authors': ['Nathan Lambert', 'Jacob Morrison', 'Valentina Pyatkin', 'Shengyi Huang', 'Hamish Ivison', 'Faeze Brahman', 'Lester James V. Miranda', 'Alisa Liu', 'Nouha Dziri', 'Shane Lyu', 'Yuling Gu', 'Saumya Malik', 'Victoria Graf', 'Jena D. Hwang', 'Jiangjiang Yang', 'Ronan Le Bras', 'Oyvind Tafjord', 'Chris Wilhelm', 'Luca Soldaini', 'Noah A. Smith', 'Yizhong Wang', 'Pradeep Dasigi', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.15124.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#open_source', '#optimization', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Открытый рецепт пост-тренировки языковых моделей', 'desc': 'Статья представляет T"ULU 3 - семейство открытых моделей, обученных с помощью пост-тренировки. Авторы описывают методы обучения, включая SFT, DPO и новый метод RLVR. T"ULU 3 превосходит многие современные модели, включая инструктированные версии Llama 3.1 и GPT-4o-mini. Исследователи предоставляют полные рецепты обучения, наборы данных и инструменты для воспроизведения результатов.'}, 'en': {'title': 'Unlocking Language Model Potential with T"ULU 3', 'desc': 'This paper presents T"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools.'}, 'zh': {'title': 'T"ULU 3：开放的后训练模型新纪元', 'desc': '本文介绍了T"ULU 3，这是一个完全开放的最新后训练模型系列，旨在提高语言模型的行为和技能。我们提供了模型的训练数据、代码和训练配方，填补了开放技术与专有技术之间的透明度差距。T"ULU 3在多个基准测试中超越了现有的语言模型，包括Llama 3.1和GPT-4o-mini等。我们还引入了一种多任务评估方案，并提供了详细的报告，以便于在更多领域中复现和适应T"ULU 3的方法。'}}}, {'id': 'https://huggingface.co/papers/2411.12946', 'title': 'A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection', 'url': 'https://huggingface.co/papers/2411.12946', 'abstract': 'Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.', 'score': 11, 'issue_id': 756, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'de5ca9118a5cab35', 'authors': ['Gabriel Chua', 'Shing Yee Chan', 'Shaun Khoo'], 'affiliations': ['Government Technology Agency Singapore', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.12946.jpg', 'data': {'categories': ['#data', '#dataset', '#hallucinations', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Гибкая защита языковых моделей без реальных данных', 'desc': 'Статья представляет новую методологию разработки защитных механизмов для больших языковых моделей (LLM) от нецелевого использования. Авторы предлагают гибкий подход, не требующий реальных данных, основанный на генерации синтетического датасета с помощью LLM. Разработанные защитные механизмы превосходят эвристические подходы и могут обобщаться на другие категории злоупотреблений. Авторы также открывают доступ к синтетическому датасету и моделям защиты для поддержки будущих исследований в области безопасности LLM.'}, 'en': {'title': 'Building Better Guardrails for Large Language Models', 'desc': 'This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety.'}, 'zh': {'title': '构建灵活的防护措施，提升大型语言模型安全性', 'desc': '本论文探讨了大型语言模型在使用中可能出现的偏离主题的误用问题。我们提出了一种灵活的、无数据的防护措施开发方法，旨在解决现有方法的高误报率和适应性不足的问题。通过对问题空间的定性定义，并利用大型语言模型生成多样化的提示，我们构建了一个合成数据集，用于基准测试和训练防护措施。最后，我们开源了合成数据集和防护模型，为大型语言模型的安全性研究和开发提供了重要资源。'}}}, {'id': 'https://huggingface.co/papers/2411.15098', 'title': 'OminiControl: Minimal and Universal Control for Diffusion Transformer', 'url': 'https://huggingface.co/papers/2411.15098', 'abstract': 'In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.', 'score': 11, 'issue_id': 754, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9cd668db99ed0902', 'authors': ['Zhenxiong Tan', 'Songhua Liu', 'Xingyi Yang', 'Qiaochu Xue', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.15098.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#data', '#diffusion', '#dataset', '#architecture', '#multimodal', '#training'], 'emoji': '🎨', 'ru': {'title': 'OminiControl: Универсальное управление генерацией изображений с минимальными затратами', 'desc': 'OminiControl - это новая эффективная система для интеграции изображений в предобученные модели Diffusion Transformer (DiT). Она использует механизм повторного использования параметров, позволяя DiT кодировать условия изображений с помощью собственной архитектуры. OminiControl требует всего 0,1% дополнительных параметров и может решать широкий спектр задач условной генерации изображений. Система превосходит существующие модели на основе UNet и адаптированные DiT как в генерации, управляемой субъектом, так и в пространственно-согласованной условной генерации.'}, 'en': {'title': 'Efficient Image Conditioning with OminiControl', 'desc': 'OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research.'}, 'zh': {'title': 'OminiControl：高效整合图像条件的创新框架', 'desc': '本文介绍了OminiControl，这是一个高度灵活且参数高效的框架，能够将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl利用参数重用机制，使DiT能够使用自身作为强大的基础，编码图像条件，并通过灵活的多模态注意力处理器进行处理。与现有方法不同，OminiControl仅需约0.1%的额外参数，就能有效地整合注入的图像条件，并以统一的方式处理多种图像条件任务。通过在DiT自身生成的图像上进行训练，OminiControl在主题驱动生成和空间对齐条件生成方面的表现优于现有的UNet和DiT适应模型。'}}}, {'id': 'https://huggingface.co/papers/2411.14982', 'title': 'Large Multi-modal Models Can Interpret Features in Large Multi-modal Models', 'url': 'https://huggingface.co/papers/2411.14982', 'abstract': "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.", 'score': 8, 'issue_id': 761, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '7d4fae86425adb6c', 'authors': ['Kaichen Zhang', 'Yifei Shen', 'Bo Li', 'Ziwei Liu'], 'affiliations': ['LMMs-Lab Team, S-Lab, NTU, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.14982.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны мышления больших мультимодальных моделей', 'desc': 'Статья представляет новый подход к интерпретации внутренних нейронных представлений в больших мультимодальных моделях (LMM). Авторы применяют разреженный автоэнкодер для выделения понятных человеку признаков, а затем используют автоматическую систему интерпретации для анализа этих признаков. Исследование проводится на модели LLaVA-NeXT-8B с помощью LLaVA-OV-72B, демонстрируя, как выделенные признаки влияют на поведение модели. Результаты помогают лучше понять принципы работы LMM и проводят параллели с когнитивными процессами человеческого мозга.'}, 'en': {'title': 'Unlocking the Secrets of Large Multimodal Models', 'desc': "This paper explores how we can understand the internal workings of Large Multimodal Models (LMMs) by using a Sparse Autoencoder (SAE) to break down their representations into features that humans can comprehend. It introduces a framework for automatically interpreting these features, which are learned by the LMMs themselves. The study specifically analyzes the LLaVA-NeXT-8B model in relation to the LLaVA-OV-72B model, showing that the identified features can influence the model's performance on various tasks. The findings enhance our understanding of LMMs' strengths and weaknesses, drawing parallels to human cognitive processes."}, 'zh': {'title': '揭示大型多模态模型的内部机制', 'desc': '本文探讨了大型多模态模型（LMMs）的内部神经表示如何被理解。我们首先使用稀疏自编码器（SAE）将表示解耦为人类可理解的特征。接着，我们提出了一个自动解释框架，用于解释LMMs自身学习的开放语义特征。我们的研究结果加深了对LMMs在特定任务中表现优异原因的理解，并揭示了它们错误的性质及可能的纠正策略。'}}}, {'id': 'https://huggingface.co/papers/2411.14794', 'title': 'VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection', 'url': 'https://huggingface.co/papers/2411.14794', 'abstract': 'The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso', 'score': 6, 'issue_id': 757, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'f03437948b77773f', 'authors': ['Songhao Han', 'Wei Huang', 'Hairong Shi', 'Le Zhuo', 'Xiu Su', 'Shifeng Zhang', 'Xu Zhou', 'Xiaojuan Qi', 'Yue Liao', 'Si Liu'], 'affiliations': ['Beihang University', 'CUHK', 'Central South University', 'Sangfor Technologies Inc.', 'Shanghai AI Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2411.14794.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#cv', '#games', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'VideoEspresso: новый подход к созданию данных и рассуждению для задач видеопонимания', 'desc': 'Статья представляет новый набор данных VideoEspresso для задач видеопонимания и видео-вопросно-ответных систем. Авторы разработали метод создания высококачественных пар вопрос-ответ с сохранением пространственно-временной информации и промежуточных шагов рассуждения. Предложена архитектура Hybrid LVLMs Collaboration, включающая селектор ключевых кадров и двухэтапную модель рассуждений на основе больших мультимодальных языковых моделей. Эксперименты показали превосходство предложенного подхода над существующими методами на большинстве задач видеопонимания.'}, 'en': {'title': 'Enhancing VideoQA with VideoEspresso: A New Era of Multimodal Reasoning', 'desc': 'This paper presents VideoEspresso, a new dataset designed to enhance video question-answering (VideoQA) by providing high-quality pairs that maintain spatial and temporal coherence. It addresses the limitations of existing datasets, which often rely on expensive manual annotations or ineffective automatic methods. The authors introduce a semantic-aware construction pipeline that reduces redundancy and generates QA pairs using GPT-4o, along with video Chain-of-Thought (CoT) annotations to improve reasoning. Additionally, they propose a Hybrid LVLMs Collaboration framework that optimally selects frames and performs reasoning, achieving superior performance on various tasks compared to existing models.'}, 'zh': {'title': 'VideoEspresso：提升视频推理的新数据集与框架', 'desc': '本论文介绍了一种新的视频问答数据集VideoEspresso，旨在解决现有数据集在视频推理任务中的不足。该数据集保留了重要的空间细节和时间连贯性，并提供了中间推理步骤的多模态注释。我们提出了一种混合的大型视觉语言模型协作框架，能够自适应选择核心帧并进行连锁思维推理。实验结果表明，我们的方法在多个任务上优于现有基线，展示了更强的视频推理能力。'}}}, {'id': 'https://huggingface.co/papers/2411.14762', 'title': 'Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction', 'url': 'https://huggingface.co/papers/2411.14762', 'abstract': 'Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.', 'score': 5, 'issue_id': 756, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9490a40884583735', 'authors': ['Huiwon Jang', 'Sihyun Yu', 'Jinwoo Shin', 'Pieter Abbeel', 'Younggyo Seo'], 'affiliations': ['KAIST', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.14762.jpg', 'data': {'categories': ['#long_context', '#3d', '#diffusion', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'CoordTok: Эффективная токенизация длинных видео с помощью координатного представления', 'desc': 'CoordTok - это новый токенизатор для видео, который использует координатное представление для кодирования длинных видеоклипов. Он обучается восстанавливать патчи, соответствующие случайно выбранным координатам (x,y,t), что позволяет эффективно обрабатывать длинные видео. CoordTok значительно сокращает количество токенов, необходимых для кодирования видео, по сравнению с базовыми методами. Это делает возможным эффективное по памяти обучение диффузионного трансформера для генерации длинных видео.'}, 'en': {'title': 'Efficient Video Tokenization with CoordTok', 'desc': 'This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality.'}, 'zh': {'title': '高效视频标记化，降低训练成本！', 'desc': '本论文提出了一种名为CoordTok的视频标记器，旨在高效处理长视频的标记化问题。CoordTok通过学习坐标表示与输入视频补丁之间的映射，利用了视频的时间一致性。与传统方法相比，CoordTok显著减少了编码长视频所需的标记数量，从而降低了训练成本。实验表明，CoordTok能够在保持重建质量的同时，将128帧视频的标记数量减少到1280个。'}}}, {'id': 'https://huggingface.co/papers/2411.13543', 'title': 'BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games', 'url': 'https://huggingface.co/papers/2411.13543', 'abstract': 'Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.', 'score': 3, 'issue_id': 762, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'c4fe6e6278490fc9', 'authors': ['Davide Paglieri', 'Bartłomiej Cupiał', 'Samuel Coward', 'Ulyana Piterbarg', 'Maciej Wolczyk', 'Akbir Khan', 'Eduardo Pignatelli', 'Łukasz Kuciński', 'Lerrel Pinto', 'Rob Fergus', 'Jakob Nicolaus Foerster', 'Jack Parker-Holder', 'Tim Rocktäschel'], 'affiliations': ['AI Centre, University College London', 'Anthropic', 'IDEAS NCBR', 'New York University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.13543.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark', '#reasoning', '#open_source', '#games'], 'emoji': '🎮', 'ru': {'title': 'BALROG: игровой полигон для испытания искусственного интеллекта', 'desc': 'BALROG - это новый бенчмарк для оценки возможностей больших языковых и визуально-языковых моделей в сложных игровых средах. Он включает ряд задач разной сложности, от простых до экстремально сложных, таких как NetHack Learning Environment. Исследователи провели обширное тестирование популярных моделей и обнаружили, что они справляются с простыми играми, но испытывают значительные трудности с более сложными задачами. Особенно заметны проблемы в принятии решений на основе визуальной информации.'}, 'en': {'title': 'BALROG: Benchmarking Agentic Capabilities of LLMs and VLMs', 'desc': 'This paper introduces BALROG, a new benchmark aimed at evaluating the agentic capabilities of Large Language Models (LLMs) and Vision Language Models (VLMs) in complex environments. The benchmark includes a variety of challenging games that test advanced skills like spatial reasoning and long-term planning, which are essential for real-world tasks. The authors implement fine-grained metrics to assess model performance across different difficulty levels, revealing that current models excel in simpler tasks but struggle with more complex ones, particularly in vision-based decision-making. BALROG is released as an open resource to support further research in enhancing the capabilities of LLMs and VLMs.'}, 'zh': {'title': '评估智能代理能力的新基准：BALROG', 'desc': '大型语言模型（LLMs）和视觉语言模型（VLMs）在知识和推理能力上表现出色，但在复杂动态环境中仍然面临挑战。现实任务需要处理复杂的交互、高级空间推理、长期规划和持续探索新策略，而我们在这些领域缺乏有效的评估方法。为了解决这个问题，我们提出了BALROG，一个新颖的基准，旨在通过多样化的挑战性游戏评估LLMs和VLMs的代理能力。我们的研究表明，尽管当前模型在简单游戏中取得了一定成功，但在更具挑战性的任务中表现显著不足，尤其是在基于视觉的决策方面。'}}}, {'id': 'https://huggingface.co/papers/2411.14208', 'title': 'Novel View Extrapolation with Video Diffusion Priors', 'url': 'https://huggingface.co/papers/2411.14208', 'abstract': 'The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.', 'score': 3, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'dddb7a1ecc9850f3', 'authors': ['Kunhao Liu', 'Ling Shao', 'Shijian Lu'], 'affiliations': ['Nanyang Technological University', 'UCAS-Terminus AI Lab, UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2411.14208.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': '🔭', 'ru': {'title': 'Реалистичная экстраполяция новых ракурсов с помощью генеративных моделей', 'desc': 'ViewExtrapolator - это новый подход к синтезу видов, использующий генеративные возможности Stable Video Diffusion (SVD) для реалистичной экстраполяции новых ракурсов. Метод улучшает качество изображений, полученных с помощью радиационных полей, значительно повышая четкость и реалистичность синтезированных видов. ViewExtrapolator может работать с различными типами 3D-рендеринга, включая облака точек, и не требует дополнительного обучения SVD. Эксперименты подтверждают превосходство ViewExtrapolator в задаче экстраполяции новых ракурсов.'}, 'en': {'title': 'Enhancing Novel View Extrapolation with ViewExtrapolator', 'desc': 'This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation.'}, 'zh': {'title': '提升新视图外推的清晰度与真实感', 'desc': '本论文介绍了一种新的视图合成方法，称为ViewExtrapolator，旨在改善新视图外推的效果。传统的辐射场技术在新视图插值方面表现良好，但在新视图外推时效果较差。ViewExtrapolator利用稳定视频扩散（SVD）的生成先验，通过重新设计去噪过程，显著提高了合成新视图的清晰度和真实感。该方法适用于不同类型的3D渲染，并且无需对SVD进行微调，具有数据和计算效率。'}}}, {'id': 'https://huggingface.co/papers/2411.14521', 'title': 'MyTimeMachine: Personalized Facial Age Transformation', 'url': 'https://huggingface.co/papers/2411.14521', 'abstract': "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.", 'score': 3, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'd104e8a00be886bb', 'authors': ['Luchao Qi', 'Jiaye Wu', 'Bang Gong', 'Annie N. Wang', 'David W. Jacobs', 'Roni Sengupta'], 'affiliations': ['University of Maryland, College Park', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.14521.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#cv', '#video'], 'emoji': '👴', 'ru': {'title': 'Персонализированное старение лиц с помощью глубокого обучения', 'desc': 'Статья представляет новый метод персонализированного старения лиц на изображениях, называемый MyTimeMachine (MyTM). Авторы комбинируют глобальную модель старения с личной коллекцией фотографий для создания индивидуализированной трансформации возраста. В работе вводится Адаптерная Сеть, объединяющая персонализированные и глобальные признаки старения для генерации состаренного изображения с помощью StyleGAN2. Метод также применим к видео, обеспечивая качественные, сохраняющие идентичность и темпорально согласованные эффекты старения.'}, 'en': {'title': 'Personalized Aging: Your Face, Your Time', 'desc': "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."}, 'zh': {'title': '个性化老化，真实再现', 'desc': '面部老化是一个复杂的过程，受到性别、种族、生活方式等多种因素的影响，因此很难学习到一个通用的老化模型来准确预测个体的老化情况。现有技术虽然能够生成逼真的老化效果，但重塑的图像往往与目标年龄的外貌不符，因此需要个性化处理。我们提出了MyTimeMachine（MyTM），它结合了全球老化模型和个人照片集（仅需50张图像）来学习个性化的年龄转换。我们的创新在于引入了适配器网络和三种损失函数，使得生成的老化图像能够保持身份一致性，并在视频中实现高质量的老化效果。'}}}, {'id': 'https://huggingface.co/papers/2411.15033', 'title': 'One to rule them all: natural language to bind communication, perception and action', 'url': 'https://huggingface.co/papers/2411.15033', 'abstract': 'In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.', 'score': 2, 'issue_id': 758, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'f76bbfe3f8854ad7', 'authors': ['Simone Colombani', 'Dimitri Ognibene', 'Giuseppe Boccignone'], 'affiliations': ['Oversonic Robotics, Carate Brianza, Italy', 'University of Milan, Italy', 'University of Milano-Bicocca, Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2411.15033.jpg', 'data': {'categories': ['#games', '#architecture', '#agents', '#robotics', '#optimization', '#agi', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Интеллектуальные роботы: взаимодействие с человеком на новом уровне', 'desc': 'Статья представляет передовую архитектуру для планирования действий роботов, интегрирующую коммуникацию, восприятие и планирование с использованием больших языковых моделей (LLM). Система переводит команды на естественном языке в исполняемые действия робота, учитывая информацию об окружающей среде и динамически обновляя планы на основе обратной связи в реальном времени. Ключевым компонентом является Модуль Планировщика, где LLM, встроенные в модифицированную структуру ReAct, интерпретируют и выполняют команды пользователя. Архитектура повышает адаптивность робота, выполнение задач и взаимодействие с пользователями в динамических средах.'}, 'en': {'title': 'Empowering Robots with Natural Language Understanding and Dynamic Adaptability', 'desc': "This paper discusses a new architecture for robots that helps them understand and follow human instructions in real-time. It combines Large Language Models (LLMs) with a planning system that allows robots to interpret natural language commands and adapt to changing environments. The core of this system is a Planner Module that uses LLMs to translate user requests into actions while considering the current state of the environment. By incorporating feedback loops and a dynamic semantic map, the architecture improves the robot's ability to work alongside humans and adjust its plans as needed."}, 'zh': {'title': '智能机器人：自然语言与动态环境的完美结合', 'desc': '近年来，人机交互领域的研究集中在开发能够理解复杂人类指令并在动态多样环境中执行任务的机器人。这些系统在个人助理和工业机器人等多个应用中具有广泛的应用，强调机器人与人类灵活、自然和安全的互动。本文提出了一种先进的机器人行动规划架构，结合了通信、感知和规划，利用大型语言模型（LLMs）将自然语言表达的命令转化为可执行的机器人动作。通过实时反馈动态更新计划，该系统提高了机器人在共享和动态环境中适应性、任务执行和与人类用户的无缝协作能力。'}}}, {'id': 'https://huggingface.co/papers/2411.15131', 'title': 'WildLMa: Long Horizon Loco-Manipulation in the Wild', 'url': 'https://huggingface.co/papers/2411.15131', 'abstract': "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.", 'score': 2, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'c38df92e9599db09', 'authors': ['Ri-Zhao Qiu', 'Yuchen Song', 'Xuanbin Peng', 'Sai Aneesh Suryadevara', 'Ge Yang', 'Minghuan Liu', 'Mazeyu Ji', 'Chengzhe Jia', 'Ruihan Yang', 'Xueyan Zou', 'Xiaolong Wang'], 'affiliations': ['MIT', 'NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2411.15131.jpg', 'data': {'categories': ['#rl', '#robotics', '#training', '#agi', '#transfer_learning', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'WildLMa: Универсальный робот-манипулятор для реального мира', 'desc': 'Статья представляет систему WildLMa для мобильной манипуляции роботов в реальных условиях. Система включает адаптивный низкоуровневый контроллер для телеоперации, библиотеку обобщаемых визуомоторных навыков и планировщик на основе языковых моделей для выполнения долгосрочных задач. WildLMa использует имитационное обучение и CLIP для генерализации на новые объекты, демонстрируя высокую точность захвата при небольшом количестве обучающих примеров. Система показала свою эффективность в практических задачах, таких как уборка мусора и перестановка предметов.'}, 'en': {'title': 'Empowering Robots for Real-World Manipulation with WildLMa', 'desc': 'This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training.'}, 'zh': {'title': 'WildLMa：提升机器人在真实环境中的操控能力', 'desc': '本论文提出了一种名为WildLMa的移动操控机器人系统，旨在解决在多样化真实环境中执行复杂任务的挑战。该系统包括三个主要组件：适应性低级控制器、通用视觉运动技能库和长时间任务规划接口。通过模仿学习和高质量训练数据，WildLMa在抓取成功率上超越了现有的强化学习基线。该研究展示了机器人在实际应用中的潜力，如清理校园走廊垃圾和操作复杂物体。'}}}, {'id': 'https://huggingface.co/papers/2411.15115', 'title': 'VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement', 'url': 'https://huggingface.co/papers/2411.15115', 'abstract': 'Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VideoRepair, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VideoRepair consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using a combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VideoRepair substantially outperforms recent baselines across various text-video alignment metrics. We provide a comprehensive analysis of VideoRepair components and qualitative examples.', 'score': 1, 'issue_id': 763, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9d767f888609fdd4', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.15115.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#alignment', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умное исправление видео с помощью ИИ', 'desc': 'VideoRepair - это новая модель для улучшения качества видео, созданных с помощью text-to-video диффузионных моделей. Она автоматически определяет несоответствия между текстовым описанием и сгенерированным видео, особенно для сложных сцен с множеством объектов. VideoRepair генерирует детальные пространственные и текстовые рекомендации для целенаправленного улучшения проблемных областей видео. Модель состоит из четырех этапов: оценка видео, планирование улучшений, декомпозиция регионов и локализованное улучшение.'}, 'en': {'title': 'Enhancing Text-Video Alignment with VideoRepair', 'desc': 'The paper presents VideoRepair, a new framework designed to improve the alignment between text prompts and generated videos in text-to-video (T2V) diffusion models. It identifies and corrects misalignments by generating specific evaluation questions and using a multi-layer language model (MLLM) to assess the video. The framework operates in four stages: evaluating the video for misalignments, planning refinements, segmenting correctly generated areas, and finally refining the misaligned regions while keeping the accurate parts intact. VideoRepair shows significant improvements over existing methods on standard benchmarks, demonstrating its effectiveness in enhancing text-video coherence.'}, 'zh': {'title': 'VideoRepair：提升文本到视频生成的精确度', 'desc': '最近的文本到视频（T2V）扩散模型在多个领域展示了出色的生成能力。然而，这些模型在生成视频时，常常与文本提示存在不一致，尤其是在描述复杂场景时。为了解决这个问题，我们提出了VideoRepair，这是一种新颖的无模型、无训练的视频修复框架，能够自动识别细粒度的文本视频不对齐，并生成明确的空间和文本反馈，从而使T2V扩散模型能够进行有针对性的局部修正。VideoRepair通过视频评估、修复规划、区域分解和局部修复四个阶段来实现视频的优化。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi (2)', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (5)', '#cv (3)', '#data (3)', '#dataset (4)', '#diffusion (5)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (2)', '#plp', '#rag', '#reasoning (2)', '#rl (2)', '#rlhf', '#robotics (2)', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (3)', '#training (5)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-25 13:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-25 13:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-25 13:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    