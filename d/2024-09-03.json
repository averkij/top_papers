{
    "date": {
        "ru": "3 сентября",
        "en": "September 3",
        "zh": "9月3日"
    },
    "time_utc": "2024-09-03 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-03",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2408.16725",
            "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
            "url": "https://huggingface.co/papers/2408.16725",
            "abstract": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
            "score": 52,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "ec2a8657c71d7ff4",
            "data": {
                "categories": [
                    "#training",
                    "#audio",
                    "#reasoning",
                    "#open_source",
                    "#synthetic",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в речевом ИИ: модель Mini-Omni для общения в реальном времени",
                    "desc": "В статье представлена модель Mini-Omni - первая полностью сквозная модель для речевого взаимодействия в реальном времени. Авторы предлагают метод генерации речи на основе текстовых инструкций и стратегии пакетно-параллельного вывода для повышения производительности. Модель способна вести диалог на естественном языке без использования дополнительных систем синтеза речи. Для обучения модели был создан набор данных VoiceAssistant-400K."
                },
                "en": {
                    "title": "Revolutionizing Real-Time Speech Interaction with Mini-Omni",
                    "desc": "This paper presents Mini-Omni, a groundbreaking end-to-end conversational model designed for real-time speech interaction. Unlike traditional models that rely on separate text-to-speech (TTS) systems, Mini-Omni integrates audio processing directly, reducing latency and enhancing user experience. The authors introduce a novel text-instructed speech generation method and batch-parallel strategies to optimize performance while preserving the model's language capabilities. Additionally, they provide the VoiceAssistant-400K dataset to further refine models for effective speech output, marking a significant advancement in human-computer interaction."
                },
                "zh": {
                    "title": "Mini-Omni：实时语音交互的新突破",
                    "desc": "最近语言模型取得了显著进展，GPT-4o作为一个新里程碑，实现了与人类的实时对话，展现出接近人类的自然流畅性。为了实现这种人机交互，模型需要具备直接进行音频推理的能力，并能够实时生成输出。然而，目前的学术模型通常依赖额外的文本转语音（TTS）系统进行语音合成，导致不必要的延迟。本文介绍了Mini-Omni，一个基于音频的端到端对话模型，能够实现实时语音交互，并提出了一种文本指导的语音生成方法，结合推理过程中的批量并行策略，以提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.17253",
            "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters",
            "url": "https://huggingface.co/papers/2408.17253",
            "abstract": "Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either fine-tune large language models (LLMs) or build large-scale time-series datasets to develop TSF foundation models. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. In this paper, we explore a new road to building a TSF foundation model from rich and high-quality natural images, based on the intrinsic similarities between images and time series. To bridge the gap between the two domains, we reformulate the TSF task as an image reconstruction task, which is further processed by a visual masked autoencoder (MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly, without further adaptation in the time-series domain, the proposed VisionTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With minimal fine-tuning, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. These findings suggest that visual models could be a free lunch for TSF and highlight the potential for future cross-domain research between computer vision and TSF. Our code is publicly available at https://github.com/Keytoyze/VisionTS.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2024-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "91bf312b245e7fb3",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#transfer_learning",
                    "#dataset",
                    "#graphs"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Изображения как ключ к прогнозированию временных рядов",
                    "desc": "Статья представляет новый подход к созданию фундаментальной модели для прогнозирования временных рядов (TSF), используя богатые и качественные естественные изображения. Авторы переформулируют задачу TSF как задачу реконструкции изображения, которая обрабатывается визуальным маскированным автоэнкодером (MAE), предварительно обученным на наборе данных ImageNet. Предложенная модель VisionTS достигает превосходной производительности в прогнозировании с нулевым обучением по сравнению с существующими фундаментальными моделями TSF. Эти результаты показывают, что визуальные модели могут быть эффективным инструментом для TSF и открывают потенциал для будущих междоменных исследований между компьютерным зрением и прогнозированием временных рядов."
                },
                "en": {
                    "title": "Bridging Time Series and Vision: A New Era in Forecasting",
                    "desc": "This paper introduces a novel approach to time series forecasting (TSF) by leveraging foundation models derived from natural images. The authors propose to treat the TSF task as an image reconstruction problem, utilizing a visual masked autoencoder (MAE) that has been pre-trained on the ImageNet dataset. Their method, named VisionTS, demonstrates impressive zero-shot forecasting capabilities without needing extensive adaptation to the time-series domain. With minimal fine-tuning, VisionTS achieves state-of-the-art performance, suggesting that visual models can significantly enhance TSF and encouraging further exploration of cross-domain applications between computer vision and time series analysis."
                },
                "zh": {
                    "title": "视觉模型助力时间序列预测的未来",
                    "desc": "本论文探讨了如何利用丰富的高质量自然图像来构建时间序列预测（TSF）基础模型。我们将TSF任务重新表述为图像重建任务，并使用在ImageNet数据集上自监督预训练的视觉掩码自编码器（MAE）进行处理。研究发现，VisionTS在没有进一步适应时间序列领域的情况下，能够实现优越的零-shot预测性能。通过最小的微调，VisionTS在大多数情况下进一步提高了预测性能，达到了最先进的水平，显示了计算机视觉与时间序列预测之间的跨领域研究潜力。"
                }
            }
        }
    ],
    "link_prev": "2024-09-02.html",
    "link_next": "2024-09-04.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9月2日"
    },
    "short_date_next": {
        "ru": "04.09",
        "en": "09/04",
        "zh": "9月4日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}