{
    "date": {
        "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 3",
        "zh": "9æœˆ3æ—¥"
    },
    "time_utc": "2024-09-03 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-03",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2408.16725",
            "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
            "url": "https://huggingface.co/papers/2408.16725",
            "abstract": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
            "score": 52,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "ec2a8657c71d7ff4",
            "data": {
                "categories": [
                    "#training",
                    "#audio",
                    "#reasoning",
                    "#open_source",
                    "#synthetic",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¼ Ğ˜Ğ˜: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mini-Omni Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mini-Omni - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾-Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VoiceAssistant-400K."
                },
                "en": {
                    "title": "Revolutionizing Real-Time Speech Interaction with Mini-Omni",
                    "desc": "This paper presents Mini-Omni, a groundbreaking end-to-end conversational model designed for real-time speech interaction. Unlike traditional models that rely on separate text-to-speech (TTS) systems, Mini-Omni integrates audio processing directly, reducing latency and enhancing user experience. The authors introduce a novel text-instructed speech generation method and batch-parallel strategies to optimize performance while preserving the model's language capabilities. Additionally, they provide the VoiceAssistant-400K dataset to further refine models for effective speech output, marking a significant advancement in human-computer interaction."
                },
                "zh": {
                    "title": "Mini-Omniï¼šå®æ—¶è¯­éŸ³äº¤äº’çš„æ–°çªç ´",
                    "desc": "æœ€è¿‘è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒGPT-4oä½œä¸ºä¸€ä¸ªæ–°é‡Œç¨‹ç¢‘ï¼Œå®ç°äº†ä¸äººç±»çš„å®æ—¶å¯¹è¯ï¼Œå±•ç°å‡ºæ¥è¿‘äººç±»çš„è‡ªç„¶æµç•…æ€§ã€‚ä¸ºäº†å®ç°è¿™ç§äººæœºäº¤äº’ï¼Œæ¨¡å‹éœ€è¦å…·å¤‡ç›´æ¥è¿›è¡ŒéŸ³é¢‘æ¨ç†çš„èƒ½åŠ›ï¼Œå¹¶èƒ½å¤Ÿå®æ—¶ç”Ÿæˆè¾“å‡ºã€‚ç„¶è€Œï¼Œç›®å‰çš„å­¦æœ¯æ¨¡å‹é€šå¸¸ä¾èµ–é¢å¤–çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè¿›è¡Œè¯­éŸ³åˆæˆï¼Œå¯¼è‡´ä¸å¿…è¦çš„å»¶è¿Ÿã€‚æœ¬æ–‡ä»‹ç»äº†Mini-Omniï¼Œä¸€ä¸ªåŸºäºéŸ³é¢‘çš„ç«¯åˆ°ç«¯å¯¹è¯æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶è¯­éŸ³äº¤äº’ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–‡æœ¬æŒ‡å¯¼çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆæ¨ç†è¿‡ç¨‹ä¸­çš„æ‰¹é‡å¹¶è¡Œç­–ç•¥ï¼Œä»¥æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.17253",
            "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters",
            "url": "https://huggingface.co/papers/2408.17253",
            "abstract": "Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either fine-tune large language models (LLMs) or build large-scale time-series datasets to develop TSF foundation models. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. In this paper, we explore a new road to building a TSF foundation model from rich and high-quality natural images, based on the intrinsic similarities between images and time series. To bridge the gap between the two domains, we reformulate the TSF task as an image reconstruction task, which is further processed by a visual masked autoencoder (MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly, without further adaptation in the time-series domain, the proposed VisionTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With minimal fine-tuning, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. These findings suggest that visual models could be a free lunch for TSF and highlight the potential for future cross-domain research between computer vision and TSF. Our code is publicly available at https://github.com/Keytoyze/VisionTS.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2024-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "91bf312b245e7fb3",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#transfer_learning",
                    "#dataset",
                    "#graphs"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² (TSF), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ TSF ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ (MAE), Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VisionTS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ TSF. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ TSF Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging Time Series and Vision: A New Era in Forecasting",
                    "desc": "This paper introduces a novel approach to time series forecasting (TSF) by leveraging foundation models derived from natural images. The authors propose to treat the TSF task as an image reconstruction problem, utilizing a visual masked autoencoder (MAE) that has been pre-trained on the ImageNet dataset. Their method, named VisionTS, demonstrates impressive zero-shot forecasting capabilities without needing extensive adaptation to the time-series domain. With minimal fine-tuning, VisionTS achieves state-of-the-art performance, suggesting that visual models can significantly enhance TSF and encouraging further exploration of cross-domain applications between computer vision and time series analysis."
                },
                "zh": {
                    "title": "è§†è§‰æ¨¡å‹åŠ©åŠ›æ—¶é—´åºåˆ—é¢„æµ‹çš„æœªæ¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ä¸°å¯Œçš„é«˜è´¨é‡è‡ªç„¶å›¾åƒæ¥æ„å»ºæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å°†TSFä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨åœ¨ImageNetæ•°æ®é›†ä¸Šè‡ªç›‘ç£é¢„è®­ç»ƒçš„è§†è§‰æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰è¿›è¡Œå¤„ç†ã€‚ç ”ç©¶å‘ç°ï¼ŒVisionTSåœ¨æ²¡æœ‰è¿›ä¸€æ­¥é€‚åº”æ—¶é—´åºåˆ—é¢†åŸŸçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°ä¼˜è¶Šçš„é›¶-shoté¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡æœ€å°çš„å¾®è°ƒï¼ŒVisionTSåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¿›ä¸€æ­¥æé«˜äº†é¢„æµ‹æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾ç¤ºäº†è®¡ç®—æœºè§†è§‰ä¸æ—¶é—´åºåˆ—é¢„æµ‹ä¹‹é—´çš„è·¨é¢†åŸŸç ”ç©¶æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-02.html",
    "link_next": "2024-09-04.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.09",
        "en": "09/04",
        "zh": "9æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}