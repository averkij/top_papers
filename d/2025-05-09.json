{
    "date": {
        "ru": "9 мая",
        "en": "May 9",
        "zh": "5月9日"
    },
    "time_utc": "2025-05-09 03:36",
    "weekday": 4,
    "issue_id": 3671,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.02847",
            "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.02847",
            "abstract": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.",
            "score": 8,
            "issue_id": 3671,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "9204f0ca97eb8bc7",
            "authors": [
                "Bang Zhang",
                "Ruotian Ma",
                "Qingxuan Jiang",
                "Peisong Wang",
                "Jiaqi Chen",
                "Zheng Xie",
                "Xingyu Chen",
                "Yue Wang",
                "Fanghua Ye",
                "Jian Li",
                "Yifan Yang",
                "Zhaopeng Tu",
                "Xiaolong Li"
            ],
            "affiliations": [
                "Hunyuan AI Digital Human, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02847.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#alignment",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAGE: Измерение эмпатии и социального интеллекта языковых моделей",
                    "desc": "Статья представляет SAGE - новую систему оценки способности больших языковых моделей (LLM) понимать человека, а не просто текст. SAGE использует 'Разумного Агента', который симулирует человеческие эмоции и мысли во время взаимодействия, обеспечивая более реалистичную оценку тестируемой модели в многоходовых диалогах. Эксперименты показали, что эмоциональные оценки SAGE коррелируют с психологическими метриками. Система также используется для создания публичного рейтинга LLM, выявляющего значительные различия между передовыми и базовыми моделями."
                },
                "en": {
                    "title": "Measuring Empathy in AI: The SAGE Framework",
                    "desc": "The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems."
                },
                "zh": {
                    "title": "评估语言模型的情感理解能力",
                    "desc": "本文介绍了一种名为SAGE的自动评估框架，用于测量大型语言模型（LLM）对人类情感和社交认知的理解能力。SAGE通过模拟人类情感变化和内心想法，提供了更真实的多轮对话评估。实验结果表明，SAGE的情感评分与心理学评估工具的评分高度相关，验证了其心理学的真实性。该框架还建立了一个公开的Sentient排行榜，揭示了不同模型之间的显著差距，推动了对更具同理心和社交能力的语言代理的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04620",
            "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
            "url": "https://huggingface.co/papers/2505.04620",
            "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/",
            "score": 7,
            "issue_id": 3671,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "57991e528141671e",
            "authors": [
                "Hao Fei",
                "Yuan Zhou",
                "Juncheng Li",
                "Xiangtai Li",
                "Qingshan Xu",
                "Bobo Li",
                "Shengqiong Wu",
                "Yaoting Wang",
                "Junbao Zhou",
                "Jiahao Meng",
                "Qingyu Shi",
                "Zhiyuan Zhou",
                "Liangtao Shi",
                "Minghe Gao",
                "Daoan Zhang",
                "Zhiqi Ge",
                "Weiming Wu",
                "Siliang Tang",
                "Kaihang Pan",
                "Yaobo Ye",
                "Haobo Yuan",
                "Tao Zhang",
                "Tianjie Ju",
                "Zixiang Meng",
                "Shilin Xu",
                "Liyu Jia",
                "Wentao Hu",
                "Meng Luo",
                "Jiebo Luo",
                "Tat-Seng Chua",
                "Shuicheng Yan",
                "Hanwang Zhang"
            ],
            "affiliations": [
                "HFUT",
                "KAUST",
                "NJU",
                "NTU",
                "NUS",
                "PKU",
                "SJTU",
                "UR",
                "WHU",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04620.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый подход к оценке мультимодальных ИИ-систем на пути к AGI",
                    "desc": "Статья описывает новую систему оценки мультимодальных больших языковых моделей (MLLM) под названием General-Level. Эта система определяет 5 уровней производительности и обобщаемости MLLM, предлагая методологию для сравнения моделей и оценки прогресса существующих систем. Ключевым понятием в этой системе является концепция Синергии, которая измеряет согласованность возможностей моделей в понимании и генерации контента, а также в работе с различными модальностями. Для поддержки этой системы оценки авторы представляют General-Bench - набор из более чем 700 задач и 325 800 примеров, охватывающий широкий спектр навыков, модальностей и форматов."
                },
                "en": {
                    "title": "Towards Multimodal Generalists: Evaluating MLLM Progress",
                    "desc": "The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "迈向真正的多模态通用人工智能",
                    "desc": "多模态大型语言模型（MLLM）正在快速发展，得益于大型语言模型（LLM）的先进能力。现有的MLLM正朝着多模态通用主义者的方向演变，不仅能够理解多种模态，还能在不同模态之间生成内容。本文提出了一种新的评估框架——General-Level，定义了MLLM性能和通用性的五个等级，以便比较不同模型的能力。通过General-Bench，我们提供了一个更广泛的技能和任务评估，揭示了当前多模态通用模型在实现真正人工智能方面的挑战。"
                }
            }
        }
    ],
    "link_prev": "2025-05-08.html",
    "link_next": "2025-05-12.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "08.05",
        "en": "05/08",
        "zh": "5月8日"
    },
    "short_date_next": {
        "ru": "12.05",
        "en": "05/12",
        "zh": "5月12日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLMs）搜索能力的重要性。最近的研究使用强化学习（RL）与实时搜索引擎互动来改进LLMs的搜索能力，但面临文档质量不可控和API费用高昂的挑战。为解决这些问题，作者提出了ZeroSearch，一种不需要与实际搜索引擎互动的RL框架。通过轻量级的监督微调和基于课程的滚动策略，ZeroSearch能够有效提升LLMs的搜索能力，并且在不同参数规模的模型上表现良好。",
        "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
        "pinyin": "Zhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLMs) sōusuǒ nénglì de zhòngyàoxìng. Zuìjìn de yánjiū shǐyòng qiángzhì xuéxí (RL) yǔ shíshí sōusuǒ yǐnqíng hùdòng lái gǎijìn LLMs de sōusuǒ nénglì, dàn miànlín wénjiàn zhìliàng bù kě kòng hé API fèiyòng gāo'áng de tiǎozhàn. Wèi jiějué zhèxiē wèntí, zuòzhě tíchū le ZeroSearch, yīzhǒng bù xūyào yǔ shíjì sōusuǒ yǐnqíng hùdòng de RL kuàngjià. Tōngguò qīngliàngjí de jiàndū wēitiáo hé jīyú kèchéng de gǔndòng cèlüè, ZeroSearch nénggòu yǒuxiào tíshēng LLMs de sōusuǒ nénglì, bìngqiě zài bùtóng cānshù guīmó de móxíng shàng biǎoxiàn liánghǎo.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improve'},\n{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '搜索', 'pinyin': 'sōu suǒ', 'trans': 'search'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'},\n{'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'},\n{'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'},\n{'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '不可控', 'pinyin': 'bù kě kòng', 'trans': 'uncontrollable'},\n{'word': 'API', 'pinyin': 'API', 'trans': 'API'},\n{'word': '费用', 'pinyin': 'fèi yòng', 'trans': 'cost'},\n{'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '轻量级', 'pinyin': 'qīng liàng jí', 'trans': 'lightweight'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'},\n{'word': '课程', 'pinyin': 'kè chéng', 'trans': 'course'},\n{'word': '滚动', 'pinyin': 'gǔn dòng', 'trans': 'rolling'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '良好', 'pinyin': 'liáng hǎo', 'trans': 'good'},\n{'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameter'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'}]",
        "trans": "This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes.",
        "update_ts": "2025-05-08 09:12"
    }
}