{
    "date": {
        "ru": "9 мая",
        "en": "May 9",
        "zh": "5月9日"
    },
    "time_utc": "2025-05-09 05:12",
    "weekday": 4,
    "issue_id": 3673,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04620",
            "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
            "url": "https://huggingface.co/papers/2505.04620",
            "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/",
            "score": 26,
            "issue_id": 3671,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "57991e528141671e",
            "authors": [
                "Hao Fei",
                "Yuan Zhou",
                "Juncheng Li",
                "Xiangtai Li",
                "Qingshan Xu",
                "Bobo Li",
                "Shengqiong Wu",
                "Yaoting Wang",
                "Junbao Zhou",
                "Jiahao Meng",
                "Qingyu Shi",
                "Zhiyuan Zhou",
                "Liangtao Shi",
                "Minghe Gao",
                "Daoan Zhang",
                "Zhiqi Ge",
                "Weiming Wu",
                "Siliang Tang",
                "Kaihang Pan",
                "Yaobo Ye",
                "Haobo Yuan",
                "Tao Zhang",
                "Tianjie Ju",
                "Zixiang Meng",
                "Shilin Xu",
                "Liyu Jia",
                "Wentao Hu",
                "Meng Luo",
                "Jiebo Luo",
                "Tat-Seng Chua",
                "Shuicheng Yan",
                "Hanwang Zhang"
            ],
            "affiliations": [
                "HFUT",
                "KAUST",
                "NJU",
                "NTU",
                "NUS",
                "PKU",
                "SJTU",
                "UR",
                "WHU",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04620.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый подход к оценке мультимодальных ИИ-систем на пути к AGI",
                    "desc": "Статья описывает новую систему оценки мультимодальных больших языковых моделей (MLLM) под названием General-Level. Эта система определяет 5 уровней производительности и обобщаемости MLLM, предлагая методологию для сравнения моделей и оценки прогресса существующих систем. Ключевым понятием в этой системе является концепция Синергии, которая измеряет согласованность возможностей моделей в понимании и генерации контента, а также в работе с различными модальностями. Для поддержки этой системы оценки авторы представляют General-Bench - набор из более чем 700 задач и 325 800 примеров, охватывающий широкий спектр навыков, модальностей и форматов."
                },
                "en": {
                    "title": "Towards Multimodal Generalists: Evaluating MLLM Progress",
                    "desc": "The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "迈向真正的多模态通用人工智能",
                    "desc": "多模态大型语言模型（MLLM）正在快速发展，得益于大型语言模型（LLM）的先进能力。现有的MLLM正朝着多模态通用主义者的方向演变，不仅能够理解多种模态，还能在不同模态之间生成内容。本文提出了一种新的评估框架——General-Level，定义了MLLM性能和通用性的五个等级，以便比较不同模型的能力。通过General-Bench，我们提供了一个更广泛的技能和任务评估，揭示了当前多模态通用模型在实现真正人工智能方面的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02847",
            "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.02847",
            "abstract": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.",
            "score": 10,
            "issue_id": 3671,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "9204f0ca97eb8bc7",
            "authors": [
                "Bang Zhang",
                "Ruotian Ma",
                "Qingxuan Jiang",
                "Peisong Wang",
                "Jiaqi Chen",
                "Zheng Xie",
                "Xingyu Chen",
                "Yue Wang",
                "Fanghua Ye",
                "Jian Li",
                "Yifan Yang",
                "Zhaopeng Tu",
                "Xiaolong Li"
            ],
            "affiliations": [
                "Hunyuan AI Digital Human, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02847.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#alignment",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAGE: Измерение эмпатии и социального интеллекта языковых моделей",
                    "desc": "Статья представляет SAGE - новую систему оценки способности больших языковых моделей (LLM) понимать человека, а не просто текст. SAGE использует 'Разумного Агента', который симулирует человеческие эмоции и мысли во время взаимодействия, обеспечивая более реалистичную оценку тестируемой модели в многоходовых диалогах. Эксперименты показали, что эмоциональные оценки SAGE коррелируют с психологическими метриками. Система также используется для создания публичного рейтинга LLM, выявляющего значительные различия между передовыми и базовыми моделями."
                },
                "en": {
                    "title": "Measuring Empathy in AI: The SAGE Framework",
                    "desc": "The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems."
                },
                "zh": {
                    "title": "评估语言模型的情感理解能力",
                    "desc": "本文介绍了一种名为SAGE的自动评估框架，用于测量大型语言模型（LLM）对人类情感和社交认知的理解能力。SAGE通过模拟人类情感变化和内心想法，提供了更真实的多轮对话评估。实验结果表明，SAGE的情感评分与心理学评估工具的评分高度相关，验证了其心理学的真实性。该框架还建立了一个公开的Sentient排行榜，揭示了不同模型之间的显著差距，推动了对更具同理心和社交能力的语言代理的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05315",
            "title": "Scalable Chain of Thoughts via Elastic Reasoning",
            "url": "https://huggingface.co/papers/2505.05315",
            "abstract": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.",
            "score": 8,
            "issue_id": 3672,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "0ce3be6057da3ed2",
            "authors": [
                "Yuhui Xu",
                "Hanze Dong",
                "Lei Wang",
                "Doyen Sahoo",
                "Junnan Li",
                "Caiming Xiong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.05315.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эластичное рассуждение: эффективные цепочки мысли в условиях ограниченных ресурсов",
                    "desc": "Эта статья представляет новый подход под названием 'Эластичное рассуждение' для крупных моделей рассуждения (LRM). Метод разделяет процесс рассуждения на две фазы - мышление и решение - с независимыми бюджетами, что позволяет эффективно работать в условиях ограниченных ресурсов. Авторы предлагают стратегию обучения с ограниченным бюджетом, интегрированную в GRPO, которая учит модель адаптивно рассуждать при сокращении процесса мышления. Эксперименты на математических и программистских задачах показывают, что 'Эластичное рассуждение' работает надежно при строгих ограничениях бюджета и производит более краткие и эффективные рассуждения даже в неограниченных условиях."
                },
                "en": {
                    "title": "Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models",
                    "desc": "This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs."
                },
                "zh": {
                    "title": "弹性推理：可控推理的新解决方案",
                    "desc": "大型推理模型（LRMs）在复杂任务上取得了显著进展，但其输出长度不受控制，给实际应用带来了挑战。我们提出了一种名为弹性推理的新框架，将推理过程分为思考和解决两个阶段，并为每个阶段分配独立的预算。在测试时，弹性推理优先考虑解决方案的完整性，从而在资源紧张的情况下显著提高可靠性。我们的实验证明，弹性推理在严格的预算限制下表现出色，同时训练成本显著低于基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05474",
            "title": "3D Scene Generation: A Survey",
            "url": "https://huggingface.co/papers/2505.05474",
            "abstract": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.",
            "score": 5,
            "issue_id": 3672,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "06bda1a6228b8f26",
            "authors": [
                "Beichen Wen",
                "Haozhe Xie",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore 637335"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05474.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#multimodal",
                    "#synthetic",
                    "#survey"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Новые горизонты в генерации трехмерных сцен: от процедурных методов к нейронным сетям",
                    "desc": "Эта статья представляет собой обзор современных методов генерации трехмерных сцен. Авторы систематизируют подходы по четырем парадигмам: процедурная генерация, нейронная генерация на основе 3D, генерация на основе изображений и генерация на основе видео. В работе анализируются технические основы, компромиссы и репрезентативные результаты каждого подхода, а также рассматриваются наборы данных, протоколы оценки и прикладные задачи. Статья завершается обсуждением ключевых проблем и перспективных направлений в области генерации 3D-сцен, включая повышение точности, физически корректную и интерактивную генерацию."
                },
                "en": {
                    "title": "Advancing 3D Scene Generation with Deep Learning",
                    "desc": "This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation."
                },
                "zh": {
                    "title": "3D场景生成的未来方向与挑战",
                    "desc": "3D场景生成旨在合成具有空间结构、语义意义和照片真实感的环境，广泛应用于沉浸式媒体、机器人、自动驾驶和具身人工智能等领域。早期基于程序规则的方法虽然具有可扩展性，但多样性有限。近年来，深度生成模型（如GAN和扩散模型）以及3D表示（如NeRF和3D高斯）取得了进展，使得能够学习真实世界场景的分布，从而提高了生成的真实感、多样性和视图一致性。本文综述了最新的3D场景生成方法，分析了其技术基础、权衡和代表性结果，并讨论了生成能力、3D表示、数据和评估等关键挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03981",
            "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
            "url": "https://huggingface.co/papers/2505.03981",
            "abstract": "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.",
            "score": 2,
            "issue_id": 3672,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "0e6c2f37e1536f9f",
            "authors": [
                "Qianchu Liu",
                "Sheng Zhang",
                "Guanghui Qin",
                "Timothy Ossowski",
                "Yu Gu",
                "Ying Jin",
                "Sid Kiblawi",
                "Sam Preston",
                "Mu Wei",
                "Paul Vozila",
                "Tristan Naumann",
                "Hoifung Poon"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03981.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#transfer_learning",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обобщаемые рассуждения: от текста к мультимодальности и специализированным доменам",
                    "desc": "Эта статья исследует возможность обобщения способностей к рассуждению на различные модальности и домены. Авторы представляют X-Reasoner - мультимодальную модель, обученную только на текстовых данных общего домена для обобщаемых рассуждений. Эксперименты показывают, что X-Reasoner успешно переносит навыки рассуждения на мультимодальные и узкоспециализированные задачи, превосходя существующие модели. Также представлена медицинская версия модели - X-Reasoner-Med, достигающая новых рекордных результатов на ряде текстовых и мультимодальных медицинских бенчмарков."
                },
                "en": {
                    "title": "Unlocking Generalizable Reasoning Across Modalities with X-Reasoner",
                    "desc": "This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks."
                },
                "zh": {
                    "title": "推理能力的跨模态推广",
                    "desc": "最近的专有模型（如o3）展示了强大的多模态推理能力。然而，大多数现有的开源研究主要集中在训练仅基于文本的推理模型，评估也主要限于数学和一般领域任务。因此，如何有效地将推理能力扩展到文本输入和一般领域之外仍然不清楚。本文探讨了一个基本的研究问题：推理是否可以跨模态和领域进行推广？"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19314",
            "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
            "url": "https://huggingface.co/papers/2504.19314",
            "abstract": "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.",
            "score": 2,
            "issue_id": 3673,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "06aff0f566bd3817",
            "authors": [
                "Peilin Zhou",
                "Bruce Leon",
                "Xiang Ying",
                "Can Zhang",
                "Yifan Shao",
                "Qichen Ye",
                "Dading Chong",
                "Zhiling Jin",
                "Chenxuan Xie",
                "Meng Cao",
                "Yuxin Gu",
                "Sixin Hong",
                "Jing Ren",
                "Jian Chen",
                "Chao Liu",
                "Yining Hua"
            ],
            "affiliations": [
                "Alibaba Group",
                "HSBC",
                "Harvard T.H. Chan School of Public Health",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "MBZUAI",
                "Mindverse AI",
                "NIO",
                "Peking University",
                "Zhejiang University",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19314.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "BrowseComp-ZH: испытание языковых моделей в китайском интернете",
                    "desc": "Статья представляет BrowseComp-ZH - новый бенчмарк для оценки способностей языковых моделей (ЯМ) работать с китайским веб-контентом. Бенчмарк состоит из 289 сложных многоэтапных вопросов в 11 различных областях. Несмотря на свои сильные разговорные и поисковые возможности, большинство современных ЯМ показывают низкую точность на этом тесте. Результаты демонстрируют, что для успеха в BrowseComp-ZH требуются не только эффективные стратегии поиска, но и сложные рассуждения и согласование информации."
                },
                "en": {
                    "title": "Evaluating LLMs: The Challenge of Chinese Web Browsing",
                    "desc": "This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments."
                },
                "zh": {
                    "title": "中文网络智能体评估新基准：BrowseComp-ZH",
                    "desc": "随着大型语言模型（LLMs）逐渐演变为使用工具的智能体，实时浏览网络的能力成为衡量其推理和检索能力的重要标准。现有的基准测试如BrowseComp主要集中在英语，忽视了其他主要信息生态系统（尤其是中文）在语言、基础设施和审查方面的复杂性。为了解决这一问题，我们推出了BrowseComp-ZH，这是一个专门为全面评估中文网络上的LLM智能体而设计的高难度基准测试。该基准测试包含289个跨越11个不同领域的多跳问题，旨在考察模型的检索策略、推理能力和信息整合能力。"
                }
            }
        }
    ],
    "link_prev": "2025-05-08.html",
    "link_next": "2025-05-12.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "08.05",
        "en": "05/08",
        "zh": "5月8日"
    },
    "short_date_next": {
        "ru": "12.05",
        "en": "05/12",
        "zh": "5月12日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLMs）搜索能力的重要性。最近的研究使用强化学习（RL）与实时搜索引擎互动来改进LLMs的搜索能力，但面临文档质量不可控和API费用高昂的挑战。为解决这些问题，作者提出了ZeroSearch，一种不需要与实际搜索引擎互动的RL框架。通过轻量级的监督微调和基于课程的滚动策略，ZeroSearch能够有效提升LLMs的搜索能力，并且在不同参数规模的模型上表现良好。",
        "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
        "pinyin": "Zhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLMs) sōusuǒ nénglì de zhòngyàoxìng. Zuìjìn de yánjiū shǐyòng qiángzhì xuéxí (RL) yǔ shíshí sōusuǒ yǐnqíng hùdòng lái gǎijìn LLMs de sōusuǒ nénglì, dàn miànlín wénjiàn zhìliàng bù kě kòng hé API fèiyòng gāo'áng de tiǎozhàn. Wèi jiějué zhèxiē wèntí, zuòzhě tíchū le ZeroSearch, yīzhǒng bù xūyào yǔ shíjì sōusuǒ yǐnqíng hùdòng de RL kuàngjià. Tōngguò qīngliàngjí de jiàndū wēitiáo hé jīyú kèchéng de gǔndòng cèlüè, ZeroSearch nénggòu yǒuxiào tíshēng LLMs de sōusuǒ nénglì, bìngqiě zài bùtóng cānshù guīmó de móxíng shàng biǎoxiàn liánghǎo.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improve'},\n{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '搜索', 'pinyin': 'sōu suǒ', 'trans': 'search'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'},\n{'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'},\n{'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'},\n{'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '不可控', 'pinyin': 'bù kě kòng', 'trans': 'uncontrollable'},\n{'word': 'API', 'pinyin': 'API', 'trans': 'API'},\n{'word': '费用', 'pinyin': 'fèi yòng', 'trans': 'cost'},\n{'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '轻量级', 'pinyin': 'qīng liàng jí', 'trans': 'lightweight'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'},\n{'word': '课程', 'pinyin': 'kè chéng', 'trans': 'course'},\n{'word': '滚动', 'pinyin': 'gǔn dòng', 'trans': 'rolling'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '良好', 'pinyin': 'liáng hǎo', 'trans': 'good'},\n{'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameter'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'}]",
        "trans": "This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes.",
        "update_ts": "2025-05-08 09:12"
    }
}