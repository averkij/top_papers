{
    "date": {
        "ru": "15 мая",
        "en": "May 15",
        "zh": "5月15日"
    },
    "time_utc": "2025-05-15 07:12",
    "weekday": 3,
    "issue_id": 3773,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 7,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "统一图像理解与生成的创新模型",
                    "desc": "本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09343",
            "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
            "url": "https://huggingface.co/papers/2505.09343",
            "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
            "score": 3,
            "issue_id": 3773,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "3c249078ec32a334",
            "authors": [
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chong Ruan",
                "Damai Dai",
                "Huazuo Gao",
                "Jiashi Li",
                "Liyue Zhang",
                "Panpan Huang",
                "Shangyan Zhou",
                "Shirong Ma",
                "Wenfeng Liang",
                "Ying He",
                "Yuqing Wang",
                "Yuxuan Liu",
                "Y. X. Wei"
            ],
            "affiliations": [
                "DeepSeek-AI Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09343.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Совместное проектирование моделей и оборудования для масштабирования ИИ",
                    "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограничений современного аппаратного обеспечения при обучении больших языковых моделей (LLM). Авторы представляют ключевые инновации, включая Multi-head Latent Attention (MLA) для повышения эффективности использования памяти и архитектуру Mixture of Experts (MoE) для оптимизации баланса между вычислениями и коммуникацией. В работе также обсуждается применение смешанной точности FP8 для максимального использования возможностей оборудования и использование многоплоскостной сетевой топологии для минимизации накладных расходов на уровне кластера. На основе опыта разработки DeepSeek-V3 авторы предлагают направления для будущих исследований в области аппаратного обеспечения для ИИ."
                },
                "en": {
                    "title": "Innovating AI: Bridging Hardware and Model Design for Scalable Solutions",
                    "desc": "This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design."
                },
                "zh": {
                    "title": "硬件与模型共同设计，推动AI创新",
                    "desc": "这篇论文讨论了大型语言模型（LLMs）在硬件架构上的限制，包括内存容量、计算效率和互连带宽等问题。DeepSeek-V3模型在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型共同设计如何有效解决这些挑战。论文分析了DeepSeek-V3/R1模型架构及其AI基础设施，介绍了多头潜在注意力（MLA）、专家混合（MoE）架构和FP8混合精度训练等创新。最后，作者与学术界和工业界同行探讨了未来硬件的发展方向，强调了硬件与模型共同设计在满足AI工作负载需求中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08455",
            "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
            "url": "https://huggingface.co/papers/2505.08455",
            "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.",
            "score": 1,
            "issue_id": 3773,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "b7bc69bb40029690",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08455.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях",
                    "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Моделей (LVLM) к причинно-следственному рассуждению на основе видео. VCRBench использует процедурные видео повседневных действий с перемешанными шагами для проверки способности моделей идентифицировать и упорядочивать ключевые причинные события. Оценка современных LVLM на VCRBench показала их трудности с моделированием долгосрочных причинно-следственных зависимостей напрямую из визуальных наблюдений. Авторы предложили модульный подход Recognition-Reasoning Decomposition (RRD), который значительно повысил точность на VCRBench."
                },
                "en": {
                    "title": "Enhancing Video Causal Reasoning with RRD",
                    "desc": "This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains."
                },
                "zh": {
                    "title": "提升视频因果推理能力的关键",
                    "desc": "尽管视频理解技术有所进步，但大型视频语言模型（LVLMs）在视频基础的因果推理方面的能力仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准测试，名为视频基础的长形式因果推理（VCRBench），通过对日常活动的视频进行处理，测试LVLMs能否识别、推理并正确排序实现特定目标所需的事件。我们还提出了一种模块化的方法，称为识别-推理分解（RRD），将视频基础的因果推理分为视频识别和因果推理两个子任务，从而显著提高了模型的准确性。我们的分析表明，LVLMs在复杂的长形式因果推理任务中主要依赖语言知识。"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5月14日"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。\n\nWǒmen jièshào le MiniMax-Speech, yī zhǒng jīyú Transformer de wénběn zhuǎn yǔyīn móxíng. Tā néng shēngchéng gāo zhìliàng de yǔyīn. Móxíng de guǎnjiàn chuàngxīn shì kě xuéxí de yǎnjiǎngzhě biānmǎqì, cóng cānkǎo yīnpín zhōng tíquān yīnsè tèzhēng, wúxū zhuǎnlù. Zhè shǐ de MiniMax-Speech néng zài líng yàngběn qíngkuàng xià shēngchéng biǎoxiànlì qiáng, yīnsè yīzhì de yǔyīn, bìng zhīchí yī yàngběn shēngyīn kèlóng. Tōngguò Flow-VAE, héchéng yīnpín de zhěngtǐ zhìliàng dédào tíshēng. Móxíng zhīchí 32 zhǒng yǔyán, zài duō zhǒng pínggū zhǐbiāo shàng biǎoxiàn chūsè, dádào le zuì xiānjìn de jiéguǒ.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"文本转语音\", \"pinyin\": \"wén běn zhuǎn yǔ yīn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔ yīn\", \"trans\": \"speech\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàng xīn\", \"trans\": \"innovation\"},\n    {\"word\": \"可学习\", \"pinyin\": \"kě xué xí\", \"trans\": \"learnable\"},\n    {\"word\": \"演讲者\", \"pinyin\": \"yǎn jiǎng zhě\", \"trans\": \"speaker\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"从\", \"pinyin\": \"cóng\", \"trans\": \"from\"},\n    {\"word\": \"参考\", \"pinyin\": \"cān kǎo\", \"trans\": \"reference\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"音色\", \"pinyin\": \"yīn sè\", \"trans\": \"timbre\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"features\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"no need\"},\n    {\"word\": \"转录\", \"pinyin\": \"zhuǎn lù\", \"trans\": \"transcription\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"强\", \"pinyin\": \"qiáng\", \"trans\": \"strong\"},\n    {\"word\": \"一致\", \"pinyin\": \"yī zhì\", \"trans\": \"consistent\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"一样本\", \"pinyin\": \"yī yàng běn\", \"trans\": \"one-shot\"},\n    {\"word\": \"声音\", \"pinyin\": \"shēng yīn\", \"trans\": \"voice\"},\n    {\"word\": \"克隆\", \"pinyin\": \"kè lóng\", \"trans\": \"clone\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"合成\", \"pinyin\": \"hé chéng\", \"trans\": \"synthesis\"},\n    {\"word\": \"整体\", \"pinyin\": \"zhěng tǐ\", \"trans\": \"overall\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"得到\", \"pinyin\": \"dé dào\", \"trans\": \"obtain\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"},\n    {\"word\": \"多种\", \"pinyin\": \"duō zhǒng\", \"trans\": \"various\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"指标\", \"pinyin\": \"zhǐ biāo\", \"trans\": \"metrics\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuì xiān jìn\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}