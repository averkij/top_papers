{
    "date": {
        "ru": "15 мая",
        "en": "May 15",
        "zh": "5月15日"
    },
    "time_utc": "2025-05-15 03:37",
    "weekday": 3,
    "issue_id": 3769,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 4,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "统一图像理解与生成的创新模型",
                    "desc": "本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5月14日"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。\n\nWǒmen jièshào le MiniMax-Speech, yī zhǒng jīyú Transformer de wénběn zhuǎn yǔyīn móxíng. Tā néng shēngchéng gāo zhìliàng de yǔyīn. Móxíng de guǎnjiàn chuàngxīn shì kě xuéxí de yǎnjiǎngzhě biānmǎqì, cóng cānkǎo yīnpín zhōng tíquān yīnsè tèzhēng, wúxū zhuǎnlù. Zhè shǐ de MiniMax-Speech néng zài líng yàngběn qíngkuàng xià shēngchéng biǎoxiànlì qiáng, yīnsè yīzhì de yǔyīn, bìng zhīchí yī yàngběn shēngyīn kèlóng. Tōngguò Flow-VAE, héchéng yīnpín de zhěngtǐ zhìliàng dédào tíshēng. Móxíng zhīchí 32 zhǒng yǔyán, zài duō zhǒng pínggū zhǐbiāo shàng biǎoxiàn chūsè, dádào le zuì xiānjìn de jiéguǒ.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"文本转语音\", \"pinyin\": \"wén běn zhuǎn yǔ yīn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔ yīn\", \"trans\": \"speech\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàng xīn\", \"trans\": \"innovation\"},\n    {\"word\": \"可学习\", \"pinyin\": \"kě xué xí\", \"trans\": \"learnable\"},\n    {\"word\": \"演讲者\", \"pinyin\": \"yǎn jiǎng zhě\", \"trans\": \"speaker\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"从\", \"pinyin\": \"cóng\", \"trans\": \"from\"},\n    {\"word\": \"参考\", \"pinyin\": \"cān kǎo\", \"trans\": \"reference\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"音色\", \"pinyin\": \"yīn sè\", \"trans\": \"timbre\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"features\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"no need\"},\n    {\"word\": \"转录\", \"pinyin\": \"zhuǎn lù\", \"trans\": \"transcription\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"强\", \"pinyin\": \"qiáng\", \"trans\": \"strong\"},\n    {\"word\": \"一致\", \"pinyin\": \"yī zhì\", \"trans\": \"consistent\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"一样本\", \"pinyin\": \"yī yàng běn\", \"trans\": \"one-shot\"},\n    {\"word\": \"声音\", \"pinyin\": \"shēng yīn\", \"trans\": \"voice\"},\n    {\"word\": \"克隆\", \"pinyin\": \"kè lóng\", \"trans\": \"clone\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"合成\", \"pinyin\": \"hé chéng\", \"trans\": \"synthesis\"},\n    {\"word\": \"整体\", \"pinyin\": \"zhěng tǐ\", \"trans\": \"overall\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"得到\", \"pinyin\": \"dé dào\", \"trans\": \"obtain\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"},\n    {\"word\": \"多种\", \"pinyin\": \"duō zhǒng\", \"trans\": \"various\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"指标\", \"pinyin\": \"zhǐ biāo\", \"trans\": \"metrics\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuì xiān jìn\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}