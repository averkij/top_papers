{
    "date": {
        "ru": "15 мая",
        "en": "May 15",
        "zh": "5月15日"
    },
    "time_utc": "2025-05-15 09:12",
    "weekday": 3,
    "issue_id": 3775,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04410",
            "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
            "url": "https://huggingface.co/papers/2505.04410",
            "abstract": "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
            "score": 27,
            "issue_id": 3774,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "24fee436fe24f861",
            "authors": [
                "Junjie Wang",
                "Bin Chen",
                "Yulin Li",
                "Bin Kang",
                "Yichi Chen",
                "Zhuotao Tian"
            ],
            "affiliations": [
                "International Research Institute for Artificial Intelligence, HIT, Shenzhen",
                "School of Computer Science and Technology, HIT, Shenzhen",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04410.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DeCLIP: Новый шаг к универсальному компьютерному зрению",
                    "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотного предсказания с открытым словарем. Авторы предлагают разделить модуль самовнимания CLIP на 'содержательные' и 'контекстные' признаки для повышения локальной различимости и пространственной согласованности. DeCLIP показывает значительное улучшение результатов в задачах обнаружения объектов и семантической сегментации по сравнению с существующими методами. Этот подход позволяет преодолеть ограничения предопределенных категорий в задачах компьютерного зрения."
                },
                "en": {
                    "title": "Enhancing Dense Visual Predictions with DeCLIP",
                    "desc": "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."
                },
                "zh": {
                    "title": "DeCLIP：提升视觉语言模型的密集预测能力",
                    "desc": "本论文提出了一种新的框架DeCLIP，旨在改善视觉语言模型CLIP在密集视觉预测任务中的表现。我们发现CLIP的图像标记在聚合空间或语义相关区域的信息时存在困难，导致特征缺乏局部可区分性和空间一致性。DeCLIP通过解耦自注意力模块，分别提取“内容”和“上下文”特征，从而提高局部可区分性并保持空间相关性。实验结果表明，DeCLIP在多个开放词汇密集预测任务中显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 13,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "统一图像理解与生成的创新模型",
                    "desc": "本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09343",
            "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
            "url": "https://huggingface.co/papers/2505.09343",
            "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
            "score": 10,
            "issue_id": 3773,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "3c249078ec32a334",
            "authors": [
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chong Ruan",
                "Damai Dai",
                "Huazuo Gao",
                "Jiashi Li",
                "Liyue Zhang",
                "Panpan Huang",
                "Shangyan Zhou",
                "Shirong Ma",
                "Wenfeng Liang",
                "Ying He",
                "Yuqing Wang",
                "Yuxuan Liu",
                "Y. X. Wei"
            ],
            "affiliations": [
                "DeepSeek-AI Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09343.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Совместное проектирование моделей и оборудования для масштабирования ИИ",
                    "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограничений современного аппаратного обеспечения при обучении больших языковых моделей (LLM). Авторы представляют ключевые инновации, включая Multi-head Latent Attention (MLA) для повышения эффективности использования памяти и архитектуру Mixture of Experts (MoE) для оптимизации баланса между вычислениями и коммуникацией. В работе также обсуждается применение смешанной точности FP8 для максимального использования возможностей оборудования и использование многоплоскостной сетевой топологии для минимизации накладных расходов на уровне кластера. На основе опыта разработки DeepSeek-V3 авторы предлагают направления для будущих исследований в области аппаратного обеспечения для ИИ."
                },
                "en": {
                    "title": "Innovating AI: Bridging Hardware and Model Design for Scalable Solutions",
                    "desc": "This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design."
                },
                "zh": {
                    "title": "硬件与模型共同设计，推动AI创新",
                    "desc": "这篇论文讨论了大型语言模型（LLMs）在硬件架构上的限制，包括内存容量、计算效率和互连带宽等问题。DeepSeek-V3模型在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型共同设计如何有效解决这些挑战。论文分析了DeepSeek-V3/R1模型架构及其AI基础设施，介绍了多头潜在注意力（MLA）、专家混合（MoE）架构和FP8混合精度训练等创新。最后，作者与学术界和工业界同行探讨了未来硬件的发展方向，强调了硬件与模型共同设计在满足AI工作负载需求中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08455",
            "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
            "url": "https://huggingface.co/papers/2505.08455",
            "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.",
            "score": 2,
            "issue_id": 3773,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "b7bc69bb40029690",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08455.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях",
                    "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Моделей (LVLM) к причинно-следственному рассуждению на основе видео. VCRBench использует процедурные видео повседневных действий с перемешанными шагами для проверки способности моделей идентифицировать и упорядочивать ключевые причинные события. Оценка современных LVLM на VCRBench показала их трудности с моделированием долгосрочных причинно-следственных зависимостей напрямую из визуальных наблюдений. Авторы предложили модульный подход Recognition-Reasoning Decomposition (RRD), который значительно повысил точность на VCRBench."
                },
                "en": {
                    "title": "Enhancing Video Causal Reasoning with RRD",
                    "desc": "This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains."
                },
                "zh": {
                    "title": "提升视频因果推理能力的关键",
                    "desc": "尽管视频理解技术有所进步，但大型视频语言模型（LVLMs）在视频基础的因果推理方面的能力仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准测试，名为视频基础的长形式因果推理（VCRBench），通过对日常活动的视频进行处理，测试LVLMs能否识别、推理并正确排序实现特定目标所需的事件。我们还提出了一种模块化的方法，称为识别-推理分解（RRD），将视频基础的因果推理分为视频识别和因果推理两个子任务，从而显著提高了模型的准确性。我们的分析表明，LVLMs在复杂的长形式因果推理任务中主要依赖语言知识。"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5月14日"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。",
        "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
        "pinyin": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。\n\nZhè piān wénzhāng tǎolùn le chuántǒng de shìjué yùcè rènwù shòuxiàn zài yùdìngyì lèibié, ér shìjué-yǔyán móxíng (VLMs) rú CLIP zài kāifàng cíhuì rènwù zhōng biǎoxiàn chū qiánlì. Rán'ér, CLIP zài mìjī yùcè zhōng biǎoxiàn bùjiā, yīnwèi qí júbù tèzhēng biǎoshì yǒuxiàn. Yánjiū fāxiàn, CLIP de túxiàng biāojiàn nán yǐ yǒuxiào jùhé kōngjiān huò yùyì xiāngguān qūyù de xìnxī. Wèi jiějué zhè yī wèntí, zuòzhě tíchū le DeCLIP kuàngjià, tōngguò jiěkǒu zìzhùyì mókuài huòdé “nèiróng” hé “shàngxìatú” tèzhēng, fēnbié tīgāo júbù biànbiéxìng hé kōngjiān yīzhìxìng. Shíyàn zhèngmíng, DeCLIP zài duōgè kāifàng cíhuì mìjī yùcè rènwù zhōng biǎoxiàn yōuyùn.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]",
        "trans": "This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains \"content\" and \"context\" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks.",
        "update_ts": "2025-05-15 09:12"
    }
}