
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 45 papers. March 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">45 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-10.html">â¬…ï¸ <span id="prev-date">10.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-12.html">â¡ï¸ <span id="next-date">12.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'};
        let feedDateNext = {'ru': '12.03', 'en': '03/12', 'zh': '3æœˆ12æ—¥'};
        let feedDatePrev = {'ru': '10.03', 'en': '03/10', 'zh': '3æœˆ10æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.03601', 'title': 'Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2503.03601', 'abstract': 'Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.', 'score': 120, 'issue_id': 2634, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'd045a8e2a39262c9', 'authors': ['Kristian Kuznetsov', 'Laida Kushnareva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Anastasia Voznyuk', 'Irina Piontkovskaya', 'Evgeny Burnaev', 'Serguei Barannikov'], 'affiliations': ['AI Foundation and Algorithm Lab', 'Artificial Intelligence Research Institute (AIRI)', 'CNRS, UniversitÃ© Paris CitÃ©, France', 'Moscow Institute of Physics and Technology', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.03601.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (ATD) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemma-2-2b. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Text Detection with Interpretability in AI Models', 'desc': 'This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text.'}, 'zh': {'title': 'æå‡äººå·¥æ–‡æœ¬æ£€æµ‹çš„å¯è§£é‡Šæ€§', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œäººå·¥æ–‡æœ¬æ£€æµ‹ï¼ˆATDï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å°½ç®¡å·²æœ‰è®¸å¤šåŠªåŠ›ï¼Œä½†æ²¡æœ‰å•ä¸€ç®—æ³•èƒ½å¤Ÿåœ¨ä¸åŒç±»å‹çš„æœªè§æ–‡æœ¬ä¸­å§‹ç»ˆè¡¨ç°è‰¯å¥½ï¼Œä¹Ÿæ— æ³•ä¿è¯å¯¹æ–°LLMçš„æœ‰æ•ˆæ³›åŒ–ã€‚å¯è§£é‡Šæ€§åœ¨å®ç°è¿™ä¸€ç›®æ ‡ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä»Gemma-2-2bæ®‹å·®æµä¸­æå–ç‰¹å¾ï¼Œå¢å¼ºäº†ATDçš„å¯è§£é‡Šæ€§ï¼Œåˆ†æäº†æ–‡æœ¬ä¸äººç±»å†™ä½œå†…å®¹çš„å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07605', 'title': 'SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models', 'url': 'https://huggingface.co/papers/2503.07605', 'abstract': "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.", 'score': 56, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'eaf9c07e3673cecc', 'authors': ['Xun Liang', 'Hanyu Wang', 'Huayi Lai', 'Simin Niu', 'Shichao Song', 'Jiawei Yang', 'Jihao Zhao', 'Feiyu Xiong', 'Bo Tang', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07605.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sparse Expert Activation Pruning (SEAP) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SEAP Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEAP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Optimize LLMs with Sparse Expert Activation Pruning!', 'desc': "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."}, 'zh': {'title': 'ç¨€ç–ä¸“å®¶æ¿€æ´»å‰ªæï¼šä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¨€ç–ä¸“å®¶æ¿€æ´»å‰ªæï¼ˆSEAPï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™ä¸ä»»åŠ¡ç›¸å…³çš„å‚æ•°ï¼Œä»¥å‡å°‘æ¨ç†å¼€é”€ã€‚SEAPé€šè¿‡åˆ†æéšè—çŠ¶æ€å’Œæ¿€æ´»çš„èšç±»æ¨¡å¼ï¼Œè¯†åˆ«ç‰¹å®šä»»åŠ¡çš„ä¸“å®¶æ¿€æ´»æ¨¡å¼ï¼Œä»è€Œåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEAPåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07365', 'title': 'MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.07365', 'abstract': "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA", 'score': 44, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '765d475b38d9f289', 'authors': ['Fanqing Meng', 'Lingxiao Du', 'Zongkai Liu', 'Zhixiang Zhou', 'Quanfeng Lu', 'Daocheng Fu', 'Botian Shi', 'Wenhai Wang', 'Junjun He', 'Kaipeng Zhang', 'Ping Luo', 'Yu Qiao', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07365.jpg', 'data': {'categories': ['#rl', '#multimodal', '#reasoning', '#rag', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MM-Eureka â€“ ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MM-Eureka!', 'desc': 'MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning.'}, 'zh': {'title': 'MM-Eurekaï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†MM-Eurekaï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒæˆåŠŸåœ°å°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚è™½ç„¶åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬é¢†åŸŸæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨å¤šæ¨¡æ€ç©ºé—´ä¸­é‡ç°äº†æ–‡æœ¬åŸºç¡€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢åŠ ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ— ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤è°ƒä¼˜å’Œé¢„è®­ç»ƒæ¨¡å‹éƒ½èƒ½é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä¸”æ•°æ®æ•ˆç‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07002', 'title': 'Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning', 'url': 'https://huggingface.co/papers/2503.07002', 'abstract': 'Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.', 'score': 31, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '1f57fb5c5398efbc', 'authors': ['Jiazheng Liu', 'Sipeng Zheng', 'BÃ¶rje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07002.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#games', '#multimodal', '#dataset', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼: Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMDiag - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ñ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DiagNote - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. DiagNote Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ»Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Multimodal Dialogue with MMDiag and DiagNote', 'desc': 'This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€å¯¹è¯çš„æ™ºèƒ½åŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†MMDiagï¼Œæ—¨åœ¨æ”¹å–„ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ã€‚MMDiagé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è§„åˆ™å’ŒGPTçš„è¾…åŠ©ç”Ÿæˆï¼ŒåŒ…å«äº†é—®é¢˜ä¹‹é—´ã€é—®é¢˜ä¸å›¾åƒä¹‹é—´ä»¥åŠä¸åŒå›¾åƒåŒºåŸŸä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œæ›´è´´è¿‘çœŸå®çš„äººç±»å¯¹è¯åœºæ™¯ã€‚è®ºæ–‡è¿˜æå‡ºäº†DiagNoteï¼Œä¸€ä¸ªå…·å¤‡å¤šæ¨¡æ€åŸºç¡€å’Œæ¨ç†èƒ½åŠ›çš„MLLMï¼ŒåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å—ï¼ˆDeliberateå’ŒGazeï¼‰ï¼Œç”¨äºåœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œæ€ç»´é“¾å’Œæ³¨é‡Šã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒDiagNoteåœ¨åŸºç¡€å’Œå…±åŒå¤„ç†è§†è§‰ä¸è¯­è¨€ä¿¡æ¯çš„æ¨ç†èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„MLLMsã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07314', 'title': 'Automated Movie Generation via Multi-Agent CoT Planning', 'url': 'https://huggingface.co/papers/2503.07314', 'abstract': 'Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.', 'score': 25, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '2e5c000925250863', 'authors': ['Weijia Wu', 'Zeyu Zhu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07314.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#story_generation', '#video', '#agents'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MovieAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ (Chain of Thought). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑĞ¶ĞµÑ‚Ğ¾Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ². MovieAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MovieAgent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½ÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Movie Magic with MovieAgent!', 'desc': 'This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ç”µå½±ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'ç°æœ‰çš„é•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ç¼ºä¹è‡ªåŠ¨åŒ–è§„åˆ’ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥æ•…äº‹æƒ…èŠ‚ã€åœºæ™¯ã€æ‘„å½±å’Œè§’è‰²äº’åŠ¨ï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œä½æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MovieAgentï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’å®ç°è‡ªåŠ¨åŒ–ç”µå½±ç”Ÿæˆã€‚MovieAgentçš„ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿æ˜¯ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢å¹¶å®šä¹‰äº†è‡ªåŠ¨åŒ–ç”µå½±/é•¿è§†é¢‘ç”Ÿæˆçš„èŒƒå¼ï¼›å…¶æ¬¡ï¼ŒMovieAgentå¼•å…¥äº†åŸºäºå±‚æ¬¡çš„CoTæ¨ç†è¿‡ç¨‹ï¼Œè‡ªåŠ¨æ„å»ºåœºæ™¯ã€æ‘„åƒæœºè®¾ç½®å’Œæ‘„å½±ï¼Œå¤§å¤§å‡å°‘äº†äººåŠ›æŠ•å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒMovieAgentåœ¨å‰§æœ¬å¿ å®åº¦ã€è§’è‰²ä¸€è‡´æ€§å’Œå™äº‹è¿è´¯æ€§æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07216', 'title': 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates', 'url': 'https://huggingface.co/papers/2503.07216', 'abstract': "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", 'score': 24, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '3d23c61b24a599ee', 'authors': ['Sangwoo Park', 'Seanie Lee', 'Byungjoo Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.07216.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#ethics', '#multimodal', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'FedRand: Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ FedRand. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Low-Rank Adaptation (LoRA) Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼. FedRand Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² LoRA Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞº ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FedRand Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ñ‡Ğ»ĞµĞ½ÑÑ‚Ğ²Ğ° (MIA) Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Privacy in Federated Learning with FedRand', 'desc': 'Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters.'}, 'zh': {'title': 'FedRandï¼šä¿æŠ¤æ•°æ®éšç§çš„è”é‚¦å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç¡®ä¿ä¸­å¤®æœåŠ¡å™¨æ— æ³•ç›´æ¥è®¿é—®æœ¬åœ°å®¢æˆ·ç«¯çš„æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨èšåˆè¿‡ç¨‹ä¸­ï¼Œæœ¬åœ°å®¢æˆ·ç«¯çš„æ¨¡å‹ä»å¯èƒ½æš´éœ²ç»™ä¸­å¤®æœåŠ¡å™¨ï¼Œä»è€Œå½±å“æ•°æ®éšç§ã€‚ç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ—¶ï¼Œè¿™ç§é£é™©æ›´ä¸ºä¸¥é‡ï¼Œå› ä¸ºVLMså®¹æ˜“è®°ä½è®­ç»ƒæ•°æ®å®ä¾‹ï¼Œå®¹æ˜“å—åˆ°æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAsï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedRandæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡éšæœºé€‰æ‹©ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å­å‚æ•°ï¼Œé¿å…æ³„éœ²å®Œæ•´çš„å®¢æˆ·ç«¯å‚æ•°ï¼Œä»è€Œå¢å¼ºæ•°æ®éšç§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'url': 'https://huggingface.co/papers/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'score': 20, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '8eb39990e619611e', 'authors': ['Jongwoo Ko', 'Tianyi Chen', 'Sungnyun Kim', 'Tianyu Ding', 'Luming Liang', 'Ilya Zharkov', 'Se-Young Yun'], 'affiliations': ['KAIST AI', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.07067.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#training', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DistiLLM-2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ½ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DistiLLM-2 ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing LLM Distillation with Contrastive Learning', 'desc': 'This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation.'}, 'zh': {'title': 'å¯¹æ¯”æ–¹æ³•æå‡å¤§å‹è¯­è¨€æ¨¡å‹è’¸é¦æ•ˆæœ', 'desc': 'å°½ç®¡è’¸é¦åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶å¯¹æ•™å¸ˆå’Œå­¦ç”Ÿç”Ÿæˆçš„æ•°æ®ä½¿ç”¨ç›¸åŒçš„æŸå¤±å‡½æ•°ã€‚è¿™ç§ç­–ç•¥å¿½è§†äº†æŸå¤±å…¬å¼ä¸æ•°æ®ç±»å‹ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½æå‡ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DistiLLM-2ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶æé«˜æ•™å¸ˆå“åº”çš„å¯èƒ½æ€§å¹¶é™ä½å­¦ç”Ÿå“åº”çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDistiLLM-2ä¸ä»…åœ¨å¤šç§ä»»åŠ¡ä¸­æ„å»ºäº†é«˜æ€§èƒ½çš„å­¦ç”Ÿæ¨¡å‹ï¼Œè¿˜æ”¯æŒå¤šæ ·åŒ–çš„åº”ç”¨ï¼Œå¦‚åå¥½å¯¹é½å’Œè§†è§‰-è¯­è¨€æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07027', 'title': 'EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2503.07027', 'abstract': 'Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.', 'score': 18, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '1b1589f9873720c7', 'authors': ['Yuxuan Zhang', 'Yirui Yuan', 'Yiren Song', 'Haofan Wang', 'Jiaming Liu'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'ShanghaiTech University', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07027.jpg', 'data': {'categories': ['#cv', '#architecture', '#training', '#optimization', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'EasyControl: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyControl - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ KV-ĞºÑÑˆĞµĞ¼. EasyControl Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers', 'desc': 'This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning.'}, 'zh': {'title': 'é«˜æ•ˆçµæ´»çš„æ¡ä»¶ç”Ÿæˆæ¡†æ¶EasyControl', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEasyControlçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šé¦–å…ˆï¼Œè½»é‡çº§çš„æ¡ä»¶æ³¨å…¥LoRAæ¨¡å—å¯ä»¥ç‹¬ç«‹å¤„ç†æ¡ä»¶ä¿¡å·ï¼Œé¿å…ä¿®æ”¹åŸºç¡€æ¨¡å‹æƒé‡ï¼Œä»è€Œå®ç°ä¸å®šåˆ¶æ¨¡å‹çš„å…¼å®¹æ€§ã€‚å…¶æ¬¡ï¼Œä½ç½®æ„ŸçŸ¥è®­ç»ƒèŒƒå¼æ ‡å‡†åŒ–è¾“å…¥æ¡ä»¶ï¼Œä½¿å¾—ç”Ÿæˆä»»æ„é•¿å®½æ¯”å’Œçµæ´»åˆ†è¾¨ç‡çš„å›¾åƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œç»“åˆKVç¼“å­˜æŠ€æœ¯çš„å› æœæ³¨æ„æœºåˆ¶æ˜¾è‘—é™ä½äº†å›¾åƒåˆæˆçš„å»¶è¿Ÿï¼Œæå‡äº†æ•´ä½“æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06680', 'title': 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation', 'url': 'https://huggingface.co/papers/2503.06680', 'abstract': "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", 'score': 15, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '4394ce17a18696a3', 'authors': ['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06680.jpg', 'data': {'categories': ['#plp', '#dataset', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'FEA-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ', 'desc': 'FEA-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑƒĞ»-Ñ€ĞµĞºĞ²ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ· 83 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ FEA-Bench, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ².'}, 'en': {'title': 'FEA-Bench: Evaluating Code Generation in Repositories', 'desc': "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."}, 'zh': {'title': 'è¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ–°åŸºå‡†ï¼šFEA-Bench', 'desc': 'åœ¨ä»£ç ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå®ç°æ–°ç‰¹æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„åº”ç”¨ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ç¼ºä¹ä¸“é—¨è¯„ä¼°è¿™ä¸€èƒ½åŠ›çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FEA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç åº“ä¸­è¿›è¡Œå¢é‡å¼€å‘èƒ½åŠ›çš„åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨FEA-Benchä¸­çš„è¡¨ç°æ˜¾è‘—è¾ƒå·®ï¼Œçªæ˜¾äº†åœ¨ä»£ç åº“çº§åˆ«å¢é‡å¼€å‘ä¸­é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning', 'url': 'https://huggingface.co/papers/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '74dfc85ba9f7bcc7', 'authors': ['Bo Jiang', 'Shaoyu Chen', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.07608.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#reasoning', '#agents'], 'emoji': 'ğŸš—', 'ru': {'title': 'AlphaDrive: Ğ˜Ğ˜ Ğ·Ğ° Ñ€ÑƒĞ»ĞµĞ¼ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'AlphaDrive - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. AlphaDrive Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning', 'desc': 'This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency.'}, 'zh': {'title': 'AlphaDriveï¼šæå‡è‡ªåŠ¨é©¾é©¶çš„æ™ºèƒ½è§„åˆ’ä¸æ¨ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†AlphaDriveï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ¡†æ¶ã€‚AlphaDriveå¼•å…¥äº†å››ç§åŸºäºGRPOçš„RLå¥–åŠ±ï¼Œä¸“é—¨é’ˆå¯¹è§„åˆ’ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLçš„ä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒç­–ç•¥ã€‚ä¸ä»…ä½¿ç”¨SFTæˆ–ä¸è¿›è¡Œæ¨ç†çš„æƒ…å†µç›¸æ¯”ï¼ŒAlphaDriveæ˜¾è‘—æé«˜äº†è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç»è¿‡RLè®­ç»ƒåï¼ŒAlphaDriveè¿˜å±•ç°å‡ºä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04629', 'title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing', 'url': 'https://huggingface.co/papers/2503.04629', 'abstract': 'Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.', 'score': 13, 'issue_id': 2633, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'a229855316ab195d', 'authors': ['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'Bo Zhang', 'Lei Bai'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04629.jpg', 'data': {'categories': ['#survey', '#multimodal', '#agents', '#dataset', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'SurveyForge: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'SurveyForge - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑÑŒ Ğº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑŒÑĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, SurveyForge Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SurveyBench, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 100 Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ˜Ğ˜ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Automated Survey Generation with SurveyForge', 'desc': 'This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey.'}, 'zh': {'title': 'SurveyForgeï¼šæå‡æ–‡çŒ®ç»¼è¿°ç”Ÿæˆè´¨é‡çš„åˆ©å™¨', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SurveyForgeï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨ç”Ÿæˆæ–‡çŒ®ç»¼è¿°çš„å·¥å…·ã€‚å®ƒé€šè¿‡åˆ†æäººç±»æ’°å†™çš„ç»¼è¿°å¤§çº²çš„é€»è¾‘ç»“æ„ï¼Œå¹¶å‚è€ƒç›¸å…³é¢†åŸŸçš„æ–‡ç« ï¼Œé¦–å…ˆç”Ÿæˆå¤§çº²ã€‚ç„¶åï¼ŒSurveyForgeåˆ©ç”¨é«˜è´¨é‡çš„è®ºæ–‡æ¥è‡ªåŠ¨ç”Ÿæˆå’Œå®Œå–„æ–‡ç« å†…å®¹ã€‚ç ”ç©¶è¿˜æ„å»ºäº†SurveyBenchï¼Œç”¨äºè¯„ä¼°AIç”Ÿæˆçš„ç»¼è¿°è®ºæ–‡åœ¨å‚è€ƒæ–‡çŒ®ã€ç»“æ„å’Œå†…å®¹è´¨é‡ç­‰ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07602', 'title': 'DreamRelation: Relation-Centric Video Customization', 'url': 'https://huggingface.co/papers/2503.07602', 'abstract': "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'a32c943630a808fc', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Biao Gong', 'Longxiang Tang', 'Xiang Wang', 'Haonan Qiu', 'Hengjia Li', 'Shuai Tan', 'Yingya Zhang', 'Hongming Shan'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Fudan University', 'Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07602.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#video', '#interpretability', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'DreamRelation: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'DreamRelation - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Relational Decoupling Learning Ğ¸ Relational Dynamics Enhancement. Relational Decoupling Learning Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ relation LoRA triplet Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼. Relational Dynamics Enhancement Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ… Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'DreamRelation: Personalizing Video Relationships with Precision', 'desc': 'This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design.'}, 'zh': {'title': 'DreamRelationï¼šä¸ªæ€§åŒ–è§†é¢‘å…³ç³»å»ºæ¨¡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamRelationçš„æ–°æ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–è§†é¢‘ä¸­çš„å…³ç³»å»ºæ¨¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³ç³»è§£è€¦å­¦ä¹ å’Œå…³ç³»åŠ¨æ€å¢å¼ºä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œè§£å†³äº†å¤æ‚å…³ç³»è§†é¢‘å®šåˆ¶ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ†ææŸ¥è¯¢ã€é”®å’Œå€¼ç‰¹å¾åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½œç”¨ï¼ŒDreamRelationå®ç°äº†å¯è§£é‡Šçš„å…³ç³»è§†é¢‘ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamRelationåœ¨å…³ç³»è§†é¢‘å®šåˆ¶æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '403914241aa8967c', 'authors': ['Wenxuan Huang', 'Bohan Jia', 'Zijie Zhai', 'Shaosheng Cao', 'Zheyu Ye', 'Fei Zhao', 'Yao Hu', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.06749.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#open_source', '#benchmark', '#reasoning', '#dataset', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Vision-R1: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Vision-R1, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ MLLM Ğ¸ DeepSeek-R1. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Progressive Thinking Suppression Training Ğ¸ Group Relative Policy Optimization. Vision-R1-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 73.5% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MathVista, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¸ÑˆÑŒ Ğ½Ğ° 0.4% Ğ½Ğ¸Ğ¶Ğµ Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI O1.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Reinforcement Learning', 'desc': "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."}, 'zh': {'title': 'é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'DeepSeek-R1-Zeroå±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¯èƒ½æ€§ã€‚åŸºäºè¿™ä¸€çªç ´ï¼Œæœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç”±äºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†Vision-R1æ¨¡å‹ï¼Œä»¥æ”¹å–„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æ¸è¿›æ€ç»´æŠ‘åˆ¶è®­ç»ƒï¼ˆPTSTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­–ç•¥æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05244', 'title': 'WritingBench: A Comprehensive Benchmark for Generative Writing', 'url': 'https://huggingface.co/papers/2503.05244', 'abstract': "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.", 'score': 12, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '679eb0b19323e2d2', 'authors': ['Yuning Wu', 'Jiahao Mei', 'Ming Yan', 'Chenliang Li', 'SHaopeng Lai', 'Yuran Ren', 'Zijia Wang', 'Ji Zhang', 'Mengyue Wu', 'Qin Jin', 'Fei Huang'], 'affiliations': ['Alibaba Group', 'Renmin University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05244.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#story_generation', '#open_source'], 'emoji': 'âœï¸', 'ru': {'title': 'WritingBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¸ÑÑŒĞ¼Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WritingBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ğ¸ÑÑŒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 100 Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰ĞµĞµ, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'WritingBench: A New Standard for Evaluating Language Models in Writing', 'desc': 'This paper introduces WritingBench, a new benchmark for evaluating large language models (LLMs) in generative writing across six key domains and 100 subdomains. It addresses the limitations of existing benchmarks that do not adequately assess the quality of writing in various contexts. The authors propose a query-dependent evaluation framework that allows LLMs to create specific assessment criteria based on the writing task at hand. Additionally, a fine-tuned critic model is introduced to provide criteria-aware scoring, enhancing the evaluation of style, format, and length in generated texts.'}, 'zh': {'title': 'WritingBenchï¼šå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å†™ä½œèƒ½åŠ›', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¯„ä¼°å…¶åœ¨ç”Ÿæˆå†™ä½œä¸­çš„è¡¨ç°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰åˆ°é«˜è´¨é‡ä¹¦é¢å†…å®¹åœ¨ä¸åŒé¢†åŸŸçš„å¤šæ ·åŒ–éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸçš„è¡¨ç°ï¼ŒåŒ…æ‹¬åˆ›æ„ã€è¯´æœæ€§ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¾èµ–æŸ¥è¯¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„è¯„ä¼°æ¨¡å‹è¿›è¡Œé£æ ¼ã€æ ¼å¼å’Œé•¿åº¦çš„è¯„åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models', 'url': 'https://huggingface.co/papers/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA', 'score': 11, 'issue_id': 2631, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '01588376bab86ceb', 'authors': ['Yuxiang Zhang', 'Yuqi Yang', 'Jiangming Shu', 'Xinyan Wen', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06580.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#rl', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Large Agent Models (LAM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AutoCoA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AutoCoA, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Autonomous Reasoning with AutoCoA', 'desc': 'This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions.'}, 'zh': {'title': 'è‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä»£ç†æ¨¡å‹', 'desc': 'ä¼ ç»Ÿçš„æ™ºèƒ½å·¥ä½œæµç¨‹ä¾èµ–å¤–éƒ¨æç¤ºæ¥ç®¡ç†ä¸å·¥å…·å’Œç¯å¢ƒçš„äº¤äº’ï¼Œè¿™é™åˆ¶äº†æ¨ç†æ¨¡å‹çš„è‡ªä¸»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤§å‹ä»£ç†æ¨¡å‹ï¼ˆLAMsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿå†…éƒ¨ç”Ÿæˆè¡ŒåŠ¨é“¾ï¼ˆCoAï¼‰ï¼Œä»è€Œè‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·ã€‚æˆ‘ä»¬æå‡ºçš„AutoCoAæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶æœ‰æ•ˆç®¡ç†ä¸ç¯å¢ƒçš„äº¤äº’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç»è¿‡AutoCoAè®­ç»ƒçš„ä»£ç†æ¨¡å‹åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºäºReActçš„å·¥ä½œæµç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œå¤šæ­¥éª¤è¡ŒåŠ¨çš„ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04812', 'title': 'LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.04812', 'abstract': "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.", 'score': 10, 'issue_id': 2631, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a1fec2227e343e88', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04812.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#rag', '#multimodal', '#training'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Multimodal Embeddings with LLaVE for Better Performance', 'desc': 'This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval.'}, 'zh': {'title': 'LLaVEï¼šæå‡å¤šæ¨¡æ€åµŒå…¥çš„å¼ºå¤§å·¥å…·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹LLaVEï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ­£è´Ÿæ ·æœ¬æ—¶ç›¸ä¼¼åº¦åˆ†å¸ƒé‡å çš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è´Ÿæ ·æœ¬çš„è¡¨ç¤ºå­¦ä¹ ï¼ŒLLaVEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŒºåˆ†å›°éš¾çš„è´Ÿæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨å›¾åƒ-æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒåï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07334', 'title': 'Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment', 'url': 'https://huggingface.co/papers/2503.07334', 'abstract': "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.", 'score': 9, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'da94bc1af48685ac', 'authors': ['Xing Xie', 'Jiawei Liu', 'Ziyue Lin', 'Huijie Fan', 'Zhi Han', 'Yandong Tang', 'Liangqiong Qu'], 'affiliations': ['Shenyang Institute of Automation, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.07334.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#open_source', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ARRA: Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ARRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ARRA Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½ĞµÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARRA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Coherent Text-to-Image Generation with ARRA', 'desc': "The paper introduces Autoregressive Representation Alignment (ARRA), a novel training framework that enhances text-to-image generation in autoregressive language models (LLMs) without changing their architecture. ARRA achieves this by aligning the hidden states of LLMs with visual representations from external models using a global visual alignment loss and a special hybrid token, <HYBNEXT>. This token imposes both local next-token prediction and global semantic distillation, allowing LLMs to learn spatial and contextual coherence effectively. The results show significant improvements in image generation quality, as evidenced by reduced FrÃ©chet Inception Distance (FID) scores across various datasets, demonstrating ARRA's effectiveness and versatility in enhancing autoregressive models."}, 'zh': {'title': 'è‡ªå›å½’æ¨¡å‹çš„æ–°çªç ´ï¼šå…¨çƒä¸€è‡´æ€§ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºè‡ªå›å½’è¡¨ç¤ºå¯¹é½ï¼ˆARRAï¼‰ï¼Œæ—¨åœ¨å®ç°è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…¨çƒä¸€è‡´æ€§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€‚ARRAé€šè¿‡å…¨å±€è§†è§‰å¯¹é½æŸå¤±å’Œæ··åˆæ ‡è®°<HYBNEXT>ï¼Œå°†LLMçš„éšè—çŠ¶æ€ä¸å¤–éƒ¨è§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰è¡¨ç¤ºå¯¹é½ã€‚è¯¥æ ‡è®°æ–½åŠ äº†å±€éƒ¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå…¨å±€è¯­ä¹‰è’¸é¦çš„åŒé‡çº¦æŸï¼Œä½¿LLMèƒ½å¤Ÿåœ¨ä¿æŒè‡ªå›å½’èŒƒå¼çš„åŒæ—¶ï¼Œéšå¼å­¦ä¹ ç©ºé—´å’Œä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœéªŒè¯äº†ARRAçš„çµæ´»æ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†FIDå€¼ï¼Œè¯æ˜äº†è®­ç»ƒç›®æ ‡çš„é‡æ–°è®¾è®¡å¯ä»¥è§£å†³è·¨æ¨¡æ€å…¨çƒä¸€è‡´æ€§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07507', 'title': 'PE3R: Perception-Efficient 3D Reconstruction', 'url': 'https://huggingface.co/papers/2503.07507', 'abstract': 'Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.', 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6a53d341839fc41f', 'authors': ['Jie Hu', 'Shizun Wang', 'Xinchao Wang'], 'affiliations': ['xML Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07507.jpg', 'data': {'categories': ['#3d', '#architecture', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'PE3R - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹. PE3R Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with PE3R', 'desc': 'This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception.'}, 'zh': {'title': 'æ„ŸçŸ¥é«˜æ•ˆ3Dé‡å»ºï¼Œé€Ÿåº¦ä¸ç²¾åº¦çš„åŒé‡æå‡', 'desc': 'æœ€è¿‘åœ¨2Dåˆ°3Dæ„ŸçŸ¥æ–¹é¢çš„è¿›å±•æ˜¾è‘—æé«˜äº†ä»2Då›¾åƒç†è§£3Dåœºæ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç€åœºæ™¯æ³›åŒ–èƒ½åŠ›æœ‰é™ã€æ„ŸçŸ¥ç²¾åº¦ä¸ä½³å’Œé‡å»ºé€Ÿåº¦æ…¢ç­‰å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ„ŸçŸ¥é«˜æ•ˆ3Dé‡å»ºï¼ˆPE3Rï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚PE3Ré‡‡ç”¨å‰é¦ˆæ¶æ„ï¼Œèƒ½å¤Ÿå¿«é€Ÿé‡å»º3Dè¯­ä¹‰åœºï¼Œå¹¶åœ¨å¤šæ ·åœºæ™¯å’Œç‰©ä½“ä¸Šå±•ç¤ºå‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜é‡å»ºé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07459', 'title': 'MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning', 'url': 'https://huggingface.co/papers/2503.07459', 'abstract': 'Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.', 'score': 8, 'issue_id': 2634, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '4d9aba5593231609', 'authors': ['Xiangru Tang', 'Daniel Shao', 'Jiwoong Sohn', 'Jiapeng Chen', 'Jiayi Zhang', 'Jinyu Xiang', 'Fang Wu', 'Yilun Zhao', 'Chenglin Wu', 'Wenqi Shi', 'Arman Cohan', 'Mark Gerstein'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07459.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#healthcare'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº <PRE_TAG>MedAgentsBench</POST_TAG> Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DeepSeek R1 Ğ¸ OpenAI o3, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'MedAgentsBench: Elevating Medical Question-Answering Evaluation', 'desc': 'This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources.'}, 'zh': {'title': 'åŒ»å­¦é—®ç­”çš„æ–°åŸºå‡†ï¼šæŒ‘æˆ˜å¤æ‚æ¨ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•â€”â€”MedAgentsBenchï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åŒ»å­¦é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“æ³¨äºéœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯å½“å‰æ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜çš„é¢†åŸŸã€‚é€šè¿‡å¯¹ä¸ƒä¸ªå·²å»ºç«‹çš„åŒ»å­¦æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œæœ¬æ–‡è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼ŒåŒ…æ‹¬ç®€å•é—®é¢˜çš„æ™®éæ€§å’Œè¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€æ–°çš„æ€ç»´æ¨¡å‹åœ¨å¤æ‚åŒ»å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•åœ¨æ€§èƒ½ä¸æˆæœ¬æ¯”æ–¹é¢å…·æœ‰è‰¯å¥½çš„å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07197', 'title': 'Effective and Efficient Masked Image Generation Models', 'url': 'https://huggingface.co/papers/2503.07197', 'abstract': "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.", 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '4740cc0178bbf099', 'authors': ['Zebin You', 'Jingyang Ou', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Method', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07197.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ eMIGM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ImageNet. eMIGM Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Unifying Masked Models for Efficient Image Generation', 'desc': "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of FrÃ©chet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."}, 'zh': {'title': 'ç»Ÿä¸€æ©è”½æ¨¡å‹ï¼Œæå‡å›¾åƒç”Ÿæˆæ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ©è”½å›¾åƒç”Ÿæˆæ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†è®­ç»ƒå’Œé‡‡æ ·çš„è®¾è®¡ç©ºé—´ï¼Œè¯†åˆ«å‡ºå½±å“æ€§èƒ½å’Œæ•ˆç‡çš„å…³é”®å› ç´ ã€‚åŸºäºè¿™äº›æ”¹è¿›ï¼Œæˆ‘ä»¬å¼€å‘äº†åä¸ºeMIGMçš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒeMIGMåœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è¾ƒä½çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ä¸‹è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs', 'url': 'https://huggingface.co/papers/2503.05856', 'abstract': "Mixture of <PRE_TAG>large language model (LLMs) Agents (MoA)</POST_TAG> architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a single carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'score': 7, 'issue_id': 2639, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'a7ca72375d4cd423', 'authors': ['Lorenz Wolf', 'Sangwoong Yoon', 'Ilija Bogunovic'], 'affiliations': ['University College London Center for Artificial Intelligence, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2503.05856.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark', '#agents', '#hallucinations'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture of large language model Agents (MoA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ MoA Ğº Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼, Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ MoA Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ AlpacaEval 2.0 Ğ¸ QuALITY. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ”Ğ¾Ğ¶Ğ° Ğ’ĞµĞ½ĞµÑ†Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Strengthening MoA: Defending Against Deceptive Agents', 'desc': 'This paper investigates the robustness of Mixture of Large Language Model Agents (MoA) architectures, which have shown impressive results in various benchmarks. The authors highlight a significant gap in understanding how these models handle deceptive agents that can provide misleading information. Their experiments reveal that even a single deceptive agent can drastically reduce the performance of MoA systems, indicating vulnerabilities in their design. To address these issues, the paper proposes unsupervised defense mechanisms inspired by historical voting processes to enhance the reliability of MoA against such threats.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸å¯é æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ··åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»£ç†æ¶æ„çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚å°½ç®¡è¿™äº›æ¶æ„åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹å…¶æŠµå¾¡è¯¯å¯¼æ€§ä¿¡æ¯çš„èƒ½åŠ›ç¼ºä¹è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå•ä¸ªè¯¯å¯¼æ€§ä»£ç†çš„å¼•å…¥ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„æ€§èƒ½ï¼Œç”šè‡³æŠµæ¶ˆæ‰€æœ‰çš„ä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—æ— ç›‘ç£çš„é˜²å¾¡æœºåˆ¶ï¼Œä»¥æ¢å¤å¤§éƒ¨åˆ†ä¸¢å¤±çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06520', 'title': 'Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement', 'url': 'https://huggingface.co/papers/2503.06520', 'abstract': "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.", 'score': 6, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'b21eb23a448d282e', 'authors': ['Yuqi Liu', 'Bohao Peng', 'Zhisheng Zhong', 'Zihao Yue', 'Fanbin Lu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'RUC'], 'pdf_title_img': 'assets/pdf/title_img/2503.06520.jpg', 'data': {'categories': ['#rl', '#benchmark', '#architecture', '#reasoning', '#rlhf', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Seg-Zero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Seg-Zero-7B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 18% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ReasonSeg.'}, 'en': {'title': 'Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization', 'desc': 'The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models.'}, 'zh': {'title': 'Seg-Zeroï¼šçªç ´æ€§æ¨ç†ä¸åˆ†å‰²çš„ç»“åˆ', 'desc': 'ä¼ ç»Ÿçš„åˆ†å‰²æ¨ç†æ–¹æ³•ä¾èµ–äºå¸¦æœ‰ç±»åˆ«æ ‡ç­¾çš„ç›‘ç£å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg-Zeroï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è®¤çŸ¥å¼ºåŒ–æ¨å¯¼å‡ºæ˜ç¡®çš„æ¨ç†é“¾ã€‚Seg-Zeroå¼•å…¥äº†ä¸€ä¸ªè§£è€¦æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œæ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼Œå¹¶äº§ç”Ÿä½ç½®æç¤ºï¼Œéšåç”±åˆ†å‰²æ¨¡å‹ç”Ÿæˆç²¾ç¡®çš„åƒç´ çº§æ©ç ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒSeg-Zeroåœ¨æ²¡æœ‰æ˜ç¡®æ¨ç†æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºçªå‡ºçš„æµ‹è¯•æ—¶æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06121', 'title': 'BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling', 'url': 'https://huggingface.co/papers/2503.06121', 'abstract': "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.", 'score': 5, 'issue_id': 2631, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': '3f03abe6317e5bba', 'authors': ['Li weile', 'Liu Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.06121.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#open_source'], 'emoji': 'â³', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ RWKV-7', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ RWKV-7. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ RWKV-7 Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Timer Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance', 'desc': "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."}, 'zh': {'title': 'åˆ›æ–°æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡', 'desc': 'æ—¶é—´åºåˆ—æ¨¡å‹åœ¨å¤„ç†å¤§å‹å¤æ‚æ•°æ®é›†æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œç±»ä¼¼äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•èƒ½åŠ›ã€‚æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹æ€§å’Œæ¨¡å‹æ‰©å±•çš„è®¡ç®—éœ€æ±‚éœ€è¦åˆ›æ–°çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨RWKV-7å°†å…ƒå­¦ä¹ èå…¥çŠ¶æ€æ›´æ–°æœºåˆ¶ã€‚é€šè¿‡å°†RWKV-7çš„æ—¶é—´æ··åˆå’Œé€šé“æ··åˆç»„ä»¶æ•´åˆåˆ°åŸºäºå˜æ¢å™¨çš„æ—¶é—´åºåˆ—æ¨¡å‹Timerä¸­ï¼Œæˆ‘ä»¬å®ç°äº†çº¦1.13åˆ°43.3å€çš„æ€§èƒ½æå‡ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´å‡å°‘äº†4.5å€ï¼ŒåŒæ—¶å‚æ•°æ•°é‡ä»…ä¸ºåŸæ¥çš„1/23ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07274', 'title': 'Efficient Distillation of Classifier-Free Guidance using Adapters', 'url': 'https://huggingface.co/papers/2503.07274', 'abstract': 'While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (sim2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (sim2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method.', 'score': 4, 'issue_id': 2639, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '84e5b0fd1b6f7f9f', 'authors': ['Cristian Perez Jensen', 'Seyedmorteza Sadat'], 'affiliations': ['ETH ZÃ¼rich'], 'pdf_title_img': 'assets/pdf/title_img/2503.07274.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ' (AGD) Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. AGD ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° (CFG) Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ ÑĞµÑ‚Ğ¸, ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ 2% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹. AGD Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼, Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ñ CFG."}, 'en': {'title': 'Speeding Up Diffusion Models with Adapter Guidance Distillation', 'desc': 'This paper presents adapter guidance distillation (AGD), a new method that enhances the efficiency of conditional diffusion models by simulating classifier-free guidance (CFG) in a single forward pass. AGD uses lightweight adapters to approximate CFG, which allows for faster sampling without sacrificing sample quality. The approach keeps the base model unchanged and only trains a small number of additional parameters, significantly reducing resource requirements. Experimental results demonstrate that AGD achieves similar or better performance compared to CFG while halving the number of neural function evaluations needed, making it feasible to distill large models on consumer-grade hardware.'}, 'zh': {'title': 'é€‚é…å™¨å¼•å¯¼è’¸é¦ï¼šæå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºé€‚é…å™¨å¼•å¯¼è’¸é¦ï¼ˆAGDï¼‰ï¼Œæ—¨åœ¨æé«˜æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚AGDé€šè¿‡è½»é‡çº§é€‚é…å™¨åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­æ¨¡æ‹Ÿæ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ï¼Œä»è€Œå®ç°äº†é‡‡æ ·é€Ÿåº¦çš„åŠ å€ï¼ŒåŒæ—¶ä¿æŒæˆ–æ”¹å–„æ ·æœ¬è´¨é‡ã€‚ä¸ä»¥å¾€çš„å¼•å¯¼è’¸é¦æ–¹æ³•ä¸åŒï¼ŒAGDåªè®­ç»ƒå°‘é‡é¢å¤–å‚æ•°ï¼Œè€Œä¿æŒåŸºç¡€æ¨¡å‹ä¸å˜ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜AGDåœ¨å¤šä¸ªæ¶æ„ä¸Šå®ç°äº†ä¸CFGç›¸å½“æˆ–æ›´ä¼˜çš„FIDï¼ŒåŒæ—¶åªéœ€ä¸€åŠçš„ç¥ç»å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07603', 'title': 'Should VLMs be Pre-trained with Image Data?', 'url': 'https://huggingface.co/papers/2503.07603', 'abstract': 'Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '16f17eb6db67a418', 'authors': ['Sedrick Keh', 'Jean Mercat', 'Samir Yitzhak Gadre', 'Kushal Arora', 'Igor Vasiljevic', 'Benjamin Burchfiel', 'Shuran Song', 'Russ Tedrake', 'Thomas Kollar', 'Ludwig Schmidt', 'Achal Dave'], 'affiliations': ['Columbia University', 'MIT', 'Stanford', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.07603.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#transfer_learning', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…) Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 80% ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°ĞµÑ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 2% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Vision-Language Synergy: Timing Matters!', 'desc': 'This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model.'}, 'zh': {'title': 'å›¾åƒä¸æ–‡æœ¬æ··åˆé¢„è®­ç»ƒæå‡è§†è§‰è¯­è¨€ä»»åŠ¡è¡¨ç°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ å…¥å›¾åƒæ•°æ®ååœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¸­åŠ å…¥å›¾åƒæ•°æ®å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ä¸æ¸…æ¥šè¿™ç§ä¸¤æ­¥è®­ç»ƒæµç¨‹ä¸æ—©æœŸæ•´åˆå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›¸æ¯”ï¼Œç©¶ç«Ÿæ˜¯å¢ç›Šè¿˜æ˜¯æŸå¤±ã€‚é€šè¿‡è®­ç»ƒä¸åŒæ•°æ®é›†ã€è§„æ¨¡å’Œå›¾åƒæ–‡æœ¬æ¯”ä¾‹çš„æ¨¡å‹ï¼Œç ”ç©¶è€…å‘ç°æ··åˆå›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„é¢„è®­ç»ƒå¯ä»¥æé«˜è§†è§‰è¯­è¨€ä»»åŠ¡çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨ä»…æ–‡æœ¬çš„è¯„ä¼°ä¸­ä¹Ÿä¿æŒè‰¯å¥½è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºä¸€ä¸ª10äº¿å‚æ•°çš„æ¨¡å‹ï¼Œåœ¨é¢„è®­ç»ƒçš„80%æ—¶å¼•å…¥è§†è§‰æ ‡è®°ï¼Œå¹³å‡æå‡äº†2%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07265', 'title': 'WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2503.07265', 'abstract': 'Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6174088b65d232ba', 'authors': ['Yuwei Niu', 'Munan Ning', 'Mengren Zheng', 'Bin Lin', 'Peng Jin', 'Jiaqi Liao', 'Kunpeng Ning', 'Bin Zhu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'Peking University', 'PengCheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07265.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'WISE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WISE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. WISE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² 25 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ», Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ WiScore Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing T2I Models with World Knowledge Evaluation', 'desc': 'This paper introduces WISE, a new benchmark for evaluating Text-to-Image (T2I) models that focuses on their ability to integrate world knowledge and complex semantic understanding. Unlike previous assessments that primarily measure image realism and basic text-image alignment, WISE challenges models with 1,000 detailed prompts across various domains, including cultural common sense and natural science. The authors also present WiScore, a new metric designed to quantitatively assess how well models align knowledge with generated images. Testing reveals that many current T2I models struggle to effectively incorporate world knowledge, indicating areas for improvement in future model development.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„çŸ¥è¯†æ•´åˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†WISEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºä¸–ç•ŒçŸ¥è¯†é©±åŠ¨çš„è¯­ä¹‰è¯„ä¼°è®¾è®¡çš„åŸºå‡†ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒçš„çœŸå®æ„Ÿå’Œç®€å•çš„æ–‡æœ¬-å›¾åƒå¯¹é½ï¼Œç¼ºä¹å¯¹å¤æ‚è¯­ä¹‰ç†è§£å’Œä¸–ç•ŒçŸ¥è¯†æ•´åˆçš„å…¨é¢è¯„ä¼°ã€‚WISEé€šè¿‡1000ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ¶µç›–æ–‡åŒ–å¸¸è¯†ã€æ—¶ç©ºæ¨ç†å’Œè‡ªç„¶ç§‘å­¦ç­‰25ä¸ªå­é¢†åŸŸï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†WiScoreè¿™ä¸€æ–°é¢–çš„å®šé‡æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°çŸ¥è¯†ä¸å›¾åƒçš„å¯¹é½ç¨‹åº¦ï¼Œæµ‹è¯•ç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨æœ‰æ•ˆæ•´åˆå’Œåº”ç”¨ä¸–ç•ŒçŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06273', 'title': 'Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations', 'url': 'https://huggingface.co/papers/2503.06273', 'abstract': 'We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded <PRE_TAG>Zero-AVSR</POST_TAG>. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': '66b650ba2f5b0404', 'authors': ['Jeong Hun Yeo', 'Minsu Kim', 'Chae Won Kim', 'Stavros Petridis', 'Yong Man Ro'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06273.jpg', 'data': {'categories': ['#low_resource', '#audio', '#dataset', '#multilingual', '#machine_translation'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Zero-AVSR Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Audio-Visual Speech Romanizer Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³Ñ€Ğ°Ñ„ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Zero-AVSR, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ MARC, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2916 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° 82 ÑĞ·Ñ‹ĞºĞ°Ñ….'}, 'en': {'title': 'Zero-AVSR: Speech Recognition Without Language-Specific Data', 'desc': 'The paper presents a new framework called Zero-AVSR for Audio-Visual Speech Recognition that can recognize speech in languages without needing any specific audio-visual data for those languages. It introduces the Audio-Visual Speech Romanizer (AV-Romanizer), which creates language-independent speech representations by predicting Roman text. By utilizing Large Language Models (LLMs), the framework converts these predictions into specific language graphemes, enhancing multilingual capabilities. Additionally, a new dataset, the Multilingual Audio-Visual Romanized Corpus (MARC), is introduced to support the training and evaluation of this framework across 82 languages.'}, 'zh': {'title': 'é›¶æ ·æœ¬éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«çš„åˆ›æ–°æ¢ç´¢', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç§°ä¸ºZero-AVSRï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç›®æ ‡è¯­è¨€éŸ³è§†é¢‘è¯­éŸ³æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†éŸ³è§†é¢‘è¯­éŸ³ç½—é©¬åŒ–å™¨ï¼ˆAV-Romanizerï¼‰ï¼Œé€šè¿‡é¢„æµ‹ç½—é©¬æ–‡æœ¬æ¥å­¦ä¹ ä¸è¯­è¨€æ— å…³çš„è¯­éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§å¤šè¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œå°†é¢„æµ‹çš„ç½—é©¬æ–‡æœ¬è½¬æ¢ä¸ºç‰¹å®šè¯­è¨€çš„å­—å½¢ï¼Œå½¢æˆçº§è”çš„Zero-AVSRã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æ–¹æ¡ˆå¾®è°ƒé€‚é…å™¨å’ŒLLMï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†ç»Ÿä¸€çš„Zero-AVSRæ–¹æ³•ï¼Œç›´æ¥å°†AV-Romanizerç¼–ç çš„éŸ³è§†é¢‘è¯­éŸ³è¡¨ç¤ºé›†æˆåˆ°LLMä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03499', 'title': 'State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models', 'url': 'https://huggingface.co/papers/2503.03499', 'abstract': 'State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.', 'score': 3, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '80ab6abd822f2976', 'authors': ['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.03499.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…, ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ SSM. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State-offset Tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Fine-Tuning with State-Based Methods for SSMs', 'desc': 'This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks.'}, 'zh': {'title': 'åŸºäºçŠ¶æ€çš„å¾®è°ƒï¼šè¶…è¶Šæç¤ºçš„æ–¹æ³•', 'desc': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºå˜æ¢å™¨çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå‡è½»å…¶äºŒæ¬¡è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨SSMsä¸Šçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºåŸºäºçŠ¶æ€çš„æ–¹æ³•ï¼Œä½œä¸ºä¼˜äºåŸºäºæç¤ºçš„æ–¹æ³•çš„æ–°é€‰æ‹©ï¼Œè¿™äº›æ–¹æ³•ç›´æ¥è°ƒæ•´ä¸çŠ¶æ€ç›¸å…³çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºçŠ¶æ€çš„PEFTæ–¹æ³•ï¼šçŠ¶æ€åç§»å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç›´æ¥å½±å“å½“å‰çŠ¶æ€ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„é€‚åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02199', 'title': 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?', 'url': 'https://huggingface.co/papers/2503.02199', 'abstract': "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.", 'score': 3, 'issue_id': 2630, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a354f8de058f0f84', 'authors': ['Ailin Deng', 'Tri Cao', 'Zhirui Chen', 'Bryan Hooi'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.02199.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ VLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ 'ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚', Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ°ÑĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ñƒ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞµĞ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Balancing Vision and Text: Overcoming Bias in Vision-Language Models', 'desc': "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."}, 'zh': {'title': 'å¹³è¡¡è®­ç»ƒï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹æ¨¡æ€ä¸ä¸€è‡´æ—¶çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æ¢è®¨äº†VLMsåœ¨è§†è§‰æ•°æ®å’Œä¸åŒæ–‡æœ¬è¾“å…¥ä¸‹çš„æ¨¡æ€åå¥½ï¼Œå‘ç°äº†â€œå¯¹æ–‡æœ¬çš„ç›²ç›®ä¿¡ä»»â€ç°è±¡ï¼šå½“å‡ºç°ä¸ä¸€è‡´æ—¶ï¼ŒVLMsè¿‡åº¦ä¾èµ–æ–‡æœ¬æ•°æ®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬åˆ†æäº†å½±å“è¿™ç§æ–‡æœ¬åè§çš„å› ç´ ï¼ŒåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€è¯­è¨€æ¨¡å‹å¤§å°ã€æ–‡æœ¬ç›¸å…³æ€§ã€æ ‡è®°é¡ºåºä»¥åŠè§†è§‰å’Œæ–‡æœ¬ç¡®å®šæ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¸¦æœ‰æ–‡æœ¬å¢å¼ºçš„ç›‘ç£å¾®è°ƒï¼Œå¹¶è¯æ˜å…¶åœ¨å‡å°‘æ–‡æœ¬åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07595', 'title': 'Detection Avoidance Techniques for Large Language Models', 'url': 'https://huggingface.co/papers/2503.07595', 'abstract': "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.", 'score': 2, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '0b6929d80e047189', 'authors': ['Sinclair Schneider', 'Florian Steuber', 'Joao A. G. Schneider', 'Gabi Dreo Rodosek'], 'affiliations': ['Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.07595.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#data', '#ethics', '#rl', '#hallucinations'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞ±Ğ¼Ğ°Ğ½ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: ĞºĞ°Ğº LLM Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DetectGPT. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Evasion Techniques Against Language Model Detectors', 'desc': "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."}, 'zh': {'title': 'åº”å¯¹å‡æ–°é—»çš„æ™ºèƒ½æ£€æµ‹æŒ‘æˆ˜', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™®åŠï¼Œå‡æ–°é—»ä¼ æ’­çš„é£é™©ä¹Ÿéšä¹‹å¢åŠ ã€‚å› æ­¤ï¼Œå¼€å‘åƒDetectGPTè¿™æ ·çš„åˆ†ç±»ç³»ç»Ÿå˜å¾—è‡³å…³é‡è¦ã€‚è¿™äº›æ£€æµ‹å™¨å®¹æ˜“å—åˆ°è§„é¿æŠ€æœ¯çš„å½±å“ï¼Œå®éªŒè¡¨æ˜ï¼Œç”Ÿæˆæ¨¡å‹æ¸©åº¦çš„ç³»ç»Ÿæ€§å˜åŒ–ä½¿å¾—æµ…å±‚å­¦ä¹ æ£€æµ‹å™¨çš„å¯é æ€§æœ€ä½ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒç”Ÿæˆæ¨¡å‹å¯ä»¥ç»•è¿‡åŸºäºBERTçš„æ£€æµ‹å™¨ï¼Œè€Œé‡æ–°è¡¨è¿°æ–‡æœ¬åˆ™ä½¿å¾—åƒDetectGPTè¿™æ ·çš„é›¶æ ·æœ¬æ£€æµ‹å™¨çš„è§„é¿ç‡è¶…è¿‡90%ï¼Œå°½ç®¡æ–‡æœ¬ä¸åŸæ–‡é«˜åº¦ç›¸ä¼¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07465', 'title': 'YOLOE: Real-Time Seeing Anything', 'url': 'https://huggingface.co/papers/2503.07465', 'abstract': "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3times less training cost and 1.4times inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less training time. Code and models are available at https://github.com/THU-MIG/yoloe.", 'score': 2, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'faab5c7007c9cc4d', 'authors': ['Ao Wang', 'Lihao Liu', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07465.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#optimization', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'YOLOE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ³Ğ¾ ÑƒĞ³Ğ¾Ğ´Ğ½Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'YOLOE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ RepRTA Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SAVPE Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ LRPC Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. YOLOE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'YOLOE: Real-Time Object Detection and Segmentation for Open-Set Scenarios', 'desc': 'This paper presents YOLOE, a novel model that enhances object detection and segmentation in open-set scenarios by integrating various prompt mechanisms. It introduces a Re-parameterizable Region-Text Alignment (RepRTA) strategy for text prompts, which refines textual embeddings efficiently without additional inference costs. For visual prompts, the Semantic-Activated Visual Prompt Encoder (SAVPE) improves visual accuracy while maintaining low complexity. Additionally, the Lazy Region-Prompt Contrast (LRPC) strategy allows for prompt-free object identification, significantly reducing training costs and improving inference speed compared to traditional models.'}, 'zh': {'title': 'YOLOEï¼šé«˜æ•ˆçš„å¼€æ”¾åœºæ™¯ç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²æ¨¡å‹YOLOEï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿæ¨¡å‹åœ¨å¼€æ”¾åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚YOLOEé€šè¿‡é›†æˆå¤šç§å¼€æ”¾æç¤ºæœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶æ£€æµ‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†å¯é‡å‚æ•°åŒ–åŒºåŸŸ-æ–‡æœ¬å¯¹é½ç­–ç•¥ï¼ˆRepRTAï¼‰å’Œè¯­ä¹‰æ¿€æ´»è§†è§‰æç¤ºç¼–ç å™¨ï¼ˆSAVPEï¼‰ï¼Œä»¥æé«˜è§†è§‰å’Œæ–‡æœ¬çš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOEåœ¨é›¶æ ·æœ¬æ€§èƒ½å’Œè¿ç§»èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®­ç»ƒæˆæœ¬ä½ï¼Œæ¨ç†æ•ˆç‡é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07389', 'title': 'TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.07389', 'abstract': "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.", 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'd88e626045dd077e', 'authors': ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu'], 'affiliations': ['The School of Electrical and Information Engineering, Tianjin University', 'The School of New Media and Communication, Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07389.jpg', 'data': {'categories': ['#benchmark', '#security', '#open_source', '#training', '#cv', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ”’', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TRCE Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. TRCE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ TRCE Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰ÑƒÑ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'TRCE: Safeguarding Image Generation with Smart Concept Erasure', 'desc': "This paper presents TRCE, a novel approach to mitigate the risk of generating malicious content in text-to-image diffusion models. It employs a two-stage concept erasure strategy that effectively removes harmful semantics from prompts while maintaining the model's ability to generate normal content. The first stage focuses on optimizing cross-attention layers to transform malicious prompts into safer alternatives. The second stage utilizes contrastive learning to guide the denoising process towards safe outputs, ensuring that the model does not produce NSFW images while preserving its generative capabilities."}, 'zh': {'title': 'å®‰å…¨ç”Ÿæˆï¼ŒæŠ¹é™¤æ¶æ„æ¦‚å¿µçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆæ¶æ„å†…å®¹çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äº†æ¦‚å¿µæŠ¹é™¤æ–¹æ³•ï¼Œä»¥å¸®åŠ©æ¨¡å‹å¿˜è®°ç‰¹å®šçš„æ¶æ„æ¦‚å¿µã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†TRCEï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„æ¦‚å¿µæŠ¹é™¤ç­–ç•¥ï¼Œåœ¨å¯é æŠ¹é™¤å’ŒçŸ¥è¯†ä¿ç•™ä¹‹é—´å®ç°æœ‰æ•ˆçš„å¹³è¡¡ã€‚é€šè¿‡ä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å±‚ï¼ŒTRCEèƒ½å¤Ÿå°†æ¶æ„æç¤ºæ˜ å°„åˆ°å®‰å…¨çš„æ¦‚å¿µï¼Œä»è€Œé¿å…ç”Ÿæˆæ¶æ„å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'url': 'https://huggingface.co/papers/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.', 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '35668e47701edcd0', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'Xiaojuan Qi'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.06960.jpg', 'data': {'categories': ['#robotics', '#open_source', '#training', '#cv', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SlotMIM: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (PVM) ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡Ğ¾Ğ¼ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ PVM ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SlotMIM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SlotMIM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Object-Centric Learning in Robotics with SlotMIM', 'desc': 'This paper explores the effectiveness of pre-trained vision models (PVMs) in robotics, particularly focusing on their ability to learn object-centric representations. The authors find that models like DINO and iBOT perform better than MAE in various tasks but struggle with non-object-centric data. They introduce SlotMIM, a new method that enhances object-centric learning by using a semantic bottleneck and cross-view consistency regularization. Their experiments show that SlotMIM significantly improves representation learning across different datasets and tasks, demonstrating better data efficiency and scalability.'}, 'zh': {'title': 'å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ˜¯æˆåŠŸçš„å…³é”®', 'desc': 'é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMsï¼‰åœ¨ç°ä»£æœºå™¨äººæŠ€æœ¯ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶æœ€ä½³é…ç½®å°šä¸æ˜ç¡®ã€‚ç ”ç©¶å‘ç°ï¼ŒDINOå’ŒiBOTåœ¨è§†è§‰è¿åŠ¨æ§åˆ¶å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºMAEï¼Œä½†åœ¨éå•å¯¹è±¡ä¸­å¿ƒï¼ˆNOCï¼‰æ•°æ®ä¸Šè®­ç»ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™ä¸å®ƒä»¬å­¦ä¹ å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„èƒ½åŠ›ä¸‹é™å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»éå¯¹è±¡ä¸­å¿ƒçš„æœºå™¨äººæ•°æ®é›†ä¸­å½¢æˆå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„èƒ½åŠ›æ˜¯PVMsæˆåŠŸçš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†SlotMIMæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰ç“¶é¢ˆæ¥å‡å°‘åŸå‹æ•°é‡ï¼Œä¿ƒè¿›å¯¹è±¡æ€§å‡ºç°ï¼Œå¹¶é€šè¿‡äº¤å‰è§†å›¾ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¥é¼“åŠ±å¤šè§†å›¾ä¸å˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06885', 'title': 'ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks', 'url': 'https://huggingface.co/papers/2503.06885', 'abstract': 'Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.', 'score': 2, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '196c83578a251f8e', 'authors': ['Yan Yang', 'Dongxu Li', 'Haoning Wu', 'Bei Chen', 'Liu Liu', 'Liyuan Pan', 'Junnan Li'], 'affiliations': ['ANU', 'BITSZ & School of CSAT, BIT', 'KooMap, Huawei', 'NTU', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.06885.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agi', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ProBench: Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ProBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 4000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 10 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 56 Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°ÑƒĞºÑƒ, Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¸Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 24 Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° MLLM-as-a-Judge Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ProBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'ProBench: Benchmarking Multimodal Intelligence for Expert Tasks', 'desc': 'This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ™ºèƒ½è¯„ä¼°çš„æ–°åŸºå‡†ï¼šProBench', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ProBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨ä¸“ä¸šé¢†åŸŸçš„æ™ºèƒ½è¡¨ç°ã€‚ProBenchåŒ…å«4000ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–ç§‘å­¦ã€è‰ºæœ¯ã€äººæ–‡å­¦ç§‘ã€ç¼–ç¨‹ã€æ•°å­¦å’Œåˆ›æ„å†™ä½œç­‰10ä¸ªé¢†åŸŸå’Œ56ä¸ªå­é¢†åŸŸã€‚é€šè¿‡å¯¹24ä¸ªæœ€æ–°æ¨¡å‹çš„å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå°½ç®¡ä¸€äº›å¼€æºæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œä½†åœ¨è§†è§‰æ„ŸçŸ¥ã€æ–‡æœ¬ç†è§£ã€é¢†åŸŸçŸ¥è¯†å’Œé«˜çº§æ¨ç†æ–¹é¢ï¼ŒProBenchä»ç„¶æå‡ºäº†æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦çš„å‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06626', 'title': 'DiffCLIP: Differential Attention Meets CLIP', 'url': 'https://huggingface.co/papers/2503.06626', 'abstract': "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP.", 'score': 2, 'issue_id': 2641, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'fff7aaf70b1d6ed0', 'authors': ['Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06626.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#benchmark', '#multimodal', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'DiffCLIP: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ DiffCLIP - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CLIP. Ğ”Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑˆÑƒĞ¼Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. DiffCLIP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ² Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ CLIP Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ….'}, 'en': {'title': 'Enhancing CLIP with Differential Attention for Better Image-Text Understanding', 'desc': "DiffCLIP is a new vision-language model that improves the CLIP architecture by using a technique called differential attention. This method helps the model focus on important information while ignoring irrelevant details, which was initially designed for large language models. By incorporating differential attention into CLIP's dual encoder system, DiffCLIP enhances its ability to understand images and text together. The model shows better performance in various tasks like zero-shot classification and retrieval, all while maintaining low computational costs."}, 'zh': {'title': 'DiffCLIPï¼šé«˜æ•ˆå¢å¼ºå¤šæ¨¡æ€è¡¨ç¤ºçš„åˆ›æ–°æ¨¡å‹', 'desc': 'æˆ‘ä»¬æå‡ºäº†DiffCLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶æ‰©å±•åˆ°CLIPæ¶æ„ä¸­ã€‚å·®åˆ†æ³¨æ„åŠ›æœ€åˆæ˜¯ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘çš„ï¼Œæ—¨åœ¨æ”¾å¤§ç›¸å…³ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶æ¶ˆé™¤å™ªå£°ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸€æœºåˆ¶é›†æˆåˆ°CLIPçš„åŒç¼–ç å™¨ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰æ¡†æ¶ä¸­ã€‚DiffCLIPåœ¨å›¾åƒ-æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”å‡ ä¹æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œè¯æ˜äº†å·®åˆ†æ³¨æ„åŠ›å¯ä»¥æ˜¾è‘—å¢å¼ºå¤šæ¨¡æ€è¡¨ç¤ºè€Œä¸ç‰ºç‰²æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06362', 'title': 'Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2503.06362', 'abstract': 'Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels.', 'score': 2, 'issue_id': 2638, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'e35aea6b25fbc264', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis'], 'affiliations': ['Imperial College London', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.06362.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#audio'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ AVSR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ñ‚Ñ€ĞµÑˆĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Llama-MTSK - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¼Ğ°Ñ‚Ñ€ĞµÑˆĞºĞ¸ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (AVSR). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Matryoshka Representation Learning Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ LoRA-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Llama-MTSK: Efficient AVSR with Flexible Token Allocation', 'desc': 'This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) designed for Audio-Visual Speech Recognition (AVSR). It addresses the challenge of integrating lengthy speech representations with LLMs by allowing flexible audio-visual token allocation based on computational constraints. The model encodes audio-visual data at various granularities, which helps maintain high performance without the need for separate models for different compression levels. Additionally, the paper presents three LoRA-based strategies for fine-tuning the model, achieving state-of-the-art results on major AVSR datasets.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«è§£å†³æ–¹æ¡ˆ', 'desc': 'éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œä»¥æé«˜åœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºäº†å®ƒä»¬åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬AVSRã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³è¡¨ç¤ºçš„é•¿åº¦è¾ƒå¤§ï¼Œç›´æ¥ä¸LLMsé›†æˆä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-MTSKï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¥—å¨ƒçš„å¤šæ¨¡æ€LLMï¼Œèƒ½å¤Ÿæ ¹æ®ç‰¹å®šçš„è®¡ç®—çº¦æŸçµæ´»è°ƒæ•´éŸ³è§†é¢‘æ ‡è®°çš„åˆ†é…ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'url': 'https://huggingface.co/papers/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.', 'score': 2, 'issue_id': 2640, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '689a25e37e909986', 'authors': ['Jian Liu', 'Wei Sun', 'Kai Zeng', 'Jin Zheng', 'Hui Yang', 'Lin Wang', 'Hossein Rahmani', 'Ajmal Mian'], 'affiliations': ['Central South University', 'Hunan University', 'Lancaster University', 'Nanyang Technological University', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.05578.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 6D-Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SinRef-6D Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 6D-Ğ¿Ğ¾Ğ·Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ RGB-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. SinRef-6D ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ·Ğµ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Single View, Full Pose: Simplifying 6D Object Estimation', 'desc': 'This paper introduces a new method for estimating the 6D pose of novel objects using only a single reference view, which is more scalable than traditional methods that rely on CAD models or multiple views. The proposed SinRef-6D method utilizes iterative point-wise alignment in the camera coordinate system, effectively addressing challenges posed by large pose discrepancies. By employing state space models (SSMs), the method captures long-range dependencies and spatial information, achieving linear complexity and enhanced spatial modeling. Experimental results show that SinRef-6D performs comparably to existing methods while simplifying the pose estimation process.'}, 'zh': {'title': 'å•è§†å›¾ä¸‹çš„å…­ç»´å§¿æ€ä¼°è®¡æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•å‚è€ƒè§†å›¾çš„å…­ç»´å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œç§°ä¸ºSinRef-6Dã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç›¸æœºåæ ‡ç³»ä¸­è¿­ä»£å»ºç«‹ç‚¹å¯¹é½ï¼Œè§£å†³äº†å¤§å§¿æ€å·®å¼‚å’Œç©ºé—´ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚SinRef-6Dåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¹¶å…·æœ‰çº¿æ€§å¤æ‚åº¦å’Œä¼˜è¶Šçš„ç©ºé—´å»ºæ¨¡èƒ½åŠ›ã€‚ç»è¿‡åœ¨åˆæˆæ•°æ®ä¸Šçš„é¢„è®­ç»ƒåï¼ŒSinRef-6Dèƒ½å¤Ÿä»…ä½¿ç”¨å•ä¸€å‚è€ƒè§†å›¾ä¼°è®¡æ–°ç‰©ä½“çš„å…­ç»´å§¿æ€ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæˆ–CADæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07598', 'title': 'VACE: All-in-One Video Creation and Editing', 'url': 'https://huggingface.co/papers/2503.07598', 'abstract': 'Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked <PRE_TAG>video-to-video editing</POST_TAG>. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.', 'score': 1, 'issue_id': 2644, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6bf5924fe3265297', 'authors': ['Zeyinzi Jiang', 'Zhen Han', 'Chaojie Mao', 'Jingfeng Zhang', 'Yulin Pan', 'Yu Liu'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.07598.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VACE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VACE - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Video Condition Unit Ğ´Ğ»Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Context Adapter Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. VACE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑÑƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'VACE: Unifying Video Creation and Editing in One Framework', 'desc': 'The paper introduces VACE, a unified framework for video generation and editing that leverages the capabilities of the Diffusion Transformer. It addresses the challenges of maintaining consistency in both temporal and spatial dynamics during video synthesis. VACE organizes various video tasks, such as reference-to-video generation and video-to-video editing, into a single interface called the Video Condition Unit (VCU). By employing a Context Adapter structure, VACE allows for flexible handling of different video synthesis tasks while achieving performance comparable to specialized models.'}, 'zh': {'title': 'VACEï¼šè§†é¢‘åˆ›ä½œä¸ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒå’Œè§†é¢‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†ç»Ÿä¸€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ï¼Œç ”ç©¶è€…ä»¬åœ¨å›¾åƒå†…å®¹åˆ›ä½œé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºæ—¶é—´å’Œç©ºé—´åŠ¨æ€çš„ä¸€è‡´æ€§è¦æ±‚ï¼Œè§†é¢‘åˆæˆçš„ç»Ÿä¸€æ–¹æ³•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†VACEï¼Œå®ƒå…è®¸ç”¨æˆ·åœ¨ä¸€ä¸ªç»¼åˆæ¡†æ¶å†…æ‰§è¡Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‚è€ƒè§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘å’Œæ©ç è§†é¢‘ç¼–è¾‘ç­‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07597', 'title': 'HumanMM: Global Human Motion Recovery from Multi-shot Videos', 'url': 'https://huggingface.co/papers/2503.07597', 'abstract': 'In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '161213a2a054dd4d', 'authors': ['Yuhong Zhang', 'Guanlin Wu', 'Ling-Hao Chen', 'Zhuokai Zhao', 'Jing Lin', 'Xiaoke Jiang', 'Jiamin Wu', 'Zhuoheng Li', 'Hao Frank Yang', 'Haoqian Wang', 'Lei Zhang'], 'affiliations': ['HKU', 'HKUST', 'IDEA Research', 'Johns Hopkins University', 'Tsinghua University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.07597.jpg', 'data': {'categories': ['#3d', '#dataset', '#long_context', '#video'], 'emoji': 'ğŸƒ', 'ru': {'title': 'Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¼ĞµĞ½Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ·ĞºĞ¸Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ¼ĞµĞ½Ñ‹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞºĞ¾Ğ»ÑŒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ³ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Reconstructing Realistic 3D Human Motion Across Multiple Video Shots', 'desc': 'This paper introduces a new framework for reconstructing long sequences of 3D human motion from videos that have multiple camera angles and transitions. The challenge lies in dealing with issues like sudden changes in shots, parts of the person being blocked, and moving backgrounds. Unlike previous methods that only work with single camera views, this approach combines advanced camera pose estimation with Human Motion Recovery (HMR) and includes a shot transition detector for better accuracy. The results show that the method effectively reduces foot sliding and maintains consistent human poses over time, proving its effectiveness on a specially created multi-shot dataset.'}, 'zh': {'title': 'é‡å»ºçœŸå®äººç±»è¿åŠ¨çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤šé•œå¤´åˆ‡æ¢çš„é‡å¤–è§†é¢‘ä¸­é‡å»ºé•¿åºåˆ—çš„ä¸‰ç»´äººç±»è¿åŠ¨ã€‚ç”±äºè§†é¢‘ä¸­çš„çªç„¶é•œå¤´åˆ‡æ¢ã€éƒ¨åˆ†é®æŒ¡å’ŒåŠ¨æ€èƒŒæ™¯ï¼Œè¿™ç§é•¿åºåˆ—çš„è¿åŠ¨æ¢å¤é¢ä¸´å¾ˆå¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆå¢å¼ºçš„ç›¸æœºå§¿æ€ä¼°è®¡å’Œäººç±»è¿åŠ¨æ¢å¤ï¼ˆHMRï¼‰ï¼Œå¼•å…¥äº†é•œå¤´åˆ‡æ¢æ£€æµ‹å™¨å’Œç¨³å¥çš„å¯¹é½æ¨¡å—ï¼Œä»¥ç¡®ä¿å§¿æ€å’Œæ–¹å‘åœ¨é•œå¤´é—´çš„è¿ç»­æ€§ã€‚é€šè¿‡è‡ªå®šä¹‰çš„è¿åŠ¨æ•´åˆå™¨ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å‡è½»äº†è„šæ»‘é—®é¢˜ï¼Œå¹¶ç¡®ä¿äº†äººç±»å§¿æ€çš„æ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'url': 'https://huggingface.co/papers/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter beta, subsequent methods like SimPO reintroduce complexity through dual parameters (beta, gamma). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates beta via two advances: (1) retaining SimPO's reference-free margins but removing beta through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case (beta to infty), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '619f3a64896639ee', 'authors': ['Junkang Wu', 'Kexin Huang', 'Xue Wang', 'Jinyang Gao', 'Bolin Ding', 'Jiancan Wu', 'Xiangnan He', 'Xiang Wang'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07426.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'RePO: Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RePO. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DPO Ğ¸ SimPO, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğµ beta. RePO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ReLU-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RePO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DPO Ğ¸ SimPO Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°.'}, 'en': {'title': 'Streamlining Preference Optimization for LLMs with RePO', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences using a new method called ReLU-based Preference Optimization (RePO). RePO simplifies the optimization process by eliminating the need for a complex hyperparameter, beta, while still maintaining effective performance through gradient analysis and a ReLU-based max-margin loss. The authors demonstrate that RePO can be seen as a special case of an existing method, SimPO, when certain conditions are met, leading to a more efficient training process. Empirical results indicate that RePO consistently outperforms previous methods like DPO and SimPO, while only requiring the tuning of a single hyperparameter.'}, 'zh': {'title': 'ç®€åŒ–åå¥½ä¼˜åŒ–ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯¹é½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç§°ä¸ºåŸºäºReLUçš„åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ï¼Œæ—¨åœ¨ç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½è¿‡ç¨‹ã€‚RePOé€šè¿‡ä¸¤ä¸ªåˆ›æ–°ç‚¹æ¶ˆé™¤äº†è¶…å‚æ•°betaï¼Œé¦–å…ˆä¿ç•™äº†SimPOçš„æ— å‚è€ƒè¾¹é™…ï¼Œä½†é€šè¿‡æ¢¯åº¦åˆ†æå»é™¤äº†betaï¼Œå…¶æ¬¡é‡‡ç”¨åŸºäºReLUçš„æœ€å¤§è¾¹é™…æŸå¤±ï¼Œè‡ªç„¶è¿‡æ»¤æ‰æ— å…³çš„é…å¯¹ã€‚ç†è®ºä¸Šï¼ŒRePOè¢«æè¿°ä¸ºSimPOçš„æé™æƒ…å†µï¼Œå½“betaè¶‹å‘äºæ— ç©·å¤§æ—¶ï¼Œé€»è¾‘åŠ æƒæ”¶æ•›ä¸ºäºŒå…ƒé˜ˆå€¼ï¼Œå½¢æˆ0-1æŸå¤±çš„å‡¸åŒ…ã€‚å®éªŒè¯æ˜ï¼ŒRePOåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šä¼˜äºDPOå’ŒSimPOï¼Œä»…éœ€è°ƒæ•´ä¸€ä¸ªè¶…å‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05283', 'title': "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", 'url': 'https://huggingface.co/papers/2503.05283', 'abstract': 'Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional <PRE_TAG>subspaces</POST_TAG> the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.', 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '85fcbc1ddbe7dbea', 'authors': ['Souhail Hadgi', 'Luca Moschella', 'Andrea Santilli', 'Diego Gomez', 'Qixing Huang', 'Emanuele RodolÃ ', 'Simone Melzi', 'Maks Ovsjanikov'], 'affiliations': ['Ecole polytechnique', 'Sapienza University of Rome', 'The University of Texas at Austin', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2503.05283.jpg', 'data': {'categories': ['#alignment', '#3d', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞĞ´Ğ½Ğ°ĞºĞ¾, Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing 3D and Text Feature Alignment through Subspace Projection', 'desc': 'This paper explores the alignment of features from uni-modal 3D encoders with text-based representations. It reveals that simply aligning these features after training does not yield good results. Instead, the authors find that projecting these features into carefully selected lower-dimensional subspaces significantly improves alignment quality. This work establishes a baseline for understanding how 3D data relates to text features, highlighting both their similarities and differences.'}, 'zh': {'title': 'æ¢ç´¢3Dç¼–ç å™¨ä¸æ–‡æœ¬ç‰¹å¾çš„å¯¹é½ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†3Dç¼–ç å™¨åœ¨ä¸å…¶ä»–æ¨¡æ€çš„å…³ç³»ä¸­çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯ä¸æ–‡æœ¬ç‰¹å¾ç©ºé—´çš„å¯¹æ¯”ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•çš„åæœŸè®­ç»ƒç‰¹å¾å¯¹é½æ–¹æ³•åœ¨å•æ¨¡æ€æ–‡æœ¬å’Œ3Dç¼–ç å™¨ä¹‹é—´çš„æ€§èƒ½æœ‰é™ã€‚é€šè¿‡æå–ç‰¹å¾ç©ºé—´çš„å­ç©ºé—´ï¼Œå¹¶å°†å­¦ä¹ åˆ°çš„è¡¨ç¤ºæŠ•å½±åˆ°ç²¾å¿ƒé€‰æ‹©çš„ä½ç»´å­ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†å¯¹é½è´¨é‡ï¼Œä»è€Œåœ¨åŒ¹é…å’Œæ£€ç´¢ä»»åŠ¡ä¸­æå‡äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡ä¸º3Då•æ¨¡æ€å’Œæ–‡æœ¬ç‰¹å¾ç©ºé—´çš„åæœŸè®­ç»ƒå¯¹é½å»ºç«‹äº†åŸºçº¿ï¼Œå¹¶çªå‡ºäº†3Dæ•°æ®ä¸å…¶ä»–è¡¨ç¤ºä¹‹é—´çš„å…±äº«å’Œç‹¬ç‰¹ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04973', 'title': 'Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning', 'url': 'https://huggingface.co/papers/2503.04973', 'abstract': 'Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'a3117b7e2b2c099c', 'authors': ['Giulio Corallo', 'Orion Weller', 'Fabio Petroni', 'Paolo Papotti'], 'affiliations': ['EURECOM', 'Johns Hopkins University', 'SAP Labs', 'Samaya AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.04973.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#long_context', '#rag', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) - ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº RAG, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ LongBench v2 Ğ¾Ğ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 7 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ RAG Ğ¿Ñ€Ğ¸ 30-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Knowledge Integration for Enhanced Language Model Performance', 'desc': 'This paper presents a new method for enhancing large language models (LLMs) by incorporating external knowledge more effectively. The proposed task-aware key-value (KV) cache compression allows LLMs to efficiently reason over a compact representation of relevant information, improving performance in zero- or few-shot scenarios. Compared to existing methods like Retrieval-Augmented Generation (RAG), this approach achieves better accuracy and significantly reduces inference time. Experiments demonstrate that while RAG excels with sparse evidence, the new compression technique is more effective for tasks requiring extensive knowledge.'}, 'zh': {'title': 'ä»»åŠ¡æ„ŸçŸ¥å‹ç¼©ï¼Œæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å‹ç¼©æ¥æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸‹å‹ç¼©å¤–éƒ¨çŸ¥è¯†ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†ç›¸å…³ä¿¡æ¯æ—¶æ›´åŠ é«˜æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ¨ç†å»¶è¿Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä»»åŠ¡æ— å…³çš„å‹ç¼©æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨LongBench v2æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§æé«˜äº†7ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿä»0.43ç§’å‡å°‘åˆ°0.16ç§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07413', 'title': 'REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding', 'url': 'https://huggingface.co/papers/2503.07413', 'abstract': 'Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM.', 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '52ecfef5a3f1d715', 'authors': ['Yan Tai', 'Luhao Zhu', 'Zhiqiang Chen', 'Ynan Ding', 'Yiying Dong', 'Xiaohong Liu', 'Guodong Guo'], 'affiliations': ['Hong Kong Polytechnic University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07413.jpg', 'data': {'categories': ['#interpretability', '#cv', '#dataset', '#benchmark', '#multimodal', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'REF-VLM: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ REF-VLM, Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Triplet-Based Referring Paradigm (TRP) Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VTInstruct, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ 25 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ REF-VLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'REF-VLM: Unifying Visual Decoding for Enhanced Multimodal Learning', 'desc': 'This paper introduces REF-VLM, a new framework designed to improve the performance of Multimodal Large Language Models (MLLMs) on complex visual tasks like semantic segmentation and keypoint detection. It addresses the limitations of current MLLMs by using a Triplet-Based Referring Paradigm (TRP) that separates concepts, decoding types, and targets for better structured learning. The authors also present a large-scale dataset, VTInstruct, which includes multimodal dialogue samples and various visual prompts to enhance training. Experimental results show that REF-VLM significantly outperforms existing MLLMs on standard benchmarks, demonstrating its effectiveness in handling diverse visual decoding tasks.'}, 'zh': {'title': 'REF-VLMï¼šç»Ÿä¸€è§†è§‰è§£ç ä»»åŠ¡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒåï¼Œå±•ç°å‡ºåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„å¼ºå¤§é›¶-shotèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ï¼Œä»…ç”¨æ–‡æœ¬è¾“å‡ºè¡¨ç¤ºæ—¶ï¼ŒMLLMsé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å¤æ‚çš„è§†è§‰è§£ç åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†REF-VLMï¼Œä¸€ä¸ªç”¨äºç»Ÿä¸€è®­ç»ƒå„ç§è§†è§‰è§£ç ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†åŸºäºä¸‰å…ƒç»„çš„å¼•ç”¨èŒƒå¼ï¼ˆTRPï¼‰ï¼Œä»¥æ˜ç¡®è§£è€¦è§†è§‰è§£ç ä»»åŠ¡ä¸­çš„æ¦‚å¿µã€è§£ç ç±»å‹å’Œç›®æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒREF-VLMåœ¨å¤šé¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–MLLMsï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šç²’åº¦åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05641', 'title': 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning', 'url': 'https://huggingface.co/papers/2503.05641', 'abstract': "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'cd41a7296c5c9225', 'authors': ['Justin Chih-Yao Chen', 'Sukwon Yun', 'Elias Stengel-Eskin', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05641.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#agents', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Symbolic-MoE: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Symbolic-MoE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Symbolic-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ Ğ±ĞµĞ·Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Symbolic-MoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Adaptive Expert Selection for Enhanced Reasoning in LLMs', 'desc': 'The paper introduces Symbolic-MoE, a novel Mixture-of-Experts framework designed to enhance the performance of large language models (LLMs) by enabling instance-level expert selection. This approach allows for the dynamic recruitment of LLMs based on specific skills required for diverse reasoning tasks, such as algebra or molecular biology. By synthesizing outputs from multiple experts, Symbolic-MoE generates high-quality responses while addressing computational efficiency through a batch inference strategy. The results show significant performance improvements over existing models and methods, demonstrating the effectiveness of fine-grained expert selection in machine learning tasks.'}, 'zh': {'title': 'å®ä¾‹çº§ä¸“å®¶é€‰æ‹©ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSymbolic-MoEçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å®ä¾‹çº§çš„ä¸“å®¶é€‰æ‹©æ¥æé«˜é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç¬¦å·åŒ–ã€åŸºäºæ–‡æœ¬ä¸”æ— æ¢¯åº¦çš„æ··åˆä¸“å®¶æ–¹æ³•ï¼Œå¼ºè°ƒæ ¹æ®ä»»åŠ¡çš„ä¸åŒéœ€æ±‚é€‰æ‹©åˆé€‚çš„ä¸“å®¶ã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶ï¼ŒSymbolic-MoEèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSymbolic-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼ºå¤§LLMå’Œå¤šä»£ç†æ–¹æ³•ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03511', 'title': 'NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection', 'url': 'https://huggingface.co/papers/2503.03511', 'abstract': 'Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.', 'score': 0, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'ba660b9e0f676df1', 'authors': ['Qingyu Fan', 'Yinghao Cai', 'Chao Li', 'Wenzhe He', 'Xudong Zheng', 'Tao Lu', 'Bin Liang', 'Shuo Wang'], 'affiliations': ['Qiyuan Lab', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.03511.jpg', 'data': {'categories': ['#robotics', '#agents', '#architecture', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'NeuGrasp: Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'NeuGrasp - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. NeuGrasp Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeuGrasp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'NeuGrasp: Mastering Grasping with Transparent and Specular Objects', 'desc': 'This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments.'}, 'zh': {'title': 'NeuGraspï¼šé€æ˜ç‰©ä½“æŠ“å–çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNeuGraspçš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é€æ˜å’Œé•œé¢ç‰©ä½“æŠ“å–ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èƒŒæ™¯å…ˆéªŒè¿›è¡Œææ–™æ— å…³çš„æŠ“å–æ£€æµ‹ï¼Œç»“åˆäº†å˜æ¢å™¨å’Œå…¨å±€å…ˆéªŒä½“ç§¯ï¼Œä»¥èšåˆå¤šè§†è§’ç‰¹å¾å¹¶è¿›è¡Œç©ºé—´ç¼–ç ã€‚NeuGraspé€šè¿‡æ®‹å·®ç‰¹å¾å¢å¼ºèšç„¦äºå‰æ™¯ç‰©ä½“ï¼Œå¹¶åˆ©ç”¨å ç”¨å…ˆéªŒä½“ç§¯æ¥æ”¹å–„ç©ºé—´æ„ŸçŸ¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é€æ˜å’Œé•œé¢è¡¨é¢çš„ç‰©ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuGraspåœ¨æŠ“å–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„é‡å»ºè´¨é‡ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (7)', '#agi (1)', '#alignment (5)', '#architecture (11)', '#audio (2)', '#benchmark (20)', '#cv (11)', '#data (4)', '#dataset (9)', '#diffusion (6)', '#ethics (2)', '#games (3)', '#graphs', '#hallucinations (3)', '#healthcare (1)', '#inference (4)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (22)', '#open_source (11)', '#optimization (19)', '#plp (1)', '#rag (4)', '#reasoning (11)', '#rl (7)', '#rlhf (3)', '#robotics (3)', '#science', '#security (4)', '#small_models', '#story_generation (2)', '#survey (2)', '#synthetic (1)', '#training (20)', '#transfer_learning (5)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-11 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-11 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-11 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    