{
    "date": {
        "ru": "4 –æ–∫—Ç—è–±—Ä—è",
        "en": "October 4",
        "zh": "10Êúà4Êó•"
    },
    "time_utc": "2024-10-04 09:00",
    "weekday": 4,
    "issue_id": 10,
    "home_page_url": "https://huggingface.co/papers?date=2024-10-04",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.02740",
            "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
            "url": "https://huggingface.co/papers/2410.02740",
            "abstract": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models.",
            "score": 52,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "81cb4e856bb4642c",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–±–µ–∂–¥–∞–µ—Ç",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏ –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ AltText, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º –ø–æ–¥–ø–∏—Å–µ–π, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Optimizing Multimodal Models with Hybrid Captioning Strategies",
                    "desc": "This paper discusses the importance of using rewritten captions to enhance the performance of multimodal models, which combine text and images. It highlights the challenges of understanding how synthetic captions interact with original AltTexts during pre-training. The authors introduce a new captioning pipeline that generates various caption formats tailored to different models, such as CLIP and multimodal LLMs. Their research shows that using both synthetic captions and AltTexts together leads to better model performance and alignment than using synthetic captions alone."
                },
                "zh": {
                    "title": "‰ºòÂåñÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊ†áÈ¢òÁîüÊàêÁ≠ñÁï•",
                    "desc": "ÊúÄËøëÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑËøõÂ±ïÊòæÁ§∫ÔºåÈáçÂÜôÁöÑÊ†áÈ¢òÂØπÊèêÈ´òÊÄßËÉΩÊúâÈáçË¶Å‰ª∑ÂÄºÔºå‰ΩÜ‰ªçÈù¢‰∏¥‰∏Ä‰∫õÊåëÊàò„ÄÇ‰æãÂ¶ÇÔºåËôΩÁÑ∂ÂêàÊàêÊ†áÈ¢òÈÄöÂ∏∏Êèê‰æõÊõ¥Â•ΩÁöÑË¥®ÈáèÂíåÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÔºå‰ΩÜÂ∞ö‰∏çÊ∏ÖÊ•öÂÆÉ‰ª¨ÊòØÂê¶ÂèØ‰ª•ÂÆåÂÖ®Êõø‰ª£AltTexts„ÄÇ‰∏çÂêåÁöÑÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂèØËÉΩÂØπÁâπÂÆöÁöÑÊ†áÈ¢òÊ†ºÂºèÊúâÁã¨ÁâπÁöÑÂÅèÂ•ΩÔºå‰ΩÜËØÜÂà´ÊØè‰∏™Ê®°ÂûãÁöÑÊúÄ‰Ω≥Ê†áÈ¢òÁöÑÂä™Âäõ‰ªçÁÑ∂ÊúâÈôê„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñ„ÄÅÂèØÊéß‰∏îÂèØÊâ©Â±ïÁöÑÊ†áÈ¢òÁîüÊàêÁÆ°ÈÅìÔºåÊó®Âú®‰∏∫ÂêÑÁßçÂ§öÊ®°ÊÄÅÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊ†áÈ¢òÊ†ºÂºè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02367",
            "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
            "url": "https://huggingface.co/papers/2410.02367",
            "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation.",
            "score": 45,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "9c0f4a698ce3fbd7",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "SageAttention: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ SageAttention –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention2 –∏ xformers, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Revolutionizing Attention: SageAttention for Efficient Quantization",
                    "desc": "This paper addresses the computational challenges of the attention mechanism in transformer models, which has a complexity of O(N^2). It highlights that while quantization techniques have been effective for linear layers, they have not been adequately applied to attention mechanisms. The authors introduce SageAttention, a novel quantization method specifically designed for attention, which significantly improves operational efficiency. Experimental results demonstrate that SageAttention not only accelerates processing speed but also maintains high accuracy across various applications, including language processing and image generation."
                },
                "zh": {
                    "title": "È´òÊïàÈáèÂåñÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑSageAttention",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂèòÊç¢Âô®Êû∂ÊûÑ‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊåáÂá∫ÂÖ∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰∏∫O(N^2)ÔºåÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂Êàê‰∏∫‰∏ªË¶ÅÁöÑÊó∂Èó¥Ê∂àËÄóÈÉ®ÂàÜ„ÄÇÂ∞ΩÁÆ°ÈáèÂåñÊäÄÊúØÂú®Âä†ÈÄüÊ®°ÂûãÊé®ÁêÜÊñπÈù¢ÊúâÊïàÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ºòÂåñÁ∫øÊÄßÂ±Ç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáËØ¶ÁªÜÂàÜÊûê‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÈáèÂåñÁöÑÂèØË°åÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïà‰∏îÂáÜÁ°ÆÁöÑÈáèÂåñÊñπÊ≥ïSageAttention„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSageAttentionÂú®Êìç‰ΩúÊØèÁßíÔºàOPSÔºâÊñπÈù¢‰ºò‰∫éFlashAttention2ÂíåxformersÔºå‰∏îÂú®Â§öÁßçÊ®°Âûã‰∏äÂá†‰πéÊ≤°ÊúâÊçüÂ§±Á´ØÂà∞Á´ØÊåáÊ†á„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02073",
            "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
            "url": "https://huggingface.co/papers/2410.02073",
            "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
            "score": 40,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "b8db8c8f9dcb5574",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã: –±—ã—Å—Ç—Ä–æ, —Ç–æ—á–Ω–æ –∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è zero-shot –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã –ø–æ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ú–æ–¥–µ–ª—å Depth Pro —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ, —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º, –∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–º–µ—Ä—ã. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ, —Å–æ–∑–¥–∞–≤–∞—è –∫–∞—Ä—Ç—É –≥–ª—É–±–∏–Ω—ã —Ä–∞–∑–º–µ—Ä–æ–º 2.25 –º–µ–≥–∞–ø–∏–∫—Å–µ–ª—è –∑–∞ 0.3 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º GPU."
                },
                "en": {
                    "title": "Depth Pro: Fast and Accurate Depth Estimation Without Metadata",
                    "desc": "This paper introduces Depth Pro, a foundation model designed for zero-shot metric monocular depth estimation. It generates high-resolution depth maps that are sharp and detailed, without needing camera metadata for scale. The model operates quickly, producing a 2.25-megapixel depth map in just 0.3 seconds on a standard GPU. Key innovations include a multi-scale vision transformer for dense predictions and a training method that effectively combines real and synthetic data to enhance accuracy and boundary detail."
                },
                "zh": {
                    "title": "Depth ProÔºöÂø´ÈÄüÈ´òÊïàÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°Âûã",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éÈõ∂-shotÂ∫¶ÈáèÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåÁß∞‰∏∫Depth Pro„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÂêàÊàêÈ´òÂàÜËæ®ÁéáÁöÑÊ∑±Â∫¶ÂõæÔºåÂÖ∑ÊúâÊó†‰∏é‰º¶ÊØîÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÈ´òÈ¢ëÁªÜËäÇ„ÄÇÂÆÉÁöÑÈ¢ÑÊµãÊòØÂ∫¶ÈáèÁöÑÔºåÂÖ∑ÊúâÁªùÂØπÂ∞∫Â∫¶Ôºå‰∏ç‰æùËµñ‰∫éÁõ∏Êú∫ÂÜÖÂèÇÁ≠âÂÖÉÊï∞ÊçÆ„ÄÇDepth ProÂú®Ê†áÂáÜGPU‰∏ä‰ª•0.3ÁßíÁöÑÈÄüÂ∫¶ÁîüÊàê2.25Áôæ‰∏áÂÉèÁ¥†ÁöÑÊ∑±Â∫¶ÂõæÔºåÂ±ïÁé∞‰∫ÜÂÖ∂È´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02757",
            "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
            "url": "https://huggingface.co/papers/2410.02757",
            "abstract": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video.",
            "score": 36,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "8003554c44032bf4",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "Loong: –ø—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ Loong, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ –∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫—É –≤–∏–¥–µ–æ—Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å Loong –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ 10-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –≤–∏–¥–µ–æ, –Ω–æ —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ —Ä–æ–ª–∏–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±–ª–∞—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LLM."
                },
                "en": {
                    "title": "Unlocking Long Video Generation with Loong!",
                    "desc": "This paper addresses the challenge of generating long videos using autoregressive large language models (LLMs), which have been successful in natural language processing but struggle with video generation. The authors introduce Loong, a novel video generator that treats text and video tokens as a unified sequence, allowing for the creation of minute-long videos. They implement a progressive training approach that gradually increases video length while addressing loss imbalance, and explore various inference strategies to reduce errors. The results show that Loong can effectively generate longer videos based on text prompts, marking a significant advancement in video generation technology."
                },
                "zh": {
                    "title": "ÁîüÊàêÂàÜÈíüÁ∫ßÈïøËßÜÈ¢ëÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Ëá™ÂõûÂΩíÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁîüÊàêÈïøËßÜÈ¢ëÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ°LLMsÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂú®ËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÔºåÁé∞ÊúâÊñπÊ≥ï‰ªÖËÉΩÁîüÊàêÂá†ÁßíÈíüÁöÑÁü≠ËßÜÈ¢ë„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËá™ÂõûÂΩíLLMËßÜÈ¢ëÁîüÊàêÂô®LoongÔºåËÉΩÂ§üÁîüÊàêÂàÜÈíüÁ∫ßÈïøËßÜÈ¢ë„ÄÇÈÄöËøáÂ∞ÜÊñáÊú¨ÂíåËßÜÈ¢ëÊ†áËÆ∞Âª∫Ê®°‰∏∫Áªü‰∏ÄÂ∫èÂàóÔºåÂπ∂ÈááÁî®ÈÄêÊ≠•Áü≠Âà∞ÈïøÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊàë‰ª¨ÊúâÊïàËß£ÂÜ≥‰∫ÜÈïøËßÜÈ¢ëËÆ≠ÁªÉ‰∏≠ÁöÑÊçüÂ§±‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02713",
            "title": "Video Instruction Tuning With Synthetic Data",
            "url": "https://huggingface.co/papers/2410.02713",
            "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.",
            "score": 36,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "f4ccb1c1c9671dde",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–¥–µ–æ-–ò–ò",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑–∞–≤—à–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø–ª–∞–Ω–∏—Ä—É—é—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Ü–µ—Å—Å –µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Synthetic Data for Superior Video Understanding",
                    "desc": "This paper presents a solution to the challenge of gathering high-quality data for training video large multimodal models (LMMs). The authors introduce a synthetic dataset called LLaVA-Video-178K, designed for video instruction-following tasks, which includes detailed captioning and question-answering. By utilizing this dataset alongside existing visual instruction tuning data, they develop a new video LMM named LLaVA-Video. Their experiments show that LLaVA-Video performs well on various video benchmarks, demonstrating the dataset's effectiveness in enhancing model training."
                },
                "zh": {
                    "title": "ÂêàÊàêÊï∞ÊçÆÈõÜÂä©ÂäõËßÜÈ¢ëÂ§öÊ®°ÊÄÅÊ®°ÂûãÂèëÂ±ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫ÜËßÜÈ¢ëÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Ëé∑ÂèñÈ´òË¥®ÈáèÂéüÂßãÊï∞ÊçÆÊó∂ÁöÑÂõ∞Èöæ„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËßÜÈ¢ëÊåá‰ª§Ë∑üÈöèÁöÑÈ´òË¥®ÈáèÂêàÊàêÊï∞ÊçÆÈõÜÔºåÁß∞‰∏∫LLaVA-Video-178K„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ËØ¶ÁªÜÁöÑÂ≠óÂπï„ÄÅÂºÄÊîæÂºèÈóÆÁ≠îÂíåÂ§öÈ°πÈÄâÊã©ÈóÆÁ≠îÁ≠âÂÖ≥ÈîÆ‰ªªÂä°„ÄÇÈÄöËøáÂú®Ëøô‰∏™Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂπ∂ÁªìÂêàÁé∞ÊúâÁöÑËßÜËßâÊåá‰ª§Ë∞É‰ºòÊï∞ÊçÆÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜÊñ∞ÁöÑËßÜÈ¢ëLMM‚Äî‚ÄîLLaVA-VideoÔºåÂπ∂Âú®Â§ö‰∏™ËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02712",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "url": "https://huggingface.co/papers/2410.02712",
            "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (2) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.",
            "score": 34,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "529e51b7f382eb97",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#interpretability",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#rlhf",
                    "#multimodal"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "LLaVA-Critic: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∏ –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLaVA-Critic - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –±–æ–ª—å—à—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (LMM) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏. LLaVA-Critic –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —Ä–æ–ª–∏ —Å—É–¥—å–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è LMM."
                },
                "en": {
                    "title": "LLaVA-Critic: The Future of Multimodal Evaluation",
                    "desc": "LLaVA-Critic is a groundbreaking open-source large multimodal model (LMM) that serves as a generalist evaluator for various multimodal tasks. It is trained on a comprehensive dataset that includes diverse evaluation criteria, allowing it to assess performance effectively. The model excels in two main areas: providing reliable evaluation scores comparable to advanced GPT models and generating reward signals for preference learning to improve model alignment. This research highlights the promise of open-source LMMs in self-evaluation and sets a foundation for future advancements in alignment feedback mechanisms."
                },
                "zh": {
                    "title": "LLaVA-CriticÔºöÂºÄÊ∫êÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑËØÑ‰º∞Êñ∞Á∫™ÂÖÉ",
                    "desc": "LLaVA-CriticÊòØÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊó®Âú®‰Ωú‰∏∫ÈÄöÁî®ËØÑ‰º∞ËÄÖÔºåËØÑ‰º∞Â§öÁßçÂ§öÊ®°ÊÄÅ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇËØ•Ê®°ÂûãÈÄöËøáÈ´òË¥®ÈáèÁöÑÊâπËØÑÊåá‰ª§Ë∑üÈöèÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊ∂µÁõñ‰∫ÜÂ§öÊ†∑ÁöÑËØÑ‰º∞Ê†áÂáÜÂíåÂú∫ÊôØ„ÄÇÂÆûÈ™åË°®ÊòéÔºåLLaVA-CriticÂú®‰∏§‰∏™ÂÖ≥ÈîÆÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤Ôºö‰Ωú‰∏∫ËØÑÂà§ËÄÖÊèê‰æõÂèØÈù†ÁöÑËØÑ‰º∞ÂàÜÊï∞ÔºåÂπ∂Âú®ÂÅèÂ•ΩÂ≠¶‰π†‰∏≠ÁîüÊàêÂ•ñÂä±‰ø°Âè∑ÔºåÂ¢ûÂº∫Ê®°ÂûãÁöÑÂØπÈΩêËÉΩÂäõ„ÄÇÊ≠§È°πÂ∑•‰ΩúÂ±ïÁ§∫‰∫ÜÂºÄÊ∫êÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Ëá™ÊàëÊâπËØÑÂíåËØÑ‰º∞‰∏≠ÁöÑÊΩúÂäõÔºå‰∏∫Êú™Êù•ÂèØÊâ©Â±ïÁöÑË∂Ö‰∫∫Á±ªÂØπÈΩêÂèçÈ¶àÊú∫Âà∂Á†îÁ©∂Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02724",
            "title": "Large Language Models as Markov Chains",
            "url": "https://huggingface.co/papers/2410.02724",
            "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.",
            "score": 31,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "faad779778001c6f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞ –Ω–∞ –∫–æ–Ω–µ—á–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –û–Ω–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –≤–ª–∏—è–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ —ç—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –æ–±–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –ø–æ–¥–∫—Ä–µ–ø–ª—è—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM."
                },
                "en": {
                    "title": "Unraveling the Power of Large Language Models through Markov Chains",
                    "desc": "This paper explores the theoretical foundations of large language models (LLMs) by establishing a connection between autoregressive language models and Markov chains. It reveals that LLMs can be understood through the lens of Markov chains with a specific state space, which helps explain their performance and convergence properties. The authors derive important results regarding the stationary distribution of these chains and how temperature affects convergence speed. Additionally, they provide generalization bounds and validate their theoretical insights through experiments on contemporary LLMs, demonstrating the practical implications of their findings."
                },
                "zh": {
                    "title": "Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁêÜËÆ∫Âü∫Á°Ä",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÂèäÂÖ∂ÁêÜËÆ∫Âü∫Á°Ä„ÄÇÊàë‰ª¨Â∞ÜÈÄöÁî®Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°Âûã‰∏éÊúâÈôêÁä∂ÊÄÅÁ©∫Èó¥‰∏äÁöÑÈ©¨Â∞îÂèØÂ§´ÈìæÂª∫Á´ã‰∫ÜÁ≠â‰ª∑ÂÖ≥Á≥ª„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÈ©¨Â∞îÂèØÂ§´ÈìæÁöÑÂπ≥Á®≥ÂàÜÂ∏ÉËÉΩÂ§üÊçïÊçâLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂ÂàÜÊûê‰∫ÜÊ∏©Â∫¶ÂØπÊî∂ÊïõÈÄüÂ∫¶ÁöÑÂΩ±Âìç„ÄÇÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÁêÜËÆ∫‰øùËØÅÔºåÂ±ïÁ§∫‰∫ÜLLMsÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑË°å‰∏∫„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02746",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "url": "https://huggingface.co/papers/2410.02746",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.",
            "score": 31,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "cc226eaa6867dda5",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#transfer_learning",
                    "#alignment",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "CLOC: –£–ª—É—á—à–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ —Ä–µ–≥–∏–æ–Ω-—Ç–µ–∫—Å—Ç. CLOC –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å CLIP –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö."
                },
                "en": {
                    "title": "Enhancing Image Understanding with CLOC for Multimodal Models",
                    "desc": "This paper introduces Contrastive Localized Language-Image Pre-training (CLOC), an enhancement of the CLIP model that improves its ability to understand images at a finer level. CLOC incorporates region-text contrastive loss and new modules to better align image regions with corresponding text descriptions. The authors propose a novel concept called promptable embeddings, which allows the model to easily convert image embeddings into detailed region representations using spatial hints. By leveraging a large-scale, visually-enriched captioning framework, CLOC generates high-quality regional embeddings, making it suitable for tasks that require precise image localization and retrieval in multimodal large language models."
                },
                "zh": {
                    "title": "ÊèêÂçáCLIPÁöÑÂå∫ÂüüÁêÜËß£ËÉΩÂäõ",
                    "desc": "ÂØπÊØîËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºàCLIPÔºâÊòØ‰∏ÄÁßçÁî®‰∫éËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÁîüÊàêÂõæÂÉèÂíåÊñáÊú¨ÁöÑË°®Á§∫ÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÂØπÊØîÂ±ÄÈÉ®ÂåñËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºàCLOCÔºâÔºåÈÄöËøáÂå∫Âüü-ÊñáÊú¨ÂØπÊØîÊçüÂ§±Â¢ûÂº∫CLIPÁöÑÂÆö‰ΩçËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂèØÊèêÁ§∫ÂµåÂÖ•ÁöÑÊ¶ÇÂøµÔºå‰ΩøÂæóÁºñÁ†ÅÂô®ËÉΩÂ§üÊ†πÊçÆÁ©∫Èó¥ÊèêÁ§∫ËΩªÊùæËΩ¨Êç¢‰∏∫Âå∫ÂüüË°®Á§∫„ÄÇCLOCÈÄöËøáÁîüÊàêÂ§ßËßÑÊ®°ÁöÑÂå∫Âüü-ÊñáÊú¨‰º™Ê†áÁ≠æÔºåÊèêÂçá‰∫ÜÂõæÂÉèÂå∫ÂüüËØÜÂà´ÂíåÊ£ÄÁ¥¢‰ªªÂä°ÁöÑË¥®ÈáèÔºåËÉΩÂ§üÊúâÊïàÊõø‰ª£CLIPÔºåÁâπÂà´ÊòØÂú®ÂºïÁî®ÂíåÂÆö‰Ωç‰ªªÂä°‰∏≠„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02416",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
            "url": "https://huggingface.co/papers/2410.02416",
            "abstract": "Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.",
            "score": 25,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "d40106a5a7b0cb85",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–µ—Ä–µ—Å—ã—â–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG —Å–Ω–∏–∂–∞–µ—Ç –≤–µ—Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –º–∞—Å—à—Ç–∞–±—ã —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –ø–µ—Ä–µ—Å—ã—â–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ª–µ–≥–∫–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π FID –∏ recall –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Adaptive Projected Guidance",
                    "desc": "This paper focuses on improving classifier-free guidance (CFG) in diffusion models, which is essential for generating high-quality outputs that align well with input conditions. The authors identify that while a high guidance scale enhances generation quality, it can also lead to oversaturation and unrealistic artifacts. They propose a new method called adaptive projected guidance (APG) that modifies the CFG update rule by down-weighting the oversaturating component, allowing for better image quality without the negative effects of high guidance scales. Extensive experiments show that APG outperforms standard CFG in various metrics, making it a practical and efficient alternative for enhancing diffusion model outputs."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÊäïÂΩ±ÂºïÂØºÔºöÊèêÂçáÁîüÊàêË¥®ÈáèÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÔºàCFGÔºâÂú®Êâ©Êï£Ê®°Âûã‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÁâπÂà´ÊòØÂú®ÁîüÊàêË¥®ÈáèÂíåËæìÂÖ•Êù°‰ª∂‰∏éÊúÄÁªàËæìÂá∫‰πãÈó¥ÁöÑÂØπÈΩêÊñπÈù¢„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåCFGÁöÑÊõ¥Êñ∞ËßÑÂàôÂèØ‰ª•ÂàÜËß£‰∏∫Âπ≥Ë°åÂíåÊ≠£‰∫§‰∏§‰∏™ÈÉ®ÂàÜÔºåÂÖ∂‰∏≠Âπ≥Ë°åÈÉ®ÂàÜ‰ºöÂØºËá¥ËøáÈ•±ÂíåÁé∞Ë±°ÔºåËÄåÊ≠£‰∫§ÈÉ®ÂàÜÂàôÊèêÂçáÂõæÂÉèË¥®Èáè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËøáÈ•±ÂíåÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËá™ÈÄÇÂ∫îÊäïÂΩ±ÂºïÂØºÔºàAPGÔºâÊñπÊ≥ïÔºåÈÄöËøáÈôç‰ΩéÂπ≥Ë°åÈÉ®ÂàÜÁöÑÊùÉÈáçÊù•ÂÆûÁé∞È´òË¥®ÈáèÁîüÊàêÔºåÂêåÊó∂ÂÖÅËÆ∏‰ΩøÁî®Êõ¥È´òÁöÑÂºïÂØºÊØî‰æã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPGÂú®Â§öÁßçÊù°‰ª∂Êâ©Êï£Ê®°ÂûãÂíåÈááÊ†∑Âô®‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèêÂçá‰∫ÜFID„ÄÅÂè¨ÂõûÁéáÂíåÈ•±ÂíåÂ∫¶ÂàÜÊï∞ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éCFGÁõ∏ÂΩìÁöÑÁ≤æÂ∫¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01679",
            "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
            "url": "https://huggingface.co/papers/2410.01679",
            "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.",
            "score": 22,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "70e8bd770c7d370b",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "VinePPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –º–µ—Ç–æ–¥–µ Proximal Policy Optimization (PPO) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∑–∞–¥–∞—á–∞–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ VinePPO, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –≤–º–µ—Å—Ç–æ —Å–µ—Ç–µ–π –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. VinePPO –ø–æ–∫–∞–∑–∞–ª –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö MATH –∏ GSM8K, —Ç—Ä–µ–±—É—è –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤."
                },
                "en": {
                    "title": "VinePPO: A Better Way to Assign Credit in Complex Reasoning Tasks",
                    "desc": "This paper discusses the challenges of credit assignment in reinforcement learning (RL) for large language models (LLMs) when performing complex reasoning tasks. It highlights the limitations of using value networks in the Proximal Policy Optimization (PPO) algorithm, which often leads to inaccurate predictions and poor performance. The authors introduce VinePPO, a new method that utilizes unbiased Monte Carlo estimates to improve credit assignment without relying on large value networks. Their experiments show that VinePPO significantly outperforms PPO and other baselines, achieving better results with fewer updates and less computation time."
                },
                "zh": {
                    "title": "VinePPOÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë∂äÊù•Ë∂äÂ§öÂú∞Ë¢´Â∫îÁî®ÔºåËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÂú®Ëé∑ÂæóÂ•ñÂä±‰πãÂâçÊâßË°åÂ§ö‰∏™Â§çÊùÇÊ≠•È™§„ÄÇÊ≠£Á°ÆÂú∞ÂàÜÈÖçËøô‰∫õÊ≠•È™§ÁöÑ‰ø°Áî®ÂØπ‰∫éÊèêÈ´òÊ®°ÂûãÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïVinePPOÔºåÂÆÉÂà©Áî®ËØ≠Ë®ÄÁéØÂ¢ÉÁöÑÁÅµÊ¥ªÊÄßÊù•ËÆ°ÁÆóÊó†ÂÅèÁöÑËíôÁâπÂç°Ê¥õ‰º∞ËÆ°ÔºåÈÅøÂÖç‰∫ÜÂ§ßÂûã‰ª∑ÂÄºÁΩëÁªúÁöÑÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåVinePPOÂú®MATHÂíåGSM8KÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑPPOÁÆóÊ≥ïÔºå‰∏îÊâÄÈúÄÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞Ê¨°Êï∞ÂíåÊó∂Èó¥ÊòæËëóÂáèÂ∞ë„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02678",
            "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
            "url": "https://huggingface.co/papers/2410.02678",
            "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute.",
            "score": 22,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "c0f8d0752bf51e22",
            "data": {
                "categories": [
                    "#audio",
                    "#multilingual",
                    "#training",
                    "#machine_translation",
                    "#transfer_learning",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Distilled Voice Assistant) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –∑–∞–¥–∞—á–∞—Ö —É—Å—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DiVA –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ."
                },
                "en": {
                    "title": "Revolutionizing Voice Assistants with Self-Supervised Learning",
                    "desc": "This paper introduces a new approach for training Speech Large Language Models (LLMs) that combines audio and text processing more effectively. Instead of relying on supervised finetuning with instruction data, the proposed method uses self-supervision by leveraging responses from a text-only LLM to improve speech understanding. The resulting model, called Distilled Voice Assistant (DiVA), demonstrates strong performance in tasks like Spoken Question Answering, Classification, and Translation. Notably, DiVA achieves a higher user satisfaction rate while requiring significantly less computational resources compared to existing models."
                },
                "zh": {
                    "title": "ÂàõÊñ∞ËØ≠Èü≥Âä©ÊâãÔºöËá™ÊàëÁõëÁù£ÁöÑËØ≠Èü≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉËØ≠Èü≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüËØ≠Èü≥Âä©ÊâãÂú®Èü≥È¢ëÂíåÊñáÊú¨Âª∫Ê®°‰∏≠‰ø°ÊÅØ‰∏¢Â§±ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®ÊñáÊú¨-only LLMÂØπËΩ¨ÂΩïÊñáÊú¨ÁöÑÂìçÂ∫î‰Ωú‰∏∫Ëá™ÊàëÁõëÁù£ÔºåËÄåÊó†ÈúÄ‰ΩøÁî®Ê†áÊ≥®Êï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑDistilled Voice AssistantÔºàDiVAÔºâÂú®Âè£ËØ≠ÈóÆÁ≠î„ÄÅÂàÜÁ±ªÂíåÁøªËØë‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÂú®Áî®Êà∑ÂÅèÂ•Ω‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåDiVAÂú®ËÆ≠ÁªÉËÆ°ÁÆóËµÑÊ∫ê‰∏äËäÇÁúÅ‰∫ÜË∂ÖËøá100ÂÄç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.19291",
            "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
            "url": "https://huggingface.co/papers/2409.19291",
            "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems.",
            "score": 18,
            "issue_id": 10,
            "pub_date": "2024-09-28",
            "pub_date_card": {
                "ru": "28 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 28",
                "zh": "9Êúà28Êó•"
            },
            "hash": "c79e62159ed005c9",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#benchmark",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ CLIP —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–ø–ª–µ—Ç–Ω–æ–≥–æ –∞–ø—Å–∞–π–∫–ª–∏–Ω–≥–∞",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ CLIP –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—É—Ç–µ–º —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–ª–æ—Ç–Ω–æ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ CLIP. –≠—Ç–∏ –º–æ–¥–µ–ª–∏ –∑–∞—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö."
                },
                "en": {
                    "title": "Enhancing CLIP with Diversified Multiplet Upcycling for Better Multimodal Learning",
                    "desc": "This paper introduces Diversified Multiplet Upcycling (DMU), a novel strategy to enhance the performance of Contrastive Language-Image Pre-training (CLIP) models. DMU fine-tunes multiple CLIP models that focus on different feature spaces while sharing most parameters, except for the Feed-Forward Network (FFN). This approach transforms these models into a CLIP-Mixture of Experts (MoE), which increases model capacity and improves performance on tasks like zero-shot retrieval and image classification. The method allows for easy integration of CLIP-MoEs into existing systems, paving the way for more efficient multimodal learning."
                },
                "zh": {
                    "title": "Â§öÊ†∑ÂåñÂ§öÈáçÂçáÁ∫ßÔºöÊèêÂçáCLIPÊ®°ÂûãÊÄßËÉΩÁöÑÊñ∞Á≠ñÁï•",
                    "desc": "ËøëÂπ¥Êù•ÔºåÂØπÊØîËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºàCLIPÔºâÂ∑≤Êàê‰∏∫Â§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÂü∫Áü≥„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂ÂèëÁé∞CLIPÁºñÁ†ÅËøáÁ®ã‰∏≠Â≠òÂú®ÊòæËëóÁöÑ‰ø°ÊÅØÊçüÂ§±Ôºå‰∏îCLIPÂæÄÂæÄÂè™ËÉΩÊçïÊçâËæìÂÖ•ÁöÑÁ≤óÁ≤íÂ∫¶ÁâπÂæÅ„ÄÇËøô‰∏ÄÁº∫Èô∑ÈôêÂà∂‰∫ÜÂçï‰∏ÄCLIPÊ®°ÂûãÂ§ÑÁêÜËßÜËßâÁªÜËäÇ‰∏∞ÂØåÁöÑÂõæÂÉèÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊ®°ÂûãÊó†ÂÖ≥Á≠ñÁï•‚Äî‚ÄîÂ§öÊ†∑ÂåñÂ§öÈáçÂçáÁ∫ßÔºàDMUÔºâÔºåÈÄöËøáÈ´òÊïàÂæÆË∞É‰∏ÄÁ≥ªÂàóÊçïÊçâ‰∏çÂêåÁâπÂæÅÁ©∫Èó¥ÁöÑCLIPÊ®°ÂûãÔºåÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02525",
            "title": "Contextual Document Embeddings",
            "url": "https://huggingface.co/papers/2410.02525",
            "abstract": "Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.",
            "score": 16,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "b45b19a592899862",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∏—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏."
                },
                "en": {
                    "title": "Context Matters: Enhancing Document Embeddings with Contextualization",
                    "desc": "This paper discusses the importance of dense document embeddings in neural retrieval systems. The authors argue that traditional embeddings do not consider the context provided by neighboring documents, which can limit their effectiveness. They propose two new methods for creating contextualized document embeddings that incorporate information from related documents. Their approach shows significant improvements in retrieval performance, especially in challenging scenarios, and sets new benchmarks without relying on complex training techniques."
                },
                "zh": {
                    "title": "‰∏ä‰∏ãÊñáÂåñÊñáÊ°£ÂµåÂÖ•ÔºåÊèêÂçáÊ£ÄÁ¥¢ÊÄßËÉΩÔºÅ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂØÜÈõÜÊñáÊ°£ÂµåÂÖ•Âú®Á•ûÁªèÊ£ÄÁ¥¢‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÁé∞ÊúâÁöÑÂµåÂÖ•ÊñπÊ≥ïËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜÂú®ÁâπÂÆöÊ£ÄÁ¥¢‰ªªÂä°‰∏≠Áº∫‰πè‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊñπÊ≥ïÊù•ÁîüÊàê‰∏ä‰∏ãÊñáÂåñÁöÑÊñáÊ°£ÂµåÂÖ•ÔºåÂàÜÂà´ÊòØÈÄöËøáÂØπÊØîÂ≠¶‰π†ÁõÆÊ†áÂíåÊñ∞ÁöÑ‰∏ä‰∏ãÊñáÂåñÊû∂ÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøô‰∏§ÁßçÊñπÊ≥ïÂú®Â§ö‰∏™ËÆæÁΩÆ‰∏≠Âùá‰ºò‰∫é‰º†ÁªüÁöÑÂèåÁºñÁ†ÅÂô®ÔºåÂ∞§ÂÖ∂Âú®ÂüüÂ§ñ‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥‰∏∫Á™ÅÂá∫„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02749",
            "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
            "url": "https://huggingface.co/papers/2410.02749",
            "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems \"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode.",
            "score": 12,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "07238a3d10e1aa04",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#plp",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "‚úèÔ∏è",
                "ru": {
                    "title": "LintSeq: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –ø—Ä–∞–≤–æ–∫",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏–Ω—Ç–µ—Ä –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–¥–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ LintSeq –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ä–∞–≤–Ω–∏–ª–∏ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –ø—Ä–∞–≤–æ–∫, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–∏."
                },
                "en": {
                    "title": "Enhancing Code Synthesis with Synthetic Edit Sequences",
                    "desc": "This paper introduces LintSeq, a synthetic data generation algorithm designed to create high-quality edit sequences for code synthesis. By utilizing a linter, LintSeq refactors existing code into a series of error-free edits, producing text strings that represent program diffs. The authors demonstrate that fine-tuning smaller language models on these synthetic edit sequences leads to improved performance in generating diverse programs compared to traditional training methods. Notably, their approach allows smaller models to achieve competitive results against larger models like GPT-4, showcasing the effectiveness of using synthetic edit data for code synthesis tasks."
                },
                "zh": {
                    "title": "Áî®LintSeqÂ°´Ë°•‰ª£Á†ÅÁºñËæëÊï∞ÊçÆÁöÑÁ©∫ÁôΩ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫LintSeqÁöÑÂêàÊàêÊï∞ÊçÆÁîüÊàêÁÆóÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥È´òË¥®Èáè‰ª£Á†ÅÁºñËæëÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøá‰ΩøÁî®linterÂØπÁé∞Êúâ‰ª£Á†ÅËøõË°åÈáçÊûÑÔºåÁîüÊàê‰∏ÄÁ≥ªÂàó‰ª£Á†ÅÁºñËæëÂ∫èÂàóÔºå‰ª•‰æø‰∫éÁ®ãÂ∫èÁöÑÈÄêÊ≠•ÁºñÂÜô„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøáLintSeqÂ§ÑÁêÜÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰ª£Á†ÅÂêàÊàêÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÁîüÊàêÊõ¥ÂÖ∑Â§öÊ†∑ÊÄßÁöÑÁ®ãÂ∫è„ÄÇÊúÄÁªàÔºå‰ΩúËÄÖÁöÑ150MÂèÇÊï∞Ê®°ÂûãÂú®‰ª£Á†ÅÁêÜËß£ÂíåÂêàÊàêÊñπÈù¢ÁöÑË°®Áé∞Ë∂ÖËøá‰∫ÜËÆ∏Â§öÂèÇÊï∞Êõ¥Â§öÁöÑÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01782",
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "url": "https://huggingface.co/papers/2410.01782",
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "score": 10,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "dbbea1382f0a84e5",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#inference",
                    "#transfer_learning",
                    "#open_source",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Open-RAG: –ü–æ–≤—ã—à–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ RAG (Retrieval-Augmented Generation). –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE), —Å–ø–æ—Å–æ–±–Ω—É—é —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RAG –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Enhancing Reasoning in RAG with Open-RAG Framework",
                    "desc": "This paper presents Open-RAG, a new framework that improves the reasoning abilities of Retrieval-Augmented Generation (RAG) models using open-source Large Language Models (LLMs). Open-RAG transforms a dense LLM into a sparse mixture of experts (MoE) model, allowing it to tackle complex reasoning tasks more effectively. The framework is designed to help the model distinguish between relevant and misleading information, enhancing its ability to provide accurate responses. Additionally, it introduces a hybrid adaptive retrieval method to optimize the balance between performance and speed during inference."
                },
                "zh": {
                    "title": "Open-RAGÔºöÊèêÂçáÂºÄÊ∫êLLMÊé®ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂Open-RAGÔºåÊó®Âú®ÊèêÈ´òÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Â∞Ü‰ªªÊÑèÁ®†ÂØÜLLMËΩ¨Âèò‰∏∫‰∏ÄÁßçÂèÇÊï∞È´òÊïàÁöÑÁ®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoEÔºâÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÁöÑÊé®ÁêÜ‰ªªÂä°ÔºåÂåÖÊã¨ÂçïË∑≥ÂíåÂ§öË∑≥Êü•ËØ¢„ÄÇOpen-RAGÈÄöËøáËÆ≠ÁªÉÊ®°ÂûãËØÜÂà´Áõ∏ÂÖ≥‰ΩÜÂÖ∑ÊúâËØØÂØºÊÄßÁöÑÂπ≤Êâ∞‰ø°ÊÅØÔºå‰ªéËÄåÊúâÊïàÊï¥ÂêàÂ§ñÈÉ®Áü•ËØÜÔºåÊèê‰æõÊõ¥ÂáÜÁ°ÆÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑÂõûÁ≠î„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàËá™ÈÄÇÂ∫îÊ£ÄÁ¥¢ÊñπÊ≥ïÔºå‰ª•Âπ≥Ë°°ÊÄßËÉΩÊèêÂçá‰∏éÊé®ÁêÜÈÄüÂ∫¶‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02115",
            "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
            "url": "https://huggingface.co/papers/2410.02115",
            "abstract": "Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM outputs is insufficient, as it is quite challenging for humans to verify the results from the extremely lengthy context. Yet, although some efforts have been made to assess whether LCMs respond truly based on the context, these works either are limited to specific tasks or heavily rely on external evaluation resources like GPT-4.In this work, we introduce L-CiteEval, a comprehensive multi-task benchmark for long-context understanding with citations, aiming to evaluate both the understanding capability and faithfulness of LCMs. L-CiteEval covers 11 tasks from diverse domains, spanning context lengths from 8K to 48K, and provides a fully automated evaluation suite. Through testing with 11 cutting-edge closed-source and open-source LCMs, we find that although these models show minor differences in their generated results, open-source models substantially trail behind their closed-source counterparts in terms of citation accuracy and recall. This suggests that current open-source LCMs are prone to responding based on their inherent knowledge rather than the given context, posing a significant risk to the user experience in practical applications. We also evaluate the RAG approach and observe that RAG can significantly improve the faithfulness of LCMs, albeit with a slight decrease in the generation quality. Furthermore, we discover a correlation between the attention mechanisms of LCMs and the citation generation process.",
            "score": 10,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "0d1ec9ec865dc20b",
            "data": {
                "categories": [
                    "#long_context",
                    "#rag",
                    "#interpretability",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "üìè",
                "ru": {
                    "title": "L-CiteEval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π LCM (Long-context models). –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8–ö –¥–æ 48–ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LCM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –∑–∞–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ RAG –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å LCM, —Ö–æ—Ç—è –∏ —Å –Ω–µ–±–æ–ª—å—à–∏–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Evaluating Long-Context Models: Faithfulness and Understanding with L-CiteEval",
                    "desc": "This paper presents L-CiteEval, a new benchmark designed to evaluate long-context models (LCMs) on their understanding and faithfulness when handling extensive text. It includes 11 diverse tasks with context lengths ranging from 8K to 48K, allowing for a comprehensive assessment of LCM performance. The study reveals that while closed-source LCMs outperform open-source models in citation accuracy and recall, the latter often rely on their pre-existing knowledge rather than the provided context. Additionally, the research highlights the effectiveness of the RAG approach in enhancing the faithfulness of LCM outputs, despite a slight trade-off in generation quality."
                },
                "zh": {
                    "title": "ÊèêÂçáÈïø‰∏ä‰∏ãÊñáÊ®°ÂûãÁöÑÁêÜËß£‰∏éÁúüÂÆûÊÄß",
                    "desc": "Èïø‰∏ä‰∏ãÊñáÊ®°ÂûãÔºàLCMsÔºâÂú®Â§ÑÁêÜÈïøÊñáÊú¨‰ªªÂä°ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊñáÊ°£ÊëòË¶ÅÊñπÈù¢„ÄÇÈöèÁùÄÂØπÁîüÊàêÁªìÊûúÁöÑÁúüÂÆûÊÄßÁöÑÈáçËßÜÔºå‰ªÖ‰ªÖ‰øùËØÅËæìÂá∫ÁöÑÂáÜÁ°ÆÊÄßÂ∑≤‰∏çË∂≥‰ª•Êª°Ë∂≥ÈúÄÊ±ÇÔºåÂõ†‰∏∫‰∫∫Á±ªÂæàÈöæÈ™åËØÅÊù•Ëá™ÊûÅÈïø‰∏ä‰∏ãÊñáÁöÑÁªìÊûú„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜL-CiteEvalÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§ö‰ªªÂä°Âü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞LCMsÁöÑÁêÜËß£ËÉΩÂäõÂíåÁúüÂÆûÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂºÄÊ∫êÊ®°ÂûãÂú®ÂºïÁî®ÂáÜÁ°ÆÊÄßÂíåÂè¨ÂõûÁéáÊñπÈù¢ÊòéÊòæËêΩÂêé‰∫éÈó≠Ê∫êÊ®°ÂûãÔºåËøôË°®ÊòéÂΩìÂâçÁöÑÂºÄÊ∫êLCMsÊõ¥ÂÄæÂêë‰∫éÂü∫‰∫éÂõ∫ÊúâÁü•ËØÜ‰ΩúÁ≠îÔºåËÄåÈùû‰æùËµñ‰∫éÁªôÂÆöÁöÑ‰∏ä‰∏ãÊñá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02762",
            "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
            "url": "https://huggingface.co/papers/2410.02762",
            "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "8ecc2787bf47eaca",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#hallucinations",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ VLM —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –û–Ω–∏ –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –º–æ–¥–µ–ª–∏ –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —É–¥–∞–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ª–∏–Ω–µ–π–Ω–æ–π –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Å–Ω–∏–∑–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –¥–æ 25.7% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö COCO2014 –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Enhancing VLM Reliability by Reducing Hallucinations",
                    "desc": "This paper explores how vision-language models (VLMs) represent images and words to tackle the issue of hallucinations, which are incorrect outputs generated by the models. The authors find that VLMs are more confident in identifying real objects compared to hallucinated ones, and they use this insight to improve object localization. They propose a knowledge erasure algorithm that modifies the model's internal features to reduce hallucinations while maintaining overall performance. Their results indicate that by refining the latent representations of VLMs, hallucinations can be decreased significantly, leading to more reliable model outputs and new functionalities like zero-shot segmentation."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØÈù†ÊÄß‰∏éËÉΩÂäõ",
                    "desc": "Êàë‰ª¨Á†îÁ©∂‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ª•Ëß£ÂÜ≥ÂπªËßâÈóÆÈ¢òÔºåËøôÂú®Ê®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉËøõÂ±ïÁöÑÊÉÖÂÜµ‰∏ã‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊåÅÁª≠ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨Â∞ÜVLMsÁöÑÂÜÖÈÉ®ÂõæÂÉèË°®Á§∫ÊäïÂΩ±Âà∞ÂÖ∂ËØ≠Ë®ÄËØçÊ±á‰∏äÔºåËßÇÂØüÂà∞ÁúüÂÆûÁâ©‰ΩìÁöÑËæìÂá∫Ê¶ÇÁéáÊØîÂπªËßâÁâ©‰ΩìÊõ¥Êúâ‰ø°ÂøÉ„ÄÇÊàë‰ª¨ËøòÂà©Áî®Ëøô‰∫õËæìÂá∫Ê¶ÇÁéáÂØπÁúüÂÆûÁâ©‰ΩìËøõË°åÁ©∫Èó¥ÂÆö‰Ωç„ÄÇÂü∫‰∫éÊ≠§ÊñπÊ≥ïÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁü•ËØÜÊ∂àÈô§ÁÆóÊ≥ïÔºåÈÄöËøáÁ∫øÊÄßÊ≠£‰∫§ÂåñÂõæÂÉèÁâπÂæÅ‰∏éÂπªËßâÁâ©‰ΩìÁâπÂæÅÊù•Ê∂àÈô§ÂπªËßâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02052",
            "title": "Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning",
            "url": "https://huggingface.co/papers/2410.02052",
            "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning.",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "026dc6add144373f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#training",
                    "#agi",
                    "#rl",
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "R-MCTS: –ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º Reflective Monte Carlo Tree Search (R-MCTS), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –¥–æ–±–∞–≤–ª—è—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ GPT-4o, –∏—Å–ø–æ–ª—å–∑—É—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ R-MCTS. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ VisualWebArena –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing AI Decision-Making with R-MCTS",
                    "desc": "This paper presents Reflective Monte Carlo Tree Search (R-MCTS), a new algorithm that improves the decision-making abilities of AI agents, particularly those using vision-language models like GPT-4o. R-MCTS enhances traditional Monte Carlo Tree Search by integrating contrastive reflection for learning from past experiences and employing multi-agent debate for better state evaluations. The authors demonstrate that their approach leads to significant performance improvements on the VisualWebArena benchmark, achieving up to 30% better results compared to previous models. Additionally, the fine-tuned GPT-4o retains most of the R-MCTS performance while being more efficient in terms of computational resources."
                },
                "zh": {
                    "title": "ÊèêÂçáÊô∫ËÉΩ‰ΩìÂÜ≥Á≠ñËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Ëá™‰∏ªÊô∫ËÉΩ‰ΩìÂú®Ëá™Âä®ÂåñÂ§çÊùÇÁöÑÂ§öÊ≠•È™§ÂÜ≥Á≠ñ‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÊòæËëóÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÔºåÂ¶ÇGPT-4oÔºåÂú®Â§çÊùÇÁöÑÁΩëÁªúÁéØÂ¢ÉÂíåÈïøËøúËßÑÂàí‰ªªÂä°‰∏≠‰ªçÁÑ∂Êó†Ê≥ïËææÂà∞‰∫∫Á±ªÊ∞¥Âπ≥ÁöÑË°®Áé∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊµãËØïÊó∂ÁÆóÊ≥ï‚Äî‚ÄîÂèçÊÄùËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàR-MCTSÔºâÔºåÊó®Âú®Â¢ûÂº∫AIÊô∫ËÉΩ‰ΩìÁöÑÂÜ≥Á≠ñÁ©∫Èó¥Êé¢Á¥¢ËÉΩÂäõ„ÄÇÈÄöËøáÂØπ‰º†ÁªüMCTSÁöÑÊâ©Â±ïÔºåR-MCTSÁªìÂêà‰∫ÜÂØπÊØîÂèçÊÄùÂíåÂ§öÊô∫ËÉΩ‰ΩìËæ©ËÆ∫ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊêúÁ¥¢ÊïàÁéáÂíåÁä∂ÊÄÅËØÑ‰º∞ËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02458",
            "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
            "url": "https://huggingface.co/papers/2410.02458",
            "abstract": "Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://bit.ly/3zf2CVs",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "28682265fba39b78",
            "data": {
                "categories": [
                    "#science",
                    "#cv",
                    "#healthcare",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ü©ª",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é LLM-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ Vision Transformers (ViT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –±–ª–æ–∫ –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è Dice —Å 0.74 –¥–æ 0.79. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª LLM-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Boosting Medical Image Segmentation with LLMs and Vision Transformers",
                    "desc": "This paper investigates the use of Large Language Models (LLMs) to improve medical image segmentation, which is essential for accurate diagnostics. The authors enhance Vision Transformers (ViTs) by integrating pre-trained LLM transformer blocks, resulting in better segmentation performance. They introduce a Hybrid Attention Mechanism that effectively combines global and local feature learning, along with a Multi-Scale Fusion Block to aggregate features from different scales. The proposed model shows significant improvements in key metrics, demonstrating the potential of LLMs to enhance the accuracy and robustness of medical image segmentation tasks."
                },
                "zh": {
                    "title": "Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊèêÂçáÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÊÄßËÉΩ",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•ÊèêÂçáÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑLLMÂèòÊç¢Âô®Ê®°ÂùóÈõÜÊàêÂà∞ËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâÊ®°ÂûãÁöÑÁºñÁ†ÅÂô®‰∏≠Ôºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫Ü‰∏çÂêåÂåªÂ≠¶ÊàêÂÉèÊ®°Âºè‰∏ãÁöÑÂàÜÂâ≤ÊïàÊûú„ÄÇÈÄöËøáÊèêÂá∫Ê∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁªìÂêàÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅÂ≠¶‰π†Ôºå‰ª•ÂèäÂ§öÂ∞∫Â∫¶ËûçÂêàÊ®°ÂùóÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÁâπÂæÅÁöÑÊúâÊïàËÅöÂêà„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊ®°ÂûãÁöÑDiceÂàÜÊï∞‰ªé0.74ÊèêÈ´òÂà∞0.79ÔºåÂáÜÁ°ÆÁéá„ÄÅÁ≤æÁ°ÆÁéáÂíåJaccardÊåáÊï∞‰πüÊúâÊòæËëóÊèêÂçáÔºåËØÅÊòé‰∫ÜLLMÂèòÊç¢Âô®Âú®ÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02103",
            "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
            "url": "https://huggingface.co/papers/2410.02103",
            "abstract": "Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
            "score": 8,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "eac41a9d952f603f",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ú–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D Gaussian Splatting",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è 3D Gaussian Splatting (3DGS), —É–ª—É—á—à–∞—é—â–∏–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–¥–Ω–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–∞—Ö. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç —Å—Ö–µ–º—É –∫—Ä–æ—Å—Å-–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—Ä–æ—Å—Å-–ª—É—á–µ–≤–æ–≥–æ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏–µ. –≠—Ç–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –ø–æ–≤—ã—à–∞—é—Ç –æ–±—â—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing 3D Gaussian Splatting with Multi-View Optimization",
                    "desc": "This paper presents a new optimization method for 3D Gaussian Splatting (3DGS) to enhance rendering quality and efficiency. The authors shift from a single-view to a multi-view training strategy, which helps prevent overfitting and improves the accuracy of novel-view synthesis. They introduce a cross-intrinsic guidance scheme for a more effective training process and a cross-ray densification strategy to increase the density of Gaussian kernels in critical areas. Finally, they propose a multi-view augmented densification approach to ensure sufficient Gaussian representation, leading to better 3D reconstruction accuracy."
                },
                "zh": {
                    "title": "Â§öËßÜÂõæ‰ºòÂåñÊèêÂçá3DÊ∏≤ÊüìÁ≤æÂ∫¶",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ3DÈ´òÊñØÁÇπÊ∏≤Êüì‰ºòÂåñÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÂçïËßÜÂõæËÆ≠ÁªÉÂØºËá¥ÁöÑËøáÊãüÂêàÈóÆÈ¢ò„ÄÇÈÄöËøáÂºïÂÖ•Â§öËßÜÂõæËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ºòÂåñ3DÈ´òÊñØÂ±ûÊÄßÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊï¥‰ΩìÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∫§ÂèâÂÜÖÂú®ÂºïÂØºÊñπÊ°àÂíå‰∫§ÂèâÂÖâÁ∫øÁ®†ÂØÜÂåñÁ≠ñÁï•Ôºå‰ª•Â¢ûÂº∫ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÁªÜËäÇË°®Áé∞„ÄÇÊúÄÁªàÔºåÈááÁî®Â§öËßÜÂõæÂ¢ûÂº∫Á®†ÂØÜÂåñÁ≠ñÁï•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈáçÂª∫Á≤æÂ∫¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02763",
            "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
            "url": "https://huggingface.co/papers/2410.02763",
            "abstract": "There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understanding long-form videos. However, is this really the case? Our studies indicate that LMMs still lack many fundamental reasoning capabilities even when dealing with short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs. We demonstrate that existing LMMs severely struggle to distinguish temporal differences between different actions and object transformations. For example, the best model GPT-4o only obtains ~50% on our text and video scores, showing a large gap compared to the human baseline of ~90%. All open-source multimodal models and CLIP-based models perform much worse, producing mostly random chance performance. Through this work, we shed light onto the fact that temporal reasoning in short videos is a problem yet to be fully solved. The dataset and evaluation code are available at https://vinoground.github.io.",
            "score": 7,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "3b6bf220f7ee6708",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ë–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –≤—Ä–µ–º—è –≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 1000 –ø–∞—Ä –≤–∏–¥–µ–æ-–ø–æ–¥–ø–∏—Å—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ LMM, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ª—é–¥—è–º –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ, –¥–æ—Å—Ç–∏–≥–∞—è –ª–∏—à—å 50% —Ç–æ—á–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ –≤—Å–µ –µ—â–µ –¥–∞–ª–µ–∫–∞ –æ—Ç –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Unveiling the Gaps in Video Comprehension: The Vinoground Challenge",
                    "desc": "This paper discusses the limitations of large multimodal models (LMMs) in understanding short videos, despite recent claims of their effectiveness. The authors introduce Vinoground, a benchmark designed to evaluate LMMs on their ability to reason about temporal aspects in video content. Their findings reveal that even the best-performing model, GPT-4o, only achieves around 50% accuracy in distinguishing temporal differences, significantly lower than the human baseline of approximately 90%. This highlights that the challenge of temporal reasoning in video comprehension remains unresolved, indicating a need for further research and development in this area."
                },
                "zh": {
                    "title": "Áü≠ËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÊó∂Èó¥Êé®ÁêÜÊåëÊàò",
                    "desc": "ËøëÂπ¥Êù•ÔºåÁé∞‰ª£Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Áü≠ËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄÂÆöËøõÂ±ïÔºå‰ΩÜÂú®ÁêÜËß£ÈïøËßÜÈ¢ëÊó∂‰ªçÈù¢‰∏¥Êõ¥Â§çÊùÇÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÂú®Â§ÑÁêÜÁü≠ËßÜÈ¢ëÊó∂ÔºåLMMs‰ªçÁÑ∂Áº∫‰πèÂü∫Êú¨ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜVinogroundÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´1000ÂØπÁü≠ËßÜÈ¢ëÂíåËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÁöÑÊó∂Èó¥Âèç‰∫ãÂÆûËØÑ‰º∞Âü∫ÂáÜ„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÁé∞ÊúâÁöÑLMMsÂú®Âå∫ÂàÜ‰∏çÂêåÂä®‰ΩúÂíåÁâ©‰ΩìÂèòÂåñÁöÑÊó∂Èó¥Â∑ÆÂºÇÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥ÔºåË°®ÊòéÁü≠ËßÜÈ¢ë‰∏≠ÁöÑÊó∂Èó¥Êé®ÁêÜÈóÆÈ¢òÂ∞öÊú™ÂÆåÂÖ®Ëß£ÂÜ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02056",
            "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
            "url": "https://huggingface.co/papers/2410.02056",
            "abstract": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet.",
            "score": 6,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "e4975d472a070194",
            "data": {
                "categories": [
                    "#reasoning",
                    "#audio",
                    "#dataset",
                    "#training",
                    "#data",
                    "#alignment",
                    "#diffusion",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "üéµ",
                "ru": {
                    "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∞—É–¥–∏–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–∑–¥–∞—é—Ç –∏ —É—Ç–æ—á–Ω—è—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∞—É–¥–∏–æ."
                },
                "en": {
                    "title": "Enhancing Audio Classification with Synthetic Data",
                    "desc": "Synthio is a new method designed to enhance small audio classification datasets by adding synthetic data. It aims to improve classification accuracy when there is limited labeled data available. Unlike traditional augmentation techniques that may not capture real-world audio diversity, Synthio uses text-to-audio diffusion models to generate more representative synthetic audio. The approach includes aligning generated audio with existing data and using advanced caption generation to ensure both acoustic consistency and compositional diversity."
                },
                "zh": {
                    "title": "SynthioÔºöÁî®ÂêàÊàêÊï∞ÊçÆÊèêÂçáÈü≥È¢ëÂàÜÁ±ªÂáÜÁ°ÆÊÄß",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SynthioÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÈÄöËøáÂêàÊàêÊï∞ÊçÆÂ¢ûÂº∫Â∞èËßÑÊ®°Èü≥È¢ëÂàÜÁ±ªÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÊèêÈ´òÂú®ÊúâÈôêÊ†áËÆ∞Êï∞ÊçÆ‰∏ãÁöÑÈü≥È¢ëÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÇ‰º†ÁªüÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÈöæ‰ª•ÁîüÊàêÁúüÂÆû‰∏ñÁïåÈü≥È¢ëÁöÑÂ§öÊ†∑ÊÄßÔºåÂõ†Ê≠§Êàë‰ª¨ÈááÁî®ÊñáÊú¨Âà∞Èü≥È¢ëÔºàT2AÔºâÊâ©Êï£Ê®°ÂûãÁîüÊàêÂêàÊàêÈü≥È¢ëÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇÈÄöËøá‰ºòÂåñÁîüÊàêÂÅèÂ•ΩÔºåÊàë‰ª¨Á°Æ‰øùÁîüÊàêÁöÑÊï∞ÊçÆÂú®Â£∞Â≠¶ÁâπÊÄß‰∏ä‰∏éÂ∞èËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏ÄËá¥ÔºåÂêåÊó∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈü≥È¢ëÊèèËø∞Ôºå‰ª•ÊèêÈ´òÂêàÊàêÊï∞ÊçÆÁöÑË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01335",
            "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
            "url": "https://huggingface.co/papers/2410.01335",
            "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "5828b5e66fffb240",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#training",
                    "#math",
                    "#interpretability",
                    "#transfer_learning",
                    "#benchmark",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —è–∑—ã–∫–∞—Ö, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∞—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –Ω–∞ –æ–±—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ. –ó–∞—Ç–µ–º –æ–Ω–∏ –∑–∞–º–µ–Ω—è—é—Ç –≤–µ—Ä—Ö–Ω–∏–µ –∏ –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ —Å–ª–æ—è–º–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ 10% –≤ —á–µ—Ç—ã—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MGSM."
                },
                "en": {
                    "title": "Enhancing Math Performance in Non-English LLMs through Layer Swapping",
                    "desc": "This paper introduces a novel approach to model merging, specifically for Large Language Models (LLMs) that need to perform mathematical reasoning in non-English languages. The authors propose a method where two separate 'expert' models are fine-tuned: one on math data in English and the other on general instruction data in the target language. By swapping the transformer layers between these experts, they enhance the math performance of the model in the target language without requiring additional training data. The results show a significant improvement in performance on the math benchmark, demonstrating the effectiveness of this layer swapping technique for cross-lingual transfer and modular model solutions."
                },
                "zh": {
                    "title": "Ë∑®ËØ≠Ë®ÄÊ®°ÂûãÂêàÂπ∂ÔºåÊèêÂçáÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ®°ÂûãÂêàÂπ∂ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Âú®ÈùûËã±ËØ≠ËØ≠Ë®Ä‰∏≠ÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂõ∞Èöæ„ÄÇÊàë‰ª¨ÈÄöËøáÂ∞ÜÊï∞Â≠¶ËÉΩÂäõ‰∏éËØ≠Ë®ÄËÉΩÂäõÁªìÂêàÔºå‰øÉËøõË∑®ËØ≠Ë®ÄËΩ¨ÁßªÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèÁâπÂÆö‰ªªÂä°Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨‰ªéÂêå‰∏ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂºÄÂßãÔºåÂàÜÂà´Âú®Ëã±ËØ≠ÁöÑÊï∞Â≠¶Êåá‰ª§Êï∞ÊçÆÂíåÁõÆÊ†áËØ≠Ë®ÄÁöÑÈÄöÁî®Êåá‰ª§Êï∞ÊçÆ‰∏äÂæÆË∞É‰∏çÂêåÁöÑ‚Äú‰∏ìÂÆ∂‚Äù„ÄÇÈÄöËøáÁõ¥Êé•ÊõøÊç¢Êï∞Â≠¶‰∏ìÂÆ∂ÁöÑÈ°∂ÈÉ®ÂíåÂ∫ïÈÉ®ÂèòÊç¢Âô®Â±ÇÔºåÊàë‰ª¨ÁöÑÂêàÂπ∂Ê®°ÂûãÂú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÂçïÁã¨ÁöÑ‰∏ìÂÆ∂ÂíåÂÖ∂‰ªñÂêàÂπ∂ÊñπÊ≥ïÔºåÊèêÂçá‰∫Ü10%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02536",
            "title": "Intelligence at the Edge of Chaos",
            "url": "https://huggingface.co/papers/2410.02536",
            "abstract": "We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "b962196b43ec4ddd",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agi",
                    "#training",
                    "#rl",
                    "#games",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–∂–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: —É—Ä–æ–∫–∏ –∫–ª–µ—Ç–æ—á–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–æ–≤",
                    "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª –∫–ª–µ—Ç–æ—á–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–æ–≤ –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –Ω–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –º–æ–¥–µ–ª—è–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–º –±–æ–ª—å—à–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —à–∞—Ö–º–∞—Ç–Ω—ã—Ö —Ö–æ–¥–æ–≤. –ü—Ä–æ—Å—Ç—ã–µ –æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã, –∞ —Ç–∞–∫–∂–µ —Å–∏–ª—å–Ω–æ —Ö–∞–æ—Ç–∏—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –¥–∞–≤–∞–ª–∏ —Ö—É–¥—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Complexity Fuels Intelligence in AI Models",
                    "desc": "This paper investigates how the complexity of rule-based systems affects the intelligence of models trained to predict these rules. It specifically examines elementary cellular automata (ECA), which are simple one-dimensional systems that can produce a wide range of behaviors. By training Large Language Models (LLMs) on various ECAs, the study finds that more complex rules lead to better performance in tasks requiring reasoning and prediction. The results suggest that there is an optimal level of complexity that enhances model intelligence, indicating that exposure to complexity may be key to developing intelligent systems."
                },
                "zh": {
                    "title": "Â§çÊùÇÊÄß‰∏éÊô∫ËÉΩÁöÑÂÖ≥Á≥ª",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∫∫Â∑•Á≥ªÁªü‰∏≠Êô∫ËÉΩË°å‰∏∫ÁöÑÂá∫Áé∞ÔºåÈáçÁÇπÂàÜÊûê‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÁ≥ªÁªüÂ§çÊùÇÊÄßÂ¶Ç‰ΩïÂΩ±ÂìçÊ®°ÂûãÁöÑÈ¢ÑÊµãËÉΩÂäõ„ÄÇÊàë‰ª¨‰ΩøÁî®ÁÆÄÂçïËÄåÂº∫Â§ßÁöÑÂàùÁ≠âÂÖÉËÉûËá™Âä®Êú∫ÔºàECAÔºâ‰Ωú‰∏∫Á†îÁ©∂ÂØπË±°ÔºåËøô‰∫õÁ≥ªÁªüËÉΩÂ§üÁîüÊàê‰ªéÁÆÄÂçïÂà∞Â§çÊùÇÁöÑÂ§öÁßçË°å‰∏∫„ÄÇÈÄöËøáÂØπ‰∏çÂêåECAËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜËßÑÂàôË°å‰∏∫ÁöÑÂ§çÊùÇÊÄß‰∏éLLMË°®Áé∞Âá∫ÁöÑÊô∫ËÉΩ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ§çÊùÇÊÄßËæÉÈ´òÁöÑËßÑÂàô‰ΩøÂæóÊ®°ÂûãÂú®Êé®ÁêÜÂíåÂõΩÈôÖË±°Ê£ãËµ∞Ê≥ïÈ¢ÑÊµã‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºåÊöóÁ§∫Êô∫ËÉΩÁöÑ‰∫ßÁîü‰∏éÂØπÂ§çÊùÇÊÄßÁöÑÈ¢ÑÊµãËÉΩÂäõÂØÜÂàáÁõ∏ÂÖ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.00255",
            "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
            "url": "https://huggingface.co/papers/2410.00255",
            "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-09-30",
            "pub_date_card": {
                "ru": "30 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 30",
                "zh": "9Êúà30Êó•"
            },
            "hash": "537bc5070d162d76",
            "data": {
                "categories": [
                    "#security",
                    "#training",
                    "#agi",
                    "#data",
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#architecture",
                    "#synthetic",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "Robin3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ 3D-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Robin3D - –º–æ—â–Ω–∞—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (3DLLM), –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –î–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é –Ω–æ–≤–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –∫–∞–∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. Robin3D –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –ø—è—Ç–∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ 3D, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏."
                },
                "en": {
                    "title": "Robin3D: Elevating 3D Language Models with Robust Instruction Data",
                    "desc": "This paper presents Robin3D, an advanced 3D Large Language Model (3DLLM) designed to improve instruction-following capabilities in 3D environments. The model is trained using a novel data generation engine called Robust Instruction Generation (RIG), which creates high-quality instruction data, including adversarial and diverse samples. By incorporating techniques like Relation-Augmented Projector and ID-Feature Bonding, Robin3D enhances its spatial understanding and object grounding abilities. The results show significant performance improvements over previous models on multiple 3D multimodal learning benchmarks, demonstrating its effectiveness without requiring task-specific fine-tuning."
                },
                "zh": {
                    "title": "Robin3DÔºöÊèêÂçá3DËØ≠Ë®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÁêÜËß£ËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ3DÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãRobin3DÔºåËØ•Ê®°ÂûãÈÄöËøáÊàë‰ª¨ÁöÑÊñ∞Êï∞ÊçÆÂºïÊìéRIGÁîüÊàêÁöÑÂ§ßËßÑÊ®°Êåá‰ª§Ë∑üÈöèÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇRIGÁîüÊàê‰∫Ü‰∏§ÁßçÂÖ≥ÈîÆÁöÑÊåá‰ª§Êï∞ÊçÆÔºöÂØπÊäóÊÄßÊåá‰ª§Ë∑üÈöèÊï∞ÊçÆÂíåÂ§öÊ†∑ÊÄßÊåá‰ª§Ë∑üÈöèÊï∞ÊçÆÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑÂå∫ÂàÜËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇRobin3DÂú®Â§ÑÁêÜÂ§çÊùÇÊåá‰ª§Êó∂ÔºåÈááÁî®‰∫ÜÂÖ≥Á≥ªÂ¢ûÂº∫ÊäïÂΩ±Âô®ÂíåIDÁâπÂæÅÁªëÂÆöÊäÄÊúØÔºåÊèêÂçá‰∫ÜÁ©∫Èó¥ÁêÜËß£ÂíåÁâ©‰ΩìÂºïÁî®ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRobin3DÂú®Â§ö‰∏™3DÂ§öÊ®°ÊÄÅÂ≠¶‰π†Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02426",
            "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
            "url": "https://huggingface.co/papers/2410.02426",
            "abstract": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella \"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 3",
                "zh": "10Êúà3Êó•"
            },
            "hash": "8713f059b61a607f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#hallucinations",
                    "#training",
                    "#games",
                    "#small_models"
                ],
                "emoji": "‚ôüÔ∏è",
                "ru": {
                    "title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—Å–≤–∞–∏–≤–∞—é—Ç —à–∞—Ö–º–∞—Ç—ã —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º–∏ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–æ–≤–µ–ª–ª–æ–π –°—Ç–µ—Ñ–∞–Ω–∞ –¶–≤–µ–π–≥–∞ '–®–∞—Ö–º–∞—Ç–Ω–∞—è –Ω–æ–≤–µ–ª–ª–∞', –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å 28 –∏ 125 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –æ–±—É—á–∏—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª–∞–º —à–∞—Ö–º–∞—Ç, –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ª–µ–≥–∞–ª—å–Ω—ã–µ —Ö–æ–¥—ã –∏ —Ä–µ—à–∞—Ç—å —à–∞—Ö–º–∞—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö. –ò–∑—É—á–µ–Ω–æ –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Small Models, Big Moves: Learning Chess with Language Models",
                    "desc": "This paper shows that small pretrained generative language models, with millions of parameters, can effectively learn the underlying rules of a specific process, such as chess, from provided data. By fine-tuning these models with varying amounts of examples, they can generate legal chess moves and solve chess problems accurately. The study highlights the importance of the number of fine-tuning epochs, showing that more training leads to better performance. Additionally, it finds that increasing the number of examples reduces the occurrence of model hallucinations, improving the reliability of the generated outputs."
                },
                "zh": {
                    "title": "Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂäõÔºöÂ≠¶‰π†ÂõΩÈôÖË±°Ê£ãËßÑÂàô",
                    "desc": "ËøôÁØáËÆ∫ÊñáÂ±ïÁ§∫‰∫ÜÂ∞èÂûãÈ¢ÑËÆ≠ÁªÉÁîüÊàêËØ≠Ë®ÄÊ®°ÂûãÂ¶Ç‰Ωï‰ªé‰∏éÁâπÂÆöËøáÁ®ãÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÊΩúÂú®ËßÑÂàô„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÖ∑Êúâ2800‰∏áÂíå1.25‰∫øÂèÇÊï∞ÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂèØ‰ª•ÈÄöËøáÊåá‰ª§ÂæÆË∞ÉÔºåÂà©Áî®1000Âà∞1000000‰∏™Á§∫‰æãÂ≠¶‰π†ÂõΩÈôÖË±°Ê£ãÁöÑËßÑÂàôÔºåÊèêÂá∫ÂêàÊ≥ïÁöÑËµ∞Ê≥ïÔºåÂπ∂ÂáÜÁ°ÆËß£ÂÜ≥Ê£ãÂ±ÄÈóÆÈ¢ò„ÄÇËÆ∫ÊñáËøòÊé¢ËÆ®‰∫ÜËøûÁª≠ÂæÆË∞ÉÂë®ÊúüÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂπ∂ÈÄöËøáÂ¢ûÂä†ÂæÆË∞ÉÁ§∫‰æãÁöÑÊï∞ÈáèÊù•ÂáèÂ∞ëÊ®°ÂûãÁöÑÂπªËßâÁé∞Ë±°„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåËøôÈ°πÁ†îÁ©∂Ë°®ÊòéÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01946",
            "title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
            "url": "https://huggingface.co/papers/2410.01946",
            "abstract": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.",
            "score": 4,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 2",
                "zh": "10Êúà2Êó•"
            },
            "hash": "6a0f04b3d6aec2b3",
            "data": {
                "categories": [
                    "#science",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#transfer_learning",
                    "#low_resource"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –æ—Ü–µ–Ω–∫–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏ –Ω–æ–≤—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–º."
                },
                "en": {
                    "title": "Automating Domain-Specific Labeling for Better Text Classification",
                    "desc": "This paper presents SciPrompt, a framework that enhances prompt-based fine-tuning for low-resource multi-class text classification tasks, particularly in scientific domains. It addresses the challenge of manually selecting domain-specific label terms for verbalizers by automatically retrieving relevant terms from scientific literature. The proposed method uses correlation scores to weight these terms, improving the model's prediction performance during tuning. SciPrompt demonstrates superior results compared to existing prompt-based methods, especially for fine-grained and emerging scientific topics in few and zero-shot scenarios."
                },
                "zh": {
                    "title": "SciPromptÔºöËá™Âä®ÂåñÁßëÂ≠¶ÊñáÊú¨ÂàÜÁ±ªÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SciPromptÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëá™Âä®Ê£ÄÁ¥¢‰∏éÁßëÂ≠¶‰∏ªÈ¢òÁõ∏ÂÖ≥ÁöÑÊúØËØ≠Ôºå‰ª•Â∫îÂØπ‰ΩéËµÑÊ∫êÊñáÊú¨ÂàÜÁ±ª‰ªªÂä°„ÄÇÈÄöËøáÈÄâÊã©ËØ≠‰πâÁõ∏ÂÖ≥‰∏îÁâπÂÆöÈ¢ÜÂüüÁöÑÊ†áÁ≠æÊúØËØ≠ÔºåSciPromptÂ¢ûÂº∫‰∫ÜÊèêÁ§∫Ê®°ÊùøÁöÑÊïàÊûú„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË°®Ëø∞Á≠ñÁï•ÔºåÂà©Áî®Áõ∏ÂÖ≥ÊÄßÂæóÂàÜ‰Ωú‰∏∫È¢ùÂ§ñÊùÉÈáçÔºåÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â∞ëÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÔºåÂ∞§ÂÖ∂Âú®ÁªÜÁ≤íÂ∫¶ÂíåÊñ∞ÂÖ¥ÁßëÂ≠¶‰∏ªÈ¢òÁöÑÂàÜÁ±ª‰ªªÂä°‰∏≠Ôºå‰ºò‰∫éÁé∞ÊúâÁöÑÊèêÁ§∫Âü∫Á°ÄÂæÆË∞ÉÊñπÊ≥ï„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-10-03.html",
    "link_next": "2024-10-07.html",
    "link_month": "2024-10.html",
    "short_date_prev": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10Êúà3Êó•"
    },
    "short_date_next": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10Êúà7Êó•"
    },
    "categories": {
        "#dataset": 7,
        "#data": 6,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 9,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 1,
        "#inference": 3,
        "#3d": 2,
        "#audio": 2,
        "#video": 4,
        "#multimodal": 8,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 18,
        "#healthcare": 1,
        "#training": 21,
        "#robotics": 0,
        "#agi": 3,
        "#games": 3,
        "#interpretability": 5,
        "#reasoning": 10,
        "#transfer_learning": 7,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 12,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 6,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 3,
        "#science": 2,
        "#low_resource": 2
    }
}