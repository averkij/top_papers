{
    "date": {
        "ru": "4 октября",
        "en": "October 4",
        "zh": "10月4日"
    },
    "time_utc": "2024-10-04 09:00",
    "weekday": 4,
    "issue_id": 10,
    "home_page_url": "https://huggingface.co/papers?date=2024-10-04",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.02740",
            "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
            "url": "https://huggingface.co/papers/2410.02740",
            "abstract": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models.",
            "score": 52,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "81cb4e856bb4642c",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Оптимизация подписей для мультимодальных моделей: гибридный подход побеждает",
                    "desc": "Статья исследует влияние синтетических подписей к изображениям на производительность мультимодальных моделей машинного обучения. Авторы предлагают новый метод генерации разнообразных форматов подписей, адаптированных под различные модели. Исследование показывает, что гибридный подход, сочетающий синтетические подписи и оригинальные AltText, превосходит использование только синтетических подписей. Результаты демонстрируют, что каждая модель имеет предпочтения к определенным форматам подписей, что важно для оптимизации стратегий предобучения мультимодальных моделей."
                },
                "en": {
                    "title": "Optimizing Multimodal Models with Hybrid Captioning Strategies",
                    "desc": "This paper discusses the importance of using rewritten captions to enhance the performance of multimodal models, which combine text and images. It highlights the challenges of understanding how synthetic captions interact with original AltTexts during pre-training. The authors introduce a new captioning pipeline that generates various caption formats tailored to different models, such as CLIP and multimodal LLMs. Their research shows that using both synthetic captions and AltTexts together leads to better model performance and alignment than using synthetic captions alone."
                },
                "zh": {
                    "title": "优化多模态模型的标题生成策略",
                    "desc": "最近多模态模型的进展显示，重写的标题对提高性能有重要价值，但仍面临一些挑战。例如，虽然合成标题通常提供更好的质量和图像-文本对齐，但尚不清楚它们是否可以完全替代AltTexts。不同的多模态基础模型可能对特定的标题格式有独特的偏好，但识别每个模型的最佳标题的努力仍然有限。我们的研究提出了一种新颖、可控且可扩展的标题生成管道，旨在为各种多模态模型生成多样化的标题格式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02367",
            "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
            "url": "https://huggingface.co/papers/2410.02367",
            "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation.",
            "score": 45,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "9c0f4a698ce3fbd7",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "SageAttention: Революция в квантизации механизма внимания",
                    "desc": "В статье представлен метод SageAttention для эффективной квантизации механизма внимания в трансформерах. Авторы анализируют возможность квантизации внимания и предлагают подход, превосходящий существующие методы по скорости и точности. SageAttention демонстрирует значительное ускорение по сравнению с FlashAttention2 и xformers, сохраняя высокую точность. Эксперименты подтверждают эффективность метода для различных моделей обработки языка, генерации изображений и видео."
                },
                "en": {
                    "title": "Revolutionizing Attention: SageAttention for Efficient Quantization",
                    "desc": "This paper addresses the computational challenges of the attention mechanism in transformer models, which has a complexity of O(N^2). It highlights that while quantization techniques have been effective for linear layers, they have not been adequately applied to attention mechanisms. The authors introduce SageAttention, a novel quantization method specifically designed for attention, which significantly improves operational efficiency. Experimental results demonstrate that SageAttention not only accelerates processing speed but also maintains high accuracy across various applications, including language processing and image generation."
                },
                "zh": {
                    "title": "高效量化注意力机制的SageAttention",
                    "desc": "本文探讨了变换器架构中的注意力机制，指出其计算复杂度为O(N^2)，在处理长序列时成为主要的时间消耗部分。尽管量化技术在加速模型推理方面有效，但现有方法主要集中在优化线性层。为此，本文详细分析了注意力机制中量化的可行性，并提出了一种高效且准确的量化方法SageAttention。实验结果表明，SageAttention在操作每秒（OPS）方面优于FlashAttention2和xformers，且在多种模型上几乎没有损失端到端指标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02073",
            "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
            "url": "https://huggingface.co/papers/2410.02073",
            "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
            "score": 40,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "b8db8c8f9dcb5574",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Революция в оценке глубины: быстро, точно и без дополнительных данных",
                    "desc": "Авторы представляют фундаментальную модель для zero-shot оценки метрической глубины по монокулярным изображениям. Модель Depth Pro синтезирует высокодетализированные карты глубины с беспрецедентной четкостью и высокочастотными деталями. Предсказания модели метрические, с абсолютным масштабом, и не требуют дополнительных метаданных, таких как параметры камеры. Модель работает быстро, создавая карту глубины размером 2.25 мегапикселя за 0.3 секунды на стандартном GPU."
                },
                "en": {
                    "title": "Depth Pro: Fast and Accurate Depth Estimation Without Metadata",
                    "desc": "This paper introduces Depth Pro, a foundation model designed for zero-shot metric monocular depth estimation. It generates high-resolution depth maps that are sharp and detailed, without needing camera metadata for scale. The model operates quickly, producing a 2.25-megapixel depth map in just 0.3 seconds on a standard GPU. Key innovations include a multi-scale vision transformer for dense predictions and a training method that effectively combines real and synthetic data to enhance accuracy and boundary detail."
                },
                "zh": {
                    "title": "Depth Pro：快速高效的单目深度估计模型",
                    "desc": "我们提出了一种用于零-shot度量单目深度估计的基础模型，称为Depth Pro。该模型能够合成高分辨率的深度图，具有无与伦比的清晰度和高频细节。它的预测是度量的，具有绝对尺度，不依赖于相机内参等元数据。Depth Pro在标准GPU上以0.3秒的速度生成2.25百万像素的深度图，展现了其高效性和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02757",
            "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
            "url": "https://huggingface.co/papers/2410.02757",
            "abstract": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video.",
            "score": 36,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "8003554c44032bf4",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Loong: прорыв в генерации длинных видео с помощью LLM",
                    "desc": "Статья представляет новый генератор видео Loong, основанный на авторегрессивных языковых моделях (LLM). Авторы анализируют проблемы генерации длинных видео и предлагают решения, включая прогрессивное обучение от коротких к длинным видео и переоценку видеотокенов. Модель Loong обучается на 10-секундных видео, но способна генерировать минутные ролики по текстовым запросам. Это значительный прогресс в области генерации длинных видео с помощью LLM."
                },
                "en": {
                    "title": "Unlocking Long Video Generation with Loong!",
                    "desc": "This paper addresses the challenge of generating long videos using autoregressive large language models (LLMs), which have been successful in natural language processing but struggle with video generation. The authors introduce Loong, a novel video generator that treats text and video tokens as a unified sequence, allowing for the creation of minute-long videos. They implement a progressive training approach that gradually increases video length while addressing loss imbalance, and explore various inference strategies to reduce errors. The results show that Loong can effectively generate longer videos based on text prompts, marking a significant advancement in video generation technology."
                },
                "zh": {
                    "title": "生成分钟级长视频的新突破",
                    "desc": "本论文探讨了使用自回归大型语言模型（LLMs）生成长视频的挑战。尽管LLMs在自然语言处理领域取得了成功，但在视频生成方面，现有方法仅能生成几秒钟的短视频。我们提出了一种新的自回归LLM视频生成器Loong，能够生成分钟级长视频。通过将文本和视频标记建模为统一序列，并采用逐步短到长的训练策略，我们有效解决了长视频训练中的损失不平衡问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02713",
            "title": "Video Instruction Tuning With Synthetic Data",
            "url": "https://huggingface.co/papers/2410.02713",
            "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.",
            "score": 36,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "f4ccb1c1c9671dde",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтетические данные открывают новые возможности для видео-ИИ",
                    "desc": "Исследователи представили новый подход к созданию мультимодальных моделей для работы с видео. Они создали синтетический набор данных LLaVA-Video-178K для обучения выполнению инструкций в видео. На основе этого набора данных была разработана модель LLaVA-Video, показавшая высокие результаты в различных видео-ориентированных задачах. Авторы планируют опубликовать набор данных, процесс его генерации и чекпоинты модели."
                },
                "en": {
                    "title": "Synthetic Data for Superior Video Understanding",
                    "desc": "This paper presents a solution to the challenge of gathering high-quality data for training video large multimodal models (LMMs). The authors introduce a synthetic dataset called LLaVA-Video-178K, designed for video instruction-following tasks, which includes detailed captioning and question-answering. By utilizing this dataset alongside existing visual instruction tuning data, they develop a new video LMM named LLaVA-Video. Their experiments show that LLaVA-Video performs well on various video benchmarks, demonstrating the dataset's effectiveness in enhancing model training."
                },
                "zh": {
                    "title": "合成数据集助力视频多模态模型发展",
                    "desc": "本论文提出了一种新方法，解决了视频大规模多模态模型（LMMs）在获取高质量原始数据时的困难。我们创建了一个专门用于视频指令跟随的高质量合成数据集，称为LLaVA-Video-178K。该数据集包含详细的字幕、开放式问答和多项选择问答等关键任务。通过在这个数据集上训练，并结合现有的视觉指令调优数据，我们推出了新的视频LMM——LLaVA-Video，并在多个视频基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02712",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "url": "https://huggingface.co/papers/2410.02712",
            "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (2) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.",
            "score": 34,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "529e51b7f382eb97",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#interpretability",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#rlhf",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLaVA-Critic: открытая мультимодальная модель для самокритики и оценки ИИ",
                    "desc": "Статья представляет LLaVA-Critic - первую открытую большую мультимодальную модель (LMM) для оценки различных мультимодальных задач. Модель обучена на высококачественном наборе данных с инструкциями для критики. LLaVA-Critic показывает эффективность в роли судьи, предоставляя надежные оценки, и в обучении с подкреплением, генерируя сигналы вознаграждения. Это исследование открывает перспективы для создания масштабируемых механизмов обратной связи для выравнивания LMM."
                },
                "en": {
                    "title": "LLaVA-Critic: The Future of Multimodal Evaluation",
                    "desc": "LLaVA-Critic is a groundbreaking open-source large multimodal model (LMM) that serves as a generalist evaluator for various multimodal tasks. It is trained on a comprehensive dataset that includes diverse evaluation criteria, allowing it to assess performance effectively. The model excels in two main areas: providing reliable evaluation scores comparable to advanced GPT models and generating reward signals for preference learning to improve model alignment. This research highlights the promise of open-source LMMs in self-evaluation and sets a foundation for future advancements in alignment feedback mechanisms."
                },
                "zh": {
                    "title": "LLaVA-Critic：开源多模态模型的评估新纪元",
                    "desc": "LLaVA-Critic是首个开源的大型多模态模型，旨在作为通用评估者，评估多种多模态任务的表现。该模型通过高质量的批评指令跟随数据集进行训练，涵盖了多样的评估标准和场景。实验表明，LLaVA-Critic在两个关键领域表现出色：作为评判者提供可靠的评估分数，并在偏好学习中生成奖励信号，增强模型的对齐能力。此项工作展示了开源多模态模型在自我批评和评估中的潜力，为未来可扩展的超人类对齐反馈机制研究奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02724",
            "title": "Large Language Models as Markov Chains",
            "url": "https://huggingface.co/papers/2410.02724",
            "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.",
            "score": 31,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "faad779778001c6f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны эффективности больших языковых моделей через призму цепей Маркова",
                    "desc": "Статья исследует теоретические основы эффективности больших языковых моделей (LLM). Авторы проводят аналогию между авторегрессионными языковыми моделями и цепями Маркова на конечном пространстве состояний. Они анализируют существование стационарного распределения, скорость сходимости и влияние температуры на эти характеристики. Исследователи также доказывают границы предобучения и обобщения в контексте, подкрепляя теоретические выводы экспериментами на современных LLM."
                },
                "en": {
                    "title": "Unraveling the Power of Large Language Models through Markov Chains",
                    "desc": "This paper explores the theoretical foundations of large language models (LLMs) by establishing a connection between autoregressive language models and Markov chains. It reveals that LLMs can be understood through the lens of Markov chains with a specific state space, which helps explain their performance and convergence properties. The authors derive important results regarding the stationary distribution of these chains and how temperature affects convergence speed. Additionally, they provide generalization bounds and validate their theoretical insights through experiments on contemporary LLMs, demonstrating the practical implications of their findings."
                },
                "zh": {
                    "title": "揭示大型语言模型的理论基础",
                    "desc": "本文探讨了大型语言模型（LLMs）在自然语言处理任务中的表现及其理论基础。我们将通用自回归语言模型与有限状态空间上的马尔可夫链建立了等价关系。研究发现，马尔可夫链的平稳分布能够捕捉LLMs的推理能力，并分析了温度对收敛速度的影响。通过实验验证了我们的理论保证，展示了LLMs在实际应用中的行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02746",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "url": "https://huggingface.co/papers/2410.02746",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.",
            "score": 31,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "cc226eaa6867dda5",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#transfer_learning",
                    "#alignment",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "CLOC: Улучшение локализации в предобучении визуально-языковых моделей",
                    "desc": "Статья представляет новый метод предварительного обучения под названием CLOC (Contrastive Localized Language-Image Pre-training), который улучшает возможности локализации модели CLIP. CLOC дополняет CLIP контрастивной потерей на уровне регионов и текста, а также вводит концепцию 'promptable embeddings'. Для поддержки масштабного предобучения авторы разработали фреймворк для генерации псевдо-меток регион-текст. CLOC демонстрирует улучшенные результаты в задачах распознавания и поиска регионов изображений, а также может заменить CLIP в мультимодальных больших языковых моделях."
                },
                "en": {
                    "title": "Enhancing Image Understanding with CLOC for Multimodal Models",
                    "desc": "This paper introduces Contrastive Localized Language-Image Pre-training (CLOC), an enhancement of the CLIP model that improves its ability to understand images at a finer level. CLOC incorporates region-text contrastive loss and new modules to better align image regions with corresponding text descriptions. The authors propose a novel concept called promptable embeddings, which allows the model to easily convert image embeddings into detailed region representations using spatial hints. By leveraging a large-scale, visually-enriched captioning framework, CLOC generates high-quality regional embeddings, making it suitable for tasks that require precise image localization and retrieval in multimodal large language models."
                },
                "zh": {
                    "title": "提升CLIP的区域理解能力",
                    "desc": "对比语言-图像预训练（CLIP）是一种用于训练视觉编码器的方法，能够生成图像和文本的表示，广泛应用于多模态大语言模型（MLLMs）。本文提出了一种改进的预训练方法，称为对比局部化语言-图像预训练（CLOC），通过区域-文本对比损失增强CLIP的定位能力。我们引入了可提示嵌入的概念，使得编码器能够根据空间提示轻松转换为区域表示。CLOC通过生成大规模的区域-文本伪标签，提升了图像区域识别和检索任务的质量，能够有效替代CLIP，特别是在引用和定位任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02416",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
            "url": "https://huggingface.co/papers/2410.02416",
            "abstract": "Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.",
            "score": 25,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "d40106a5a7b0cb85",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение качества генерации изображений без пересыщения",
                    "desc": "Статья представляет новый метод адаптивного проецируемого руководства (APG) для улучшения качества генерации изображений в диффузионных моделях. Авторы предлагают модификацию классического метода руководства без классификатора (CFG), разделяя обновление на параллельные и ортогональные компоненты. APG снижает вес параллельного компонента, что позволяет использовать более высокие масштабы руководства без пересыщения. Метод легко реализуется, практически не увеличивает вычислительные затраты и демонстрирует улучшение показателей FID и recall при сохранении точности."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Adaptive Projected Guidance",
                    "desc": "This paper focuses on improving classifier-free guidance (CFG) in diffusion models, which is essential for generating high-quality outputs that align well with input conditions. The authors identify that while a high guidance scale enhances generation quality, it can also lead to oversaturation and unrealistic artifacts. They propose a new method called adaptive projected guidance (APG) that modifies the CFG update rule by down-weighting the oversaturating component, allowing for better image quality without the negative effects of high guidance scales. Extensive experiments show that APG outperforms standard CFG in various metrics, making it a practical and efficient alternative for enhancing diffusion model outputs."
                },
                "zh": {
                    "title": "自适应投影引导：提升生成质量的无分类器引导新方法",
                    "desc": "本文探讨了无分类器引导（CFG）在扩散模型中的重要性，特别是在生成质量和输入条件与最终输出之间的对齐方面。我们发现，CFG的更新规则可以分解为平行和正交两个部分，其中平行部分会导致过饱和现象，而正交部分则提升图像质量。为了解决过饱和问题，我们提出了一种新的自适应投影引导（APG）方法，通过降低平行部分的权重来实现高质量生成，同时允许使用更高的引导比例。实验结果表明，APG在多种条件扩散模型和采样器中表现出色，提升了FID、召回率和饱和度分数，同时保持了与CFG相当的精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01679",
            "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
            "url": "https://huggingface.co/papers/2410.01679",
            "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.",
            "score": 22,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "70e8bd770c7d370b",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VinePPO: Эффективное обучение языковых моделей для сложных рассуждений",
                    "desc": "Статья представляет новый подход к обучению больших языковых моделей (LLM) для решения сложных задач рассуждения. Авторы выявили недостатки в существующем методе Proximal Policy Optimization (PPO) при работе с задачами, требующими многоступенчатых рассуждений. Они предложили альтернативный метод VinePPO, который использует оценки Монте-Карло вместо сетей значений для более точного присвоения вознаграждений. VinePPO показал превосходные результаты на наборах данных MATH и GSM8K, требуя меньше вычислительных ресурсов."
                },
                "en": {
                    "title": "VinePPO: A Better Way to Assign Credit in Complex Reasoning Tasks",
                    "desc": "This paper discusses the challenges of credit assignment in reinforcement learning (RL) for large language models (LLMs) when performing complex reasoning tasks. It highlights the limitations of using value networks in the Proximal Policy Optimization (PPO) algorithm, which often leads to inaccurate predictions and poor performance. The authors introduce VinePPO, a new method that utilizes unbiased Monte Carlo estimates to improve credit assignment without relying on large value networks. Their experiments show that VinePPO significantly outperforms PPO and other baselines, achieving better results with fewer updates and less computation time."
                },
                "zh": {
                    "title": "VinePPO：提升大型语言模型推理性能的新方法",
                    "desc": "大型语言模型（LLMs）在复杂推理任务中越来越多地被应用，这些任务需要在获得奖励之前执行多个复杂步骤。正确地分配这些步骤的信用对于提高模型性能至关重要。我们提出了一种新的方法VinePPO，它利用语言环境的灵活性来计算无偏的蒙特卡洛估计，避免了大型价值网络的需求。我们的实验表明，VinePPO在MATH和GSM8K数据集上表现优于传统的PPO算法，且所需的梯度更新次数和时间显著减少。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02678",
            "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
            "url": "https://huggingface.co/papers/2410.02678",
            "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute.",
            "score": 22,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "c0f8d0752bf51e22",
            "data": {
                "categories": [
                    "#audio",
                    "#multilingual",
                    "#training",
                    "#machine_translation",
                    "#transfer_learning",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Эффективное обучение речевых ассистентов без размеченных данных",
                    "desc": "В статье представлен новый подход к обучению речевых моделей больших языковых моделей (LLM) без использования размеченных данных. Авторы предлагают метод, основанный на самообучении, используя ответы текстовой LLM на транскрипты речи в качестве целевых данных. Разработанная модель DiVA (Distilled Voice Assistant) демонстрирует способность к обобщению в задачах устного ответа на вопросы, классификации и перевода. Результаты показывают, что DiVA лучше соответствует предпочтениям пользователей, превосходя современные модели при значительно меньших вычислительных затратах на обучение."
                },
                "en": {
                    "title": "Revolutionizing Voice Assistants with Self-Supervised Learning",
                    "desc": "This paper introduces a new approach for training Speech Large Language Models (LLMs) that combines audio and text processing more effectively. Instead of relying on supervised finetuning with instruction data, the proposed method uses self-supervision by leveraging responses from a text-only LLM to improve speech understanding. The resulting model, called Distilled Voice Assistant (DiVA), demonstrates strong performance in tasks like Spoken Question Answering, Classification, and Translation. Notably, DiVA achieves a higher user satisfaction rate while requiring significantly less computational resources compared to existing models."
                },
                "zh": {
                    "title": "创新语音助手：自我监督的语音大型语言模型",
                    "desc": "本论文提出了一种新的训练语音大型语言模型（LLMs）的方法，旨在解决传统语音助手在音频和文本建模中信息丢失的问题。我们的方法利用文本-only LLM对转录文本的响应作为自我监督，而无需使用标注数据。实验结果表明，我们的Distilled Voice Assistant（DiVA）在口语问答、分类和翻译任务中表现出色，并且在用户偏好上优于现有的最先进模型。值得注意的是，DiVA在训练计算资源上节省了超过100倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.19291",
            "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
            "url": "https://huggingface.co/papers/2409.19291",
            "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems.",
            "score": 18,
            "issue_id": 10,
            "pub_date": "2024-09-28",
            "pub_date_card": {
                "ru": "28 сентября",
                "en": "September 28",
                "zh": "9月28日"
            },
            "hash": "c79e62159ed005c9",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#benchmark",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Повышение эффективности CLIP с помощью разнообразного мультиплетного апсайклинга",
                    "desc": "В статье представлен новый подход к улучшению модели CLIP под названием Diversified Multiplet Upcycling (DMU). DMU позволяет создать серию моделей CLIP, захватывающих различные пространства признаков, путем тонкой настройки плотной предобученной модели CLIP. Эти модели затем преобразуются в CLIP-MoE с большей емкостью модели, что значительно повышает производительность при минимальных вычислительных затратах. Эксперименты показывают значительное улучшение результатов в задачах нулевого обучения и мультимодальных языковых моделях."
                },
                "en": {
                    "title": "Enhancing CLIP with Diversified Multiplet Upcycling for Better Multimodal Learning",
                    "desc": "This paper introduces Diversified Multiplet Upcycling (DMU), a novel strategy to enhance the performance of Contrastive Language-Image Pre-training (CLIP) models. DMU fine-tunes multiple CLIP models that focus on different feature spaces while sharing most parameters, except for the Feed-Forward Network (FFN). This approach transforms these models into a CLIP-Mixture of Experts (MoE), which increases model capacity and improves performance on tasks like zero-shot retrieval and image classification. The method allows for easy integration of CLIP-MoEs into existing systems, paving the way for more efficient multimodal learning."
                },
                "zh": {
                    "title": "多样化多重升级：提升CLIP模型性能的新策略",
                    "desc": "近年来，对比语言-图像预训练（CLIP）已成为多模态智能的基石。然而，研究发现CLIP编码过程中存在显著的信息损失，且CLIP往往只能捕捉输入的粗粒度特征。这一缺陷限制了单一CLIP模型处理视觉细节丰富的图像的能力。我们提出了一种简单而有效的模型无关策略——多样化多重升级（DMU），通过高效微调一系列捕捉不同特征空间的CLIP模型，显著提升了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02525",
            "title": "Contextual Document Embeddings",
            "url": "https://huggingface.co/papers/2410.02525",
            "abstract": "Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.",
            "score": 16,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "b45b19a592899862",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Контекст имеет значение: новый подход к эмбеддингам документов",
                    "desc": "Статья предлагает новый подход к созданию контекстуализированных векторных представлений документов для нейронного поиска. Авторы утверждают, что традиционные методы создания эмбеддингов документов не учитывают контекст их использования. Они предлагают два метода: новую контрастивную функцию потерь, учитывающую соседние документы, и архитектуру, кодирующую информацию о соседях в представление документа. Результаты показывают улучшение производительности по сравнению с биэнкодерами, особенно на данных вне обучающей выборки."
                },
                "en": {
                    "title": "Context Matters: Enhancing Document Embeddings with Contextualization",
                    "desc": "This paper discusses the importance of dense document embeddings in neural retrieval systems. The authors argue that traditional embeddings do not consider the context provided by neighboring documents, which can limit their effectiveness. They propose two new methods for creating contextualized document embeddings that incorporate information from related documents. Their approach shows significant improvements in retrieval performance, especially in challenging scenarios, and sets new benchmarks without relying on complex training techniques."
                },
                "zh": {
                    "title": "上下文化文档嵌入，提升检索性能！",
                    "desc": "本文探讨了密集文档嵌入在神经检索中的重要性。我们认为，现有的嵌入方法虽然有效，但在特定检索任务中缺乏上下文信息。为此，我们提出了两种方法来生成上下文化的文档嵌入，分别是通过对比学习目标和新的上下文化架构。实验结果表明，这两种方法在多个设置中均优于传统的双编码器，尤其在域外任务中表现更为突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02749",
            "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
            "url": "https://huggingface.co/papers/2410.02749",
            "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems \"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode.",
            "score": 12,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "07238a3d10e1aa04",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#plp",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "LintSeq: Улучшение генерации кода через обучение на синтетических последовательностях правок",
                    "desc": "Статья представляет новый алгоритм LintSeq для генерации синтетических данных редактирования кода. Этот метод преобразует существующие программы в последовательности правок, используя линтер для пошагового создания кода. Исследователи применили LintSeq для обучения небольших языковых моделей и сравнили их производительность с базовыми моделями на задачах синтеза кода. Результаты показывают, что модели, обученные на синтетических последовательностях правок, генерируют более разнообразные программы и достигают лучших показателей при многократном сэмплировании."
                },
                "en": {
                    "title": "Enhancing Code Synthesis with Synthetic Edit Sequences",
                    "desc": "This paper introduces LintSeq, a synthetic data generation algorithm designed to create high-quality edit sequences for code synthesis. By utilizing a linter, LintSeq refactors existing code into a series of error-free edits, producing text strings that represent program diffs. The authors demonstrate that fine-tuning smaller language models on these synthetic edit sequences leads to improved performance in generating diverse programs compared to traditional training methods. Notably, their approach allows smaller models to achieve competitive results against larger models like GPT-4, showcasing the effectiveness of using synthetic edit data for code synthesis tasks."
                },
                "zh": {
                    "title": "用LintSeq填补代码编辑数据的空白",
                    "desc": "本文提出了一种名为LintSeq的合成数据生成算法，旨在解决高质量代码编辑数据稀缺的问题。该算法通过使用linter对现有代码进行重构，生成一系列代码编辑序列，以便于程序的逐步编写。研究表明，经过LintSeq处理的小型语言模型在代码合成基准测试中表现优异，能够生成更具多样性的程序。最终，作者的150M参数模型在代码理解和合成方面的表现超过了许多参数更多的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01782",
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "url": "https://huggingface.co/papers/2410.01782",
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "score": 10,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "dbbea1382f0a84e5",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#inference",
                    "#transfer_learning",
                    "#open_source",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Open-RAG: Повышение интеллекта открытых языковых моделей",
                    "desc": "Open-RAG - это новая система для улучшения способности рассуждения у моделей с открытым исходным кодом в контексте RAG (Retrieval-Augmented Generation). Она преобразует плотную языковую модель в разреженную смесь экспертов (MoE), способную решать сложные задачи рассуждения. Open-RAG обучает модель работать с отвлекающими факторами и использует латентное обучение для динамического выбора экспертов. Система также предлагает гибридный метод адаптивного поиска для оптимизации производительности."
                },
                "en": {
                    "title": "Enhancing Reasoning in RAG with Open-RAG Framework",
                    "desc": "This paper presents Open-RAG, a new framework that improves the reasoning abilities of Retrieval-Augmented Generation (RAG) models using open-source Large Language Models (LLMs). Open-RAG transforms a dense LLM into a sparse mixture of experts (MoE) model, allowing it to tackle complex reasoning tasks more effectively. The framework is designed to help the model distinguish between relevant and misleading information, enhancing its ability to provide accurate responses. Additionally, it introduces a hybrid adaptive retrieval method to optimize the balance between performance and speed during inference."
                },
                "zh": {
                    "title": "Open-RAG：提升开源LLM推理能力的创新框架",
                    "desc": "本文介绍了一种新框架Open-RAG，旨在提高开源大型语言模型（LLMs）在检索增强生成（RAG）中的推理能力。该框架将任意稠密LLM转变为一种参数高效的稀疏专家混合模型（MoE），能够处理复杂的推理任务，包括单跳和多跳查询。Open-RAG通过训练模型识别相关但具有误导性的干扰信息，从而有效整合外部知识，提供更准确和上下文相关的回答。此外，本文还提出了一种混合自适应检索方法，以平衡性能提升与推理速度之间的权衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02115",
            "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
            "url": "https://huggingface.co/papers/2410.02115",
            "abstract": "Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM outputs is insufficient, as it is quite challenging for humans to verify the results from the extremely lengthy context. Yet, although some efforts have been made to assess whether LCMs respond truly based on the context, these works either are limited to specific tasks or heavily rely on external evaluation resources like GPT-4.In this work, we introduce L-CiteEval, a comprehensive multi-task benchmark for long-context understanding with citations, aiming to evaluate both the understanding capability and faithfulness of LCMs. L-CiteEval covers 11 tasks from diverse domains, spanning context lengths from 8K to 48K, and provides a fully automated evaluation suite. Through testing with 11 cutting-edge closed-source and open-source LCMs, we find that although these models show minor differences in their generated results, open-source models substantially trail behind their closed-source counterparts in terms of citation accuracy and recall. This suggests that current open-source LCMs are prone to responding based on their inherent knowledge rather than the given context, posing a significant risk to the user experience in practical applications. We also evaluate the RAG approach and observe that RAG can significantly improve the faithfulness of LCMs, albeit with a slight decrease in the generation quality. Furthermore, we discover a correlation between the attention mechanisms of LCMs and the citation generation process.",
            "score": 10,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "0d1ec9ec865dc20b",
            "data": {
                "categories": [
                    "#long_context",
                    "#rag",
                    "#interpretability",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "L-CiteEval: Новый стандарт оценки достоверности моделей с длинным контекстом",
                    "desc": "Статья представляет L-CiteEval - комплексный многозадачный бенчмарк для оценки понимания длинного контекста и достоверности моделей LCM (Long-context models). Бенчмарк охватывает 11 задач из разных областей с длиной контекста от 8К до 48К токенов и предоставляет полностью автоматизированный набор для оценки. Результаты тестирования 11 современных LCM показали, что модели с открытым исходным кодом существенно отстают от закрытых моделей по точности и полноте цитирования. Исследование также выявило, что подход RAG может значительно повысить достоверность LCM, хотя и с небольшим снижением качества генерации."
                },
                "en": {
                    "title": "Evaluating Long-Context Models: Faithfulness and Understanding with L-CiteEval",
                    "desc": "This paper presents L-CiteEval, a new benchmark designed to evaluate long-context models (LCMs) on their understanding and faithfulness when handling extensive text. It includes 11 diverse tasks with context lengths ranging from 8K to 48K, allowing for a comprehensive assessment of LCM performance. The study reveals that while closed-source LCMs outperform open-source models in citation accuracy and recall, the latter often rely on their pre-existing knowledge rather than the provided context. Additionally, the research highlights the effectiveness of the RAG approach in enhancing the faithfulness of LCM outputs, despite a slight trade-off in generation quality."
                },
                "zh": {
                    "title": "提升长上下文模型的理解与真实性",
                    "desc": "长上下文模型（LCMs）在处理长文本任务方面取得了显著进展，尤其是在文档摘要方面。随着对生成结果的真实性的重视，仅仅保证输出的准确性已不足以满足需求，因为人类很难验证来自极长上下文的结果。为此，我们提出了L-CiteEval，这是一个全面的多任务基准，旨在评估LCMs的理解能力和真实性。我们的研究表明，开源模型在引用准确性和召回率方面明显落后于闭源模型，这表明当前的开源LCMs更倾向于基于固有知识作答，而非依赖于给定的上下文。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02762",
            "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
            "url": "https://huggingface.co/papers/2410.02762",
            "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "8ecc2787bf47eaca",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#hallucinations",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Борьба с галлюцинациями в VLM через анализ скрытых представлений",
                    "desc": "Исследователи изучают внутренние представления моделей компьютерного зрения и обработки естественного языка (VLM) для решения проблемы галлюцинаций. Они проецируют внутренние представления изображений на языковой словарь модели и наблюдают более уверенные вероятности для реальных объектов по сравнению с галлюцинациями. На основе этого подхода авторы разрабатывают алгоритм удаления знаний, который устраняет галлюцинации путем линейной ортогонализации признаков изображения относительно признаков галлюцинируемых объектов. Результаты показывают, что целенаправленные изменения в скрытых представлениях модели могут снизить галлюцинации до 25.7% на наборе данных COCO2014 при сохранении производительности."
                },
                "en": {
                    "title": "Enhancing VLM Reliability by Reducing Hallucinations",
                    "desc": "This paper explores how vision-language models (VLMs) represent images and words to tackle the issue of hallucinations, which are incorrect outputs generated by the models. The authors find that VLMs are more confident in identifying real objects compared to hallucinated ones, and they use this insight to improve object localization. They propose a knowledge erasure algorithm that modifies the model's internal features to reduce hallucinations while maintaining overall performance. Their results indicate that by refining the latent representations of VLMs, hallucinations can be decreased significantly, leading to more reliable model outputs and new functionalities like zero-shot segmentation."
                },
                "zh": {
                    "title": "提升视觉-语言模型的可靠性与能力",
                    "desc": "我们研究了视觉-语言模型（VLMs）的内部表示，以解决幻觉问题，这在模型规模和训练进展的情况下仍然是一个持续的挑战。我们将VLMs的内部图像表示投影到其语言词汇上，观察到真实物体的输出概率比幻觉物体更有信心。我们还利用这些输出概率对真实物体进行空间定位。基于此方法，我们引入了一种知识消除算法，通过线性正交化图像特征与幻觉物体特征来消除幻觉。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02052",
            "title": "Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning",
            "url": "https://huggingface.co/papers/2410.02052",
            "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning.",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "026dc6add144373f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#training",
                    "#agi",
                    "#rl",
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "R-MCTS: Повышение эффективности ИИ-агентов через рефлексивный поиск и самообучение",
                    "desc": "Статья представляет новый алгоритм Reflective Monte Carlo Tree Search (R-MCTS), который улучшает способность ИИ-агентов исследовать пространство решений в реальном времени. R-MCTS расширяет традиционный MCTS, добавляя контрастную рефлексию и многоагентные дебаты для более эффективного поиска и оценки состояний. Исследователи также применили самообучение для дополнительной настройки модели GPT-4o, используя траектории, сгенерированные R-MCTS. Результаты показывают значительное улучшение производительности на бенчмарке VisualWebArena по сравнению с предыдущими методами."
                },
                "en": {
                    "title": "Enhancing AI Decision-Making with R-MCTS",
                    "desc": "This paper presents Reflective Monte Carlo Tree Search (R-MCTS), a new algorithm that improves the decision-making abilities of AI agents, particularly those using vision-language models like GPT-4o. R-MCTS enhances traditional Monte Carlo Tree Search by integrating contrastive reflection for learning from past experiences and employing multi-agent debate for better state evaluations. The authors demonstrate that their approach leads to significant performance improvements on the VisualWebArena benchmark, achieving up to 30% better results compared to previous models. Additionally, the fine-tuned GPT-4o retains most of the R-MCTS performance while being more efficient in terms of computational resources."
                },
                "zh": {
                    "title": "提升智能体决策能力的新方法",
                    "desc": "自主智能体在自动化复杂的多步骤决策任务中展现出显著潜力。然而，即使是最先进的视觉-语言模型（VLMs），如GPT-4o，在复杂的网络环境和长远规划任务中仍然无法达到人类水平的表现。为了解决这些问题，我们提出了一种新颖的测试时算法——反思蒙特卡洛树搜索（R-MCTS），旨在增强AI智能体的决策空间探索能力。通过对传统MCTS的扩展，R-MCTS结合了对比反思和多智能体辩论，显著提高了智能体的搜索效率和状态评估能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02458",
            "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
            "url": "https://huggingface.co/papers/2410.02458",
            "abstract": "Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://bit.ly/3zf2CVs",
            "score": 9,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "28682265fba39b78",
            "data": {
                "categories": [
                    "#science",
                    "#cv",
                    "#healthcare",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "Улучшение сегментации медицинских изображений с помощью LLM-трансформеров",
                    "desc": "Это исследование посвящено улучшению сегментации медицинских изображений с помощью интеграции предобученных блоков трансформеров из больших языковых моделей (LLM) в Vision Transformers (ViT). Авторы предлагают гибридный механизм внимания, сочетающий глобальное и локальное обучение признаков, а также блок мультимасштабного слияния. Результаты показывают значительное улучшение производительности сегментации для различных модальностей медицинской визуализации, включая повышение среднего показателя Dice с 0.74 до 0.79. Это исследование демонстрирует потенциал LLM-трансформеров для повышения точности и надежности моделей сегментации медицинских изображений."
                },
                "en": {
                    "title": "Boosting Medical Image Segmentation with LLMs and Vision Transformers",
                    "desc": "This paper investigates the use of Large Language Models (LLMs) to improve medical image segmentation, which is essential for accurate diagnostics. The authors enhance Vision Transformers (ViTs) by integrating pre-trained LLM transformer blocks, resulting in better segmentation performance. They introduce a Hybrid Attention Mechanism that effectively combines global and local feature learning, along with a Multi-Scale Fusion Block to aggregate features from different scales. The proposed model shows significant improvements in key metrics, demonstrating the potential of LLMs to enhance the accuracy and robustness of medical image segmentation tasks."
                },
                "zh": {
                    "title": "利用大型语言模型提升医学图像分割性能",
                    "desc": "本研究探讨了如何利用大型语言模型（LLMs）来提升医学图像分割的性能。我们将预训练的LLM变换器模块集成到视觉变换器（ViT）模型的编码器中，从而显著提高了不同医学成像模式下的分割效果。通过提出混合注意力机制，结合全局和局部特征学习，以及多尺度融合模块，我们实现了特征的有效聚合。实验结果显示，模型的Dice分数从0.74提高到0.79，准确率、精确率和Jaccard指数也有显著提升，证明了LLM变换器在医学图像分割中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02103",
            "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
            "url": "https://huggingface.co/papers/2410.02103",
            "abstract": "Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
            "score": 8,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "eac41a9d952f603f",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Многоракурсная оптимизация для улучшения 3D Gaussian Splatting",
                    "desc": "Эта статья представляет новый метод оптимизации для 3D Gaussian Splatting (3DGS), улучшающий рендеринг и геометрическую точность. Авторы предлагают многоракурсную стратегию обучения вместо традиционного одноракурсного подхода, что помогает избежать переобучения на отдельных ракурсах. Они также вводят схему кросс-внутренней регуляризации, стратегию кросс-лучевого уплотнения и многоракурсное аугментированное уплотнение. Эти инновации повышают общую точность реконструкции в различных сценариях и вариантах гауссовых моделей."
                },
                "en": {
                    "title": "Enhancing 3D Gaussian Splatting with Multi-View Optimization",
                    "desc": "This paper presents a new optimization method for 3D Gaussian Splatting (3DGS) to enhance rendering quality and efficiency. The authors shift from a single-view to a multi-view training strategy, which helps prevent overfitting and improves the accuracy of novel-view synthesis. They introduce a cross-intrinsic guidance scheme for a more effective training process and a cross-ray densification strategy to increase the density of Gaussian kernels in critical areas. Finally, they propose a multi-view augmented densification approach to ensure sufficient Gaussian representation, leading to better 3D reconstruction accuracy."
                },
                "zh": {
                    "title": "多视图优化提升3D渲染精度",
                    "desc": "本文提出了一种新的3D高斯点渲染优化方法，旨在解决传统单视图训练导致的过拟合问题。通过引入多视图训练策略，优化3D高斯属性，从而提高了在不同场景下的整体准确性。我们还提出了交叉内在引导方案和交叉光线稠密化策略，以增强训练过程中的细节表现。最终，采用多视图增强稠密化策略，显著提高了重建精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02763",
            "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
            "url": "https://huggingface.co/papers/2410.02763",
            "abstract": "There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understanding long-form videos. However, is this really the case? Our studies indicate that LMMs still lack many fundamental reasoning capabilities even when dealing with short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs. We demonstrate that existing LMMs severely struggle to distinguish temporal differences between different actions and object transformations. For example, the best model GPT-4o only obtains ~50% on our text and video scores, showing a large gap compared to the human baseline of ~90%. All open-source multimodal models and CLIP-based models perform much worse, producing mostly random chance performance. Through this work, we shed light onto the fact that temporal reasoning in short videos is a problem yet to be fully solved. The dataset and evaluation code are available at https://vinoground.github.io.",
            "score": 7,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "3b6bf220f7ee6708",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Большие мультимодальные модели все еще не понимают время в коротких видео",
                    "desc": "Статья посвящена оценке способности больших мультимодальных моделей (LMM) к пониманию коротких видео. Авторы представляют новый бенчмарк Vinoground, состоящий из 1000 пар видео-подпись для оценки темпорального рассуждения моделей. Результаты показывают, что даже лучшие LMM, такие как GPT-4o, значительно уступают людям в этой задаче, достигая лишь 50% точности. Исследование демонстрирует, что проблема темпорального рассуждения в коротких видео все еще далека от полного решения."
                },
                "en": {
                    "title": "Unveiling the Gaps in Video Comprehension: The Vinoground Challenge",
                    "desc": "This paper discusses the limitations of large multimodal models (LMMs) in understanding short videos, despite recent claims of their effectiveness. The authors introduce Vinoground, a benchmark designed to evaluate LMMs on their ability to reason about temporal aspects in video content. Their findings reveal that even the best-performing model, GPT-4o, only achieves around 50% accuracy in distinguishing temporal differences, significantly lower than the human baseline of approximately 90%. This highlights that the challenge of temporal reasoning in video comprehension remains unresolved, indicating a need for further research and development in this area."
                },
                "zh": {
                    "title": "短视频理解中的时间推理挑战",
                    "desc": "近年来，现代大型多模态模型（LMMs）在短视频理解方面取得了一定进展，但在理解长视频时仍面临更复杂的挑战。我们的研究表明，即使在处理短视频时，LMMs仍然缺乏基本的推理能力。我们引入了Vinoground，这是一个包含1000对短视频和自然语言描述的时间反事实评估基准。结果显示，现有的LMMs在区分不同动作和物体变化的时间差异方面表现不佳，表明短视频中的时间推理问题尚未完全解决。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02056",
            "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
            "url": "https://huggingface.co/papers/2410.02056",
            "abstract": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet.",
            "score": 6,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "e4975d472a070194",
            "data": {
                "categories": [
                    "#reasoning",
                    "#audio",
                    "#dataset",
                    "#training",
                    "#data",
                    "#alignment",
                    "#diffusion",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Синтетическое аудио для улучшения классификации при ограниченных данных",
                    "desc": "Synthio - это новый подход к расширению небольших наборов данных для классификации аудио с помощью синтетических данных. Метод использует модели диффузии текст-в-аудио (T2A) для генерации дополнительных аудиозаписей. Для обеспечения акустической согласованности с исходным набором данных применяется оптимизация предпочтений. Разнообразие генерируемых образцов достигается с помощью языковых моделей, которые создают и уточняют разнообразные описания аудио."
                },
                "en": {
                    "title": "Enhancing Audio Classification with Synthetic Data",
                    "desc": "Synthio is a new method designed to enhance small audio classification datasets by adding synthetic data. It aims to improve classification accuracy when there is limited labeled data available. Unlike traditional augmentation techniques that may not capture real-world audio diversity, Synthio uses text-to-audio diffusion models to generate more representative synthetic audio. The approach includes aligning generated audio with existing data and using advanced caption generation to ensure both acoustic consistency and compositional diversity."
                },
                "zh": {
                    "title": "Synthio：用合成数据提升音频分类准确性",
                    "desc": "本文介绍了一种名为Synthio的新方法，用于通过合成数据增强小规模音频分类数据集。我们的目标是提高在有限标记数据下的音频分类准确性。传统的数据增强技术难以生成真实世界音频的多样性，因此我们采用文本到音频（T2A）扩散模型生成合成音频来解决这个问题。通过优化生成偏好，我们确保生成的数据在声学特性上与小规模数据集一致，同时利用大型语言模型生成多样化的音频描述，以提高合成数据的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01335",
            "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
            "url": "https://huggingface.co/papers/2410.01335",
            "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "5828b5e66fffb240",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#training",
                    "#math",
                    "#interpretability",
                    "#transfer_learning",
                    "#benchmark",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Слияние моделей для переноса математических навыков между языками",
                    "desc": "Эта статья представляет методологию слияния моделей для улучшения математических рассуждений на языках, для которых нет специализированных данных. Авторы обучают отдельные модели на математических задачах на английском и на общих инструкциях на целевом языке. Затем они заменяют верхние и нижние слои трансформера математической модели слоями языковой модели. Этот метод превосходит другие подходы на 10% в четырех основных языках на бенчмарке MGSM."
                },
                "en": {
                    "title": "Enhancing Math Performance in Non-English LLMs through Layer Swapping",
                    "desc": "This paper introduces a novel approach to model merging, specifically for Large Language Models (LLMs) that need to perform mathematical reasoning in non-English languages. The authors propose a method where two separate 'expert' models are fine-tuned: one on math data in English and the other on general instruction data in the target language. By swapping the transformer layers between these experts, they enhance the math performance of the model in the target language without requiring additional training data. The results show a significant improvement in performance on the math benchmark, demonstrating the effectiveness of this layer swapping technique for cross-lingual transfer and modular model solutions."
                },
                "zh": {
                    "title": "跨语言模型合并，提升数学推理能力",
                    "desc": "本文提出了一种模型合并方法，旨在解决在非英语语言中微调大型语言模型（LLMs）的困难。我们通过将数学能力与语言能力结合，促进跨语言转移，尤其是在缺乏特定任务数据的情况下。我们从同一预训练模型开始，分别在英语的数学指令数据和目标语言的通用指令数据上微调不同的“专家”。通过直接替换数学专家的顶部和底部变换器层，我们的合并模型在数学基准测试中表现优于单独的专家和其他合并方法，提升了10%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02536",
            "title": "Intelligence at the Edge of Chaos",
            "url": "https://huggingface.co/papers/2410.02536",
            "abstract": "We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "b962196b43ec4ddd",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agi",
                    "#training",
                    "#rl",
                    "#games",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Сложность порождает интеллект: уроки клеточных автоматов",
                    "desc": "В этой работе исследуется связь между сложностью правил клеточных автоматов и интеллектуальными способностями обученных на них языковых моделей. Авторы обнаружили, что правила с более высокой сложностью приводят к моделям, демонстрирующим большие интеллектуальные способности на задачах рассуждения и предсказания шахматных ходов. Простые однородные и периодические системы, а также сильно хаотичные системы давали худшие результаты. Исследователи предполагают, что интеллект возникает из способности предсказывать сложность."
                },
                "en": {
                    "title": "Complexity Fuels Intelligence in AI Models",
                    "desc": "This paper investigates how the complexity of rule-based systems affects the intelligence of models trained to predict these rules. It specifically examines elementary cellular automata (ECA), which are simple one-dimensional systems that can produce a wide range of behaviors. By training Large Language Models (LLMs) on various ECAs, the study finds that more complex rules lead to better performance in tasks requiring reasoning and prediction. The results suggest that there is an optimal level of complexity that enhances model intelligence, indicating that exposure to complexity may be key to developing intelligent systems."
                },
                "zh": {
                    "title": "复杂性与智能的关系",
                    "desc": "本研究探讨了人工系统中智能行为的出现，重点分析了基于规则的系统复杂性如何影响模型的预测能力。我们使用简单而强大的初等元胞自动机（ECA）作为研究对象，这些系统能够生成从简单到复杂的多种行为。通过对不同ECA训练大型语言模型（LLM），我们评估了规则行为的复杂性与LLM表现出的智能之间的关系。研究结果表明，复杂性较高的规则使得模型在推理和国际象棋走法预测任务中表现更好，暗示智能的产生与对复杂性的预测能力密切相关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.00255",
            "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
            "url": "https://huggingface.co/papers/2410.00255",
            "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "537bc5070d162d76",
            "data": {
                "categories": [
                    "#security",
                    "#training",
                    "#agi",
                    "#data",
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#architecture",
                    "#synthetic",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Robin3D: Революция в понимании 3D-инструкций искусственным интеллектом",
                    "desc": "В этой статье представлен Robin3D - мощная трехмерная языковая модель (3DLLM), обученная на масштабном наборе данных для выполнения инструкций. Данные были сгенерированы с помощью нового механизма RIG, который создает как состязательные, так и разнообразные инструкции для улучшения дискриминативной способности и обобщения модели. Robin3D использует улучшенные методы пространственного понимания и связывания объектов. Модель превосходит предыдущие методы на пяти широко используемых бенчмарках мультимодального обучения в 3D, без необходимости дообучения под конкретные задачи."
                },
                "en": {
                    "title": "Robin3D: Elevating 3D Language Models with Robust Instruction Data",
                    "desc": "This paper presents Robin3D, an advanced 3D Large Language Model (3DLLM) designed to improve instruction-following capabilities in 3D environments. The model is trained using a novel data generation engine called Robust Instruction Generation (RIG), which creates high-quality instruction data, including adversarial and diverse samples. By incorporating techniques like Relation-Augmented Projector and ID-Feature Bonding, Robin3D enhances its spatial understanding and object grounding abilities. The results show significant performance improvements over previous models on multiple 3D multimodal learning benchmarks, demonstrating its effectiveness without requiring task-specific fine-tuning."
                },
                "zh": {
                    "title": "Robin3D：提升3D语言模型的指令理解能力",
                    "desc": "本文介绍了一种新的3D大型语言模型Robin3D，该模型通过我们的新数据引擎RIG生成的大规模指令跟随数据进行训练。RIG生成了两种关键的指令数据：对抗性指令跟随数据和多样性指令跟随数据，以增强模型的区分能力和泛化能力。Robin3D在处理复杂指令时，采用了关系增强投影器和ID特征绑定技术，提升了空间理解和物体引用能力。实验结果表明，Robin3D在多个3D多模态学习基准测试中表现优异，显著提高了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02426",
            "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
            "url": "https://huggingface.co/papers/2410.02426",
            "abstract": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella \"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples.",
            "score": 5,
            "issue_id": 10,
            "pub_date": "2024-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "8713f059b61a607f",
            "data": {
                "categories": [
                    "#reasoning",
                    "#hallucinations",
                    "#training",
                    "#games",
                    "#small_models"
                ],
                "emoji": "♟️",
                "ru": {
                    "title": "Малые языковые модели осваивают шахматы через обучение на примерах",
                    "desc": "Исследователи показали, что малые языковые модели (SLM) с миллионами параметров способны изучать скрытые правила процессов на основе связанных с ними данных. Эксперимент, вдохновленный новеллой Стефана Цвейга 'Шахматная новелла', продемонстрировал, что модели с 28 и 125 миллионами параметров могут обучиться правилам шахмат, предлагать легальные ходы и решать шахматные задачи после файнтюнинга на 1000-1000000 примерах. Изучено влияние последовательных эпох обучения на улучшение результатов. Также показано, как увеличение числа обучающих примеров снижает галлюцинации модели."
                },
                "en": {
                    "title": "Small Models, Big Moves: Learning Chess with Language Models",
                    "desc": "This paper shows that small pretrained generative language models, with millions of parameters, can effectively learn the underlying rules of a specific process, such as chess, from provided data. By fine-tuning these models with varying amounts of examples, they can generate legal chess moves and solve chess problems accurately. The study highlights the importance of the number of fine-tuning epochs, showing that more training leads to better performance. Additionally, it finds that increasing the number of examples reduces the occurrence of model hallucinations, improving the reliability of the generated outputs."
                },
                "zh": {
                    "title": "小型语言模型的潜力：学习国际象棋规则",
                    "desc": "这篇论文展示了小型预训练生成语言模型如何从与特定过程相关的数据中学习潜在规则。研究表明，具有2800万和1.25亿参数的小型语言模型可以通过指令微调，利用1000到1000000个示例学习国际象棋的规则，提出合法的走法，并准确解决棋局问题。论文还探讨了连续微调周期对模型性能的影响，并通过增加微调示例的数量来减少模型的幻觉现象。总的来说，这项研究表明小型语言模型在特定任务上的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01946",
            "title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
            "url": "https://huggingface.co/papers/2410.01946",
            "abstract": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.",
            "score": 4,
            "issue_id": 10,
            "pub_date": "2024-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "6a0f04b3d6aec2b3",
            "data": {
                "categories": [
                    "#science",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#transfer_learning",
                    "#low_resource"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Автоматическое обогащение вербализатора для точной классификации научных текстов",
                    "desc": "Статья представляет SciPrompt - фреймворк для автоматического извлечения терминов, связанных с научными темами, для задач классификации текста с ограниченными ресурсами. Авторы предлагают новую стратегию вербализации, использующую оценки корреляции в качестве дополнительных весов для улучшения производительности языковой модели. Метод превосходит современные методы тонкой настройки на основе промптов при классификации научных текстов в условиях малого количества примеров и нулевого обучения. Особенно эффективен при классификации детальных и новых научных тем."
                },
                "en": {
                    "title": "Automating Domain-Specific Labeling for Better Text Classification",
                    "desc": "This paper presents SciPrompt, a framework that enhances prompt-based fine-tuning for low-resource multi-class text classification tasks, particularly in scientific domains. It addresses the challenge of manually selecting domain-specific label terms for verbalizers by automatically retrieving relevant terms from scientific literature. The proposed method uses correlation scores to weight these terms, improving the model's prediction performance during tuning. SciPrompt demonstrates superior results compared to existing prompt-based methods, especially for fine-grained and emerging scientific topics in few and zero-shot scenarios."
                },
                "zh": {
                    "title": "SciPrompt：自动化科学文本分类的新方法",
                    "desc": "本文介绍了一种名为SciPrompt的框架，旨在自动检索与科学主题相关的术语，以应对低资源文本分类任务。通过选择语义相关且特定领域的标签术语，SciPrompt增强了提示模板的效果。我们提出了一种新的表述策略，利用相关性得分作为额外权重，提高语言模型的预测性能。实验结果表明，该方法在少样本和零样本设置下，尤其在细粒度和新兴科学主题的分类任务中，优于现有的提示基础微调方法。"
                }
            }
        }
    ],
    "link_prev": "2024-10-03.html",
    "link_next": "2024-10-07.html",
    "link_month": "2024-10.html",
    "short_date_prev": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10月3日"
    },
    "short_date_next": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10月7日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 6,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 9,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 1,
        "#inference": 3,
        "#3d": 2,
        "#audio": 2,
        "#video": 4,
        "#multimodal": 8,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 18,
        "#healthcare": 1,
        "#training": 21,
        "#robotics": 0,
        "#agi": 3,
        "#games": 3,
        "#interpretability": 5,
        "#reasoning": 10,
        "#transfer_learning": 7,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 12,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 6,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 3,
        "#science": 2,
        "#low_resource": 2
    }
}