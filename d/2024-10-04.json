{
    "date": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "issue_id": 9,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.00907",
            "title": "Addition is All You Need for Energy-efficient Language Models",
            "url": "https://huggingface.co/papers/2410.00907",
            "abstract": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.",
            "score": 81,
            "issue_id": 1,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "L-Mul: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ L-Mul Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡ĞµĞ¼ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ L-Mul Ğ² Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 95% Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ½Ğ° 80% Ğ´Ğ»Ñ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ L-Mul Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Precision: Revolutionizing Neural Computation with L-Mul",
                    "desc": "This paper introduces the L-Mul algorithm, which approximates floating point multiplications using integer additions, significantly reducing computational resources while maintaining high precision. The method is particularly energy-efficient, potentially reducing energy costs by up to 95% for element-wise tensor multiplications. Theoretical and experimental evaluations demonstrate that L-Mul with a 4-bit mantissa achieves precision comparable to traditional 8-bit floating point operations. Applying L-Mul in transformer models shows almost no loss in precision, making it a promising alternative for efficient neural network computations."
                },
                "zh": {
                    "title": "ç”¨æ•´æ•°åŠ æ³•é©æ–°æµ®ç‚¹æ•°ä¹˜æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•L-Mulï¼Œå¯ä»¥ç”¨æ•´æ•°åŠ æ³•æ¥è¿‘ä¼¼æµ®ç‚¹æ•°ä¹˜æ³•ï¼Œä»è€Œå¤§å¤§å‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚ä¸ä¼ ç»Ÿçš„8ä½æµ®ç‚¹æ•°ä¹˜æ³•ç›¸æ¯”ï¼ŒL-Mulä¸ä»…è®¡ç®—ç²¾åº¦æ›´é«˜ï¼Œè€Œä¸”èƒ½æ˜¾è‘—é™ä½èƒ½è€—ã€‚å®éªŒè¡¨æ˜ï¼ŒL-Mulåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œå¸¸è¯†é—®ç­”ç­‰é¢†åŸŸã€‚é€šè¿‡å°†L-Mulåº”ç”¨äºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡ ä¹æ²¡æœ‰ç²¾åº¦æŸå¤±ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            },
            "hash": "2e806ebbdf3cc22d"
        },
        {
            "id": "https://huggingface.co/papers/2410.02613",
            "title": "NL-Eye: Abductive NLI for Images",
            "url": "https://huggingface.co/papers/2410.02613",
            "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",
            "score": 14,
            "issue_id": 4,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "NL-Eye: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NL-Eye - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. NL-Eye Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ»ÑĞ´ÑĞ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. NL-Eye Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ VLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing VLMs for Real-World Reasoning",
                    "desc": "The paper introduces NL-Eye, a benchmark designed to test the visual abductive reasoning skills of Visual Language Models (VLMs). It adapts the Natural Language Inference task to the visual domain, requiring models to assess the plausibility of images based on a given premise. The study finds that current VLMs struggle with this task, often performing no better than random chance, while humans excel. This highlights a significant gap in the reasoning abilities of VLMs, suggesting the need for further development to enable real-world applications like accident prevention."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼šNL-Eyeçš„æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ¨ç†å› æœå…³ç³»æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåä¸ºNL-Eyeçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°VLMçš„è§†è§‰æº¯å› æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„VLMåœ¨NL-Eyeä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸åªèƒ½è¾¾åˆ°éšæœºåŸºçº¿æ°´å¹³ï¼Œè€Œäººç±»åœ¨å¯è¡Œæ€§é¢„æµ‹å’Œè§£é‡Šè´¨é‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¿™è¡¨æ˜ç°ä»£VLMåœ¨æº¯å› æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚"
                }
            },
            "hash": "315844ac62249b1b"
        },
        {
            "id": "https://huggingface.co/papers/2410.02703",
            "title": "Selective Attention Improves Transformer",
            "url": "https://huggingface.co/papers/2410.02703",
            "abstract": "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (Selective Attention) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±ÑƒÑ„ĞµÑ€Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğº Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Selective Attention: Streamlining Efficiency in Language Models",
                    "desc": "The paper introduces Selective Attention, a modification to the standard attention mechanism in machine learning models that filters out unnecessary elements, enhancing performance. This approach improves language modeling across various model sizes and context lengths, making models as effective as those with twice the number of attention heads and parameters. Selective Attention also significantly reduces memory and computational requirements during inference by decreasing the size of the attention's context buffer. This innovation allows models to maintain the same level of accuracy while using substantially less memory, making them more efficient."
                },
                "zh": {
                    "title": "é€‰æ‹©æ€§æ³¨æ„åŠ›ï¼šæå‡æ€§èƒ½ï¼Œå‡å°‘èµ„æºæ¶ˆè€—",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºé€‰æ‹©æ€§æ³¨æ„åŠ›çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥å‡å°‘æ³¨æ„åŠ›æœºåˆ¶ä¸­ä¸å¿…è¦å…ƒç´ çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè¯­è¨€æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¸‹éƒ½å¾—åˆ°äº†æå‡ã€‚é€‰æ‹©æ€§æ³¨æ„åŠ›ä½¿å¾—æ¨¡å‹åœ¨ç›¸åŒçš„éªŒè¯å›°æƒ‘åº¦ä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨é€‰æ‹©æ€§æ³¨æ„åŠ›çš„å˜å‹å™¨æ¨¡å‹åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­éœ€è¦çš„å†…å­˜æ¯”ä¼ ç»Ÿæ–¹æ³•å°‘å¾—å¤šã€‚"
                }
            },
            "hash": "fd0e4fdfc820e20f"
        },
        {
            "id": "https://huggingface.co/papers/2410.01699",
            "title": "Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding",
            "url": "https://huggingface.co/papers/2410.01699",
            "abstract": "The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel decoding algorithm, has been used to accelerate the auto-regressive generation and can be executed without training. However, the Jacobi decoding relies on a deterministic criterion to determine the convergence of iterations. Thus, it works for greedy decoding but is incompatible with sampling-based decoding which is crucial for visual quality and diversity in the current auto-regressive text-to-image generation. In this paper, we propose a training-free probabilistic parallel decoding algorithm, Speculative Jacobi Decoding (SJD), to accelerate auto-regressive text-to-image generation. By introducing a probabilistic convergence criterion, our SJD accelerates the inference of auto-regressive text-to-image generation while maintaining the randomness in sampling-based token decoding and allowing the model to generate diverse images. Specifically, SJD facilitates the model to predict multiple tokens at each step and accepts tokens based on the probabilistic criterion, enabling the model to generate images with fewer steps than the conventional next-token-prediction paradigm. We also investigate the token initialization strategies that leverage the spatial locality of visual data to further improve the acceleration ratio under specific scenarios. We conduct experiments for our proposed SJD on multiple auto-regressive text-to-image generation models, showing the effectiveness of model acceleration without sacrificing the visual quality.",
            "score": 11,
            "issue_id": 7,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#inference",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Speculative Jacobi Decoding (SJD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. SJD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑˆĞ°Ğ³ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SJD Ğ² ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "\"Faster Images, Same Quality: Meet Speculative Jacobi Decoding!\"",
                    "desc": "The paper introduces Speculative Jacobi Decoding (SJD), a new algorithm to speed up the process of generating images from text using large auto-regressive models. Unlike previous methods, SJD uses a probabilistic approach to decide when to stop iterating, which allows it to work well with sampling-based decoding that is important for creating diverse and high-quality images. This method lets the model predict several tokens at once, reducing the number of steps needed to generate an image. Experiments show that SJD can make the image generation process faster without losing the quality of the images."
                },
                "zh": {
                    "title": "æ¨æµ‹é›…å¯æ¯”è§£ç ï¼šåŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "å½“å‰çš„å¤§å‹è‡ªå›å½’æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œä½†éœ€è¦å¤§é‡çš„æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œè€—æ—¶è¾ƒé•¿ã€‚ç°æœ‰çš„é›…å¯æ¯”è§£ç ç®—æ³•å¯ä»¥åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆï¼Œä½†ä¸é€‚ç”¨äºåŸºäºé‡‡æ ·çš„è§£ç ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨æµ‹é›…å¯æ¯”è§£ç ç®—æ³•ï¼Œé€šè¿‡å¼•å…¥æ¦‚ç‡æ”¶æ•›æ ‡å‡†ï¼ŒåŠ é€Ÿç”Ÿæˆè¿‡ç¨‹å¹¶ä¿æŒå›¾åƒçš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨ä¸å½±å“è§†è§‰è´¨é‡çš„æƒ…å†µä¸‹æœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚"
                }
            },
            "hash": "e4be41b1e418b1b8"
        },
        {
            "id": "https://huggingface.co/papers/2410.03017",
            "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
            "url": "https://huggingface.co/papers/2410.03017",
            "abstract": "Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#rlhf",
                    "#multimodal",
                    "#medicine"
                ],
                "emoji": "ğŸ¤–ğŸ‘¨â€ğŸ«",
                "ru": {
                    "title": "Tutor CoPilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Tutor CoPilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 900 Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ 1800 ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ² Ğ±Ñ‹Ğ»Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ²ÑˆĞ¸Ğµ Ñ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Tutor CoPilot, Ğ½Ğ° 4 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ñ‡Ğ°Ñ‰Ğµ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¼Ñ‹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· 550 000+ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€Ñ‹ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Tutor CoPilot Ñ‡Ğ°Ñ‰Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµĞ´Ğ°Ğ³Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº-Ğ˜Ğ˜ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑ‡Ğ°Ñ‰Ğ¸Ñ…ÑÑ."
                },
                "en": {
                    "title": "Scaling Expertise: AI-Powered Tutoring for All",
                    "desc": "The paper introduces Tutor CoPilot, a Human-AI system designed to provide expert-like guidance to tutors, enhancing the quality of education for students, especially in under-served communities. In a large-scale trial involving 900 tutors and 1,800 students, the system improved student mastery of topics by 4 percentage points, with the most significant gains seen in students taught by lower-rated tutors. The study highlights the cost-effectiveness of Tutor CoPilot, costing only $20 per tutor annually, and its ability to encourage tutors to use effective teaching strategies. Despite some issues with grade-level appropriateness, Tutor CoPilot demonstrates the potential of AI to scale expertise and improve educational outcomes."
                },
                "zh": {
                    "title": "Tutor CoPilotï¼šè®©ä¼˜è´¨æ•™è‚²è§¦æ‰‹å¯åŠ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTutor CoPilotçš„åˆ›æ–°äººæœºåä½œç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿä¸“å®¶æ€ç»´ä¸ºè¾…å¯¼å‘˜æä¾›ç±»ä¼¼ä¸“å®¶çš„æŒ‡å¯¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨Tutor CoPilotçš„è¾…å¯¼å‘˜èƒ½æ›´æœ‰æ•ˆåœ°å¸®åŠ©å­¦ç”ŸæŒæ¡çŸ¥è¯†ï¼Œå°¤å…¶æ˜¯é‚£äº›è¯„åˆ†è¾ƒä½çš„è¾…å¯¼å‘˜ï¼Œå…¶å­¦ç”Ÿçš„çŸ¥è¯†æŒæ¡ç‡æé«˜äº†9ä¸ªç™¾åˆ†ç‚¹ã€‚Tutor CoPilotæ¯å¹´æ¯ä½è¾…å¯¼å‘˜çš„æˆæœ¬ä»…ä¸º20ç¾å…ƒï¼Œå¹¶ä¸”èƒ½ä¿ƒä½¿è¾…å¯¼å‘˜é‡‡ç”¨æ›´é«˜è´¨é‡çš„æ•™å­¦ç­–ç•¥ã€‚å°½ç®¡Tutor CoPilotåœ¨æŸäº›æƒ…å†µä¸‹ä¼šç”Ÿæˆä¸é€‚åˆå¹´çº§çš„å»ºè®®ï¼Œä½†æ€»ä½“ä¸Šå®ƒå±•ç¤ºäº†äººæœºåä½œç³»ç»Ÿåœ¨æ•™è‚²é¢†åŸŸæ‰©å±•ä¸“ä¸šçŸ¥è¯†çš„æ½œåŠ›ã€‚"
                }
            },
            "hash": "72b9d001759c0a5d"
        },
        {
            "id": "https://huggingface.co/papers/2409.19989",
            "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models",
            "url": "https://huggingface.co/papers/2409.19989",
            "abstract": "Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.",
            "score": 8,
            "issue_id": 2,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ‘ĞµÑÑˆĞ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ² 3D",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… ÑˆĞ²Ğ¾Ğ² Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ 2D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SDXL Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ControlNet, Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ğ¸Ğ½Ğ¿Ğ°Ğ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆĞ²Ğ¾Ğ²."
                },
                "en": {
                    "title": "Seamless Textures, Perfectly Aligned: A New Era in Text-to-Texture Generation",
                    "desc": "The paper introduces a new method for generating textures from text descriptions that align well with 3D meshes, addressing common issues like view inconsistencies and seams. It uses advanced 2D diffusion models, such as SDXL and ControlNets, to capture detailed structural features in textures. The approach includes a symmetrical view synthesis strategy and regional prompts to improve view consistency. Novel techniques like texture blending and soft-inpainting are employed to minimize seam regions, showing superior performance over existing methods."
                },
                "zh": {
                    "title": "æ— ç¼å¯¹é½ï¼šé©æ–°æ–‡æœ¬åˆ°çº¹ç†ç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°çº¹ç†ç”Ÿæˆæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­è§†å›¾ä¸ä¸€è‡´ã€æ˜æ˜¾æ¥ç¼å’Œçº¹ç†ä¸ç½‘æ ¼ä¸å¯¹é½çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ€å…ˆè¿›çš„2Dæ‰©æ•£æ¨¡å‹ï¼Œå¦‚SDXLå’Œå¤šä¸ªControlNetsï¼Œæ¥æ•æ‰ç”Ÿæˆçº¹ç†ä¸­çš„ç»“æ„ç‰¹å¾å’Œå¤æ‚ç»†èŠ‚ã€‚é€šè¿‡å¯¹ç§°è§†å›¾åˆæˆç­–ç•¥å’ŒåŒºåŸŸæç¤ºï¼Œå¢å¼ºäº†è§†å›¾ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ–°çš„çº¹ç†æ··åˆå’Œè½¯ä¿®è¡¥æŠ€æœ¯ï¼Œæ˜¾è‘—å‡å°‘äº†æ¥ç¼åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            },
            "hash": "d0d2a72fcf8c7f62"
        },
        {
            "id": "https://huggingface.co/papers/2410.02362",
            "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
            "url": "https://huggingface.co/papers/2410.02362",
            "abstract": "Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.",
            "score": 7,
            "issue_id": 3,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#survey",
                    "#medicine",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Mamba: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Mamba, ÑĞ²Ğ»ÑÑÑÑŒ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (State Space Model), Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Mamba ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mamba, Ğ¸Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Mamba: Revolutionizing Medical Imaging with Efficiency and Precision",
                    "desc": "Mamba is a type of State Space Model that offers a more efficient alternative to traditional deep learning methods like transformers in medical image analysis. Unlike transformers, which struggle with high computational demands and long-range dependencies, Mamba operates with linear time complexity, making it faster and more memory-efficient. It excels in processing long sequences without attention mechanisms and effectively merges multimodal data, enhancing diagnostic accuracy. The paper details various Mamba architectures and optimizations, highlighting its potential to revolutionize medical imaging by addressing current limitations."
                },
                "zh": {
                    "title": "Mambaï¼šåŒ»å­¦å›¾åƒåˆ†æçš„æ–°çªç ´",
                    "desc": "Mambaæ˜¯ä¸€ç§ç‰¹æ®Šçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­é€æ¸æˆä¸ºæ¨¡æ¿åŒ–æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸å˜å‹å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒMambaå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ï¼Œå‡å°‘å†…å­˜éœ€æ±‚ã€‚Mambaåœ¨å¤šæ¨¡æ€æ•°æ®èåˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œæ‚£è€…æ²»ç–—æ•ˆæœã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†Mambaåœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å…¶æ¶æ„ã€ä¼˜åŒ–æŠ€æœ¯åŠæœªæ¥å‘å±•æ–¹å‘ã€‚"
                }
            },
            "hash": "dd4bc5254b2493c9"
        },
        {
            "id": "https://huggingface.co/papers/2410.02760",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "url": "https://huggingface.co/papers/2410.02760",
            "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "score": 7,
            "issue_id": 1,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "ELM: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… - Erasure of Language Memory (ELM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…: Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ELM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ¸Ñ€Ğ°ĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Erase with Grace: ELM's Breakthrough in Concept Erasure",
                    "desc": "The paper introduces a new evaluation framework for concept erasure in language models, focusing on innocence, seamlessness, and specificity. It presents Erasure of Language Memory (ELM), a method that uses low-rank updates to effectively erase concepts while maintaining the model's fluency and performance on unrelated tasks. ELM is tested on various domains, showing superior results in erasing specific topics without affecting other capabilities. The method also demonstrates robustness against adversarial attacks, ensuring reliable performance."
                },
                "zh": {
                    "title": "è¯­è¨€æ¨¡å‹ä¸­çš„æ¦‚å¿µæ¶ˆé™¤æ–°æ ‡å‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­æ¦‚å¿µæ¶ˆé™¤çš„æ•ˆæœã€‚è¯¥æ¡†æ¶åŸºäºä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼šæ— å®³æ€§ï¼ˆå®Œå…¨çŸ¥è¯†ç§»é™¤ï¼‰ã€æ— ç¼æ€§ï¼ˆä¿æŒæ¡ä»¶æµç•…ç”Ÿæˆï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆä¿ç•™æ— å…³ä»»åŠ¡æ€§èƒ½ï¼‰ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ç§åä¸ºè¯­è¨€è®°å¿†æ¶ˆé™¤ï¼ˆELMï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä½ç§©æ›´æ–°æ¥æ”¹å˜è¢«æ¶ˆé™¤æ¦‚å¿µçš„è¾“å‡ºåˆ†å¸ƒï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒELMåœ¨ç”Ÿç‰©å®‰å…¨ã€ç½‘ç»œå®‰å…¨å’Œæ–‡å­¦é¢†åŸŸçš„æ¶ˆé™¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            },
            "hash": "018d36385dbb4dea"
        },
        {
            "id": "https://huggingface.co/papers/2410.02241",
            "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
            "url": "https://huggingface.co/papers/2410.02241",
            "abstract": "Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing a single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, a single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, a novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose a novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As a result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct a comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "MIGA: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ñ‹Ğ½ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "MIGA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ñ‹Ğ½ĞºĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, MIGA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ°ĞºÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ÑƒĞ¼Ğ°Ğ³. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ‚Ñ‚ĞµĞ½Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. MIGA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ñ„Ğ¾Ğ½Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 24% Ğ³Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ½Ğ´ĞµĞºÑĞµ CSI300."
                },
                "en": {
                    "title": "\"MIGA: Tailored Expertise for Smarter Stock Predictions\"",
                    "desc": "The paper introduces MIGA, a new framework for stock market prediction that uses a Mixture of Expert with Group Aggregation approach. Unlike traditional models that apply a single model to all stocks, MIGA dynamically switches between specialized experts tailored to different stock styles. This method enhances prediction accuracy by allowing experts to share information through an inner group attention mechanism. MIGA demonstrates superior performance on Chinese Stock Index benchmarks, achieving a notable 24% excess annual return on the CSI300 benchmark."
                },
                "zh": {
                    "title": "MIGAï¼šè‚¡ç¥¨é¢„æµ‹çš„é£æ ¼ä¸“å®¶æ··åˆæ¡†æ¶",
                    "desc": "è‚¡ç¥¨å¸‚åœºé¢„æµ‹ä¸€ç›´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå¸‚åœºçš„é«˜æ³¢åŠ¨æ€§å’Œä¿¡æ¯å™ªå£°æ¯”ä½ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸ä½¿ç”¨å•ä¸€æ¨¡å‹æ¥é¢„æµ‹æ‰€æœ‰ç±»å‹çš„è‚¡ç¥¨ï¼Œä½†ç”±äºè‚¡ç¥¨é£æ ¼å’Œå¸‚åœºè¶‹åŠ¿çš„å·®å¼‚ï¼Œè¿™ç§æ–¹æ³•çš„å‡†ç¡®æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸“å®¶æ··åˆæ¡†æ¶MIGAï¼Œé€šè¿‡åŠ¨æ€åˆ‡æ¢ä¸åŒé£æ ¼çš„ä¸“å®¶æ¥ç”Ÿæˆé’ˆå¯¹ä¸åŒé£æ ¼è‚¡ç¥¨çš„é¢„æµ‹ã€‚MIGAåœ¨å¤šä¸ªä¸­å›½è‚¡ç¥¨æŒ‡æ•°åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨CSI300åŸºå‡†ä¸Šå®ç°äº†24%çš„è¶…é¢å¹´å›æŠ¥ç‡ã€‚"
                }
            },
            "hash": "9cfda67b2beb4586"
        },
        {
            "id": "https://huggingface.co/papers/2410.01273",
            "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
            "url": "https://huggingface.co/papers/2410.01273",
            "abstract": "Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.",
            "score": 4,
            "issue_id": 4,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#rl",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "CANVAS: Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ¼",
                    "desc": "CANVAS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ COMMAND Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. CANVAS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ROS NavStack Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Guiding Robots with Human-Like Understanding",
                    "desc": "The paper introduces CANVAS, a framework that helps robots navigate using both visual and linguistic instructions, making them more aligned with human expectations. It uses imitation learning to teach robots how to interpret abstract human commands by learning from human navigation behavior. The researchers created a dataset called COMMAND to train these systems, showing that CANVAS performs better than traditional rule-based systems, especially in challenging environments. The framework also demonstrates strong Sim2Real transfer, meaning it works well in real-world scenarios after being trained in simulations."
                },
                "zh": {
                    "title": "CANVASï¼šè®©æœºå™¨äººå¯¼èˆªæ›´è´´è¿‘äººç±»å¸¸è¯†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCANVASçš„æ–°æ¡†æ¶ï¼Œå®ƒç»“åˆè§†è§‰å’Œè¯­è¨€æŒ‡ä»¤æ¥å®ç°å¸¸è¯†æ„ŸçŸ¥çš„å¯¼èˆªã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥ä»äººç±»çš„å¯¼èˆªè¡Œä¸ºä¸­å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒCANVASåœ¨æ‰€æœ‰ç¯å¢ƒä¸­éƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨å™ªå£°æŒ‡ä»¤ä¸‹è¡¨ç°å‡ºè‰²ã€‚åœ¨çœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼ŒCANVASå±•ç¤ºäº†ä»æ¨¡æ‹Ÿç¯å¢ƒåˆ°ç°å®ç¯å¢ƒçš„å‡ºè‰²è½¬ç§»èƒ½åŠ›ã€‚"
                }
            },
            "hash": "db64c057695506e6"
        },
        {
            "id": "https://huggingface.co/papers/2410.03535",
            "title": "NRGBoost: Energy-Based Generative Boosted Trees",
            "url": "https://huggingface.co/papers/2410.03535",
            "abstract": "Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling.",
            "score": 3,
            "issue_id": 4,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑÑ‚Ğ¸Ğ½Ğ³: Ğ¼Ğ¾Ñ‰ÑŒ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Random Forests Ğ¸ Gradient Boosted Decision Trees, Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ±ÑƒÑÑ‚Ğ¸Ğ½Ğ³Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑÑ‚Ğ¸Ğ½Ğ³Ñƒ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ² XGBoost. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ĞºĞ°Ğº Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº."
                },
                "en": {
                    "title": "Boosting Trees Beyond Boundaries: Generative Power Unleashed",
                    "desc": "This paper explores how tree-based methods like Random Forests and Gradient Boosted Decision Trees can be extended to generative models, which can model data density and perform tasks beyond just classification or regression. The authors introduce an energy-based generative boosting algorithm that mirrors the second-order boosting used in popular tools like XGBoost. Their proposed method not only matches the discriminative performance of traditional GBDT on real-world tabular data but also competes well with neural networks in generating samples. This approach offers a versatile model that can handle both inference and sampling tasks effectively."
                },
                "zh": {
                    "title": "æ ‘æ¨¡å‹çš„ç”Ÿæˆå¼æ–°çªç ´ï¼šèƒ½é‡æå‡ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡å†³ç­–æ ‘ç­‰æ ‘æ¨¡å‹æ‰©å±•ä¸ºç”Ÿæˆæ¨¡å‹ï¼Œé‡ç‚¹åœ¨äºæ˜¾å¼å»ºæ¨¡æ•°æ®å¯†åº¦ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºèƒ½é‡çš„ç”Ÿæˆæå‡ç®—æ³•ï¼Œä¸XGBoostä¸­çš„äºŒé˜¶æå‡ç±»ä¼¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸GBDTç›¸ä¼¼çš„åˆ¤åˆ«æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–ç”Ÿæˆæ–¹æ³•ã€‚åŒæ—¶ï¼Œå®ƒåœ¨é‡‡æ ·ä»»åŠ¡ä¸­ä¹Ÿèƒ½ä¸ç¥ç»ç½‘ç»œæ¨¡å‹ç«äº‰ã€‚"
                }
            },
            "hash": "e71078964f133d48"
        },
        {
            "id": "https://huggingface.co/papers/2410.03103",
            "title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning",
            "url": "https://huggingface.co/papers/2410.03103",
            "abstract": "Fill-in-the-Middle (FIM) has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm, which reorders original training sequences and then performs regular next-token prediction (NTP), often leads to models struggling to generate content that aligns smoothly with the surrounding context. Crucially, while existing works rely on rule-based post-processing to circumvent this weakness, such methods are not practically usable in open-domain code completion tasks as they depend on restrictive, dataset-specific assumptions (e.g., generating the same number of lines as in the ground truth). Moreover, model performance on FIM tasks deteriorates significantly without these unrealistic assumptions.   We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens (i.e., horizon length) at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different models and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level, and without resorting to unrealistic post-processing methods. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP only incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.",
            "score": 2,
            "issue_id": 7,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#plp",
                    "#reasoning"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "HLP: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğµ - Horizon-Length Prediction (HLP). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Fill-in-the-Middle (FIM), HLP ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HLP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ´Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "\"Horizon-Length Prediction: Elevating Code Completion with Smarter Planning\"",
                    "desc": "The paper discusses a new approach to improve code language models' ability to fill in missing code segments, called Fill-in-the-Middle (FIM). Traditional methods struggle because they rely on reordering sequences and predicting the next token, which doesn't always align well with the surrounding code context. The authors propose a new training method called Horizon-Length Prediction (HLP), which helps models predict how many tokens are needed to complete the middle section, improving their planning and infilling capabilities. This method significantly enhances model performance without needing complex post-processing, making it more practical for real-world applications."
                },
                "zh": {
                    "title": "è§†ç•Œé•¿åº¦é¢„æµ‹ï¼šæå‡ä»£ç å¡«å……çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†ä»£ç è¯­è¨€æ¨¡å‹ä¸­çš„å¡«å……ä¸­é—´éƒ¨åˆ†ï¼ˆFIMï¼‰é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰çš„è®­ç»ƒæ–¹æ³•åœ¨ç”Ÿæˆä¸ä¸Šä¸‹æ–‡å¹³æ»‘è¡”æ¥çš„å†…å®¹æ—¶å­˜åœ¨å›°éš¾ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œç§°ä¸ºè§†ç•Œé•¿åº¦é¢„æµ‹ï¼ˆHLPï¼‰ï¼Œä»¥æ”¹å–„æ¨¡å‹åœ¨FIMä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚HLPé€šè¿‡é¢„æµ‹å‰©ä½™ä¸­é—´æ ‡è®°çš„æ•°é‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ä¸ä¾èµ–æ•°æ®é›†ç‰¹å®šåå¤„ç†çš„æƒ…å†µä¸‹å­¦ä¹ å¡«å……è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHLPæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶å¯¹è®­ç»ƒå’Œæ¨ç†çš„å¼€é”€å½±å“å¾ˆå°ã€‚"
                }
            },
            "hash": "6d595cf08f21593b"
        },
        {
            "id": "https://huggingface.co/papers/2410.01999",
            "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs",
            "url": "https://huggingface.co/papers/2410.01999",
            "abstract": "Recent advancements in Code Large Language Models (CodeLLMs) have predominantly focused on open-ended code generation tasks, often neglecting the critical aspect of code understanding and comprehension. To bridge this gap, we present CodeMMLU, a comprehensive multiple-choice question-answer benchmark designed to evaluate the depth of software and code understanding in LLMs. CodeMMLU includes over 10,000 questions sourced from diverse domains, encompassing tasks such as code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks, CodeMMLU assesses models's ability to reason about code rather than merely generate it, providing deeper insights into their grasp of complex software concepts and systems. Our extensive evaluation reveals that even state-of-the-art models face significant challenges with CodeMMLU, highlighting deficiencies in comprehension beyond code generation. By underscoring the crucial relationship between code understanding and effective generation, CodeMMLU serves as a vital resource for advancing AI-assisted software development, ultimately aiming to create more reliable and capable coding assistants.",
            "score": 1,
            "issue_id": 9,
            "pub_date": "2024-10-02",
            "pub_date_ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CodeMMLU: ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ² Ğ˜Ğ˜",
                    "desc": "CodeMMLU - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², CodeMMLU Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ¾Ğ´Ğµ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ CodeMMLU."
                },
                "en": {
                    "title": "\"From Code Generation to Code Comprehension: Elevating AI's Understanding\"",
                    "desc": "The paper introduces CodeMMLU, a benchmark designed to test the code understanding capabilities of large language models (LLMs) through multiple-choice questions. It focuses on evaluating models' ability to comprehend and reason about code, rather than just generating it. The benchmark includes over 10,000 questions from various domains, highlighting the challenges even advanced models face in understanding complex software concepts. CodeMMLU aims to improve AI-assisted software development by emphasizing the importance of code comprehension in creating reliable coding assistants."
                },
                "zh": {
                    "title": "CodeMMLUï¼šæå‡ä»£ç ç†è§£ï¼Œè¶…è¶Šç”Ÿæˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMsï¼‰ä¸»è¦å…³æ³¨äºå¼€æ”¾å¼ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¾€å¾€å¿½è§†äº†ä»£ç ç†è§£çš„é‡è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CodeMMLUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¯¹è½¯ä»¶å’Œä»£ç çš„ç†è§£æ·±åº¦ã€‚CodeMMLUåŒ…å«è¶…è¿‡10,000ä¸ªé—®é¢˜ï¼Œæ¶µç›–ä»£ç åˆ†æã€ç¼ºé™·æ£€æµ‹å’Œè½¯ä»¶å·¥ç¨‹åŸç†ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨CodeMMLUä¸Šä¹Ÿé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†ç†è§£ä»£ç ä¸ç”Ÿæˆä»£ç ä¹‹é—´çš„é‡è¦å…³ç³»ã€‚"
                }
            },
            "hash": "a61e37ff42c5cd79"
        },
        {
            "id": "https://huggingface.co/papers/2410.03645",
            "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
            "url": "https://huggingface.co/papers/2410.03645",
            "abstract": "Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated objects. To automatically generate demonstration data for these tasks at scale, we propose planning and RL solvers that generalize within object categories. The pipeline can generate data for up to 100 articulated tasks with 200 objects and reduce the required human efforts. To utilize such data, we propose an effective multi-task language-conditioned policy architecture, dubbed proprioceptive point-cloud transformer (PPT), that learns from the generated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the policy architecture, we show a promising usage of GenSim2 that the generated data can be used for zero-shot transfer or co-train with real-world collected data, which enhances the policy performance by 20% compared with training exclusively on limited real data.",
            "score": 1,
            "issue_id": 7,
            "pub_date": "1963-01-17",
            "pub_date_ru": "Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€Ñ‚Ğ¾Ğ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#transfer_learning",
                    "#robots"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "GenSim2: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "GenSim2 - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ†ĞµĞ½Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Scaling Robotic Simulations with GenSim2: Automate, Learn, Transfer!",
                    "desc": "The paper introduces GenSim2, a framework designed to automate the creation of complex simulation tasks using coding language models with multi-modal and reasoning capabilities. It addresses the scalability issues in robotic simulations by generating demonstration data for numerous tasks, reducing the need for human input. The framework includes a multi-task language-conditioned policy architecture called proprioceptive point-cloud transformer (PPT), which effectively learns from these demonstrations and shows strong sim-to-real zero-shot transfer capabilities. By combining GenSim2 with real-world data, the approach enhances policy performance significantly, demonstrating a 20% improvement over training with limited real data alone."
                },
                "zh": {
                    "title": "GenSim2ï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆå¤æ‚æ¨¡æ‹Ÿä»»åŠ¡çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGenSim2çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å’Œæ¨ç†èƒ½åŠ›çš„ç¼–ç å¤§è¯­è¨€æ¨¡å‹æ¥åˆ›å»ºå¤æ‚çš„æ¨¡æ‹Ÿä»»åŠ¡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤šè¾¾100ä¸ªä»»åŠ¡çš„æ•°æ®ï¼Œå‡å°‘äº†äººåŠ›æŠ•å…¥ã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡è¯­è¨€æ¡ä»¶ç­–ç•¥æ¶æ„ï¼Œç§°ä¸ºæœ¬ä½“ç‚¹äº‘å˜æ¢å™¨ï¼ˆPPTï¼‰ï¼Œå¯ä»¥å®ç°ä»æ¨¡æ‹Ÿåˆ°çœŸå®ç¯å¢ƒçš„é›¶æ ·æœ¬è¿ç§»ã€‚ç»“åˆç”Ÿæˆçš„æ•°æ®å’Œç­–ç•¥æ¶æ„ï¼ŒGenSim2å±•ç¤ºäº†åœ¨å¢å¼ºç­–ç•¥æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œæ¯”ä»…ä½¿ç”¨æœ‰é™çš„çœŸå®æ•°æ®è®­ç»ƒæé«˜äº†20%ã€‚"
                }
            },
            "hash": "f956848db7fdb9bc"
        }
    ],
    "weekday": 4,
    "link_prev": "2024-10-03.html",
    "link_next": "2024-10-07.html",
    "time_utc": "2024-10-06 20:26",
    "date_en": "4 October",
    "date_prev": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": "03.10",
    "short_date_next": "07.10",
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 2,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 4,
        "#medicine": 2,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#robots": 0
    }
}