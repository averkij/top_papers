{
    "date": {
        "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 12",
        "zh": "2æœˆ12æ—¥"
    },
    "time_utc": "2026-02-12 21:28",
    "weekday": 3,
    "issue_id": 1032,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.10604",
            "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
            "url": "https://huggingface.co/papers/2602.10604",
            "abstract": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
            "score": 146,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4a737b220dcc18cc",
            "authors": [
                "Ailin Huang",
                "Ang Li",
                "Aobo Kong",
                "Bin Wang",
                "Binxing Jiao",
                "Bo Dong",
                "Bojun Wang",
                "Boyu Chen",
                "Brian Li",
                "Buyun Ma",
                "Chang Su",
                "Changxin Miao",
                "Changyi Wan",
                "Chao Lou",
                "Chen Hu",
                "Chen Xu",
                "Chenfeng Yu",
                "Chengting Feng",
                "Chengyuan Yao",
                "Chunrui Han",
                "Dan Ma",
                "Dapeng Shi",
                "Daxin Jiang",
                "Dehua Ma",
                "Deshan Sun",
                "Di Qi",
                "Enle Liu",
                "Fajie Zhang",
                "Fanqi Wan",
                "Guanzhe Huang",
                "Gulin Yan",
                "Guoliang Cao",
                "Guopeng Li",
                "Han Cheng",
                "Hangyu Guo",
                "Hanshan Zhang",
                "Hao Nie",
                "Haonan Jia",
                "Haoran Lv",
                "Hebin Zhou",
                "Hekun Lv",
                "Heng Wang",
                "Heung-Yeung Shum",
                "Hongbo Huang",
                "Hongbo Peng",
                "Hongyu Zhou",
                "Hongyuan Wang",
                "Houyong Chen",
                "Huangxi Zhu",
                "Huimin Wu",
                "Huiyong Guo",
                "Jia Wang",
                "Jian Zhou",
                "Jianjian Sun",
                "Jiaoren Wu",
                "Jiaran Zhang",
                "Jiashu Lv",
                "Jiashuo Liu",
                "Jiayi Fu",
                "Jiayu Liu",
                "Jie Cheng",
                "Jie Luo",
                "Jie Yang",
                "Jie Zhou",
                "Jieyi Hou",
                "Jing Bai",
                "Jingcheng Hu",
                "Jingjing Xie",
                "Jingwei Wu",
                "Jingyang Zhang",
                "Jishi Zhou",
                "Junfeng Liu",
                "Junzhe Lin",
                "Ka Man Lo",
                "Kai Liang",
                "Kaibo Liu",
                "Kaijun Tan",
                "Kaiwen Yan",
                "Kaixiang Li",
                "Kang An",
                "Kangheng Lin",
                "Lei Yang",
                "Liang Lv",
                "Liang Zhao",
                "Liangyu Chen",
                "Lieyu Shi",
                "Liguo Tan",
                "Lin Lin",
                "Lina Chen",
                "Luck Ma",
                "Mengqiang Ren",
                "Michael Li",
                "Ming Li",
                "Mingliang Li",
                "Mingming Zhang",
                "Mingrui Chen",
                "Mitt Huang",
                "Na Wang",
                "Peng Liu",
                "Qi Han",
                "Qian Zhao",
                "Qinglin He",
                "Qinxin Du",
                "Qiuping Wu",
                "Quan Sun",
                "Rongqiu Yang",
                "Ruihang Miao",
                "Ruixin Han",
                "Ruosi Wan",
                "Ruyan Guo",
                "Shan Wang",
                "Shaoliang Pang",
                "Shaowen Yang",
                "Shengjie Fan",
                "Shijie Shang",
                "Shiliang Yang",
                "Shiwei Li",
                "Shuangshuang Tian",
                "Siqi Liu",
                "Siye Wu",
                "Siyu Chen",
                "Song Yuan",
                "Tiancheng Cao",
                "Tianchi Yue",
                "Tianhao Cheng",
                "Tianning Li",
                "Tingdan Luo",
                "Wang You",
                "Wei Ji",
                "Wei Yuan",
                "Wei Zhang",
                "Weibo Wu",
                "Weihao Xie",
                "Wen Sun",
                "Wenjin Deng",
                "Wenzhen Zheng",
                "Wuxun Xie",
                "Xiangfeng Wang",
                "Xiangwen Kong",
                "Xiangyu Liu",
                "Xiangyu Zhang",
                "Xiaobo Yang",
                "Xiaojia Liu",
                "Xiaolan Yuan",
                "Xiaoran Jiao",
                "Xiaoxiao Ren",
                "Xiaoyun Zhang",
                "Xin Li",
                "Xin Liu",
                "Xin Wu",
                "Xing Chen",
                "Xingping Yang",
                "Xinran Wang",
                "Xu Zhao",
                "Xuan He",
                "Xuanti Feng",
                "Xuedan Cai",
                "Xuqiang Zhou",
                "Yanbo Yu",
                "Yang Li",
                "Yang Xu",
                "Yanlin Lai",
                "Yanming Xu",
                "Yaoyu Wang",
                "Yeqing Shen",
                "Yibo Zhu",
                "Yichen Lv",
                "Yicheng Cao",
                "Yifeng Gong",
                "Yijing Yang",
                "Yikun Yang",
                "Yin Zhao",
                "Yingxiu Zhao",
                "Yinmin Zhang",
                "Yitong Zhang",
                "Yixuan Zhang",
                "Yiyang Chen",
                "Yongchi Zhao",
                "Yongshen Long",
                "Yongyao Wang",
                "Yousong Guan",
                "Yu Zhou",
                "Yuang Peng",
                "Yuanhao Ding",
                "Yuantao Fan",
                "Yuanzhen Yang",
                "Yuchu Luo",
                "Yudi Zhao",
                "Yue Peng",
                "Yueqiang Lin",
                "Yufan Lu",
                "Yuling Zhao",
                "Yunzhou Ju",
                "Yurong Zhang",
                "Yusheng Li",
                "Yuxiang Yang",
                "Yuyang Chen",
                "Yuzhu Cai",
                "Zejia Weng",
                "Zetao Hong",
                "Zexi Li",
                "Zhe Xie",
                "Zheng Ge",
                "Zheng Gong",
                "Zheng Zeng",
                "Zhenyi Lu",
                "Zhewei Huang",
                "Zhichao Chang",
                "Zhiguo Huang",
                "Zhiheng Hu",
                "Zidong Yang",
                "Zili Wang",
                "Ziqi Ren",
                "Zixin Zhang",
                "Zixuan Wang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.10604.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#inference",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Step 3.5 Flash, Ğ¾Ñ‚Ğ½Ğ¾ÑÑÑ‰Ğ°ÑÑÑ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture-of-Experts, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 11 Ğ¼Ğ»Ñ€Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ· 196 Ğ¼Ğ»Ñ€Ğ´ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ 3:1 Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ) Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ñ…: 85.4% Ğ½Ğ° IMO-AnswerBench, 86.4% Ğ½Ğ° LiveCodeBench, 88.2% Ğ½Ğ° tau2-Bench Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ."
                },
                "en": {
                    "title": "Efficient Intelligence with Step 3.5 Flash",
                    "desc": "Step 3.5 Flash is a sparse Mixture-of-Experts (MoE) model that enhances agentic intelligence while optimizing computational efficiency. It utilizes a large foundation of 196 billion parameters but activates only 11 billion during inference, allowing for faster processing. The model employs advanced techniques like interleaved attention and Multi-Token Prediction to minimize latency in multi-round interactions. With a robust reinforcement learning framework, it achieves impressive performance across various tasks, making it a strong contender among leading AI models."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ™ºèƒ½çš„ç¨€ç–ä¸“å®¶æ¨¡å‹",
                    "desc": "Step 3.5 Flash æ˜¯ä¸€ç§ç¨€ç–çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMixture-of-Expertsï¼‰ï¼Œé€šè¿‡é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨å’Œä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†å‰æ²¿æ°´å¹³çš„æ™ºèƒ½è¡¨ç°ã€‚è¯¥æ¨¡å‹ç»“åˆäº†1960äº¿å‚æ•°çš„åŸºç¡€å’Œ110äº¿æ´»è·ƒå‚æ•°ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚å®ƒé‡‡ç”¨äº†äº¤é”™çš„3:1æ»‘åŠ¨çª—å£/å…¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ ‡è®°é¢„æµ‹ï¼ˆMTP-3ï¼‰ï¼Œé™ä½äº†å¤šè½®äº¤äº’çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚é€šè¿‡å¯éªŒè¯ä¿¡å·ä¸åå¥½åé¦ˆç›¸ç»“åˆçš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒStep 3.5 Flash åœ¨æ•°å­¦ã€ç¼–ç å’Œå·¥å…·ä½¿ç”¨ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†ä¸å‰æ²¿æ¨¡å‹ç›¸åª²ç¾çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11144",
            "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
            "url": "https://huggingface.co/papers/2602.11144",
            "abstract": "GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.",
            "score": 40,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "63e69030ff6001c2",
            "authors": [
                "Ruichuan An",
                "Sihan Yang",
                "Ziyu Guo",
                "Wei Dai",
                "Zijun Shen",
                "Haodong Li",
                "Renrui Zhang",
                "Xinyu Wei",
                "Guopeng Li",
                "Wenshan Wu",
                "Wentao Zhang"
            ],
            "affiliations": [
                "CUHK",
                "MSRA",
                "Peking University",
                "PolyU",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11144.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ„Ğ»ÑĞ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¾Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° GENIUS Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ»ÑĞ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 12 Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ° Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¼ĞµÑ‰Ğ°Ñ Ñ„Ğ¾ĞºÑƒÑ Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Unlocking Generative Fluid Intelligence in AI Models",
                    "desc": "The paper introduces GENIUS, a new evaluation suite designed to measure Generative Fluid Intelligence (GFI) in multimodal models. Unlike traditional benchmarks that focus on Crystallized Intelligence, GENIUS assesses a model's ability to induce patterns, execute constraints, and adapt to new contexts. The study reveals that many models struggle with context comprehension, which limits their performance in GFI tasks. To address these deficiencies, the authors propose a training-free attention intervention strategy, aiming to enhance dynamic reasoning capabilities in AI systems."
                },
                "zh": {
                    "title": "è¯„ä¼°ç”Ÿæˆæµä½“æ™ºèƒ½çš„æ–°æ ‡å‡†",
                    "desc": "GENIUSè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ç”Ÿæˆæµä½“æ™ºèƒ½ï¼Œé‡ç‚¹åœ¨äºæ¨¡å¼è¯±å¯¼ã€çº¦æŸæ‰§è¡Œå’Œä¸Šä¸‹æ–‡é€‚åº”ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè€Œéç”Ÿæˆèƒ½åŠ›ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºçš„GENIUSå·¥å…·ä¸ºç”Ÿæˆæµä½“æ™ºèƒ½æä¾›äº†ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ï¼Œå¼ºè°ƒäº†æ¨¡å‹åœ¨åŠ¨æ€æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡å¯¹12ä¸ªä»£è¡¨æ€§æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å¤„ç†å³æ—¶ä¸Šä¸‹æ–‡é—®é¢˜æ—¶çš„æ˜¾è‘—æ€§èƒ½ç¼ºé™·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11124",
            "title": "PhyCritic: Multimodal Critic Models for Physical AI",
            "url": "https://huggingface.co/papers/2602.11124",
            "abstract": "PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.",
            "score": 40,
            "issue_id": 1018,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "0cdacb707106ee3c",
            "authors": [
                "Tianyi Xiong",
                "Shihao Wang",
                "Guilin Liu",
                "Yi Dong",
                "Ming Li",
                "Heng Huang",
                "Jan Kautz",
                "Zhiding Yu"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11124.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#robotics",
                    "#multimodal",
                    "#alignment",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšÑ€Ğ¸Ñ‚Ğ¸Ğº Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ‡ÑƒÑ‚ÑŒÑ‘Ğ¼: ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ",
                    "desc": "PhyCritic â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… AI Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ RLVR pipeline: Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. PhyCritic Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ°Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Physical AI with PhyCritic: A New Standard in Multimodal Evaluation",
                    "desc": "PhyCritic is a specialized multimodal critic model aimed at improving physical AI tasks by using a two-stage RLVR (Reinforcement Learning with Visual Reasoning) pipeline. The first stage focuses on enhancing perception and reasoning skills related to physical tasks, while the second stage involves fine-tuning the critic to generate its own predictions for better judgment accuracy. This approach allows PhyCritic to provide reliable evaluations, including pairwise preferences and numerical scores, specifically for tasks that require understanding of physical interactions. The model demonstrates significant performance improvements over existing baselines in both physical and general multimodal evaluation tasks."
                },
                "zh": {
                    "title": "PhyCriticï¼šç‰©ç†AIä»»åŠ¡çš„å¤šæ¨¡æ€è¯„ä¼°æ–°æ¨¡å‹",
                    "desc": "PhyCriticæ˜¯ä¸€ç§å¤šæ¨¡æ€è¯„ä¼°æ¨¡å‹ï¼Œä¸“ä¸ºç‰©ç†äººå·¥æ™ºèƒ½ä»»åŠ¡è®¾è®¡ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„RLVRç®¡é“æ¥å¢å¼ºæ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é¦–å…ˆé€šè¿‡ç‰©ç†æŠ€èƒ½çƒ­èº«é˜¶æ®µæå‡ä¸ç‰©ç†ç›¸å…³çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œæ¥ç€è¿›è¡Œè‡ªæˆ‘å‚è€ƒçš„è¯„ä¼°å¾®è°ƒï¼Œä½¿è¯„ä¼°è€…åœ¨åˆ¤æ–­å€™é€‰å“åº”ä¹‹å‰ç”Ÿæˆè‡ªå·±çš„é¢„æµ‹ï¼Œä»è€Œæé«˜åˆ¤æ–­çš„ç¨³å®šæ€§å’Œç‰©ç†æ­£ç¡®æ€§ã€‚PhyCriticåœ¨ç‰©ç†å’Œé€šç”¨å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸æ¯”å¼€æºåŸºçº¿å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åº”ç”¨äºç­–ç•¥æ¨¡å‹æ—¶ï¼ŒPhyCriticè¿›ä¸€æ­¥æ”¹å–„äº†ç‰©ç†åŸºç¡€ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04935",
            "title": "ASA: Training-Free Representation Engineering for Tool-Calling Agents",
            "url": "https://huggingface.co/papers/2602.04935",
            "abstract": "A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.",
            "score": 38,
            "issue_id": 1017,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "79e467c611d9e5d6",
            "authors": [
                "Youjin Wang",
                "Run Zhou",
                "Rong Fu",
                "Shuaishuai Cao",
                "Hongwei Zeng",
                "Jiaxuan Lu",
                "Sicheng Fan",
                "Jiaqiao Zhao",
                "Liangming Pan"
            ],
            "affiliations": [
                "Central South University",
                "Fudan University",
                "Peking University",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "University South University",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04935.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#inference",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹: Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Activation Steering Adapter (ASA) Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ \"Ğ»ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°\", ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ², Ğ½Ğ¾ Ğ½Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼. ASA Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ¼. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ MTU-Bench Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ F1 Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ 0.18 Ğ´Ğ¾ 0.50, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ğ² Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 20KB Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Tool Calling in Language Models with Activation Steering",
                    "desc": "The paper introduces the Activation Steering Adapter (ASA), a novel method for improving tool calling behavior in language models without the need for training. It addresses the issue of models being overly cautious in using tools, despite having the necessary information available in their mid-layer activations. ASA utilizes a single-shot intervention at inference time, employing router-conditioned steering vectors and a probe-guided mechanism to enhance the model's decision-making. The results show significant improvements in tool-use accuracy while maintaining a low false positive rate, demonstrating the effectiveness of this training-free approach."
                },
                "zh": {
                    "title": "æ¿€æ´»å¼•å¯¼é€‚é…å™¨ï¼šæ— éœ€è®­ç»ƒçš„å·¥å…·è°ƒç”¨ä¿®æ­£æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¿€æ´»å¼•å¯¼é€‚é…å™¨ï¼ˆActivation Steering Adapter, ASAï¼‰çš„æ–¹æ³•ï¼Œç”¨äºä¿®æ­£è¯­è¨€æ¨¡å‹ä¸­çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸­é—´å±‚æ¿€æ´»å¹²é¢„ï¼Œåˆ©ç”¨æ¢é’ˆå’Œè·¯ç”±æ¡ä»¶å¼•å¯¼çš„å‘é‡ï¼Œåœ¨æ¨ç†æ—¶è¿›è¡Œå•æ¬¡å¹²é¢„ï¼Œè€Œæ— éœ€è®­ç»ƒã€‚ASAèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å·¥å…·è°ƒç”¨çš„å¿…è¦æ€§ï¼Œå¹¶é€šè¿‡æ”¾å¤§çœŸå®æ„å›¾æ¥æŠ‘åˆ¶é”™è¯¯è§¦å‘ï¼Œä»è€Œæé«˜å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASAåœ¨MTU-Benchä¸Šæ˜¾è‘—æé«˜äº†å·¥å…·ä½¿ç”¨çš„F1åˆ†æ•°ï¼ŒåŒæ—¶é™ä½äº†è¯¯æŠ¥ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰¹å®šé¢†åŸŸå·¥å…·è°ƒç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10560",
            "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
            "url": "https://huggingface.co/papers/2602.10560",
            "abstract": "GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.",
            "score": 21,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "766dce90e9fcaf33",
            "authors": [
                "Leheng Sheng",
                "Yongtao Zhang",
                "Wenchang Ma",
                "Yaorui Shi",
                "Ting Huang",
                "Xiang Wang",
                "An Zhang",
                "Ke Shen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "ByteDance Seed",
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10560.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "GRU-Mem Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€Ğ¾Ğ²: update gate ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° exit gate Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸Ğ· Ñ†Ğ¸ĞºĞ»Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ reinforcement learning Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 400% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ MemAgent."
                },
                "en": {
                    "title": "Efficient Long-Context Reasoning with GRU-Mem",
                    "desc": "GRU-Mem is a novel approach designed to enhance long-context reasoning in large language models (LLMs) by using text-controlled gates and reinforcement learning. It addresses the limitations of previous methods, like MemAgent, which struggled with indiscriminate memory updates and lacked an efficient exit mechanism. By implementing an update gate and an exit gate, GRU-Mem ensures that memory updates occur only when necessary and allows the model to stop processing once sufficient evidence is gathered. Experimental results show that GRU-Mem significantly improves both the speed and accuracy of long-context reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "GRU-Memï¼šé«˜æ•ˆç¨³å®šçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†",
                    "desc": "GRU-Memæ˜¯ä¸€ç§æ–°å‹çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥æ–‡æœ¬æ§åˆ¶çš„é—¨æ§æœºåˆ¶å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¥ç¨³å®šå†…å­˜æ›´æ–°å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„MemAgentç›¸æ¯”ï¼ŒGRU-Memåœ¨å†…å­˜æ›´æ–°æ—¶ä»…åœ¨æ›´æ–°é—¨å¼€å¯æ—¶è¿›è¡Œï¼Œå¹¶åœ¨é€€å‡ºé—¨å¼€å¯æ—¶ç«‹å³é€€å‡ºå¾ªç¯ï¼Œä»è€Œé¿å…äº†ä¸å¿…è¦çš„è®¡ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRU-Memåœ¨å¤šç§é•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†400%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10177",
            "title": "Towards Autonomous Mathematics Research",
            "url": "https://huggingface.co/papers/2602.10177",
            "abstract": "Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.",
            "score": 21,
            "issue_id": 1017,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "576d66277cc83830",
            "authors": [
                "Tony Feng",
                "Trieu H. Trinh",
                "Garrett Bingham",
                "Dawsen Hwang",
                "Yuri Chervonyi",
                "Junehyuk Jung",
                "Joonkyung Lee",
                "Carlo Pagano",
                "Sang-hyun Kim",
                "Federico Pasqualotto",
                "Sergei Gukov",
                "Jonathan N. Lee",
                "Junsu Kim",
                "Kaiying Hou",
                "Golnaz Ghiasi",
                "Yi Tay",
                "YaGuang Li",
                "Chenkai Kuang",
                "Yuan Liu",
                "Hanzhao",
                "Lin",
                "Evan Zheran Liu",
                "Nigamaa Nayakanti",
                "Xiaomeng Yang",
                "Heng-tze Cheng",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Quoc V. Le",
                "Thang Luong"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10177.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´ Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑĞ¼: AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Aletheia â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹ÑˆĞµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´. ĞĞ³ĞµĞ½Ñ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ AI Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AI Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Aletheia: Revolutionizing Math Research with AI Autonomy",
                    "desc": "Aletheia is a math research agent that showcases advanced reasoning abilities by autonomously generating and verifying mathematical solutions in natural language. It utilizes an enhanced version of Gemini Deep Think to tackle complex problems, extending its capabilities from Olympiad-level challenges to professional research tasks. The agent has successfully produced research papers independently and in collaboration with humans, demonstrating its effectiveness in navigating mathematical literature and solving open problems. This work emphasizes the importance of defining standards for measuring the autonomy and novelty of AI-generated mathematical results."
                },
                "zh": {
                    "title": "Aletheiaï¼šæ•°å­¦ç ”ç©¶çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "Aletheia æ˜¯ä¸€ä¸ªæ•°å­¦ç ”ç©¶ä»£ç†ï¼Œèƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ä¸­ç”Ÿæˆã€éªŒè¯å’Œä¿®è®¢è§£å†³æ–¹æ¡ˆï¼Œå±•ç°å‡ºå…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§æ”¹è¿›ç‰ˆçš„ Gemini Deep Thinkï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œå¹¶é€šè¿‡å·¥å…·ä½¿ç”¨æ¥åº”å¯¹æ•°å­¦ç ”ç©¶çš„å¤æ‚æ€§ã€‚Aletheia ä»å¥¥æ—åŒ¹å…‹é—®é¢˜åˆ°åšå£«çº§ç»ƒä¹ ï¼Œå±•ç¤ºäº†å…¶åœ¨ AI è¾…åŠ©æ•°å­¦ç ”ç©¶ä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ— äººå·¥å¹²é¢„çš„ç ”ç©¶è®ºæ–‡å’Œäººæœºåä½œçš„è¯æ˜ã€‚æˆ‘ä»¬å»ºè®®å¯¹ AI è¾…åŠ©ç»“æœçš„è‡ªä¸»æ€§å’Œæ–°é¢–æ€§è¿›è¡Œæ ‡å‡†åŒ–é‡åŒ–ï¼Œä»¥å¸®åŠ©å…¬ä¼—æ›´å¥½åœ°ç†è§£ AI å’Œæ•°å­¦çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08253",
            "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
            "url": "https://huggingface.co/papers/2602.08253",
            "abstract": "A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
            "score": 21,
            "issue_id": 1017,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "034646ca5ddc25d4",
            "authors": [
                "Baoyun Zhao",
                "He Wang",
                "Liang Zeng"
            ],
            "affiliations": [
                "International Centre for Theoretical Physics Asia-Pacific, University of Chinese Academy of Sciences",
                "Software College, Northeastern University",
                "Taiji Laboratory for Gravitational Wave Universe, University of Chinese Academy of Sciences",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08253.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ G-LNS, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (Large Neighborhood Search) Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰ĞµĞ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¼Ğ¸Ğ²Ğ¾ÑĞ¶ĞµÑ€Ğ° Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ LLM-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Heuristic Design with G-LNS",
                    "desc": "This paper introduces G-LNS, a generative evolutionary framework that enhances large language models (LLMs) for creating Large Neighborhood Search (LNS) operators in combinatorial optimization problems. Unlike traditional methods that limit heuristic design to fixed forms, G-LNS allows for the co-evolution of destroy and repair operators, improving the exploration of the solution space. The framework employs a cooperative evaluation mechanism to optimize the interaction between these operators, leading to better structural disruption and reconstruction. Experimental results show that G-LNS outperforms existing LLM-based methods and classical solvers, achieving near-optimal solutions efficiently across various problem instances."
                },
                "zh": {
                    "title": "ç”Ÿæˆè¿›åŒ–æ¡†æ¶ï¼šä¼˜åŒ–ç»„åˆé—®é¢˜çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆè¿›åŒ–æ¡†æ¶G-LNSï¼Œç”¨äºè‡ªåŠ¨è®¾è®¡å¤§é‚»åŸŸæœç´¢ï¼ˆLNSï¼‰ç®—å­ï¼Œä»¥è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒG-LNSåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…±åŒè¿›åŒ–ç ´åå’Œä¿®å¤ç®—å­ï¼Œå¢å¼ºäº†æœç´¢ç©ºé—´çš„æ¢ç´¢èƒ½åŠ›ã€‚é€šè¿‡åˆä½œè¯„ä¼°æœºåˆ¶ï¼ŒG-LNSèƒ½å¤Ÿæ•æ‰ç®—å­ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œå‘ç°æœ‰æ•ˆçš„äº’è¡¥é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG-LNSåœ¨è§£å†³æ—…è¡Œå•†é—®é¢˜å’Œå®¹é‡è½¦è¾†è·¯å¾„é—®é¢˜ç­‰å¤æ‚ç»„åˆä¼˜åŒ–åŸºå‡†ä¸Šï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMåŸºç¡€è‡ªåŠ¨å¯å‘å¼è®¾è®¡æ–¹æ³•å’Œç»å…¸æ±‚è§£å™¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10622",
            "title": "How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning",
            "url": "https://huggingface.co/papers/2602.10622",
            "abstract": "Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.",
            "score": 20,
            "issue_id": 1026,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "c9359a195670ae33",
            "authors": [
                "Jiahao Yuan",
                "Yike Xu",
                "Jinyong Wen",
                "Baokun Wang",
                "Yang Chen",
                "Xiaotong Lin",
                "Wuliang Huang",
                "Ziyi Gao",
                "Xing Fu",
                "Yu Cheng",
                "Weiqiang Wang"
            ],
            "affiliations": [
                "Ant Group",
                "East China Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10622.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² decoder-only ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Gradient-Guided Soft Masking, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´a Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Alipay Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing User Embeddings with Gradient-Guided Soft Masking",
                    "desc": "This paper explores how different attention masking strategies affect the quality of user embeddings in decoder-only language models. It introduces a new technique called Gradient-Guided Soft Masking, which enhances training stability and representation quality by gradually allowing future attention during optimization. The study evaluates various attention masks within a contrastive learning framework using real-world user behavior data. Results show that the proposed method outperforms traditional masking strategies, leading to better user representation for tasks like prediction and marketing sensitivity."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ç”¨æˆ·åµŒå…¥è´¨é‡çš„æ³¨æ„åŠ›æ©è”½ç­–ç•¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒæ³¨æ„åŠ›æ©è”½ç­–ç•¥å¯¹è§£ç å™¨è¯­è¨€æ¨¡å‹ä¸­ç”¨æˆ·åµŒå…¥è´¨é‡çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„è½¯æ©è”½æŠ€æœ¯ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œç”¨æˆ·è¡Œä¸ºåˆ†æçš„è¡¨ç¤ºè´¨é‡ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®ä¸Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å› æœã€æ··åˆå’ŒåŒå‘æ³¨æ„åŠ›æ©è”½çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªç”¨æˆ·è®¤çŸ¥åŸºå‡†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œè´¨é‡ï¼Œå¼ºè°ƒäº†æ©è”½è®¾è®¡å’Œè®­ç»ƒè¿‡æ¸¡çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08711",
            "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions",
            "url": "https://huggingface.co/papers/2602.08711",
            "abstract": "Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.",
            "score": 20,
            "issue_id": 1017,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "06197a055773ca80",
            "authors": [
                "Linli Yao",
                "Yuancheng Wei",
                "Yaojie Zhang",
                "Lei Li",
                "Xinlong Chen",
                "Feifan Song",
                "Ziyue Wang",
                "Kun Ouyang",
                "Yuanxin Liu",
                "Lingpeng Kong",
                "Qi Liu",
                "Pengfei Wan",
                "Kun Gai",
                "Yuanxing Zhang",
                "Xu Sun"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Kling Team, Kuaishou Technology",
                "School of Computer Science, Peking University",
                "South China University of Technology",
                "The University of Hong Kong",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08711.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#audio",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ÑÑ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Omni Dense Captioning â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑˆĞµÑÑ‚Ğ¸Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 'ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ…' Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ†ĞµĞ½Ğ° Ğ·Ğ° ÑÑ†ĞµĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OmniDCBench Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ SodaM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TimeChat-Captioner-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ SFT Ğ¸ GRPO Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Gemini-2.5-Pro Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Crafting Cinematic Narratives with Time-Aware Captions",
                    "desc": "Omni Dense Captioning is a new approach for creating detailed audio-visual narratives that include specific timestamps. It uses a six-dimensional structural schema to produce captions that resemble a screenplay, allowing for a richer understanding of video content. The paper introduces a benchmark called OmniDCBench and a new evaluation metric, SodaM, to assess the quality of these time-aware descriptions. Additionally, it presents a strong baseline model, TimeChat-Captioner-7B, which outperforms existing models and enhances tasks related to audio-visual reasoning and temporal grounding."
                },
                "zh": {
                    "title": "å…¨æ–¹ä½å¯†é›†å­—å¹•ç”Ÿæˆï¼šéŸ³è§†é¢‘å™è¿°çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†Omni Dense Captioningï¼Œè¿™æ˜¯ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰æ˜ç¡®æ—¶é—´æˆ³çš„è¿ç»­ã€ç»†è‡´ä¸”ç»“æ„åŒ–çš„éŸ³è§†é¢‘å™è¿°ã€‚ä¸ºç¡®ä¿è¯­ä¹‰çš„å¯†é›†è¦†ç›–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…­ç»´ç»“æ„æ¡†æ¶ï¼Œåˆ›å»ºç±»ä¼¼å‰§æœ¬çš„å­—å¹•ï¼Œä½¿è¯»è€…èƒ½å¤Ÿé€å¸§ç”ŸåŠ¨æƒ³è±¡è§†é¢‘å†…å®¹ã€‚æˆ‘ä»¬æ„å»ºäº†OmniDCBenchï¼Œä¸€ä¸ªé«˜è´¨é‡çš„äººç±»æ ‡æ³¨åŸºå‡†ï¼Œå¹¶æå‡ºäº†SodaMï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ—¶é—´æ„ŸçŸ¥çš„è¯¦ç»†æè¿°ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒTimeChat-Captioner-7Bæ¨¡å‹åœ¨éŸ³è§†é¢‘æ¨ç†å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10975",
            "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
            "url": "https://huggingface.co/papers/2602.10975",
            "abstract": "FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.",
            "score": 15,
            "issue_id": 1018,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "d5865efdfd7fd28c",
            "authors": [
                "Qixing Zhou",
                "Jiacheng Zhang",
                "Haiyang Wang",
                "Rui Hao",
                "Jiahe Wang",
                "Minghao Han",
                "Yuxue Yang",
                "Shuzhe Wu",
                "Feiyang Pan",
                "Lue Fan",
                "Dandan Tu",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Huawei Technologies Co., Ltd",
                "Institute of Automation, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10975.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#plp",
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ±Ğ°Ğ³Ğ¾Ğ² Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "FeatureBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ¼Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑƒĞ·ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, FeatureBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² ĞºĞ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 11% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… FeatureBench, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FeatureBench: Advancing AI Coding Performance Evaluation",
                    "desc": "FeatureBench is a new benchmark designed to evaluate the coding performance of AI agents in software development. It focuses on feature-oriented tasks and uses execution-based assessments to ensure that the code works correctly. The benchmark automatically derives tasks from code repositories, allowing for continuous updates and minimizing human effort. Initial tests show that even advanced AI models struggle with many tasks, highlighting the need for further improvements in agentic coding capabilities."
                },
                "zh": {
                    "title": "FeatureBenchï¼šæ™ºèƒ½ç¼–ç æ€§èƒ½çš„æ–°æ ‡å‡†",
                    "desc": "FeatureBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ç¼–ç æ€§èƒ½çš„åŸºå‡†ï¼Œä¸“æ³¨äºç‰¹å¾å¯¼å‘çš„è½¯ä»¶å¼€å‘ã€‚å®ƒé€šè¿‡æ‰§è¡ŒåŸºç¡€çš„è¯„ä¼°åè®®å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡æ¨å¯¼æ–¹æ³•ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„ï¼Œèƒ½å¤Ÿä»ä»£ç åº“ä¸­æå–ä»»åŠ¡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«è·¨å¤šä¸ªæäº¤å’Œæ‹‰å–è¯·æ±‚çš„ç‰¹å¾çº§ç¼–ç ä»»åŠ¡ï¼Œå¹¶ç¡®ä¿å…¶ä»–ç‰¹å¾åœ¨åˆ†ç¦»åæ­£å¸¸è¿è¡Œã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†200ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°ä»»åŠ¡å’Œ3825ä¸ªå¯æ‰§è¡Œç¯å¢ƒï¼Œæ¨åŠ¨äº†æ™ºèƒ½ç¼–ç çš„è¿›ä¸€æ­¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11008",
            "title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
            "url": "https://huggingface.co/papers/2602.11008",
            "abstract": "ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.",
            "score": 14,
            "issue_id": 1019,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "10a6c491c3cc3d4d",
            "authors": [
                "Ammar Ali",
                "Baher Mohammad",
                "Denis Makhov",
                "Dmitriy Shopkhoev",
                "Magauiya Zhussip",
                "Stamatios Lefkimmiatis"
            ],
            "affiliations": [
                "Department of Computer Science, ITMO University, Saint-Petersburg, Russia",
                "MWS AI, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11008.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "ROCKET â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑĞºĞ·Ğ°ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑĞ¾Ğ² Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ² Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ñ‹, Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ROCKET Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ½Ğ° 30% Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "ROCKET: Efficient Model Compression Without Training",
                    "desc": "ROCKET is a novel model compression technique that does not require training, making it efficient and straightforward. It treats the problem of compressing each layer of a neural network as a multi-choice knapsack problem, optimizing the compression level for each layer to minimize errors while staying within a size limit. The method employs a unique sparse matrix factorization approach that uses a small calibration dataset to adjust weight coefficients based on their importance, avoiding the need for complex iterative processes. As a result, ROCKET achieves significant compression rates while maintaining high performance, outperforming traditional methods without the need for fine-tuning."
                },
                "zh": {
                    "title": "ROCKETï¼šæ— è®­ç»ƒçš„é«˜æ•ˆæ¨¡å‹å‹ç¼©æ–¹æ³•",
                    "desc": "ROCKETæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œå®ƒå°†é€å±‚å‹ç¼©é—®é¢˜è§†ä¸ºå¤šé€‰èƒŒåŒ…é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ç¨€ç–çŸ©é˜µåˆ†è§£å®ç°é«˜æ•ˆçš„æƒé‡ç¨€ç–åŒ–ï¼Œè€Œæ— éœ€è¿­ä»£ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•åœ¨å…¨çƒå‹ç¼©é¢„ç®—ä¸‹ï¼Œé€šè¿‡é€‰æ‹©æ¯å±‚çš„æœ€ä½³å‹ç¼©çº§åˆ«ï¼Œæœ€å°åŒ–æ€»é‡å»ºè¯¯å·®ï¼ŒåŒæ—¶æ»¡è¶³ç›®æ ‡æ¨¡å‹å¤§å°ã€‚ROCKETåœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸‹çš„å‹ç¼©ç‡è¾¾åˆ°20-50%ï¼Œå¹¶ä¸”åœ¨30%çš„å‹ç¼©ä¸‹ä¿ç•™äº†è¶…è¿‡90%çš„åŸå§‹æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡è½»å¾®çš„å¾®è°ƒé˜¶æ®µï¼Œå‹ç¼©åçš„æ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå‡ ä¹å¯ä»¥ä¸åŸå§‹æ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10224",
            "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
            "url": "https://huggingface.co/papers/2602.10224",
            "abstract": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
            "score": 13,
            "issue_id": 1017,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "e9eb1cf90cc7d4ea",
            "authors": [
                "Shiting Huang",
                "Zecheng Li",
                "Yu Zeng",
                "Qingnan Ren",
                "Zhen Fang",
                "Qisheng Su",
                "Kou Shi",
                "Lin Chen",
                "Zehui Chen",
                "Feng Zhao"
            ],
            "affiliations": [
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10224.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Meta-Experience Learning â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ°, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… (Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ¿Ñ‹Ñ‚) Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 3.92%-4.73% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Meta-Experience Learning",
                    "desc": "Meta-Experience Learning (MEL) is a new framework designed to improve the reasoning abilities of Large Language Models (LLMs) by integrating self-distilled error representations into their memory. It addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) by enabling fine-grained credit assignment and the formation of reusable knowledge from past mistakes. MEL uses contrastive trajectory analysis to pinpoint where reasoning errors occur and summarizes these insights into generalizable meta-experience. This approach not only enhances the model's learning process but also leads to significant performance improvements on various benchmarks."
                },
                "zh": {
                    "title": "å…ƒç»éªŒå­¦ä¹ ï¼šæå‡LLMæ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "Meta-Experience Learningï¼ˆMELï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å°†è‡ªæˆ‘æç‚¼çš„å…ƒç»éªŒèå…¥æ¨¡å‹çš„å‚æ•°è®°å¿†ä¸­ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ¯”è½¨è¿¹åˆ†æï¼Œè¯†åˆ«æ¨ç†é”™è¯¯çš„å…³é”®åˆ†å‰ç‚¹ï¼Œå¹¶å°†è¿™äº›é”™è¯¯æ€»ç»“ä¸ºå¯é‡ç”¨çš„å…ƒç»éªŒã€‚é€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ŒMELå°†å…ƒç»éªŒå†…åŒ–åˆ°LLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œä»è€Œä¿ƒè¿›çŸ¥è¯†çš„æœ‰æ•ˆé‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMELåœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3.92%åˆ°4.73%çš„Pass@1æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šçš„ä¸€è‡´æ€§æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11089",
            "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.11089",
            "abstract": "DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  \t\t\t\t\tAI-generated summary \t\t\t\t In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
            "score": 12,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "c18e7b1c9f3e8df4",
            "authors": [
                "Yicheng Chen",
                "Zerun Ma",
                "Xinchen Xie",
                "Yining Li",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11089.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ‘¨â€ğŸ³",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑƒĞ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ DataChef-32B â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Qwen3-1.7B-Base Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° AIME'25, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Automating Data Recipes for LLMs with DataChef-32B",
                    "desc": "DataChef-32B is a system that automates the creation of data recipes for adapting Large Language Models (LLMs) using reinforcement learning. It generates a complete data processing pipeline that transforms raw data into training sets, aiming to reduce the manual effort typically required. By utilizing a proxy reward to predict the effectiveness of these recipes, DataChef-32B can produce high-quality recipes that perform similarly to those crafted by human experts. This innovation not only streamlines the training process for LLMs but also contributes to the development of self-evolving AI systems."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æ•°æ®é…æ–¹ç”Ÿæˆï¼Œæå‡LLMæ€§èƒ½",
                    "desc": "DataChef-32B æ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œä»£ç†å¥–åŠ±è‡ªåŠ¨ç”Ÿæˆæ•°æ®é…æ–¹çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå°†åŸå§‹æ•°æ®æºè½¬åŒ–ä¸ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ŒDataChef-32B ç”Ÿæˆçš„é…æ–¹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸äººç±»ä¸“å®¶ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤ç ”ç©¶ä¸º LLM çš„è®­ç»ƒè‡ªåŠ¨åŒ–å’Œè‡ªæˆ‘è¿›åŒ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11149",
            "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
            "url": "https://huggingface.co/papers/2602.11149",
            "abstract": "Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.",
            "score": 10,
            "issue_id": 1021,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "b71f39aa275f49ce",
            "authors": [
                "Dawid J. Kopiczko",
                "Sagar Vaze",
                "Tijmen Blankevoort",
                "Yuki M. Asano"
            ],
            "affiliations": [
                "Mistral AI",
                "NVIDIA",
                "University of Technology Nuremberg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11149.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°: ĞºĞ°Ğº Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ»Ğ°ÑÑ‚ ÑƒĞ¼Ğ½ĞµĞµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ supervised fine-tuning Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¼Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Olmo3-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ 128 ÑĞ¿Ğ¾Ñ… Ğ½Ğ° 400 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ½Ğ° 12-26 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚, Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ 1 ÑĞ¿Ğ¾Ñ…Ñƒ Ğ½Ğ° 51200 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ± Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğº ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Repetition Over Quantity: Unlocking Better Performance in Language Models",
                    "desc": "This paper explores the effectiveness of training reasoning language models using repeated examples from smaller datasets instead of unique samples from larger datasets. It finds that supervised fine-tuning (SFT) benefits from training for more epochs on fewer samples, leading to better performance on benchmarks like AIME'24/25 and GPQA. The authors demonstrate that token accuracy can be a reliable measure for determining the optimal training duration, indicating when further training may not yield additional benefits. This research introduces the concept of the 'repetition advantage,' suggesting that full memorization can enhance generalization, which presents a new area for further investigation in machine learning."
                },
                "zh": {
                    "title": "é‡å¤è®­ç»ƒå°æ•°æ®é›†ï¼Œæå‡æ¨ç†æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨è¾ƒå°æ•°æ®é›†ä¸Šä½¿ç”¨é‡å¤ç¤ºä¾‹è®­ç»ƒæ¨ç†è¯­è¨€æ¨¡å‹çš„æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šè¿›è¡Œå•æ¬¡è®­ç»ƒç›¸æ¯”ï¼Œé‡å¤è®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åœ¨å›ºå®šçš„æ›´æ–°é¢„ç®—ä¸‹å¢åŠ è®­ç»ƒè½®æ¬¡ï¼Œæ¨¡å‹åœ¨è¾ƒå°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„å•è½®è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œè®­ç»ƒçš„æ ‡è®°å‡†ç¡®ç‡å¯ä»¥æœ‰æ•ˆæŒ‡ç¤ºé‡å¤è®­ç»ƒçš„é¥±å’ŒçŠ¶æ€ï¼Œä¸ºæ¨ç†æ¨¡å‹çš„ç›‘ç£å¾®è°ƒæä¾›äº†å®ç”¨çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10609",
            "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
            "url": "https://huggingface.co/papers/2602.10609",
            "abstract": "Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
            "score": 10,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "857c0a58eccaf142",
            "authors": [
                "Shuo He",
                "Lang Feng",
                "Xin Cheng",
                "Lei Feng",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "Southeast University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10609.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ°Ğ»Ğ¼Ğ°Ğ½Ğ¾Ğ²Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Online Causal Kalman Filtering Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ importance sampling Ğ² reinforcement learning Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹ importance sampling ĞºĞ°Ğº ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ ĞšĞ°Ğ»Ğ¼Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ KPO ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Stabilizing Policy Optimization with Kalman Filtering in RL",
                    "desc": "This paper introduces Online Causal Kalman Filtering (KPO) to improve the stability of policy optimization in reinforcement learning for large language models. It addresses the problem of high-variance token-level importance sampling (IS) ratios, which can destabilize training. By modeling IS ratios as evolving latent states and using Kalman filtering, the method updates these states based on past tokens, ensuring that the updates are more stable and effective. The experimental results show that KPO outperforms existing methods on complex math reasoning tasks."
                },
                "zh": {
                    "title": "åœ¨çº¿å› æœå¡å°”æ›¼æ»¤æ³¢ï¼šç¨³å®šå¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿å› æœå¡å°”æ›¼æ»¤æ³¢æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ çš„é«˜æ–¹å·®é‡è¦æ€§é‡‡æ ·é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å°†é‡è¦æ€§é‡‡æ ·æ¯”ç‡å»ºæ¨¡ä¸ºä¸æ–­æ¼”å˜çš„æ½œåœ¨çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨å¡å°”æ›¼æ»¤æ³¢è¿›è¡Œç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå±€éƒ¨çš„ç¦»çº¿ç­–ç•¥åå·®åœ¨ä»¤ç‰Œçº§åˆ«ä¸Šæ˜¯ä¸ä¸€è‡´çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´ç­–ç•¥æ¢¯åº¦æ›´æ–°çš„å¤±çœŸã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒä»¤ç‰Œé—´å±€éƒ¨ç»“æ„å˜åŒ–çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¹³æ»‘å™ªå£°ï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„ç­–ç•¥æ›´æ–°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07106",
            "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
            "url": "https://huggingface.co/papers/2602.07106",
            "abstract": "Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.",
            "score": 10,
            "issue_id": 1018,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "06e3c81476d35025",
            "authors": [
                "Haoyu Zhang",
                "Zhipeng Li",
                "Yiwen Guo",
                "Tianshu Yu"
            ],
            "affiliations": [
                "Independent Researcher",
                "LIGHTSPEED",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07106.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#audio",
                    "#3d",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ°: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸",
                    "desc": "Ex-Omni â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ñ 3D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ°Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ TQGF Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ InstructEx Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ Ğ¸ Ñ„Ğ°ÑÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Bridging Speech and 3D Animation in Language Models",
                    "desc": "Ex-Omni is an innovative framework designed to enhance omni-modal large language models (OLLMs) by integrating speech with 3D facial animation. It addresses the challenge of aligning discrete semantic reasoning with the continuous dynamics of facial motion by decoupling these processes. By using speech units as a temporal guide, Ex-Omni simplifies the learning process and improves the generation of synchronized speech and facial animations. The framework also introduces a new dataset, InstructEx, to support the training of OLLMs in this multimodal context, showing competitive performance in experiments."
                },
                "zh": {
                    "title": "è§£è€¦è¯­ä¹‰ä¸æ—¶é—´ï¼Œæå‡å¤šæ¨¡æ€äº¤äº’ä½“éªŒ",
                    "desc": "Ex-Omniæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è¯­ä¹‰æ¨ç†ä¸æ—¶é—´ç”Ÿæˆè§£è€¦ï¼Œå¢å¼ºå…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆOLLMsï¼‰ä¸è¯­éŸ³ä¼´éšçš„3Dé¢éƒ¨åŠ¨ç”»çš„ç»“åˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¯­éŸ³å•å…ƒä½œä¸ºæ—¶é—´æ”¯æ¶ï¼Œè§£å†³äº†LLMsä¸­ç¦»æ•£çš„è¯­ä¹‰æ¨ç†ä¸3Dé¢éƒ¨è¿åŠ¨æ‰€éœ€çš„ç»†ç²’åº¦æ—¶é—´åŠ¨æ€ä¹‹é—´çš„è¡¨ç¤ºä¸åŒ¹é…é—®é¢˜ã€‚Ex-Omnié‡‡ç”¨äº†ä¸€ç§ç»Ÿä¸€çš„ä»¤ç‰ŒæŸ¥è¯¢é—¨æ§èåˆæœºåˆ¶ï¼Œä¾¿äºæ§åˆ¶è¯­ä¹‰çš„æ³¨å…¥ï¼Œä»è€Œé™ä½å­¦ä¹ éš¾åº¦ã€‚é€šè¿‡å¼•å…¥InstructExæ•°æ®é›†ï¼ŒEx-Omnièƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ƒè¿›OLLMsä¸è¯­éŸ³ä¼´éšçš„3Dé¢éƒ¨åŠ¨ç”»çš„ç»“åˆï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¸ç°æœ‰çš„å¼€æºOLLMsç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09514",
            "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies",
            "url": "https://huggingface.co/papers/2602.09514",
            "abstract": "EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.",
            "score": 9,
            "issue_id": 1020,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "0293afbf8eacf7ff",
            "authors": [
                "Xavier Hu",
                "Jinxiang Xia",
                "Shengze Xu",
                "Kangqi Song",
                "Yishuo Yuan",
                "Guibin Zhang",
                "JinCheng Ren",
                "Boyu Feng",
                "Li Lu",
                "Tieyong Zeng",
                "Jiaheng Liu",
                "Minghao Liu",
                "He Zhu",
                "Yuchen Eleanor Jiang",
                "Wei Wang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09514.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "EcoGym â€” ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-based Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ (Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ñ, Ñ„Ñ€Ğ¸Ğ»Ğ°Ğ½Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾) Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 1000 ÑˆĞ°Ğ³Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ‡Ğ¸ÑÑ‚Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ñ…Ğ¾Ğ´, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¸ Ğ²ÑĞµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "EcoGym: A New Standard for Long-Horizon Planning in Economic Environments",
                    "desc": "EcoGym is a new benchmark designed to assess the long-term planning abilities of large language model (LLM) agents in dynamic economic environments. Unlike previous evaluation methods that are often limited to specific scenarios, EcoGym provides a continuous decision-making framework across three diverse environments: Vending, Freelance, and Operation. The benchmark focuses on measuring business-relevant outcomes such as net worth and income, emphasizing the importance of strategic coherence and adaptability in uncertain conditions. Experiments reveal that while no single LLM excels in all scenarios, they often struggle with either high-level strategy or efficient execution, highlighting the need for improved agent design."
                },
                "zh": {
                    "title": "EcoGymï¼šè¯„ä¼°é•¿æœŸè§„åˆ’èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "EcoGymæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨äº’åŠ¨ç»æµç¯å¢ƒä¸­é•¿æœŸè§„åˆ’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒæä¾›äº†ä¸‰ä¸ªå¤šæ ·åŒ–çš„ç¯å¢ƒï¼šè‡ªåŠ¨å”®è´§æœºã€è‡ªç”±èŒä¸šå’Œè¿è¥ï¼Œå…è®¸åœ¨ç»Ÿä¸€çš„å†³ç­–è¿‡ç¨‹ä¸­è¿›è¡ŒæŒç»­çš„è®¡åˆ’å’Œæ‰§è¡Œã€‚EcoGymçš„è¯„ä¼°åŸºäºä¸å•†ä¸šç›¸å…³çš„ç»“æœï¼Œå¦‚å‡€èµ„äº§ã€æ”¶å…¥å’Œæ—¥æ´»è·ƒç”¨æˆ·ï¼Œæ—¨åœ¨å®ç°é•¿æœŸæˆ˜ç•¥çš„ä¸€è‡´æ€§å’Œåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ€§åŠéšæœºæ€§ä¸‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ²¡æœ‰å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰åœºæ™¯ä¸­è¡¨ç°æœ€ä½³ï¼Œä¸”æ¨¡å‹åœ¨é«˜å±‚ç­–ç•¥æˆ–é«˜æ•ˆæ‰§è¡Œæ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ¬¡ä¼˜æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08099",
            "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval",
            "url": "https://huggingface.co/papers/2602.08099",
            "abstract": "Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.",
            "score": 9,
            "issue_id": 1026,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "30a96629f933c122",
            "authors": [
                "Issar Tzachor",
                "Dvir Samuel",
                "Rami Ben-Ari"
            ],
            "affiliations": [
                "OriginAI, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08099.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#video",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² MLLM Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ»Ñ‘Ğ³ĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ¿Ñ‚Ğ¸Ğ¾Ğ½Ñ‹ Ğ² ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ±ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Video-Text Retrieval with MLLMs: No Visuals Needed!",
                    "desc": "This paper explores how generative multimodal large language models (MLLMs) can be used for video-text embedding and retrieval without needing visual data. The authors analyze the intermediate layers of MLLMs, revealing that they contain valuable information for video tasks. By combining these intermediate embeddings with a calibrated MLLM head, the researchers achieve impressive zero-shot retrieval performance. Additionally, they propose a text-based alignment method that enhances video-text learning, leading to superior results compared to existing techniques, all without fine-tuning on visual data."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æå‡è§†é¢‘æ£€ç´¢æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè§†é¢‘-æ–‡æœ¬åµŒå…¥å’Œæ£€ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMLLMçš„ä¸­é—´å±‚å·²ç»ç¼–ç äº†å¤§é‡ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ç»“åˆè¿™äº›ä¸­é—´å±‚çš„åµŒå…¥ä¸ç»è¿‡æ ¡å‡†çš„MLLMå¤´æ¥å®ç°å¼ºå¤§çš„é›¶-shotæ£€ç´¢æ€§èƒ½ï¼Œè€Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§çš„åŸºäºæ–‡æœ¬çš„å¯¹é½ç­–ç•¥ï¼Œå°†å¯†é›†çš„è§†é¢‘æ ‡é¢˜æ˜ å°„åˆ°ç®€çŸ­çš„æ‘˜è¦ï¼Œä»è€Œåœ¨æ²¡æœ‰è§†è§‰ç›‘ç£çš„æƒ…å†µä¸‹å®ç°è§†é¢‘-æ–‡æœ¬åµŒå…¥å­¦ä¹ ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸¸è§çš„è§†é¢‘æ£€ç´¢åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10999",
            "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
            "url": "https://huggingface.co/papers/2602.10999",
            "abstract": "CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
            "score": 8,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "c72eb824352bcaac",
            "authors": [
                "Yusong Lin",
                "Haiyang Wang",
                "Shuzhe Wu",
                "Lue Fan",
                "Feiyang Pan",
                "Sanyuan Zhao",
                "Dandan Tu"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Huawei Technologies Co., Ltd",
                "Institute of Automation, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10999.jpg",
            "data": {
                "categories": [],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLI-Gym, ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹, Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ Dockerfile Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¾Ğ½Ğ¸ Ğ±ĞµÑ€ÑƒÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ĞµÑ‘ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸, Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ¸Ğ· 1,655 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LiberCoder Ğ¿ÑƒÑ‚ĞµĞ¼ fine-tuning Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LiberCoder Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (+21.1%) Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Terminal-Bench, ÑÑ‚Ğ°Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¼ state-of-the-art Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Task Generation for Agentic Coding with CLI-Gym and LiberCoder",
                    "desc": "This paper introduces CLI-Gym, a novel approach for generating a large set of environment-intensive tasks by simulating and exploring historical environments. It leverages execution feedback to trace back to earlier states of a system that encountered runtime failures, allowing for the creation of tasks based on these buggy states and their associated error messages. Additionally, the paper presents LiberCoder, a fine-tuned model that significantly enhances performance on the Terminal-Bench benchmark by utilizing curated successful trajectories. This work represents a significant advancement in the scalable derivation of tasks that require interaction with command line interfaces, marking a first in the field."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡ç”Ÿæˆç¯å¢ƒå¯†é›†å‹ä»»åŠ¡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCLI-Gymçš„æ–¹æ³•ï¼Œç”¨äºå¤§è§„æ¨¡ç”Ÿæˆç¯å¢ƒå¯†é›†å‹ä»»åŠ¡ï¼Œä¸»è¦é€šè¿‡æ¨¡æ‹Ÿå’Œæ¢ç´¢ç¯å¢ƒå†å²æ¥å®ç°ã€‚æˆ‘ä»¬é€šè¿‡æ‰§è¡Œåé¦ˆå¼•å¯¼ä»£ç†ï¼Œè¿½è¸ªå¥åº·ç¯å¢ƒçš„å†å²çŠ¶æ€ï¼Œä»è€Œé€†è½¬åˆ°æ—©æœŸçš„è¿è¡Œå¤±è´¥çŠ¶æ€ï¼Œå¹¶ä»ä¸­æå–ä»»åŠ¡ã€‚æ­¤æ–¹æ³•ç”Ÿæˆäº†1,655ä¸ªç¯å¢ƒå¯†é›†å‹ä»»åŠ¡ï¼Œæˆä¸ºåŒç±»ä¸­æœ€å¤§çš„é›†åˆã€‚æ­¤å¤–ï¼Œç»è¿‡ç²¾ç»†è°ƒä¼˜çš„æ¨¡å‹LiberCoderåœ¨Terminal-Benchä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç»å¯¹æé«˜äº†21.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09713",
            "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
            "url": "https://huggingface.co/papers/2602.09713",
            "abstract": "Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
            "score": 8,
            "issue_id": 1017,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "88480aeee4c64487",
            "authors": [
                "Ruisi Zhao",
                "Haoren Zheng",
                "Zongxin Yang",
                "Hehe Fan",
                "Yi Yang"
            ],
            "affiliations": [
                "DBMI, HMS, Harvard University",
                "ReLER, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09713.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞÑ‚ ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ° Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Stroke3D â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ pipeline. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´ĞµÑ€ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞºĞµĞ»ĞµÑ‚Ğ° (Sk-VAE) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ (Sk-DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞºĞµĞ»ĞµÑ‚Ğ°, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ 2D-ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ°Ğ¼Ğ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ TextuRig Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ SKA-DPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ°ÑÑĞµÑ‚Ñ‹ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼, Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "From 2D Strokes to Rigged 3D Meshes: Stroke3D Revolutionizes Animation Creation!",
                    "desc": "Stroke3D is a novel framework that generates rigged 3D meshes from 2D strokes and text prompts using a two-stage pipeline. The first stage involves Controllable Skeleton Generation, where a Skeletal Graph VAE encodes the skeleton's structure, allowing for fine control over its creation. The second stage focuses on Enhanced Mesh Synthesis, which produces a textured mesh based on the generated skeleton, utilizing a dataset of rigged meshes and a preference optimization strategy for improved quality. This approach simplifies the process of creating animatable 3D assets, making it more intuitive for users."
                },
                "zh": {
                    "title": "Stroke3Dï¼šä»2Dçº¿æ¡ç”Ÿæˆå¯åŠ¨ç”»çš„3Dç½‘æ ¼",
                    "desc": "Stroke3D æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡ç”¨æˆ·è¾“å…¥çš„ 2D çº¿æ¡å’Œæ–‡æœ¬æç¤ºç›´æ¥ç”Ÿæˆå¸¦éª¨æ¶çš„ 3D ç½‘æ ¼ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µçš„æµç¨‹ï¼Œé¦–å…ˆç”Ÿæˆå¯æ§çš„éª¨æ¶ï¼Œç„¶ååˆæˆé«˜è´¨é‡çš„ç½‘æ ¼ã€‚æˆ‘ä»¬ä½¿ç”¨äº†éª¨æ¶å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆSk-VAEï¼‰æ¥ç¼–ç éª¨æ¶çš„å›¾ç»“æ„ï¼Œå¹¶é€šè¿‡æ–‡æœ¬å’Œ 2D çº¿æ¡è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚æœ€ç»ˆï¼Œç»“åˆå¢å¼ºçš„ç½‘æ ¼åˆæˆæŠ€æœ¯ï¼ŒStroke3D æä¾›äº†ä¸€ç§æ›´ç›´è§‚çš„å·¥ä½œæµç¨‹ï¼Œä¾¿äºåˆ›å»ºå¯åŠ¨ç”»çš„ 3D å†…å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11103",
            "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development",
            "url": "https://huggingface.co/papers/2602.11103",
            "abstract": "GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.",
            "score": 7,
            "issue_id": 1032,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "23b96ef5022a92ac",
            "authors": [
                "Wayne Chi",
                "Yixiong Fang",
                "Arnav Yayavaram",
                "Siddharth Yayavaram",
                "Seth Karten",
                "Qiuhong Anna Wei",
                "Runkun Chen",
                "Alexander Wang",
                "Valerie Chen",
                "Ameet Talwalkar",
                "Chris Donahue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11103.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ³Ñ€Ñ‹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GameDevBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 132 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²ĞµĞ±-Ñ‚ÑƒÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»Ğ°Ñ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑˆĞµĞ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ¿Ñ€Ğ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ Ğ¸Ğ³Ñ€Ñ‹. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€ĞµÑˆĞ¸Ğ» Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 54,5% Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ 46,9% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ´Ğ¾ 31,6% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2D Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Claude Sonnet 4.5 Ñ 33,3% Ğ´Ğ¾ 47,7%."
                },
                "en": {
                    "title": "GameDevBench: Bridging AI and Game Development Challenges",
                    "desc": "GameDevBench is a new benchmark designed to evaluate AI agents on complex game development tasks that require both software coding skills and deep understanding of various media types. It includes 132 tasks that challenge agents to work with intricate codebases and multimodal assets like graphics and animations. The benchmark reveals that current agents struggle significantly, achieving only 54.5% success on average, with performance varying based on task complexity. To enhance agent performance, the study introduces simple feedback mechanisms using images and videos, which have shown to improve task completion rates."
                },
                "zh": {
                    "title": "GameDevBenchï¼šæ¸¸æˆå¼€å‘æ™ºèƒ½ä½“çš„è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "GameDevBenchæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨æ¸¸æˆå¼€å‘ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ï¼Œç»“åˆäº†è½¯ä»¶å¼€å‘çš„å¤æ‚æ€§å’Œæ·±åº¦å¤šæ¨¡æ€ç†è§£çš„è¦æ±‚ã€‚å°½ç®¡ç¼–ç æ™ºèƒ½ä½“å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å‘å±•å´ç›¸å¯¹æ»åã€‚æ¸¸æˆå¼€å‘æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨å¤æ‚çš„ä»£ç åº“ä¸­å¯¼èˆªï¼ŒåŒæ—¶å¤„ç†å¤šæ¨¡æ€èµ„äº§ï¼Œå¦‚ç€è‰²å™¨ã€ç²¾çµå’ŒåŠ¨ç”»ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»»åŠ¡çš„éš¾åº¦ä¸å¤šæ¨¡æ€å¤æ‚æ€§ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œå¹¶æå‡ºäº†ç®€å•çš„å›¾åƒå’Œè§†é¢‘åé¦ˆæœºåˆ¶ï¼Œä»¥æé«˜æ™ºèƒ½ä½“çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10231",
            "title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
            "url": "https://huggingface.co/papers/2602.10231",
            "abstract": "Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.",
            "score": 7,
            "issue_id": 1022,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "e10a1e05ca1c1b96",
            "authors": [
                "Kirill Pavlenko",
                "Alexander Golubev",
                "Simon Karasik",
                "Boris Yangel"
            ],
            "affiliations": [
                "Nebius",
                "The Humanoid"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10231.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Blockwise Advantage Estimation - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ²ÑĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ, ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ Group Relative Policy Optimization, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ±Ğ»Ğ¾ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ - ÑÑ‚Ğ¾ Outcome-Conditioned Baseline, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Optimizing Structured Generations with Blockwise Advantage Estimation",
                    "desc": "Blockwise Advantage Estimation is a method designed to improve reward assignment in structured text generation tasks. It addresses the problem of reward interference by giving each segment of text its own advantage score, rather than using a single score for all tokens. This approach helps to prevent misattribution of credit among unrelated objectives, making the learning process more efficient. By using an Outcome-Conditioned Baseline, the method estimates rewards without the need for costly nested rollouts, allowing for better performance in tasks that require uncertainty estimation."
                },
                "zh": {
                    "title": "å—çŠ¶ä¼˜åŠ¿ä¼°è®¡ï¼šä¼˜åŒ–ç»“æ„åŒ–ç”Ÿæˆçš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå—çŠ¶ä¼˜åŠ¿ä¼°è®¡ï¼ˆBlockwise Advantage Estimationï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç»“æ„åŒ–ç”Ÿæˆä¸­çš„å¥–åŠ±å¹²æ‰°é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ºä¸åŒçš„æ–‡æœ¬å—åˆ†é…ç‹¬ç«‹çš„ä¼˜åŠ¿ï¼Œå¹¶ä½¿ç”¨ç»“æœæ¡ä»¶åŸºçº¿æ¥é¿å…æ˜‚è´µçš„åµŒå¥—å›æ»šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå—çŠ¶ä¼˜åŠ¿ä¼°è®¡èƒ½å¤Ÿå‡å°‘å¯¹æ‰‹åŠ¨è®¾è®¡æ ‡é‡å¥–åŠ±çš„ä¾èµ–ï¼Œå¹¶è‡ªç„¶æ‰©å±•åˆ°æ›´å¤šç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ•°å­¦ä»»åŠ¡ä¸­æœ‰æ•ˆå‡è½»äº†å¥–åŠ±å¹²æ‰°ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶ä¿æŒäº†ä¿¡å¿ƒåŠ æƒé›†æˆçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09901",
            "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
            "url": "https://huggingface.co/papers/2602.09901",
            "abstract": "A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
            "score": 6,
            "issue_id": 1017,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "54bedc96b8103bef",
            "authors": [
                "Jianzhao Huang",
                "Xiaorui Huang",
                "Fei Zhao",
                "Yunpeng Liu",
                "Hui Zhang",
                "Fangcheng Shi",
                "Congfeng Li",
                "Zechen Sun",
                "Yi Wu",
                "Yao Hu",
                "Yunhan Bai",
                "Shaosheng Cao"
            ],
            "affiliations": [
                "Xiaohongshu Inc., China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09901.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QP-OneModel â€” ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ² Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ’ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ QP-OneModel Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ² 7.35% Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Xiaohongshu."
                },
                "en": {
                    "title": "Unified Generative Model for Enhanced Social Network Query Processing",
                    "desc": "This paper introduces QP-OneModel, a unified generative large language model designed to enhance query processing in social network search engines. It addresses the limitations of traditional systems that use isolated models by integrating multi-task learning and reinforcement learning to improve semantic understanding. The model reformulates various sub-tasks into a cohesive sequence generation framework, allowing for better alignment and performance across tasks. Evaluation results demonstrate significant improvements in accuracy and user engagement, showcasing the model's effectiveness in real-world applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹æå‡ç¤¾äº¤ç½‘ç»œæŸ¥è¯¢å¤„ç†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹QP-OneModelï¼Œç”¨äºç¤¾äº¤ç½‘ç»œæœç´¢æŸ¥è¯¢å¤„ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æé«˜äº†è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„ç¦»æ•£æ¨¡å‹ä¸åŒï¼ŒQP-OneModelå°†å¼‚æ„å­ä»»åŠ¡é‡æ–°æ„é€ æˆç»Ÿä¸€çš„åºåˆ—ç”ŸæˆèŒƒå¼ï¼Œé‡‡ç”¨æ¸è¿›çš„ä¸‰é˜¶æ®µå¯¹é½ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQP-OneModelåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åˆ¤åˆ«åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02192",
            "title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.02192",
            "abstract": "ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.",
            "score": 6,
            "issue_id": 1027,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "594f7922e59b5610",
            "authors": [
                "Jie Xiao",
                "Meng Chen",
                "Qingnan Ren",
                "Jingwei Song",
                "Jiaqi Huang",
                "Yangshen Deng",
                "Chris Tong",
                "Wanyi Chen",
                "Suli Wang",
                "Ziqian Bi",
                "Shuo Lu",
                "Yiqun Duan",
                "Xu Wang",
                "Rymon Yu",
                "Ween Yang",
                "Lynn Ai",
                "Eric Yang",
                "Bill Shi"
            ],
            "affiliations": [
                "Fudan University",
                "Gradient",
                "Technical University of Darmstadt",
                "The University of Hong Kong",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02192.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#inference",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "ECHO-2 â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ…, Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ² ÑĞµÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 4B Ğ¸ 8B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "ECHO-2: Efficient Post-Training for Large Language Models through Distributed Reinforcement Learning",
                    "desc": "ECHO-2 is a distributed reinforcement learning framework designed to enhance the post-training process of large language models (LLMs). It efficiently manages the overlapping of rollout generation, policy dissemination, and training while addressing issues like policy staleness and network latency. By utilizing remote inference workers and allowing user control over policy staleness, ECHO-2 optimizes resource usage and reduces costs. Experimental results demonstrate that ECHO-2 achieves significant cost efficiency improvements while maintaining competitive reinforcement learning rewards compared to existing methods."
                },
                "zh": {
                    "title": "ECHO-2ï¼šé«˜æ•ˆçš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "ECHO-2æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é‡å çš„å›åˆç”Ÿæˆã€ä¼ æ’­å’Œè®­ç»ƒæ¥é«˜æ•ˆåœ°è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨è¿œç¨‹æ¨ç†å·¥ä½œè€…å’Œæ˜¾è‘—ä¼ æ’­å»¶è¿Ÿä¸‹çš„åè°ƒå’Œç­–ç•¥ä¼ æ’­é—®é¢˜ã€‚ECHO-2ç»“åˆäº†é›†ä¸­å­¦ä¹ ä¸åˆ†å¸ƒå¼å›åˆæ‰§è¡Œï¼Œå¹¶å°†ç­–ç•¥è¿‡æ—¶è§†ä¸ºç”¨æˆ·å¯æ§å‚æ•°ï¼Œä»è€Œå®ç°äº†å›åˆç”Ÿæˆã€ä¼ æ’­å’Œè®­ç»ƒçš„é‡å ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECHO-2åœ¨å®é™…å®½åŸŸå¸¦å®½æ¡ä»¶ä¸‹æ˜¾è‘—æé«˜äº†æˆæœ¬æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¼ºåŸºçº¿ç›¸å½“çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10179",
            "title": "When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models",
            "url": "https://huggingface.co/papers/2602.10179",
            "abstract": "Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.",
            "score": 5,
            "issue_id": 1017,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "1791cf6930f61520",
            "authors": [
                "Jiacheng Hou",
                "Yining Sun",
                "Ruochong Jin",
                "Haochen Han",
                "Fangming Liu",
                "Wai Kin Victor Chan",
                "Alex Jinpeng Wang"
            ],
            "affiliations": [
                "Central South University, Changsha, China",
                "Peng Cheng Laboratory, Shenzhen, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10179.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#security",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Vision-Centric Jailbreak Attack (VJA) â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ‚Ğ°ĞºÑƒ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ IESBench, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VJA ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ´Ğ¾ 80.9%. Ğ”Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸Ğ½Ñ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Visual Attacks: A New Threat to Image Editing Safety",
                    "desc": "This paper introduces a new type of attack called Vision-Centric Jailbreak Attack (VJA), which targets image editing models by using malicious visual inputs instead of text. The authors highlight the risks associated with the shift to vision-prompt editing, where user intent is interpreted from visual cues. They also present IESBench, a benchmark designed to evaluate the safety of image editing models against these visual attacks. To counteract the vulnerabilities exposed by VJA, the paper proposes a novel defense mechanism that enhances model safety without requiring additional training or significant computational resources."
                },
                "zh": {
                    "title": "è§†è§‰è¾“å…¥çš„å®‰å…¨æŒ‘æˆ˜ä¸é˜²å¾¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰åˆ°è§†è§‰çš„è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰ä¸­å¿ƒè¶Šç‹±æ”»å‡»ï¼ˆVJAï¼‰ï¼Œå®ƒé€šè¿‡æ¶æ„çš„è§†è§‰è¾“å…¥æ¥ç ´åå›¾åƒç¼–è¾‘æ¨¡å‹çš„å®‰å…¨æ€§ã€‚éšç€å›¾åƒç¼–è¾‘æ¨¡å‹çš„å‘å±•ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥é€šè¿‡è§†è§‰è¾“å…¥è¿›è¡Œç¼–è¾‘ï¼Œè¿™è™½ç„¶æé«˜äº†å¯ç”¨æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†IESBenchï¼Œä¸€ä¸ªä¸“æ³¨äºå®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å›¾åƒç¼–è¾‘æ¨¡å‹çš„è„†å¼±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVJAèƒ½å¤Ÿæœ‰æ•ˆæ”»å‡»å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ï¼ŒæˆåŠŸç‡é«˜è¾¾80.9%ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå†…çœå¤šæ¨¡æ€æ¨ç†çš„é˜²å¾¡æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å®‰å…¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08030",
            "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models",
            "url": "https://huggingface.co/papers/2602.08030",
            "abstract": "Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.   Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.",
            "score": 5,
            "issue_id": 1018,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "1d7fefded0d3cd52",
            "authors": [
                "Yilun Zheng",
                "Dongyang Ma",
                "Tian Liang",
                "Jiahao Xu",
                "Xinting Huang",
                "Lihui Chen",
                "Haitao Mi",
                "Yan Wang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08030.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#reasoning",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Free()LM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Free-Module Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ LLM Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²ÑĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑÑ Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Free()LM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ 0% Ğ´Ğ¾ 50% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering AI with the Freedom to Forget",
                    "desc": "Free()LM is a novel approach that enhances reasoning models by incorporating a self-forgetting mechanism through a Free-Module LoRA adapter. This model addresses the issue of excessive thinking tokens, which can hinder performance due to the accumulation of redundant information. By alternating between reasoning and cleaning modes, Free()LM effectively prunes unnecessary context, resulting in a more efficient and accurate model. Experimental results demonstrate significant performance improvements across various scales, particularly in long-horizon tasks where traditional models struggle."
                },
                "zh": {
                    "title": "è‡ªç”±é—å¿˜ï¼Œæå‡æ¨ç†èƒ½åŠ›",
                    "desc": "Free()LM é€šè¿‡å¼•å…¥è‡ªæˆ‘é—å¿˜æœºåˆ¶ï¼Œè§£å†³äº†æ¨ç†æ¨¡å‹çš„å±€é™æ€§ã€‚å®ƒä½¿ç”¨ä¸€ç§å¯æ’æ‹”çš„ LoRA é€‚é…å™¨ï¼Œç§°ä¸º Free-Moduleï¼Œæ¥æé«˜æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡å’Œé•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨æ¨ç†å’Œæ¸…ç†æ¨¡å¼ä¹‹é—´åå¤åˆ‡æ¢ï¼ŒåŠ¨æ€è¯†åˆ«å¹¶ä¿®å‰ªæ— ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œä¿æŒç´§å‡‘ä¸”æ— å™ªå£°çš„çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFree()LM åœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10748",
            "title": "Benchmarking Large Language Models for Knowledge Graph Validation",
            "url": "https://huggingface.co/papers/2602.10748",
            "abstract": "Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge Graphs (KGs) store structured factual knowledge by linking entities through relationships, crucial for many applications. These applications depend on the KG's factual accuracy, so verifying facts is essential, yet challenging. Expert manual verification is ideal but impractical on a large scale. Automated methods show promise but are not ready for real-world KGs. Large Language Models (LLMs) offer potential with their semantic understanding and knowledge access, yet their suitability and effectiveness for KG fact validation remain largely unexplored.   In this paper, we introduce FactCheck, a benchmark designed to evaluate LLMs for KG fact validation across three key dimensions: (1) LLMs internal knowledge; (2) external evidence via Retrieval-Augmented Generation (RAG); and (3) aggregated knowledge employing a multi-model consensus strategy. We evaluated open-source and commercial LLMs on three diverse real-world KGs. FactCheck also includes a RAG dataset with 2+ million documents tailored for KG fact validation. Additionally, we offer an interactive exploration platform for analyzing verification decisions.   The experimental analyses demonstrate that while LLMs yield promising results, they are still not sufficiently stable and reliable to be used in real-world KG validation scenarios. Integrating external evidence through RAG methods yields fluctuating performance, providing inconsistent improvements over more streamlined approaches -- at higher computational costs. Similarly, strategies based on multi-model consensus do not consistently outperform individual models, underscoring the lack of a one-fits-all solution. These findings further emphasize the need for a benchmark like FactCheck to systematically evaluate and drive progress on this difficult yet crucial task.",
            "score": 4,
            "issue_id": 1020,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "bd0cff7ce6e888e0",
            "authors": [
                "Farzad Shami",
                "Stefano Marchesin",
                "Gianmaria Silvello"
            ],
            "affiliations": [
                "Aalto University",
                "University of Padua"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10748.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ LLM Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FactCheck Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼: Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº-Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ LLM Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¾Ğ½Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· RAG Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ°Ñ‘Ñ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "FactCheck: Benchmarking LLMs for Reliable Knowledge Graph Validation",
                    "desc": "This paper addresses the challenges of using large language models (LLMs) for validating facts in knowledge graphs (KGs), which are essential for maintaining factual accuracy in various applications. The authors introduce FactCheck, a benchmark that evaluates LLMs based on their internal knowledge, the use of external evidence through Retrieval-Augmented Generation (RAG), and multi-model consensus strategies. Despite showing potential, the results indicate that LLMs lack the stability and reliability needed for real-world KG validation, with RAG methods and consensus strategies yielding inconsistent performance. The study highlights the importance of systematic evaluation through benchmarks like FactCheck to improve the effectiveness of automated fact validation methods."
                },
                "zh": {
                    "title": "FactCheckï¼šæ¨åŠ¨çŸ¥è¯†å›¾è°±éªŒè¯çš„åŸºå‡†è¯„ä¼°",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±äº‹å®éªŒè¯ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç¼ºä¹ç¨³å®šæ€§å’Œå¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†FactCheckåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±äº‹å®éªŒè¯ä¸­çš„è¡¨ç°ï¼Œä¸»è¦ä»å†…éƒ¨çŸ¥è¯†ã€å¤–éƒ¨è¯æ®å’Œå¤šæ¨¡å‹å…±è¯†ä¸‰ä¸ªç»´åº¦è¿›è¡Œåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ä»¤äººé¼“èˆï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶ä¸å¤Ÿç¨³å®šå’Œå¯é ã€‚FactCheckåŸºå‡†çš„å»ºç«‹æœ‰åŠ©äºç³»ç»Ÿæ€§åœ°è¯„ä¼°å’Œæ¨åŠ¨è¿™ä¸€é‡è¦ä»»åŠ¡çš„è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08489",
            "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
            "url": "https://huggingface.co/papers/2602.08489",
            "abstract": "Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.",
            "score": 4,
            "issue_id": 1018,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "8337d2a84b022cca",
            "authors": [
                "Hyunseok Lee",
                "Soheil Abbasloo",
                "Jihoon Tack",
                "Jinwoo Shin"
            ],
            "affiliations": [
                "KAIST",
                "Microsoft Research",
                "Microsoft Research Asia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08489.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞšÑ€ĞµĞ¿ĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RLTR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, RLTR Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RLTR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Transferable Rewards",
                    "desc": "Reinforcement Learning with Transferable Reward (RLTR) improves the reasoning abilities of large language models (LLMs) by focusing on the stability and generalizability of their reasoning processes. Unlike previous methods that only emphasize the correctness of final answers, RLTR ensures that reasoning can be effectively transferred between models, allowing for better guidance and interpretation. This method uses transfer rewards to evaluate if a partial reasoning sequence from one model can help another model arrive at the correct conclusion. As a result, RLTR not only enhances the accuracy of answers but also does so with greater efficiency, requiring fewer training steps while maintaining robust reasoning capabilities."
                },
                "zh": {
                    "title": "å¯è½¬ç§»å¥–åŠ±æå‡æ¨ç†ç¨³å¥æ€§",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯è½¬ç§»å¥–åŠ±ï¼ˆRLTRï¼‰é€šè¿‡è½¬ç§»å¥–åŠ±å¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå¯æ¨å¹¿æ€§ï¼Œå…è®¸ä¸åŒæ¨¡å‹ä¹‹é—´çš„äº¤å‰æŒ‡å¯¼ã€‚RLTRçš„æ ¸å¿ƒåœ¨äºæµ‹è¯•ä¸€ä¸ªæ¨¡å‹çš„éƒ¨åˆ†æ¨ç†æ˜¯å¦èƒ½å¤Ÿå¼•å¯¼å¦ä¸€ä¸ªæ¨¡å‹å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä»è€Œé¼“åŠ±ç”Ÿæˆæ›´ç¨³å®šã€å¯è§£é‡Šå’ŒçœŸæ­£å¯æ¨å¹¿çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLTRåœ¨æ ·æœ¬ä¸€è‡´æ€§å’Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œä¸”è®­ç»ƒæ­¥éª¤æ˜¾è‘—å‡å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07954",
            "title": "Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation",
            "url": "https://huggingface.co/papers/2602.07954",
            "abstract": "Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65%) and very low false positive rate (0.63%) on real user prompts, outperforming HerBERT-PL-Guard (31.55% precision, 4.70% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.",
            "score": 4,
            "issue_id": 1020,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "c263a012d21218bb",
            "authors": [
                "Krzysztof WrÃ³bel",
                "Jan Maria Kowalski",
                "Jerzy Surma",
                "Igor Ciuciura",
                "Maciej SzymaÅ„ski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.07954.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#low_resource",
                    "#dataset",
                    "#benchmark",
                    "#ethics",
                    "#small_models",
                    "#multilingual"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Bielik Guard â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.1B Ğ¸ 0.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 6885 Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ¸ÑĞºĞ¾Ğ²: Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚ÑŒ/Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ñ, Ğ½ĞµÑ†ĞµĞ½Ğ·ÑƒÑ€Ğ½Ğ°Ñ Ğ»ĞµĞºÑĞ¸ĞºĞ°, ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ¿Ñ€ĞµÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 0.5B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ F1-score 0.791 Ğ½Ğ° Ğ¼Ğ¸ĞºÑ€Ğ¾-ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ğ° 0.1B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ (0.63%). ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ½Ğµ Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bielik Guard: Precision and Efficiency in Polish Content Safety Classification",
                    "desc": "Bielik Guard is a family of Polish language safety classifiers designed to efficiently categorize content into five safety domains: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. It includes two model variants, one with 0.1 billion parameters and another with 0.5 billion parameters, both fine-tuned on a dataset of 6,885 Polish texts. The models demonstrate high accuracy, with the 0.5B variant achieving impressive F1 scores and the 0.1B variant excelling in precision and low false positive rates. These classifiers aim to provide nuanced responses rather than merely blocking content, making them suitable for sensitive topics."
                },
                "zh": {
                    "title": "Bielik Guardï¼šæ³¢å…°è¯­è¨€å†…å®¹å®‰å…¨çš„é«˜æ•ˆå®ˆæŠ¤è€…",
                    "desc": "Bielik Guardæ˜¯ä¸€ç§ç´§å‡‘çš„æ³¢å…°è¯­è¨€å®‰å…¨åˆ†ç±»å™¨å®¶æ—ï¼ŒåŒ…å«ä¸¤ä¸ªå˜ä½“ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¸”å‡†ç¡®åœ°å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚å®ƒä»¬è¢«è®­ç»ƒåœ¨ä¸€ä¸ªåŒ…å«6885ä¸ªæ³¢å…°æ–‡æœ¬çš„ç¤¾åŒºæ³¨é‡Šæ•°æ®é›†ä¸Šï¼Œèƒ½å¤Ÿè¯†åˆ«ä»‡æ¨/æ”»å‡»ã€ç²—ä¿—ã€æ€§å†…å®¹ã€çŠ¯ç½ªå’Œè‡ªæ®‹ç­‰äº”ä¸ªå®‰å…¨ç±»åˆ«ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œ0.5Bå˜ä½“åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.791ï¼ˆå¾®å¹³å‡ï¼‰å’Œ0.785ï¼ˆå®å¹³å‡ï¼‰ã€‚è€Œ0.1Bå˜ä½“åˆ™åœ¨çœŸå®ç”¨æˆ·æç¤ºä¸­å±•ç°å‡ºå“è¶Šçš„ç²¾ç¡®åº¦å’Œæä½çš„è¯¯æŠ¥ç‡ï¼Œæä¾›äº†æ¯”ç®€å•å†…å®¹å±è”½æ›´åˆé€‚çš„å“åº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09014",
            "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
            "url": "https://huggingface.co/papers/2602.09014",
            "abstract": "ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
            "score": 3,
            "issue_id": 1017,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "d34bd75b555fb1b3",
            "authors": [
                "Zihan Yang",
                "Shuyuan Tu",
                "Licheng Zhang",
                "Qi Dai",
                "Yu-Gang Jiang",
                "Zuxuan Wu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research Asia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09014.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ArcFlow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ, ArcFlow Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞ¼ĞµÑÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¾Ñ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ ArcFlow Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ², Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµĞ½ĞµĞµ 5% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 40-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "ArcFlow: Fast and Efficient Few-Step Distillation for Diffusion Models",
                    "desc": "ArcFlow is a novel framework designed to improve the efficiency of diffusion models by reducing the number of inference steps needed for high-quality image generation. It achieves this by using non-linear flow trajectories to better approximate the complex behavior of teacher models during the denoising process. By parameterizing the velocity field as a mixture of continuous momentum processes, ArcFlow captures the evolving dynamics of the inference trajectory, leading to more accurate and coherent outputs. This method allows for significant speed improvements, achieving a 40x reduction in inference time while maintaining the quality of generated images with minimal parameter adjustments."
                },
                "zh": {
                    "title": "ArcFlowï¼šå¿«é€Ÿé«˜æ•ˆçš„å°‘æ­¥è’¸é¦æ¡†æ¶",
                    "desc": "ArcFlowæ˜¯ä¸€ç§å°‘æ­¥è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨éçº¿æ€§æµè½¨è¿¹æ¥è¿‘ä¼¼æ•™å¸ˆæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œå®ç°å¿«é€Ÿæ¨ç†ä¸”è´¨é‡æŸå¤±æœ€å°ã€‚ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•é€šå¸¸ä½¿ç”¨çº¿æ€§æ·å¾„æ¥è¿‘ä¼¼æ•™å¸ˆè½¨è¿¹ï¼Œè¿™å¯¼è‡´éš¾ä»¥åŒ¹é…ä¸æ–­å˜åŒ–çš„åˆ‡çº¿æ–¹å‘ï¼Œè¿›è€Œå½±å“ç”Ÿæˆè´¨é‡ã€‚ArcFlowé€šè¿‡å°†æ¨ç†è½¨è¿¹çš„é€Ÿåº¦åœºå‚æ•°åŒ–ä¸ºè¿ç»­åŠ¨é‡è¿‡ç¨‹çš„æ··åˆï¼Œèƒ½å¤Ÿæ•æ‰é€Ÿåº¦æ¼”å˜å¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å½¢æˆè¿ç»­çš„éçº¿æ€§è½¨è¿¹ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼ŒArcFlowåœ¨ä¿æŒç”Ÿæˆå¤šæ ·æ€§å’Œè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†40å€çš„é€Ÿåº¦æå‡ï¼Œä¸”ä»…éœ€å¾®è°ƒä¸åˆ°5%çš„åŸå§‹å‚æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07900",
            "title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents",
            "url": "https://huggingface.co/papers/2602.07900",
            "abstract": "Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.   To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.",
            "score": 3,
            "issue_id": 1025,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "a288320fb8d62075",
            "authors": [
                "Zhi Chen",
                "Zhensu Sun",
                "Yuling Shi",
                "Chao Peng",
                "Xiaodong Gu",
                "David Lo",
                "Lingxiao Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "Shanghai Jiao Tong University",
                "Singapore Management University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07900.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ›",
                "ru": {
                    "title": "Ğ¢ĞµÑÑ‚Ñ‹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ â€” Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ",
                    "desc": "Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ´Ğ°, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ…Ğ¾Ñ‚Ñ Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ ÑÑ€ĞµĞ´Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ print-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¾Ğº. ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Rethinking Test Writing: Limited Impact on LLM Code Agents' Performance",
                    "desc": "This paper investigates the effectiveness of test writing by Large Language Model (LLM) code agents in resolving software issues. The study finds that while many agents write tests during their debugging process, the actual impact of these tests on issue resolution is minimal. Instead, agents often rely on observation-based debugging methods, such as using print statements, which provide more immediate feedback. Ultimately, the research suggests that the practice of writing tests may not significantly enhance the performance of LLMs in software engineering tasks."
                },
                "zh": {
                    "title": "æµ‹è¯•ç¼–å†™çš„æ•ˆç”¨æœ‰é™ï¼Œè§‚å¯Ÿè°ƒè¯•æ›´æœ‰æ•ˆ",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç ä»£ç†åœ¨è§£å†³é—®é¢˜æ—¶çš„æµ‹è¯•ç¼–å†™è¡Œä¸ºã€‚å°½ç®¡è®¸å¤šé«˜æ’åçš„ä»£ç†åœ¨å·¥ä½œæµç¨‹ä¸­ä¼šåŠ¨æ€ç¼–å†™æµ‹è¯•ï¼Œä½†æˆ‘ä»¬çš„è§‚å¯Ÿæ˜¾ç¤ºï¼ŒGPT-5.2å‡ ä¹ä¸ç¼–å†™æ–°æµ‹è¯•ï¼Œå´èƒ½è¾¾åˆ°ä¸é¡¶çº§ä»£ç†ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜ï¼Œæµ‹è¯•ç¼–å†™å¯èƒ½å¹¶æœªæ˜¾è‘—æ”¹å–„é—®é¢˜è§£å†³æ•ˆæœï¼Œåè€Œå¯èƒ½åªæ˜¯æ¨¡ä»¿äººç±»çš„æµ‹è¯•æ–¹å¼ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œå½“å‰çš„æµ‹è¯•ç¼–å†™å®è·µåœ¨è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å¯èƒ½æä¾›çš„æ•ˆç”¨æœ‰é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06008",
            "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
            "url": "https://huggingface.co/papers/2602.06008",
            "abstract": "AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
            "score": 3,
            "issue_id": 1017,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "fb4093533108c1ec",
            "authors": [
                "Xianyang Liu",
                "Shangding Gu",
                "Dawn Song"
            ],
            "affiliations": [
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06008.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²ĞµÑÑ‚Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹Ğº",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° AgenticPay Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ñ‹Ğ½ĞºĞ¸, Ğ³Ğ´Ğµ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ²Ğ¾Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 110 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ñ€Ñ‹Ğ½ĞºĞ¾Ğ² Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Multi-Agent Negotiation with Language",
                    "desc": "AgenticPay is a new benchmark and simulation framework designed to evaluate how well multiple agents can negotiate and interact in economic scenarios using natural language. It focuses on the complexities of negotiation where agents have private constraints and varying valuations for products, requiring them to communicate effectively over multiple rounds. The framework includes over 110 different tasks, allowing for a comprehensive assessment of negotiation strategies and outcomes. By testing advanced language models, the study reveals significant performance gaps and highlights the difficulties in long-term strategic reasoning in these interactions."
                },
                "zh": {
                    "title": "AgenticPayï¼šå¤šæ™ºèƒ½ä½“ç»æµäº’åŠ¨çš„æ–°åŸºå‡†",
                    "desc": "AgenticPayæ˜¯ä¸€ä¸ªåŸºå‡†å’Œä»¿çœŸæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤šæ™ºèƒ½ä½“ä¹‹é—´çš„è¯­è¨€ä¸­ä»‹ç»æµäº’åŠ¨ï¼Œç‰¹åˆ«å…³æ³¨è°ˆåˆ¤è¡¨ç°å’Œå¤æ‚å¸‚åœºåœºæ™¯ä¸­çš„æˆ˜ç•¥æ¨ç†æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†ä¹°å–åŒæ–¹åœ¨å…·æœ‰ç§å¯†çº¦æŸå’Œäº§å“ä¾èµ–ä¼°å€¼çš„å¸‚åœºä¸­ï¼Œé€šè¿‡å¤šè½®è¯­è¨€è°ˆåˆ¤è¾¾æˆåè®®ï¼Œè€Œä¸ä»…ä»…ä¾èµ–æ•°å­—ç«æ ‡ã€‚AgenticPayæ”¯æŒè¶…è¿‡110ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ï¼Œä»åŒè¾¹è°ˆåˆ¤åˆ°å¤šå¯¹å¤šå¸‚åœºï¼Œæä¾›ç»“æ„åŒ–çš„è¡ŒåŠ¨æå–å’Œå¯è¡Œæ€§ã€æ•ˆç‡åŠç¦åˆ©çš„è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†è°ˆåˆ¤è¡¨ç°çš„æ˜¾è‘—å·®è·ï¼Œå¹¶çªå‡ºäº†é•¿æœŸæˆ˜ç•¥æ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œç¡®ç«‹äº†AgenticPayä½œä¸ºç ”ç©¶æ™ºèƒ½å•†ä¸šå’ŒåŸºäºè¯­è¨€çš„å¸‚åœºäº’åŠ¨çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10652",
            "title": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory",
            "url": "https://huggingface.co/papers/2602.10652",
            "abstract": "A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.",
            "score": 2,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "a5d917cb654e3d7f",
            "authors": [
                "Yongshi Ye",
                "Hui Jiang",
                "Feihu Jiang",
                "Tian Lan",
                "Yichao Du",
                "Biao Fu",
                "Xiaodong Shi",
                "Qianghuai Jia",
                "Longyue Wang",
                "Weihua Luo"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce",
                "Tongyi Lab, Alibaba Group",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10652.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ UMEM â€” ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10.67% Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Unified Memory Management for Better Generalization",
                    "desc": "This paper presents a new framework called Unified Memory Extraction and Management (UMEM) for improving how Large Language Models (LLMs) handle memory. It focuses on two key processes: extracting useful insights from experiences and managing the memory bank effectively. The authors introduce Semantic Neighborhood Modeling to enhance generalization and reduce overfitting by evaluating memory utility based on related queries. Their experiments show that UMEM significantly outperforms existing methods, leading to better performance in interactive tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è®°å¿†æå–ä¸ç®¡ç†ï¼Œæå‡æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è®°å¿†æå–å’Œç®¡ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡è‡ªæˆ‘æ¼”åŒ–çš„è®°å¿†ï¼Œä»£ç†èƒ½å¤Ÿåœ¨æå–ç»éªŒå’Œæ›´æ–°è®°å¿†åº“ä¹‹é—´å®ç°ç´§å¯†åè°ƒã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰é‚»åŸŸå»ºæ¨¡å’Œè¾¹é™…æ•ˆç”¨å¥–åŠ±ï¼Œä»¥ä¼˜åŒ–è®°å¿†çš„æå–å’Œç®¡ç†ï¼Œé¿å…è¿‡æ‹Ÿåˆç‰¹å®šå®ä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†è®°å¿†çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08995",
            "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
            "url": "https://huggingface.co/papers/2602.08995",
            "abstract": "Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
            "score": 2,
            "issue_id": 1017,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "d4e80fde2d84ca42",
            "authors": [
                "Yuting Ning",
                "Jaylen Jones",
                "Zhehao Zhang",
                "Chentao Ye",
                "Weitong Ruan",
                "Junyi Li",
                "Rahul Gupta",
                "Huan Sun"
            ],
            "affiliations": [
                "Amazon AGI",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08995.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#alignment",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DeAction Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ñ‚Ğ°Ğº (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº), Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MisActBench Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ğ½Ğ° 15% Ğ¿Ğ¾ F1-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 90% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "DeAction: Safeguarding Computer-Use Agents from Misalignment Risks",
                    "desc": "This paper introduces DeAction, a safety mechanism designed for computer-use agents (CUAs) to detect and correct misaligned actions that may arise from both external attacks and internal limitations. Misaligned actions can lead to safety risks and inefficiencies, prompting the need for a robust solution. The authors define misaligned action detection and create MisActBench, a benchmark for evaluating action alignment in CUAs. DeAction demonstrates significant improvements in performance, achieving over 15% better F1 scores and reducing attack success rates by more than 90% while maintaining task success in safe environments."
                },
                "zh": {
                    "title": "DeActionï¼šç¡®ä¿è®¡ç®—æœºä»£ç†å®‰å…¨çš„å®ˆæŠ¤è€…",
                    "desc": "è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰åœ¨è¿‡å»ä¸€å¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶ç»å¸¸äº§ç”Ÿä¸ç”¨æˆ·åŸæ„ä¸ç¬¦çš„è¡Œä¸ºã€‚è¿™äº›ä¸ä¸€è‡´çš„è¡Œä¸ºå¯èƒ½æºäºå¤–éƒ¨æ”»å‡»ï¼ˆå¦‚é—´æ¥æç¤ºæ³¨å…¥ï¼‰æˆ–å†…éƒ¨é™åˆ¶ï¼ˆå¦‚é”™è¯¯æ¨ç†ï¼‰ï¼Œä¸ä»…å¸¦æ¥å®‰å…¨é£é™©ï¼Œè¿˜é™ä½äº†ä»»åŠ¡çš„æ•ˆç‡å’Œå¯é æ€§ã€‚æœ¬æ–‡é¦–æ¬¡å®šä¹‰å¹¶ç ”ç©¶äº†CUAä¸­çš„ä¸ä¸€è‡´è¡Œä¸ºæ£€æµ‹ï¼Œæ„å»ºäº†MisActBenchåŸºå‡†ï¼Œæä¾›äº†çœŸå®è½¨è¿¹å’Œäººç±»æ ‡æ³¨çš„è¡Œä¸ºå¯¹é½æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†DeActionï¼Œä¸€ä¸ªå®ç”¨çš„ä¿æŠ¤æœºåˆ¶ï¼Œå¯ä»¥åœ¨æ‰§è¡Œå‰æ£€æµ‹å¹¶é€šè¿‡ç»“æ„åŒ–åé¦ˆè¿­ä»£çº æ­£ä¸ä¸€è‡´çš„è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11137",
            "title": "Weight Decay Improves Language Model Plasticity",
            "url": "https://huggingface.co/papers/2602.11137",
            "abstract": "Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.",
            "score": 1,
            "issue_id": 1018,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "c189df9798cb68cf",
            "authors": [
                "Tessa Han",
                "Sebastian Bordt",
                "Hanlin Zhang",
                "Sham Kakade"
            ],
            "affiliations": [
                "Broad Institute",
                "Harvard University",
                "University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11137.jpg",
            "data": {
                "categories": [
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ â€” Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² (weight decay) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "Boosting Model Adaptability with Weight Decay",
                    "desc": "This paper explores how pretraining large language models (LLMs) with higher weight decay values can enhance their adaptability to downstream tasks. Weight decay acts as a regularization technique that helps models avoid overfitting and encourages the formation of linearly separable representations. The authors demonstrate that models with larger weight decay may initially perform worse during pretraining but achieve better results after fine-tuning on specific tasks. This research highlights the need to consider model plasticity and other evaluation metrics beyond just validation loss when optimizing hyperparameters."
                },
                "zh": {
                    "title": "æƒé‡è¡°å‡æå‡æ¨¡å‹å¯å¡‘æ€§ä¸å¾®è°ƒæ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è¾ƒå¤§æƒé‡è¡°å‡å€¼å¯¹æ¨¡å‹å¯å¡‘æ€§å’Œä¸‹æ¸¸å¾®è°ƒæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¾ƒå¤§çš„æƒé‡è¡°å‡å€¼å¯ä»¥é¼“åŠ±æ¨¡å‹å­¦ä¹ çº¿æ€§å¯åˆ†çš„è¡¨ç¤ºï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œç»è¿‡è¾ƒå¤§æƒé‡è¡°å‡å€¼è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶è¡¨ç°å‡ºæ›´å¤§çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨è¶…å‚æ•°ä¼˜åŒ–ä¸­ä½¿ç”¨è¶…è¶Šäº¤å‰ç†µæŸå¤±çš„è¯„ä¼°æŒ‡æ ‡çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10870",
            "title": "FedPS: Federated data Preprocessing via aggregated Statistics",
            "url": "https://huggingface.co/papers/2602.10870",
            "abstract": "FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.",
            "score": 1,
            "issue_id": 1021,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "492b5f911a526f3a",
            "authors": [
                "Xuefeng Xu",
                "Graham Cormode"
            ],
            "affiliations": [
                "University of Oxford",
                "University of Warwick"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10870.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "FedPS â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑĞºĞ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. FedPS Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ k-Means Ğ¸ k-NN Ğ´Ğ»Ñ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ, ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient and Private Data Preparation for Federated Learning",
                    "desc": "FedPS is a framework designed for federated data preprocessing, which helps prepare data for collaborative machine learning while keeping it private. It uses aggregated statistics and data-sketching techniques to summarize local datasets without sharing raw data. This preprocessing is essential for improving model performance, especially in federated learning where data is distributed across multiple parties. FedPS offers efficient methods for handling tasks like feature scaling and missing-value imputation, making it suitable for real-world applications of federated learning."
                },
                "zh": {
                    "title": "è”é‚¦å­¦ä¹ çš„é«˜æ•ˆæ•°æ®é¢„å¤„ç†è§£å†³æ–¹æ¡ˆ",
                    "desc": "FedPSæ˜¯ä¸€ä¸ªè”é‚¦æ•°æ®é¢„å¤„ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡èšåˆç»Ÿè®¡å’Œæ•°æ®è‰å›¾æŠ€æœ¯ï¼Œå®ç°é«˜æ•ˆä¸”ä¿æŠ¤éšç§çš„æ•°æ®å‡†å¤‡ï¼Œä»¥æ”¯æŒåˆ†å¸ƒå¼æ•°æ®é›†çš„åä½œæœºå™¨å­¦ä¹ ã€‚åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œæ•°æ®é¢„å¤„ç†æ˜¯å…³é”®æ­¥éª¤ï¼Œèƒ½å¤Ÿè§£å†³ç¼ºå¤±å€¼ã€ä¸ä¸€è‡´æ ¼å¼å’Œç‰¹å¾å°ºåº¦ä¸å‡ç­‰é—®é¢˜ï¼Œä½†è¿™ä¸€ç¯èŠ‚åœ¨ç ”ç©¶ä¸­å¸¸è¢«å¿½è§†ã€‚FedPSåˆ©ç”¨æ•°æ®è‰å›¾æŠ€æœ¯é«˜æ•ˆåœ°æ€»ç»“æœ¬åœ°æ•°æ®é›†ï¼ŒåŒæ—¶ä¿ç•™é‡è¦çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®¾è®¡äº†å¤šç§è”é‚¦ç®—æ³•æ¥å¤„ç†ç‰¹å¾ç¼©æ”¾ã€ç¼–ç ã€ç¦»æ•£åŒ–å’Œç¼ºå¤±å€¼å¡«è¡¥ç­‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ä¸ºå®é™…çš„è”é‚¦å­¦ä¹ éƒ¨ç½²æä¾›äº†çµæ´»ã€é€šä¿¡é«˜æ•ˆä¸”ä¸€è‡´çš„é¢„å¤„ç†ç®¡é“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10778",
            "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
            "url": "https://huggingface.co/papers/2602.10778",
            "abstract": "GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.",
            "score": 1,
            "issue_id": 1023,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "c9c834cee1b8757f",
            "authors": [
                "Maximilian Thang",
                "Lichao Wu",
                "Sasha Behrouzi",
                "Mohamadreza Rostami",
                "Jona te Lintelo",
                "Stjepan Picek",
                "Ahmad-Reza Sadeghi"
            ],
            "affiliations": [
                "Radboud University",
                "Technical University of Darmstadt",
                "University of Zagreb"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10778.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#security",
                    "#plp",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾-Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ LLM",
                    "desc": "GoodVibe â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ° ĞºĞ¾Ğ´Ğµ, Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ´ĞµÑ€Ğ¶ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ² 2,5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² 4700 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Enhancing Code Security with Neuron-Level Fine-Tuning",
                    "desc": "GoodVibe is a novel framework designed to enhance the security of code language models by focusing on specific neurons that are critical for security reasoning. Instead of fine-tuning the entire model, GoodVibe selectively updates only the neurons identified as security-relevant, which significantly reduces training costs and avoids issues like catastrophic forgetting. The framework employs activation-driven neuron clustering to optimize updates, ensuring that the model remains efficient while improving the security of generated code. Evaluations show that GoodVibe can achieve substantial security improvements with far fewer parameters and less computational expense compared to traditional fine-tuning methods."
                },
                "zh": {
                    "title": "GoodVibeï¼šæå‡ä»£ç å®‰å…¨æ€§çš„ç¥ç»å…ƒçº§æ¡†æ¶",
                    "desc": "GoodVibeæ˜¯ä¸€ä¸ªç¥ç»å…ƒçº§æ¡†æ¶ï¼Œé€šè¿‡é’ˆå¯¹æ€§åœ°å¾®è°ƒä¸å®‰å…¨ç›¸å…³çš„ç¥ç»å…ƒæ¥å¢å¼ºä»£ç è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§å¹¶æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å®‰å…¨ç›¸å…³çš„æ¨ç†ä»…é›†ä¸­åœ¨å°‘é‡ç¥ç»å…ƒä¸Šã€‚é€šè¿‡ä½¿ç”¨åŸºäºæ¢¯åº¦çš„å½’å› æ–¹æ³•è¯†åˆ«è¿™äº›ç¥ç»å…ƒï¼ŒGoodVibeä»…å¯¹è¿™äº›å®‰å…¨å…³é”®çš„å­ç©ºé—´è¿›è¡Œé€‰æ‹©æ€§å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGoodVibeåœ¨å¤šä¸ªå®‰å…¨å…³é”®ç¼–ç¨‹è¯­è¨€ä¸Šæ˜¾è‘—æé«˜äº†ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•´ä½“æ•ˆç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10699",
            "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
            "url": "https://huggingface.co/papers/2602.10699",
            "abstract": "V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.",
            "score": 1,
            "issue_id": 1017,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4aaea540952b7aaf",
            "authors": [
                "Jie Jiang",
                "Yangru Huang",
                "Zeyu Wang",
                "Changping Wang",
                "Yuling Xiong",
                "Jun Zhang",
                "Huan Yu"
            ],
            "affiliations": [
                "Tencent Inc., China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10699.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ”ĞµÑ€ĞµĞ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµĞ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ",
                    "desc": "V-STAR â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹, Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ´ĞµÑ€ĞµĞ²Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ°Ğ»Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ÑÑ… Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ V-STAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ."
                },
                "en": {
                    "title": "Enhancing Generative Recommendations with V-STAR: Smart Exploration and Learning",
                    "desc": "V-STAR is a novel framework designed to enhance generative recommendation systems by addressing common issues in exploration and reward signal quality. It combines value-guided decoding with tree-structured advantage reinforcement to improve the decision-making process in recommendation tasks. By using Value-Guided Efficient Decoding, V-STAR efficiently explores high-potential options while avoiding the pitfalls of traditional decoding methods that can overlook valuable items. Additionally, the Sibling-GRPO component refines the learning process by focusing on relative advantages among similar options, leading to better performance in both accuracy and diversity of recommendations."
                },
                "zh": {
                    "title": "V-STARï¼šæå‡ç”Ÿæˆæ¨èçš„æ¢ç´¢ä¸å¥–åŠ±è´¨é‡",
                    "desc": "V-STARæ˜¯ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­çš„æ¢ç´¢ä¸è¶³å’Œå¥–åŠ±ä¿¡å·è´¨é‡ä½çš„é—®é¢˜ã€‚å®ƒç»“åˆäº†ä»·å€¼å¼•å¯¼è§£ç å’Œæ ‘ç»“æ„ä¼˜åŠ¿å¼ºåŒ–å­¦ä¹ ï¼Œå½¢æˆä¸€ä¸ªè‡ªæˆ‘æ¼”åŒ–çš„å¾ªç¯ã€‚é€šè¿‡ä»·å€¼å¼•å¯¼é«˜æ•ˆè§£ç ï¼ŒV-STARèƒ½å¤Ÿè¯†åˆ«å…³é”®èŠ‚ç‚¹å¹¶é€‰æ‹©æ€§åœ°æ·±å…¥é«˜æ½œåŠ›çš„å‰ç¼€ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV-STARåœ¨å‡†ç¡®æ€§å’Œå€™é€‰é›†å¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08741",
            "title": "Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing",
            "url": "https://huggingface.co/papers/2602.08741",
            "abstract": "Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.",
            "score": 1,
            "issue_id": 1023,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "191ae260ce903fdb",
            "authors": [
                "Jona te Lintelo",
                "Lichao Wu",
                "Stjepan Picek"
            ],
            "affiliations": [
                "Faculty of Electrical Engineering and Computing University of Zagreb, Croatia",
                "Radboud University, The Netherlands",
                "Technical University of Darmstadt, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#security",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "âš ï¸",
                "ru": {
                    "title": "Ğ­ĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ² MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Mixture-of-Experts (MoE) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ³Ğ´Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Large Language Lobotomy (LÂ³), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ¾Ñ‚ĞºĞ°Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ 86,3% Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 20% ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ MoE Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Exploiting Expert Dynamics: A New Threat to MoE Language Models",
                    "desc": "This paper discusses a new attack method called Large Language Lobotomy (L^3) that targets Mixture-of-Experts (MoE) language models. MoE models are efficient because they activate only a few parameters for each input, but this creates vulnerabilities in their safety mechanisms. The authors demonstrate that harmful behaviors in these models are linked to specific experts, allowing L^3 to selectively silence them to produce unsafe outputs. Their experiments show that this method significantly increases the success rate of attacks while maintaining the overall language performance of the models."
                },
                "zh": {
                    "title": "åˆ©ç”¨ä¸“å®¶è·¯ç”±åŠ¨æ€ç ´è§£å®‰å…¨å¯¹é½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ä¸­çš„å®‰å…¨æ€§é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒMoEæ¨¡å‹ä¸­çš„å®‰å…¨å…³é”®è¡Œä¸ºä¸»è¦é›†ä¸­åœ¨å°‘æ•°ä¸“å®¶ä¸Šï¼Œè€Œä¸æ˜¯å‡åŒ€åˆ†å¸ƒã€‚æå‡ºäº†ä¸€ç§åä¸ºL^3çš„æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä¸“å®¶è·¯ç”±åŠ¨æ€æ¥ç ´åå®‰å…¨å¯¹é½ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€å®ç”¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒL^3åœ¨å¤šä¸ªå¼€æºMoE LLMä¸Šæ˜¾è‘—æé«˜äº†æ”»å‡»æˆåŠŸç‡ï¼Œæ­ç¤ºäº†æ•ˆç‡é©±åŠ¨çš„MoEè®¾è®¡ä¸å®‰å…¨å¯¹é½ä¹‹é—´çš„æ ¹æœ¬çŸ›ç›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08052",
            "title": "Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling",
            "url": "https://huggingface.co/papers/2602.08052",
            "abstract": "A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  \t\t\t\t\tAI-generated summary \t\t\t\t The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.",
            "score": 1,
            "issue_id": 1026,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "f484c9425514226a",
            "authors": [
                "Bulent Soykan",
                "Sean Mondesire",
                "Ghaith Rabadi",
                "Grace Bochenek"
            ],
            "affiliations": [
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08052.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#graphs",
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Proximal Policy Optimization (PPO) Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (GNN) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ñ…. Ğ“Ñ€Ğ°Ñ„ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¼Ğ°ÑˆĞ¸Ğ½ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ»Ğ°Ğ´Ğ¾Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ PPO Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ»Ğ°Ğ´Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ°ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Tardiness and Setup Time with Deep Reinforcement Learning",
                    "desc": "This paper presents a Deep Reinforcement Learning framework that integrates Proximal Policy Optimization (PPO) with Graph Neural Networks (GNN) to tackle the Unrelated Parallel Machine Scheduling Problem (UPMSP). The challenge lies in effectively minimizing two conflicting objectives: Total Weighted Tardiness (TWT) and Total Setup Time (TST). By utilizing a GNN, the framework captures the intricate relationships between jobs, machines, and setups, enabling the PPO agent to develop an efficient scheduling policy. Experimental results indicate that this approach significantly outperforms traditional methods, offering a better balance between TWT and TST in complex manufacturing environments."
                },
                "zh": {
                    "title": "æ·±åº¦å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤šç›®æ ‡è°ƒåº¦ä¼˜åŒ–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šç›®æ ‡è°ƒåº¦é—®é¢˜ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆå¹³è¡¡äº†æ€»åŠ æƒå»¶è¿Ÿï¼ˆTWTï¼‰å’Œæ€»è®¾ç½®æ—¶é—´ï¼ˆTSTï¼‰ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚GNNèƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºä½œä¸šã€æœºå™¨å’Œè®¾ç½®çš„å¤æ‚çŠ¶æ€ï¼Œä½¿å¾—PPOä»£ç†èƒ½å¤Ÿå­¦ä¹ ç›´æ¥çš„è°ƒåº¦ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPPO-GNNä»£ç†åœ¨åŸºå‡†å®ä¾‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†è°ƒåº¦è§„åˆ™å’Œå…ƒå¯å‘å¼ç®—æ³•ï¼Œæä¾›äº†å¤æ‚åˆ¶é€ è°ƒåº¦çš„å¼ºå¤§ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03773",
            "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
            "url": "https://huggingface.co/papers/2602.03773",
            "abstract": "RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.",
            "score": 1,
            "issue_id": 1027,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "4e7349d9fcf6ab91",
            "authors": [
                "Ian Wu",
                "Yuxiao Qu",
                "Amrith Setlur",
                "Aviral Kumar"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03773.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#small_models"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RC â€” ÑÑ‚Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ RC, ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 4-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 70% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 500 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Continuous Improvement in Language Models with RC",
                    "desc": "The paper introduces RC, an innovative iterative decoding algorithm designed for large language models (LLMs) to enhance their performance on complex tasks beyond their initial training limits. By constructing reasoning chains through multiple iterations, RC allows models to adapt and improve their outputs during inference, addressing the challenges posed by distribution shifts in standard reinforcement learning. This method significantly boosts the models' ability to extrapolate, enabling them to tackle longer reasoning tasks than previously possible. Empirical results demonstrate that models trained with RC achieve substantial performance gains, showcasing their superior capabilities in generating coherent and contextually relevant responses."
                },
                "zh": {
                    "title": "RCç®—æ³•ï¼šè¶…è¶Šè®­ç»ƒçš„æ¨ç†èƒ½åŠ›",
                    "desc": "RCæ˜¯ä¸€ç§è¿­ä»£è§£ç ç®—æ³•ï¼Œèƒ½å¤Ÿä½¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒé¢„ç®—ä¹‹å¤–ä¸æ–­æ”¹è¿›ã€‚å®ƒé€šè¿‡æ„å»ºæ¨ç†é“¾ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­å¢å¼ºæ¨¡å‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶é—´è·¨åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸åŒï¼ŒRCåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ›¿ä»£äº†æ ‡å‡†çš„è‡ªå›å½’è§£ç ï¼Œåˆ©ç”¨äº†æ¨¡å‹ç”Ÿæˆå“åº”å’Œæ€»ç»“çš„èƒ½åŠ›ä¸å¯¹ç§°æ€§ã€‚ç»è¿‡RCè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è¶…è¶Šè®­ç»ƒæœŸé—´çš„æ¨ç†èŒƒå›´ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08934",
            "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
            "url": "https://huggingface.co/papers/2602.08934",
            "abstract": "StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.",
            "score": 0,
            "issue_id": 1030,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "1b978c45e63f80c7",
            "authors": [
                "Suraj Ranganath",
                "Atharv Ramesh"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08934.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#open_source",
                    "#training",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ²ĞµÑ€ÑĞ°Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ StealthRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ LoRA Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 99.9% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ AUROC Ñ‚Ñ€Ñ‘Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² (RoBERTa, FastDetectGPT Ğ¸ Binoculars). ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑĞ¼."
                },
                "en": {
                    "title": "Evasion Mastery: StealthRL Defeats AI Text Detectors!",
                    "desc": "StealthRL is a novel reinforcement learning framework designed to create adversarial paraphrases that can bypass various AI text detectors while maintaining the original meaning of the text. It employs LoRA adapters and Group Relative Policy Optimization (GRPO) to train a paraphrase policy against a multi-detector ensemble, optimizing for both evasion and semantic integrity. The framework demonstrates a remarkable ability to evade detection, achieving a near-zero detection rate and a high attack success rate across multiple detector families. This research highlights significant vulnerabilities in current AI text detection systems, suggesting that they share common weaknesses rather than being uniquely fragile."
                },
                "zh": {
                    "title": "æ­ç¤º AI æ–‡æœ¬æ£€æµ‹çš„è„†å¼±æ€§",
                    "desc": "StealthRL æ˜¯ä¸€ç§ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œ LoRA é€‚é…å™¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå¯¹æŠ—æ€§é‡Šä¹‰ï¼Œä»¥ç»•è¿‡å¤šç§ AI æ–‡æœ¬æ£€æµ‹å™¨ï¼ŒåŒæ—¶ä¿æŒåŸæ„ã€‚è¯¥æ–¹æ³•é€šè¿‡ Group Relative Policy Optimization (GRPO) è®­ç»ƒé‡Šä¹‰ç­–ç•¥ï¼Œä¼˜åŒ–æ£€æµ‹è§„é¿ä¸è¯­ä¹‰ä¿ç•™ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒStealthRL åœ¨å¤šä¸ªæ£€æµ‹å™¨ä¸Šå®ç°äº†è¿‘ä¹é›¶çš„æ£€æµ‹ç‡ï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾ 99.9%ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†å½“å‰ AI æ–‡æœ¬æ£€æµ‹ç³»ç»Ÿçš„æ˜¾è‘—è„†å¼±æ€§ï¼Œå¹¶ä¸ºå¯¹æŠ—æ€§è¯„ä¼°æä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„åè®®ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-11.html",
    "link_next": "2026-02-13.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 1,
        "#benchmark": 19,
        "#agents": 9,
        "#cv": 1,
        "#rl": 9,
        "#rlhf": 5,
        "#rag": 2,
        "#plp": 4,
        "#inference": 6,
        "#3d": 1,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 7,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 19,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 14,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 2,
        "#security": 5,
        "#optimization": 15,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 4,
        "#science": 1,
        "#low_resource": 1
    }
}