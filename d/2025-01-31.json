{
    "date": {
        "ru": "31 января",
        "en": "January 31",
        "zh": "1月31日"
    },
    "time_utc": "2025-01-31 07:09",
    "weekday": 4,
    "issue_id": 1967,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.18492",
            "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
            "url": "https://huggingface.co/papers/2501.18492",
            "abstract": "As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
            "score": 23,
            "issue_id": 1964,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "2ea0bf2a655fc703",
            "authors": [
                "Yue Liu",
                "Hongcheng Gao",
                "Shengfang Zhai",
                "Jun Xia",
                "Tianyi Wu",
                "Zhiwei Xue",
                "Yulin Chen",
                "Kenji Kawaguchi",
                "Jiaheng Zhang",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Chinese"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18492.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Разумная защита: обучение ИИ безопасному мышлению",
                    "desc": "Статья представляет GuardReasoner - новый метод обеспечения безопасности больших языковых моделей (LLM) путем обучения их рассуждению. Авторы создали датасет GuardReasonerTrain с 127 тысячами образцов и 460 тысячами шагов рассуждений. Они применили методы обучения с подкреплением для развития способности моделей к рассуждению. Эксперименты на 13 бенчмарках показали превосходство GuardReasoner над другими методами, включая GPT-4 и LLaMA Guard."
                },
                "en": {
                    "title": "GuardReasoner: Enhancing LLM Safety through Advanced Reasoning Techniques",
                    "desc": "This paper introduces GuardReasoner, a novel safeguard designed to enhance the safety of large language models (LLMs) in critical applications. It presents a new dataset, GuardReasonerTrain, containing 127,000 samples with 460,000 reasoning steps to train guard models effectively. The authors implement reasoning Supervised Fine-Tuning (SFT) and hard sample Direct Preference Optimization (DPO) to improve the reasoning capabilities of these models. Experimental results show that GuardReasoner outperforms existing models like GPT-4o+CoT and LLaMA Guard 3 in terms of performance, explainability, and generalizability across multiple benchmarks."
                },
                "zh": {
                    "title": "GuardReasoner：提升大型语言模型的安全性与推理能力",
                    "desc": "随着大型语言模型（LLMs）在安全关键应用中的影响日益增加，确保其安全性成为一项重要挑战。本文提出了一种新的保护机制GuardReasoner，通过引导保护模型学习推理来增强安全性。我们首先创建了GuardReasonerTrain数据集，包含127K样本和460K详细推理步骤，并引入推理SFT以解锁保护模型的推理能力。此外，我们还提出了困难样本DPO，以进一步增强其推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18585",
            "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
            "url": "https://huggingface.co/papers/2501.18585",
            "abstract": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source o1-like models, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty TIP that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in o1-like LLMs and offer a practical solution to enhance their problem-solving capabilities.",
            "score": 4,
            "issue_id": 1966,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "ef1309c4c491d1d5",
            "authors": [
                "Yue Wang",
                "Qiuzhi Liu",
                "Jiahao Xu",
                "Tian Liang",
                "Xingyu Chen",
                "Zhiwei He",
                "Linfeng Song",
                "Dian Yu",
                "Juntao Li",
                "Zhuosheng Zhang",
                "Rui Wang",
                "Zhaopeng Tu",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Soochow University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18585.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Глубже мысли, точнее ответ: борьба с 'недодумыванием' в ИИ",
                    "desc": "Исследователи обнаружили феномен 'недодумывания' в крупных языковых моделях типа GPT-3, когда модели часто переключаются между разными мыслями, не исследуя достаточно глубоко перспективные направления рассуждений. Это приводит к снижению производительности, особенно на сложных математических задачах. Авторы предложили метрику для количественной оценки 'недодумывания' и разработали стратегию декодирования с штрафом за переключение мыслей (TIP). Эксперименты показали, что данный подход повышает точность моделей на сложных наборах данных без необходимости дополнительного обучения."
                },
                "en": {
                    "title": "Enhancing Deep Reasoning in LLMs by Reducing Underthinking",
                    "desc": "This paper investigates a problem called 'underthinking' in large language models (LLMs) like OpenAI's o1, where the models switch between reasoning thoughts too quickly without fully exploring each option. This behavior results in shallow reasoning and poor performance, especially on difficult math problems. The authors conduct experiments to show that this frequent thought switching is linked to incorrect answers and introduce a new metric to measure this inefficiency. To combat underthinking, they propose a decoding strategy that penalizes quick transitions between thoughts, leading to improved accuracy in problem-solving without needing to change the model itself."
                },
                "zh": {
                    "title": "提升推理深度，克服思维不足",
                    "desc": "本文探讨了大型语言模型（LLMs）在复杂推理任务中的表现，特别是它们在推理过程中出现的\"思维不足\"现象。研究发现，这些模型在推理时频繁切换思路，导致对有前景的路径探索不够，从而影响了在数学问题上的表现。为了解决这一问题，作者提出了一种新的解码策略，增加了思维切换的惩罚，鼓励模型深入探索每个推理路径。实验结果表明，该方法在多个挑战性数据集上提高了模型的准确性，而无需对模型进行微调。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16411",
            "title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding",
            "url": "https://huggingface.co/papers/2501.16411",
            "abstract": "Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world -- likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding.",
            "score": 3,
            "issue_id": 1965,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "de192844b85ce40c",
            "authors": [
                "Wei Chow",
                "Jiageng Mao",
                "Boyi Li",
                "Daniel Seita",
                "Vitor Guizilini",
                "Yue Wang"
            ],
            "affiliations": [
                "Toyota Research Institute",
                "UC Berkeley",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16411.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Научим ИИ понимать физику",
                    "desc": "PhysBench - это новый комплексный бенчмарк для оценки способности мультимодальных языковых моделей (VLM) понимать физический мир. Он содержит более 10 000 примеров с видео, изображениями и текстом, охватывающих различные аспекты физики. Эксперименты показали, что современные VLM плохо справляются с задачами понимания физического мира. Авторы предложили фреймворк PhysAgent, объединяющий сильные стороны VLM и специализированных моделей компьютерного зрения для улучшения физического понимания."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing VLMs' Physical Understanding with PhysBench and PhysAgent",
                    "desc": "This paper addresses the challenge of enabling embodied AI agents to understand the physical world, which is essential for performing complex tasks safely. It introduces PhysBench, a benchmark that evaluates Vision-Language Models (VLMs) on their ability to comprehend physical phenomena through a diverse dataset of video-image-text entries. The study reveals that while VLMs are proficient in common-sense reasoning, they struggle with physical understanding due to limited training data and lack of physical knowledge. To improve this, the authors propose PhysAgent, a framework that enhances VLMs' physical understanding by integrating them with specialized vision models, resulting in significant performance improvements."
                },
                "zh": {
                    "title": "提升VLMs的物理理解能力",
                    "desc": "理解物理世界是具身人工智能的一项基本挑战，对于使智能体能够执行复杂任务和安全地在现实环境中操作至关重要。虽然视觉-语言模型（VLMs）在推理和任务规划方面表现出色，但它们理解物理现象的能力仍然非常有限。为了解决这个问题，我们引入了PhysBench，这是一个全面的基准测试，旨在评估VLMs在物理世界理解能力方面的表现，涵盖了多种任务和数据类型。我们还提出了PhysAgent，一个新框架，结合了VLMs的泛化能力和视觉模型的专业知识，显著提升了VLMs在物理理解方面的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18009",
            "title": "Large Language Models Think Too Fast To Explore Effectively",
            "url": "https://huggingface.co/papers/2501.18009",
            "abstract": "Large Language Models have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore, an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.",
            "score": 1,
            "issue_id": 1965,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 января",
                "en": "January 29",
                "zh": "1月29日"
            },
            "hash": "4eac49e81b71499d",
            "authors": [
                "Lan Pan",
                "Hanbo Xie",
                "Robert C. Wilson"
            ],
            "affiliations": [
                "School of Psychology Georgia Institute of Technology Atlanta, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18009.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#rl",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Ограничения исследовательских способностей языковых моделей",
                    "desc": "Исследование посвящено способности крупных языковых моделей (LLM) к исследованию в открытых задачах. Используя игру Little Alchemy 2 в качестве тестового полигона, авторы сравнивают эффективность исследования LLM и людей. Результаты показывают, что большинство LLM уступают людям, полагаясь в основном на стратегии, основанные на неопределенности. Анализ с помощью разреженных автоэнкодеров выявил, что LLM обрабатывают неопределенность и выбор на ранних этапах, что приводит к преждевременным решениям и ухудшает исследование."
                },
                "en": {
                    "title": "Exploring the Limits of Large Language Models in Discovery Tasks",
                    "desc": "This paper examines the exploration capabilities of Large Language Models (LLMs) in open-ended tasks, specifically using the game Little Alchemy 2 as a testbed. The study finds that most LLMs do not perform as well as humans in exploring and discovering new combinations, with the exception of the o1 model. It highlights that traditional LLMs tend to rely on uncertainty-driven strategies, while humans effectively balance uncertainty with empowerment. The research also reveals that LLMs process uncertainty and choices earlier in their architecture, leading to hasty decisions that limit their exploration effectiveness."
                },
                "zh": {
                    "title": "探索能力：LLMs的局限与潜力",
                    "desc": "大型语言模型（LLMs）展现了许多智力能力，但对其探索能力的研究相对较少。探索是发现新信息和适应新环境的重要能力，尤其在开放式任务中更为关键。研究表明，大多数LLMs在开放式任务中的探索能力不及人类，只有o1模型表现较好。分析结果显示，LLMs在决策时过于依赖不确定性策略，而人类则能更好地平衡不确定性和赋能，从而影响了有效探索的能力。"
                }
            }
        }
    ],
    "link_prev": "2025-01-30.html",
    "link_next": "2025-02-03.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1月30日"
    },
    "short_date_next": {
        "ru": "03.02",
        "en": "02/03",
        "zh": "2月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "pinyin": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。\n\nZhè piān wénzhāng tiǎozhàn le jiàndū wēitiáo (SFT) de fànshì, tíchū le pīpíng wēitiáo (CFT) cèlüè. CFT ràng móxíng xuéxí pīpíng yǒu zàoyīn de xiǎngyìng, ér bùshì jiǎndān de mófǎng zhèngquè de xiǎngyìng. Shòu qiángdiào pīpàn xìng sīwéi de rénlèi xuéxí guòchéng qǐfā, CFT gǔlì gèng shēnrù de fēnxi hé xìzhì de lǐjiě. Zuòzhě tōngguò shìyàn yànzhèng le CFT de yǒuxiàoxìng, jiéguǒ xiǎnshì CFT zài duōgè shùxué jīzhǔn shàng yōuyú SFT.",
        "vocab": "[{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervise'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '批评', 'pinyin': 'pī píng', 'trans': 'criticize'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '噪音', 'pinyin': 'zào yīn', 'trans': 'noise'},\n{'word': '响应', 'pinyin': 'xiǎng yìng', 'trans': 'response'},\n{'word': '模仿', 'pinyin': 'mó fǎng', 'trans': 'imitate'},\n{'word': '启发', 'pinyin': 'qǐ fā', 'trans': 'inspire'},\n{'word': '批判性', 'pinyin': 'pī pàn xìng', 'trans': 'critical'},\n{'word': '思维', 'pinyin': 'sī wéi', 'trans': 'thinking'},\n{'word': '鼓励', 'pinyin': 'gǔ lì', 'trans': 'encourage'},\n{'word': '深入', 'pinyin': 'shēn rù', 'trans': 'in-depth'},\n{'word': '细致', 'pinyin': 'xì zhì', 'trans': 'detailed'},\n{'word': '验证', 'pinyin': 'yàn zhèng', 'trans': 'verify'},\n{'word': '有效性', 'pinyin': 'yǒu xiào xìng', 'trans': 'effectiveness'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}]",
        "trans": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.",
        "update_ts": "2025-01-30 09:10"
    }
}