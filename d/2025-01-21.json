{
    "date": {
        "ru": "20 января",
        "en": "January 20",
        "zh": "1月20日"
    },
    "time_utc": "2025-01-20 23:09",
    "weekday": 0,
    "issue_id": 1769,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.09891",
            "title": "Evolving Deeper LLM Thinking",
            "url": "https://huggingface.co/papers/2501.09891",
            "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
            "score": 57,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "f2f5bbede5781334",
            "authors": [
                "Kuang-Huei Lee",
                "Ian Fischer",
                "Yueh-Hua Wu",
                "Dave Marwood",
                "Shumeet Baluja",
                "Dale Schuurmans",
                "Xinyun Chen"
            ],
            "affiliations": [
                "Google DeepMind",
                "UC San Diego",
                "University of Alberta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09891.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эволюция мышления: новый подход к оптимизации вывода в языковых моделях",
                    "desc": "Статья представляет эволюционную стратегию поиска для масштабирования вычислений во время вывода в больших языковых моделях. Метод, названный Mind Evolution, использует языковую модель для генерации, рекомбинации и уточнения кандидатов-ответов. Этот подход устраняет необходимость формализации исходной задачи вывода, если доступен оценщик решений. При контроле за стоимостью вычислений, Mind Evolution значительно превосходит другие стратегии вывода в задачах планирования на естественном языке."
                },
                "en": {
                    "title": "Mind Evolution: Revolutionizing Inference in Large Language Models",
                    "desc": "This paper presents Mind Evolution, an innovative evolutionary search strategy designed to enhance the inference time of Large Language Models (LLMs). By leveraging a language model, Mind Evolution generates, recombines, and refines potential responses without needing to define the inference problem formally, as long as a solution evaluator is available. The results demonstrate that Mind Evolution significantly outperforms traditional inference methods like Best-of-N and Sequential Revision in natural language planning tasks. In benchmarks such as TravelPlanner and Natural Plan, Mind Evolution successfully solves over 98% of instances using Gemini 1.5 Pro, showcasing its effectiveness without relying on a formal solver."
                },
                "zh": {
                    "title": "Mind Evolution：推理效率的新突破",
                    "desc": "本文探讨了一种用于大语言模型推理时间计算的进化搜索策略，称为Mind Evolution。该方法利用语言模型生成、重组和优化候选响应，避免了在有解决方案评估器的情况下需要形式化推理问题。通过控制推理成本，我们发现Mind Evolution在自然语言规划任务中显著优于其他推理策略，如Best-of-N和Sequential Revision。在TravelPlanner和Natural Plan基准测试中，Mind Evolution在不使用正式求解器的情况下，解决了超过98%的问题实例。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10120",
            "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
            "url": "https://huggingface.co/papers/2501.10120",
            "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholarly queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",
            "score": 18,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "bf3bfc73e6d5b31d",
            "authors": [
                "Yichen He",
                "Guanhua Huang",
                "Peiyuan Feng",
                "Yuan Lin",
                "Yuchen Zhang",
                "Hang Li",
                "Weinan E"
            ],
            "affiliations": [
                "ByteDance Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10120.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "PaSa: ИИ-агент для эффективного поиска научных статей",
                    "desc": "PaSa - это продвинутый агент для поиска научных статей, основанный на больших языковых моделях. Он способен автономно принимать решения, включая использование поисковых инструментов, чтение статей и выбор релевантных ссылок для получения комплексных и точных результатов по сложным научным запросам. PaSa оптимизирован с помощью обучения с подкреплением на синтетическом наборе данных AutoScholarQuery, содержащем 35 тысяч детализированных академических запросов и соответствующих статей из ведущих конференций по ИИ. Несмотря на обучение на синтетических данных, PaSa значительно превосходит существующие базовые модели на реальном тестовом наборе RealScholarQuery, включая Google и ChatGPT."
                },
                "en": {
                    "title": "Revolutionizing Academic Search with PaSa!",
                    "desc": "The paper presents PaSa, a sophisticated Paper Search agent that utilizes large language models to enhance academic research. PaSa autonomously navigates the search process by making decisions such as invoking search tools, analyzing papers, and selecting pertinent references to deliver thorough and precise results for complex queries. It is optimized through reinforcement learning using a synthetic dataset called AutoScholarQuery, which contains 35,000 detailed academic queries and related papers from leading AI conferences. The performance of PaSa is evaluated against real-world queries using the RealScholarQuery benchmark, demonstrating significant improvements over existing search tools, including Google and various GPT models."
                },
                "zh": {
                    "title": "PaSa：智能论文搜索的新纪元",
                    "desc": "本文介绍了一种名为PaSa的先进论文搜索代理，利用大型语言模型进行自主决策。PaSa能够调用搜索工具、阅读论文并选择相关参考文献，以获取复杂学术查询的全面和准确结果。我们通过强化学习优化PaSa，使用了一个包含35,000个细粒度学术查询的合成数据集AutoScholarQuery。尽管在合成数据上训练，PaSa在真实学术查询基准RealScholarQuery上的表现显著优于现有的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09775",
            "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong",
            "url": "https://huggingface.co/papers/2501.09775",
            "abstract": "One of the most widely used methods to evaluate LLMs are Multiple Choice Question (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on almost any topic at scale as the results can be processed automatically. To help the LLM answer, a few examples called few shots can be included in the prompt. Moreover, the LLM can be asked to answer the question directly with the selected option or to first provide the reasoning and then the selected answer, which is known as chain of thought. In addition to checking whether the selected answer is correct, the evaluation can look at the LLM-estimated probability of its response as an indication of the confidence of the LLM in the response. In this paper, we study how the LLM confidence in its answer depends on whether the model has been asked to answer directly or to provide the reasoning before answering. The results of the evaluation of questions on a wide range of topics in seven different models show that LLMs are more confident in their answers when they provide reasoning before the answer. This occurs regardless of whether the selected answer is correct. Our hypothesis is that this behavior is due to the reasoning that modifies the probability of the selected answer, as the LLM predicts the answer based on the input question and the reasoning that supports the selection made. Therefore, LLM estimated probabilities seem to have intrinsic limitations that should be understood in order to use them in evaluation procedures. Interestingly, the same behavior has been observed in humans, for whom explaining an answer increases confidence in its correctness.",
            "score": 12,
            "issue_id": 1756,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "eb8938131508de10",
            "authors": [
                "Tairan Fu",
                "Javier Conde",
                "Gonzalo Martínez",
                "María Grandury",
                "Pedro Reviriego"
            ],
            "affiliations": [
                "College of Mechanical and Electrical Engineering Nanjing University of Aeronautics and Astronautics Nanjing, China",
                "ETSI de Telecomunicación Universidad Politécnica de Madrid Madrid, Spain",
                "SomosNLP/Universidad Politécnica de Madrid Madrid, Spain",
                "Universidad Carlos III de Madrid Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09775.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🤔",
                "ru": {
                    "title": "Рассуждения повышают уверенность ИИ, но не точность",
                    "desc": "Статья исследует влияние цепочки рассуждений на уверенность языковых моделей в ответах на вопросы с множественным выбором. Авторы обнаружили, что модели более уверены в своих ответах, когда они предоставляют рассуждения перед ответом, независимо от правильности ответа. Это поведение наблюдалось у семи различных моделей на широком спектре тем. Исследователи предполагают, что это связано с тем, как рассуждения модифицируют вероятность выбранного ответа в процессе генерации."
                },
                "en": {
                    "title": "Boosting LLM Confidence Through Reasoning!",
                    "desc": "This paper investigates how the confidence of large language models (LLMs) in their answers is influenced by the method of response. Specifically, it compares direct answers to those that include reasoning, known as the chain of thought approach. The study finds that LLMs exhibit higher confidence in their answers when they provide reasoning first, regardless of the correctness of the answer. This suggests that the reasoning process alters the model's probability estimates, highlighting potential limitations in using these probabilities for evaluation purposes."
                },
                "zh": {
                    "title": "推理提升LLM回答信心的秘密",
                    "desc": "本文研究了大型语言模型（LLM）在回答多项选择题时的信心如何受到回答方式的影响。通过提供推理过程，LLM在选择答案时表现出更高的信心，无论所选答案是否正确。研究表明，推理过程会改变LLM对所选答案的概率估计，这可能是LLM信心的内在限制。类似的现象也在人的回答中观察到，解释答案会提高对其正确性的信心。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10020",
            "title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
            "url": "https://huggingface.co/papers/2501.10020",
            "abstract": "The 2D cartoon style is a prominent art form in digital character creation, particularly popular among younger audiences. While advancements in digital human technology have spurred extensive research into photorealistic digital humans and 3D characters, interactive 2D cartoon characters have received comparatively less attention. Unlike 3D counterparts, which require sophisticated construction and resource-intensive rendering, Live2D, a widely-used format for 2D cartoon characters, offers a more efficient alternative, which allows to animate 2D characters in a manner that simulates 3D movement without the necessity of building a complete 3D model. Furthermore, Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility and efficiency. In this technical report, we introduce Textoon, an innovative method for generating diverse 2D cartoon characters in the Live2D format based on text descriptions. The Textoon leverages cutting-edge language and vision models to comprehend textual intentions and generate 2D appearance, capable of creating a wide variety of stunning and interactive 2D characters within one minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.",
            "score": 12,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "828788f94bccbdc9",
            "authors": [
                "Chao He",
                "Jianqiang Ren",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10020.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Textoon: ИИ создает 2D мультперсонажей по текстовому описанию",
                    "desc": "В статье представлен метод Textoon для создания 2D мультипликационных персонажей в формате Live2D на основе текстовых описаний. Textoon использует современные языковые и визуальные модели для понимания текстовых намерений и генерации 2D внешнего вида персонажей. Метод способен создавать разнообразных интерактивных 2D персонажей менее чем за минуту. Live2D предлагает эффективную альтернативу 3D моделям, позволяя анимировать 2D персонажей, имитируя 3D движение, без необходимости создания полной 3D модели."
                },
                "en": {
                    "title": "Transforming Text into 2D Cartoon Characters with Textoon!",
                    "desc": "This paper presents Textoon, a novel approach for generating diverse 2D cartoon characters using the Live2D format. By utilizing advanced language and vision models, Textoon interprets text descriptions to create visually appealing and interactive characters efficiently. Unlike traditional 3D character models, Textoon allows for quick generation of 2D characters that simulate 3D movement without extensive resources. The method enhances accessibility and efficiency in digital character creation, catering especially to younger audiences."
                },
                "zh": {
                    "title": "Textoon：快速生成多样化2D卡通角色的创新方法",
                    "desc": "这篇论文介绍了一种名为Textoon的方法，用于根据文本描述生成多样化的2D卡通角色。与3D角色相比，2D卡通角色的动画制作更为高效，Textoon利用先进的语言和视觉模型来理解文本意图，并生成2D外观。该方法使用Live2D格式，使得角色动画能够模拟3D运动，而无需构建完整的3D模型。Textoon能够在一分钟内创建出多种令人惊叹和互动的2D角色，提升了数字角色创作的效率和可访问性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09825",
            "title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
            "url": "https://huggingface.co/papers/2501.09825",
            "abstract": "This paper investigates the challenges of developing large language models (LLMs) proficient in both multilingual understanding and medical knowledge. We demonstrate that simply translating medical data does not guarantee strong performance on clinical tasks in the target language. Our experiments reveal that the optimal language mix in training data varies significantly across different medical tasks. We find that larger models with carefully calibrated language ratios achieve superior performance on native-language clinical tasks. Furthermore, our results suggest that relying solely on fine-tuning may not be the most effective approach for incorporating new language knowledge into LLMs. Instead, data and computationally intensive pretraining methods may still be necessary to achieve optimal performance in multilingual medical settings. These findings provide valuable guidance for building effective and inclusive medical AI systems for diverse linguistic communities.",
            "score": 8,
            "issue_id": 1758,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "a2bf2d3dc7e978d7",
            "authors": [
                "Nada Saadi",
                "Tathagata Raha",
                "Clément Christophe",
                "Marco AF Pimentel",
                "Ronnie Rajan",
                "Praveen K Kanithi"
            ],
            "affiliations": [
                "M42 Health, Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09825.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#training",
                    "#science",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычные медицинские LLM: больше, чем просто перевод",
                    "desc": "Статья исследует проблемы разработки больших языковых моделей (LLM), обладающих как многоязычным пониманием, так и медицинскими знаниями. Авторы показывают, что простой перевод медицинских данных не гарантирует высокой производительности на клинических задачах в целевом языке. Эксперименты выявляют, что оптимальное соотношение языков в обучающих данных значительно варьируется для разных медицинских задач. Результаты также указывают на то, что для включения знаний нового языка в LLM может потребоваться ресурсоемкое предобучение, а не только тонкая настройка."
                },
                "en": {
                    "title": "Optimizing Multilingual Medical AI: Beyond Translation and Fine-Tuning",
                    "desc": "This paper explores the difficulties in creating large language models (LLMs) that can understand multiple languages and possess medical expertise. It shows that merely translating medical information does not ensure good performance in clinical tasks for different languages. The research indicates that the best combination of languages in training data changes depending on the specific medical task. Additionally, it suggests that larger models with well-balanced language inputs perform better, and that extensive pretraining may be more beneficial than just fine-tuning for integrating new language capabilities."
                },
                "zh": {
                    "title": "多语言医学模型的优化之道",
                    "desc": "本论文探讨了开发能够理解多种语言和医学知识的大型语言模型（LLMs）所面临的挑战。我们证明，仅仅翻译医学数据并不能保证在目标语言的临床任务中表现良好。实验结果显示，不同医学任务对训练数据中的语言组合有显著不同的最佳需求。我们的研究表明，经过精心调整语言比例的大型模型在本土语言的临床任务中表现更佳，而仅依赖微调可能不是将新语言知识有效融入LLMs的最佳方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10021",
            "title": "X-Dyna: Expressive Dynamic Human Image Animation",
            "url": "https://huggingface.co/papers/2501.10021",
            "abstract": "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna.",
            "score": 5,
            "issue_id": 1752,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "4163d7e5ec4b04ce",
            "authors": [
                "Di Chang",
                "Hongyi Xu",
                "You Xie",
                "Yipeng Gao",
                "Zhengfei Kuang",
                "Shengqu Cai",
                "Chenxu Zhang",
                "Guoxian Song",
                "Chao Wang",
                "Yichun Shi",
                "Zeyuan Chen",
                "Shijie Zhou",
                "Linjie Luo",
                "Gordon Wetzstein",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "ByteDance",
                "Stanford University",
                "University of California Los Angeles",
                "University of California San Diego",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10021.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#diffusion",
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Оживление статичных изображений с помощью ИИ: реалистичная анимация человека и окружения",
                    "desc": "X-Dyna - это новый подход к анимации изображений человека с нуля, основанный на диффузионных моделях. Он использует выражения лица и движения тела из видео-драйвера для создания реалистичной динамики как субъекта, так и окружающей среды. В основе X-Dyna лежит модуль Dynamics-Adapter, который интегрирует контекст внешнего вида в пространственное внимание диффузионной модели. Система также включает локальный модуль управления для передачи выражений лица, что повышает реалистичность анимированных сцен."
                },
                "en": {
                    "title": "X-Dyna: Realistic Animation from a Single Image",
                    "desc": "X-Dyna is a new method for animating a single human image by using expressions and movements from a video. It improves on previous techniques by maintaining dynamic details, making animations look more realistic. The key part of X-Dyna is the Dynamics-Adapter, which helps blend the appearance of the subject with their movements while keeping the animation smooth. Additionally, it includes a module for accurately transferring facial expressions, resulting in more lifelike and expressive animations."
                },
                "zh": {
                    "title": "X-Dyna：真实感动画的新突破",
                    "desc": "X-Dyna是一种新颖的零样本扩散基础管道，能够通过驱动视频中的面部表情和身体动作为单个人物图像生成动画。该方法解决了以往人类姿态控制方法中的动态细节丢失问题，增强了视频动画的真实感。X-Dyna的核心是Dynamics-Adapter模块，它有效地将参考外观上下文整合到扩散模型的空间注意力中，同时保持运动模块合成流畅动态细节的能力。通过连接局部控制模块，X-Dyna能够捕捉与身份无关的面部表情，实现更真实的动画场景中的表情转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10045",
            "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
            "url": "https://huggingface.co/papers/2501.10045",
            "abstract": "The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio).",
            "score": 4,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "8d8cd8e70ad62b51",
            "authors": [
                "Shengkui Zhao",
                "Kun Zhou",
                "Zexu Pan",
                "Yukun Ma",
                "Chong Zhang",
                "Bin Ma"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10045.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#optimization"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "HiFi-SR: Единая сеть для сверхчеткой речи",
                    "desc": "Статья представляет HiFi-SR - унифицированную нейронную сеть для высококачественного повышения разрешения речи. Модель использует единую архитектуру трансформер-сверточной сети для обработки мел-спектрограмм и генерации высококачественных аудиосигналов. Для улучшения качества высоких частот применяется многополосный дискриминатор и многомасштабная функция потерь реконструкции мел-спектрограмм. Экспериментальные результаты показывают превосходство HiFi-SR над существующими методами как по объективным метрикам, так и по субъективным тестам."
                },
                "en": {
                    "title": "HiFi-SR: Elevating Speech Quality with Unified GANs",
                    "desc": "This paper introduces HiFi-SR, a novel approach to speech super-resolution using generative adversarial networks (GANs). Unlike traditional methods that use separate networks, HiFi-SR employs a unified transformer-convolutional architecture for end-to-end training, improving the consistency and quality of generated speech. The transformer encodes low-resolution mel-spectrograms into latent representations, while the convolutional network converts these into high-resolution audio waveforms. The model also integrates a multi-band discriminator and a mel-reconstruction loss to enhance high-frequency details, achieving superior performance in various scenarios."
                },
                "zh": {
                    "title": "HiFi-SR：高保真语音超分辨率的新方法",
                    "desc": "本研究提出了一种名为HiFi-SR的统一网络，用于语音超分辨率（SR），通过端到端的对抗训练实现高保真语音重建。该模型结合了变换器和卷积网络，能够有效地将低分辨率的mel谱图转换为高分辨率的时域波形。为了提高高频细节的保真度，我们在对抗训练中引入了多带宽、多尺度的时频判别器和多尺度mel重建损失。实验结果表明，HiFi-SR在目标指标和ABX偏好测试中显著优于现有的语音超分辨率方法，适用于不同的输入语音信号。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10132",
            "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
            "url": "https://huggingface.co/papers/2501.10132",
            "abstract": "Enhancing large language models (LLMs) with real-time APIs can help generate more accurate and up-to-date responses. However, evaluating the function calling abilities of LLMs in real-world scenarios remains under-explored due to the complexity of data collection and evaluation. In this work, we introduce ComplexFuncBench, a benchmark for complex function calling across five real-world scenarios. Compared to existing benchmarks, ComplexFuncBench encompasses multi-step and constrained function calling, which requires long-parameter filing, parameter value reasoning, and 128k long context. Additionally, we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks. Through comprehensive experiments, we demonstrate the deficiencies of state-of-the-art LLMs in function calling and suggest future directions for optimizing these capabilities. The data and code are available at https://github.com/THUDM/ComplexFuncBench.",
            "score": 4,
            "issue_id": 1749,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "de405dcc4bfc8efc",
            "authors": [
                "Lucen Zhong",
                "Zhengxiao Du",
                "Xiaohan Zhang",
                "Haiyi Hu",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10132.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Новый бенчмарк для оценки сложных вызовов функций в больших языковых моделях",
                    "desc": "Данная статья представляет новый бенчмарк ComplexFuncBench для оценки способностей больших языковых моделей (LLM) вызывать сложные функции в реальных сценариях. Бенчмарк включает в себя многошаговые и ограниченные вызовы функций, требующие заполнения длинных параметров и рассуждений о значениях параметров. Авторы также предлагают автоматическую систему ComplexEval для количественной оценки задач сложного вызова функций. Эксперименты показывают недостатки современных LLM в вызове функций и предлагают направления для оптимизации этих возможностей."
                },
                "en": {
                    "title": "Benchmarking Complex Function Calling in LLMs",
                    "desc": "This paper presents ComplexFuncBench, a new benchmark designed to evaluate the function calling abilities of large language models (LLMs) in real-world scenarios. It focuses on complex tasks that involve multi-step and constrained function calling, which require advanced reasoning and handling of long contexts. The authors also introduce ComplexEval, an automatic framework for quantitatively assessing these complex function calling tasks. Through their experiments, they highlight the limitations of current state-of-the-art LLMs and propose directions for improving their performance in this area."
                },
                "zh": {
                    "title": "提升LLMs函数调用能力的基准与评估",
                    "desc": "本论文提出了ComplexFuncBench，这是一个用于评估大型语言模型（LLMs）在复杂函数调用方面的基准测试。该基准涵盖了五种真实场景，涉及多步骤和受限的函数调用，要求模型进行长参数填写和参数值推理。我们还提出了ComplexEval，一个自动化框架，用于定量评估复杂函数调用任务的能力。通过实验，我们展示了当前最先进的LLMs在函数调用方面的不足，并提出了未来优化的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09978",
            "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
            "url": "https://huggingface.co/papers/2501.09978",
            "abstract": "We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: [Project Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).",
            "score": 2,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "e5b8603f26a902f9",
            "authors": [
                "Xiangyue Liu",
                "Kunming Luo",
                "Heng Li",
                "Qi Zhang",
                "Yuan Liu",
                "Li Yi",
                "Ping Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Tencent AI Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09978.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в редактировании анимируемых 3D-аватаров с помощью гауссовых моделей",
                    "desc": "Статья представляет GaussianAvatar-Editor - инновационную систему для редактирования анимируемых гауссовых аватаров головы на основе текстовых инструкций. Авторы предлагают функцию Weighted Alpha Blending Equation (WABE) для решения проблем, связанных с окклюзией при движении и пространственно-временной несогласованностью. Система использует условное состязательное обучение для улучшения качества редактирования и обеспечения согласованности в 4D. Эксперименты показывают превосходство предложенного подхода над существующими методами в создании фотореалистичных и согласованных результатов редактирования анимируемых 4D гауссовых аватаров."
                },
                "en": {
                    "title": "Revolutionizing 4D Avatar Editing with GaussianAvatar-Editor",
                    "desc": "GaussianAvatar-Editor is a new framework designed for editing animated Gaussian head avatars using text inputs. It tackles challenges like motion occlusion and maintaining spatial-temporal consistency, which are common in 4D animations. The framework introduces the Weighted Alpha Blending Equation (WABE) to improve the blending of visible elements while minimizing the impact of non-visible ones. Additionally, it employs conditional adversarial learning to enhance the quality of edits and ensure consistency throughout the animation process, resulting in photorealistic outputs."
                },
                "zh": {
                    "title": "高斯头像编辑的创新之路",
                    "desc": "我们介绍了GaussianAvatar-Editor，这是一个创新的框架，用于基于文本驱动的可动画高斯头像编辑。与静态3D高斯编辑不同，编辑可动画的4D高斯头像面临运动遮挡和时空不一致等挑战。为了解决这些问题，我们提出了加权阿尔法混合方程（WABE），该函数增强了可见高斯的混合权重，同时抑制了对不可见高斯的影响。通过结合条件对抗学习，我们提高了编辑质量并确保了4D一致性，从而实现了逼真且一致的可动画4D高斯编辑结果。"
                }
            }
        }
    ],
    "link_prev": "2025-01-17.html",
    "link_next": "2025-01-21.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "short_date_next": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章探讨了一种用于大型语言模型推理时间计算的演化搜索策略。该方法称为“心智演化”，使用语言模型生成、重组和优化候选响应。这种方法避免了在有解决方案评估器时正式定义底层推理问题的需要。在控制推理成本的情况下，心智演化在自然语言规划任务中显著优于其他推理策略，如Best-of-N和Sequential Revision。在TravelPlanner和Natural Plan基准测试中，心智演化在不使用正式求解器的情况下，使用Gemini 1.5 Pro解决了超过98%的问题实例。",
        "title": "Evolving Deeper LLM Thinking",
        "pinyin": "这篇文章探讨了一种用于大型语言模型推理时间计算的演化搜索策略。该方法称为“心智演化”，使用语言模型生成、重组和优化候选响应。这种方法避免了在有解决方案评估器时正式定义底层推理问题的需要。在控制推理成本的情况下，心智演化在自然语言规划任务中显著优于其他推理策略，如Best-of-N和Sequential Revision。在TravelPlanner和Natural Plan基准测试中，心智演化在不使用正式求解器的情况下，使用Gemini 1.5 Pro解决了超过98%的问题实例。\n\nZhè piān wénzhāng tàntào le yī zhǒng yòngyú dàxíng yǔyán móxíng tuīlǐ shíjiān jìsuàn de yǎnhuà sōusuǒ cèlüè. Gāi fāngfǎ chēngwéi “xīnzhì yǎnhuà”, shǐyòng yǔyán móxíng shēngchéng, chóngzǔ hé yōuhuà hòuxuǎn xǐngyìng. Zhè zhǒng fāngfǎ bìmiǎn le zài yǒu jiějué fāng'àn píngjiàqì shí zhèngshì dìngyì dǐcéng tuīlǐ wèntí de xūyào. Zài kòngzhì tuīlǐ chéngběn de qíngkuàng xià, xīnzhì yǎnhuà zài zìrán yǔyán guīhuà rènwù zhōng xiǎnzhù yōuqí qítā tuīlǐ cèlüè, rú Best-of-N hé Sequential Revision. Zài TravelPlanner hé Natural Plan jīzhǔn cèshì zhōng, xīnzhì yǎnhuà zài bù shǐyòng zhèngshì qiújiěqì de qíngkuàng xià, shǐyòng Gemini 1.5 Pro jiějué le chāoguò 98% de wèntí shìlì.",
        "vocab": "[\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tào\", \"trans\": \"discuss\"},\n    {\"word\": \"演化\", \"pinyin\": \"yǎn huà\", \"trans\": \"evolution\"},\n    {\"word\": \"搜索\", \"pinyin\": \"sōu suǒ\", \"trans\": \"search\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"心智\", \"pinyin\": \"xīn zhì\", \"trans\": \"mind\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"重组\", \"pinyin\": \"chóng zǔ\", \"trans\": \"recombine\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"候选\", \"pinyin\": \"hòu xuǎn\", \"trans\": \"candidate\"},\n    {\"word\": \"响应\", \"pinyin\": \"xiǎng yìng\", \"trans\": \"response\"},\n    {\"word\": \"避免\", \"pinyin\": \"bì miǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"解决方案\", \"pinyin\": \"jiě jué fāng àn\", \"trans\": \"solution\"},\n    {\"word\": \"评估器\", \"pinyin\": \"píng gū qì\", \"trans\": \"evaluator\"},\n    {\"word\": \"定义\", \"pinyin\": \"dìng yì\", \"trans\": \"define\"},\n    {\"word\": \"底层\", \"pinyin\": \"dǐ céng\", \"trans\": \"underlying\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"优于\", \"pinyin\": \"yōu yú\", \"trans\": \"superior to\"},\n    {\"word\": \"规划\", \"pinyin\": \"guī huà\", \"trans\": \"planning\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wu\", \"trans\": \"task\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"实例\", \"pinyin\": \"shí lì\", \"trans\": \"instance\"},\n    {\"word\": \"求解器\", \"pinyin\": \"qiú jiě qì\", \"trans\": \"solver\"}\n]",
        "trans": "This article discusses an evolutionary search strategy for calculating the inference time of large language models. The method, known as \"Mental Evolution,\" employs language models to generate, recombine, and optimize candidate responses. This approach eliminates the need to formally define underlying inference problems when a solution evaluator is available. Under the constraint of controlling inference costs, Mental Evolution significantly outperforms other inference strategies, such as Best-of-N and Sequential Revision, in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mental Evolution solved over 98% of problem instances using Gemini 1.5 Pro without the need for a formal solver.",
        "update_ts": "2025-01-20 09:11"
    }
}