{
    "date": {
        "ru": "7 ноября",
        "en": "November 7",
        "zh": "11月7日"
    },
    "time_utc": "2024-11-07 16:15",
    "weekday": 3,
    "issue_id": 459,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.03823",
            "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
            "url": "https://huggingface.co/papers/2411.03823",
            "abstract": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.",
            "score": 29,
            "issue_id": 457,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "3f0a02ee67213e17",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Чистота данных - залог надёжности мультимодальных ИИ-моделей",
                    "desc": "Статья посвящена проблеме загрязнения данных в мультимодальных больших языковых моделях (MLLM). Авторы представляют фреймворк MM-Detect для обнаружения загрязнения в MLLM. Экспериментальные результаты показывают, что MM-Detect чувствителен к разным степеням загрязнения и может выявлять значительные улучшения производительности из-за утечки обучающего набора. Исследование также рассматривает возможность загрязнения на этапах предобучения и тонкой настройки моделей."
                },
                "en": {
                    "title": "Detecting Contamination in Multimodal Language Models",
                    "desc": "This paper addresses the challenge of data contamination in multimodal large language models (MLLMs), which can affect their performance evaluation. The authors propose a new framework called MM-Detect, specifically designed to identify contamination in MLLMs across different modalities and training phases. Their experiments show that MM-Detect can effectively detect varying levels of contamination and reveal how training set leakage impacts performance. Additionally, the study investigates contamination sources during both the pre-training and fine-tuning phases of MLLMs, providing valuable insights into potential contamination points."
                },
                "zh": {
                    "title": "多模态数据污染检测的新突破",
                    "desc": "本研究介绍了一种针对多模态大语言模型（MLLMs）数据污染检测的新框架MM-Detect。该框架能够有效识别在训练过程中可能出现的数据污染问题，尤其是在多模态基准测试中。实验结果表明，MM-Detect对不同程度的数据污染非常敏感，并能显著提升性能评估的准确性。此外，我们还探讨了数据污染可能源自LLMs的预训练阶段和MLLMs的微调阶段，为理解污染引入的时机提供了新的视角。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.03562",
            "title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
            "url": "https://huggingface.co/papers/2411.03562",
            "abstract": "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.",
            "score": 23,
            "issue_id": 457,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 ноября",
                "en": "November 5",
                "zh": "11月5日"
            },
            "hash": "1db584382b826315",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Agent K v1.0: Автономный ИИ-агент для решения задач в области науки о данных",
                    "desc": "Представлен Agent K v1.0 - автономный агент для автоматизации задач в области науки о данных. Он использует структурированную систему рассуждений для обработки памяти и обучения на основе опыта. Agent K v1.0 оптимизирует долгосрочную и краткосрочную память, что позволяет ему улучшать решения без дополнительной настройки. Оценка возможностей агента проводилась на соревнованиях Kaggle, где он продемонстрировал 92.5% успешность в решении разнообразных задач."
                },
                "en": {
                    "title": "Agent K v1.0: Your Autonomous Data Science Expert!",
                    "desc": "Agent K v1.0 is an autonomous data science agent that automates the entire data science life cycle, learning from its experiences to improve over time. It uses a structured reasoning framework to manage memory and handle complex tasks without the need for traditional fine-tuning methods. By employing Bayesian optimization for hyperparameter tuning and feature engineering, it effectively addresses a variety of data science challenges. The agent's performance has been validated through Kaggle competitions, where it achieved a high success rate and ranked competitively against human experts, demonstrating its advanced capabilities in multiple domains."
                },
                "zh": {
                    "title": "Agent K v1.0：自动化数据科学的未来",
                    "desc": "我们介绍了Agent K v1.0，这是一个端到端的自主数据科学代理，旨在自动化、优化和泛化各种数据科学任务。Agent K v1.0 完全自动化，能够管理整个数据科学生命周期，并通过经验学习来提升能力。它利用灵活的结构化推理框架，动态处理嵌套结构中的记忆，有效地从积累的经验中学习，以应对复杂的推理任务。通过选择性存储和检索关键信息，Agent K v1.0 优化了短期和长期记忆，基于环境奖励指导未来决策，展现出与人类专家相当的技能水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.03884",
            "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
            "url": "https://huggingface.co/papers/2411.03884",
            "abstract": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
            "score": 5,
            "issue_id": 459,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "6ed1524392784244",
            "data": {
                "categories": [
                    "#math",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "PolyCom: Революция в активационных функциях для трансформеров",
                    "desc": "Статья представляет новую категорию активационных функций для трансформеров, называемую PolyCom (полиномиальные композитные активации). Авторы теоретически обосновывают, что PolyCom обладает улучшенной выразительностью и эффективностью по сравнению с другими активационными функциями. Эксперименты показывают, что использование PolyCom в больших языковых моделях (LLM) позволяет им лучше улавливать взаимодействия высокого порядка в данных. Результаты демонстрируют значительные улучшения в точности и скорости сходимости по сравнению с традиционными активационными функциями."
                },
                "en": {
                    "title": "Unlocking Transformer Potential with Polynomial Activations",
                    "desc": "This paper introduces a new type of activation function called Polynomial Composition Activations (PolyCom) for transformer models. The authors argue that PolyCom enhances the nonlinearity of transformers, which is crucial for improving their representational capacity. Through mathematical analysis, they show that networks using PolyCom can approximate complex functions more efficiently than those using traditional activation functions. Empirical tests on large language models reveal that PolyCom leads to better performance in terms of accuracy and convergence, demonstrating its potential as a superior alternative in machine learning applications."
                },
                "zh": {
                    "title": "多项式组合激活函数：提升变换器性能的新方法",
                    "desc": "本文提出了一种新型的多项式组合激活函数（PolyCom），旨在优化变换器的动态性能。我们通过数学分析证明了PolyCom在表达能力和效率方面优于其他激活函数。实验结果表明，使用PolyCom的网络在逼近光滑函数时所需的参数更少，且在大型语言模型的预训练配置中表现出更高的准确性和收敛速度。我们的研究表明，PolyCom能够有效捕捉数据中的高阶交互，从而显著提升模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04109",
            "title": "Self-Consistency Preference Optimization",
            "url": "https://huggingface.co/papers/2411.04109",
            "abstract": "Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.",
            "score": 1,
            "issue_id": 459,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "213f2796c0bc72ae",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосогласованность как ключ к улучшению ИИ без учителя",
                    "desc": "Статья представляет новый метод самосогласованной оптимизации предпочтений (ScPO) для улучшения моделей машинного обучения без использования аннотированных данных. Этот подход расширяет концепцию самосогласованности, применяемую обычно на этапе вывода, на процесс обучения модели. ScPO итеративно обучает модель предпочитать согласованные ответы над несогласованными на новых задачах без учителя. Метод показывает значительные улучшения в задачах рассуждения, таких как GSM8K и MATH, сокращая разрыв с обучением с учителем."
                },
                "en": {
                    "title": "Empowering Models with Self-Consistency for Better Reasoning",
                    "desc": "This paper introduces a new method called self-consistency preference optimization (ScPO) to enhance the training of machine learning models without human annotations. ScPO focuses on improving the consistency of answers by iteratively training the model to prefer consistent responses over inconsistent ones. The authors demonstrate that ScPO significantly outperforms traditional reward model training on complex reasoning tasks, such as GSM8K and MATH, and even approaches the performance of supervised training. Additionally, when combined with standard supervised learning, ScPO further boosts the model's performance, achieving superior results on various benchmarks."
                },
                "zh": {
                    "title": "自我一致性优化，提升模型推理能力！",
                    "desc": "自我对齐是指模型在没有人工标注的情况下自我改进的过程，近年来这一研究领域迅速发展。然而，现有技术在复杂推理任务中常常无法有效提升性能，因为很难分配正确的奖励。本文提出了一种新的方法——自我一致性偏好优化（ScPO），它通过迭代训练一致的答案，使其优于不一致的答案，从而解决无监督新问题。实验结果表明，ScPO在推理任务上显著优于传统的奖励模型训练，且与标准监督学习结合后效果更佳。"
                }
            }
        }
    ],
    "link_prev": "2024-11-06.html",
    "link_next": "2024-11-08.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11月6日"
    },
    "short_date_next": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11月8日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。",
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "pinyin": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。\n\nzhè piān wén zhāng jiè shào le yī zhǒng gǎi jìn de jiǎn suǒ zēng qiáng shēng chéng (RAG) fāng fǎ, chēng wéi HtmlRAG. chuántǒng de RAG xì tǒng cóng wǎng yè jiǎn suǒ xìn xī, tī qǔ chún wén běn wèi gěi dà yǔ yán mó xìng (LLMs). rán ér, zhè huì diū shī HTML zhōng de jiè gòu hé yǔ yì xìn xī. HtmlRAG zhí jiē shǐ yòng HTML gē shì de zhī shì, bǎo liú gèng duō xìn xī. dàn HTML bāo hán é xiǎo de biǎo qiān hé zào shēng, zuò zhě tí chū le qīng lǐ hé yā suō cè lüè lái jiě jué zhè gè wèn tí. shí yàn zhèng míng, HtmlRAG zài liù gè wèn dá shù jù zhōng biǎo xiàn gèng hǎo.",
        "vocab": "[\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"检索\", \"pinyin\": \"jiǎn suǒ\", \"trans\": \"retrieval\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhancement\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēng wéi\", \"trans\": \"called\"},\n    {\"word\": \"传统\", \"pinyin\": \"chuán tǒng\", \"trans\": \"traditional\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"网页\", \"pinyin\": \"wǎng yè\", \"trans\": \"webpage\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"纯文本\", \"pinyin\": \"chún wén běn\", \"trans\": \"pure text\"},\n    {\"word\": \"喂给\", \"pinyin\": \"wèi gěi\", \"trans\": \"feed\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"丢失\", \"pinyin\": \"diū shī\", \"trans\": \"lose\"},\n    {\"word\": \"结构\", \"pinyin\": \"jié gòu\", \"trans\": \"structure\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantics\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìn xī\", \"trans\": \"information\"},\n    {\"word\": \"直接\", \"pinyin\": \"zhí jiē\", \"trans\": \"directly\"},\n    {\"word\": \"格式\", \"pinyin\": \"gé shì\", \"trans\": \"format\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shì\", \"trans\": \"knowledge\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎo liú\", \"trans\": \"retain\"},\n    {\"word\": \"额外\", \"pinyin\": \"é wài\", \"trans\": \"extra\"},\n    {\"word\": \"标签\", \"pinyin\": \"biāo qiān\", \"trans\": \"tag\"},\n    {\"word\": \"噪声\", \"pinyin\": \"zào shēng\", \"trans\": \"noise\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"清理\", \"pinyin\": \"qīng lǐ\", \"trans\": \"clean\"},\n    {\"word\": \"压缩\", \"pinyin\": \"yā suō\", \"trans\": \"compress\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces an improved Retrieval-Augmented Generation (RAG) method called HtmlRAG. Traditional RAG systems retrieve information from web pages and extract plain text to feed into large language models (LLMs). However, this approach loses the structural and semantic information present in HTML. HtmlRAG directly uses knowledge in HTML format, preserving more information. But since HTML contains additional tags and noise, the authors propose cleaning and compression strategies to address this issue. Experiments show that HtmlRAG performs better on six question-answering datasets.",
        "update_ts": "2024-11-07 10:12"
    }
}