{
    "date": {
        "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 5",
        "zh": "12æœˆ5æ—¥"
    },
    "time_utc": "2025-12-05 09:00",
    "weekday": 4,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-05",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.04677",
            "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
            "url": "https://huggingface.co/papers/2512.04677",
            "abstract": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
            "score": 165,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "ba6604a744e81b69",
            "authors": [
                "Yubo Huang",
                "Hailong Guo",
                "Fangtai Wu",
                "Shifeng Zhang",
                "Shijie Huang",
                "Qijun Gan",
                "Lin Liu",
                "Sirui Zhao",
                "Enhong Chen",
                "Jiaming Liu",
                "Steven Hoi"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing University of Posts and Telecommunications",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04677.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04324",
            "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
            "url": "https://huggingface.co/papers/2512.04324",
            "abstract": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io",
            "score": 146,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "67634956bb0a1146",
            "authors": [
                "Fangyu Lei",
                "Jinxiang Meng",
                "Yiming Huang",
                "Junjie Zhao",
                "Yitong Zhang",
                "Jianwen Luo",
                "Xin Zou",
                "Ruiyi Yang",
                "Wenbo Shi",
                "Yan Gao",
                "Shizhu He",
                "Zuo Wang",
                "Qian Liu",
                "Yang Wang",
                "Ke Wang",
                "Jun Zhao",
                "Kang Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute of Automation, CAS",
                "NUS",
                "TikTok",
                "UC San Diego",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04324.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#reasoning",
                    "#science"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DAComp Ğ¸Ğ· 210 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… SQL-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ…, Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· LLM-ÑÑƒĞ´ÑŒÑ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑƒÑĞ¿ĞµÑ… Ğ¼ĞµĞ½ĞµĞµ 20% Ğ½Ğ° Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ¸Ğ¶Ğµ 40% Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Unveiling the Gaps in Data Engineering and Analysis with DAComp",
                    "desc": "DAComp is a benchmark consisting of 210 tasks designed to evaluate the performance of agents in real-world data engineering and data analysis workflows. It highlights significant shortcomings in both areas, particularly in data engineering tasks where agents struggle with repository-level engineering and SQL pipeline creation. Data analysis tasks also reveal low performance, indicating challenges in open-ended reasoning and strategic planning. By identifying these deficiencies, DAComp serves as a critical tool for advancing the development of autonomous data agents in enterprise environments."
                },
                "zh": {
                    "title": "DACompï¼šè¯„ä¼°æ•°æ®æ™ºèƒ½ä½“èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "DACompæ˜¯ä¸€ä¸ªåŒ…å«210ä¸ªä»»åŠ¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®æ•°æ®å·¥ç¨‹å’Œæ•°æ®åˆ†æå·¥ä½œæµç¨‹ä¸­çš„èƒ½åŠ›ã€‚æ•°æ®å·¥ç¨‹ä»»åŠ¡æ¶‰åŠå°†åŸå§‹æ•°æ®æºè½¬åŒ–ä¸ºå¯åˆ†æçš„è¡¨æ ¼ï¼Œè€Œæ•°æ®åˆ†æä»»åŠ¡åˆ™å°†è¿™äº›è¡¨æ ¼è½¬åŒ–ä¸ºå†³ç­–å¯¼å‘çš„è§è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“åœ¨DACompä¸Šè¡¨ç°ä¸ä½³ï¼Œæ•°æ®å·¥ç¨‹ä»»åŠ¡çš„æˆåŠŸç‡ä½äº20%ï¼Œè€Œæ•°æ®åˆ†æä»»åŠ¡çš„å¹³å‡å¾—åˆ†ä¹Ÿä½äº40%ã€‚é€šè¿‡æ˜ç¡®è¯Šæ–­è¿™äº›å±€é™æ€§ï¼ŒDACompä¸ºå¼€å‘çœŸæ­£èƒ½å¤Ÿåœ¨ä¼ä¸šç¯å¢ƒä¸­å·¥ä½œçš„è‡ªä¸»æ•°æ®æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªä¸¥æ ¼ä¸”ç°å®çš„æµ‹è¯•å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04987",
            "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
            "url": "https://huggingface.co/papers/2512.04987",
            "abstract": "The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
            "score": 71,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "99887076d6ca86f9",
            "authors": [
                "Nex-AGI Team",
                ":",
                "Yuxuan Cai",
                "Lu Chen",
                "Qiaoling Chen",
                "Yuyang Ding",
                "Liwen Fan",
                "Wenjie Fu",
                "Yufei Gao",
                "Honglin Guo",
                "Pinxue Guo",
                "Zhenhua Han",
                "Zhengfu He",
                "Hanglei Hu",
                "Kai Hu",
                "Shengjia Hua",
                "Tianyu Huai",
                "Baodai Huang",
                "Li Ji",
                "Zhen Jiang",
                "Zhikai Lei",
                "Bufan Li",
                "Jiahang Lin",
                "Lizhi Lin",
                "Jinxiu Liu",
                "Shichun Liu",
                "Ziming Liu",
                "Yuchen Ni",
                "Pengfang Qian",
                "Yujiong Shen",
                "Qingyun Shi",
                "Wentao Shu",
                "Peng Sun",
                "Yiran Suo",
                "Tian Tang",
                "Boyu Tian",
                "Guoteng Wang",
                "Junzhe Wang",
                "Peixin Wang",
                "Zhiheng Xi",
                "Hang Yan",
                "Jie Yang",
                "Zhixiong Yang",
                "Tianchu Yao",
                "Guangze Ye",
                "Qianxi Yu",
                "Shuo Zhang",
                "Xinyue Zhang",
                "Yiqi Zhang",
                "Jiarong Zhao",
                "Miao Zheng",
                "Rui Zheng",
                "Enyu Zhou",
                "Jiazheng Zhou",
                "Maosen Zhou",
                "Yuhao Zhou",
                "Tao Gui",
                "Yining Zheng",
                "Xinchi Chen",
                "Jie Zhou",
                "Siyuan Feng",
                "Qin Chen",
                "Liang He",
                "Qi Zhang",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04987.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Nex Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: NexAU Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², NexA4A Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¸ NexGAP Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Nex-N1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Scaling LLMs: From Imitation to Autonomous Decision-Making",
                    "desc": "This paper introduces a new framework called Nex, which enhances the training of large language models (LLMs) as autonomous agents. It focuses on three key areas: complexity, diversity, and fidelity, allowing for more effective learning through interactive environments. NexAU provides a flexible structure for creating complex agent hierarchies, while NexA4A generates diverse agent configurations from natural language. Finally, NexGAP helps bridge the gap between simulated and real-world environments, leading to improved performance in complex tasks compared to existing models."
                },
                "zh": {
                    "title": "è‡ªä¸»ä»£ç†çš„äº’åŠ¨ç¯å¢ƒæ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†NexAUã€NexA4Aå’ŒNexGAPçš„å¼•å…¥ï¼Œè¿™äº›å·¥å…·èƒ½å¤Ÿåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»ä»£ç†çš„äº’åŠ¨ç¯å¢ƒä¸­æ‰©å±•å¤æ‚æ€§ã€å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿï¼Œä»è€Œå®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜éœ€è¦ä»é™æ€æ¨¡ä»¿è½¬å‘åŸºäºæ¿€åŠ±çš„å†³ç­–åˆ¶å®šï¼Œä½†ç¼ºä¹å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½æ¥æ„å»ºé«˜è´¨é‡çš„äº’åŠ¨ä¿¡å·ï¼Œé˜»ç¢äº†è¿™ä¸€è½¬å˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªç»´åº¦æ¥æ‰©å±•äº’åŠ¨ç¯å¢ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼šå¤æ‚æ€§ã€è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–ä»£ç†å±‚æ¬¡çš„èƒ½åŠ›ï¼Œä»¥åŠé€šè¿‡åŠ¨æ€çœŸå®ç¯å¢ƒæ¥ç¼©å°æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæˆ‘ä»¬åŸºç¡€è®¾æ–½è®­ç»ƒçš„Nex-N1åœ¨å¤æ‚çš„ä»£ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨ä¸å‰æ²¿ä¸“æœ‰æ¨¡å‹çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02589",
            "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
            "url": "https://huggingface.co/papers/2512.02589",
            "abstract": "PaperDebugger is an in-editor academic writing assistant that integrates large language models, enabling direct interaction within LaTeX editors for document state management, revision, and literature search.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
            "score": 55,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "66f5533c505f33e5",
            "authors": [
                "Junyi Hou",
                "Andre Lin Huikai",
                "Nuo Chen",
                "Yiwei Gong",
                "Bingsheng He"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02589.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05111",
            "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
            "url": "https://huggingface.co/papers/2512.05111",
            "abstract": "ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.",
            "score": 45,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "cfd3368c930c3069",
            "authors": [
                "Shengyuan Ding",
                "Xinyu Fang",
                "Ziyu Liu",
                "Yuhang Zang",
                "Yuhang Cao",
                "Xiangyu Zhao",
                "Haodong Duan",
                "Xiaoyi Dong",
                "Jianze Liang",
                "Bin Wang",
                "Conghui He",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05111.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#rl",
                    "#interpretability",
                    "#hallucinations",
                    "#alignment",
                    "#rlhf",
                    "#agents",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "ARM-Thinker â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ°Ğ´Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ reinforcement learning, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ARM-Thinker Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Empowering AI with Agentic Verification for Better Reasoning",
                    "desc": "ARM-Thinker is a novel agentic reward model designed to enhance the performance of vision-language systems by utilizing external tools for verification. This model addresses common issues in existing reward models, such as hallucination and weak visual grounding, by autonomously invoking tools to provide verifiable evidence for its judgments. By employing multi-stage reinforcement learning, ARM-Thinker optimizes both the decision to use tools and the accuracy of its assessments. The introduction of ARMBench-VL benchmarks further validates the model's effectiveness, showing significant improvements in accuracy and interpretability in complex multimodal reasoning tasks."
                },
                "zh": {
                    "title": "è‡ªä¸»å¥–åŠ±æ¨¡å‹æå‡æ¨ç†å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§",
                    "desc": "ARM-Thinkeræ˜¯ä¸€ç§è‡ªä¸»å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿä½¿ç”¨å¤–éƒ¨å·¥å…·è¿›è¡ŒéªŒè¯ï¼Œä»è€Œæé«˜å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡è°ƒç”¨å›¾åƒè£å‰ªå’Œæ–‡æ¡£æ£€ç´¢ç­‰å·¥å…·ï¼ŒåŸºäºå¯éªŒè¯çš„è¯æ®æ¥æ”¯æŒåˆ¤æ–­ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚ARM-Thinkeré‡‡ç”¨å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–å·¥å…·è°ƒç”¨å†³ç­–å’Œåˆ¤æ–­å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARM-Thinkeråœ¨å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šå¹³å‡æé«˜äº†16.2%ï¼Œåœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸Šæé«˜äº†9.6%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04926",
            "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
            "url": "https://huggingface.co/papers/2512.04926",
            "abstract": "Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
            "score": 40,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "247d709340d0e502",
            "authors": [
                "Yueming Pan",
                "Ruoyu Feng",
                "Qi Dai",
                "Yuqi Wang",
                "Wenfeng Lin",
                "Mingyu Guo",
                "Chong Luo",
                "Nanning Zheng"
            ],
            "affiliations": [
                "ByteDance",
                "IAIR, Xian Jiaotong University",
                "Microsoft Research Asia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04926.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#open_source",
                    "#diffusion",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¿ĞµÑ€ĞµĞ´Ğ¸: Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¸Ğ´ĞµĞ¸ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Semantic-First Diffusion (SFD) â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ñ Ğ¾Ğ¿ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ñ‘Ñ‚ĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Semantic VAE, Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet SFD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ FID 1.04 Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² 100 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Prioritizing Semantics for Superior Image Generation",
                    "desc": "Semantic-First Diffusion (SFD) is a novel approach in image generation that improves the quality and speed of Latent Diffusion Models (LDMs) by focusing on semantic information first. It separates the denoising process of semantic and texture latents, allowing the model to refine high-level structures before adding fine details. By using a dedicated Semantic VAE to extract semantic latents, SFD provides a clearer guide for generating textures, leading to more coherent images. This method not only enhances convergence speed but also outperforms existing techniques, showcasing the benefits of prioritizing semantics in the generation process."
                },
                "zh": {
                    "title": "è¯­ä¹‰ä¼˜å…ˆï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "è¯­ä¹‰ä¼˜å…ˆæ‰©æ•£ï¼ˆSFDï¼‰é€šè¿‡å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œå˜é‡æ¥å¢å¼ºå›¾åƒç”Ÿæˆï¼Œä»è€Œæé«˜æ”¶æ•›é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆç»“åˆä»é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨æå–çš„ç´§å‡‘è¯­ä¹‰æ½œå˜é‡ä¸çº¹ç†æ½œå˜é‡ï¼Œæ„å»ºå¤åˆæ½œå˜é‡ã€‚SFDçš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ä¸åŒçš„å™ªå£°è°ƒåº¦å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œå˜é‡ï¼Œä½¿å¾—è¯­ä¹‰åœ¨æ—¶é—´ä¸Šä¼˜å…ˆäºçº¹ç†ï¼Œä»è€Œä¸ºçº¹ç†ç»†åŒ–æä¾›æ›´æ¸…æ™°çš„é«˜å±‚æ¬¡æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSFDåœ¨ImageNetæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ”¶æ•›é€Ÿåº¦æ¯”åŸå§‹æ–¹æ³•å¿«100å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04678",
            "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
            "url": "https://huggingface.co/papers/2512.04678",
            "abstract": "The paper introduces Reward Forcing, which enhances video generation by updating sink tokens with EMA-Sink and using Rewarded Distribution Matching Distillation to prioritize dynamic content.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "afadf7ab06f72e40",
            "authors": [
                "Yunhong Lu",
                "Yanhong Zeng",
                "Haobo Li",
                "Hao Ouyang",
                "Qiuyu Wang",
                "Ka Leong Cheng",
                "Jiapeng Zhu",
                "Hengyuan Cao",
                "Zhipeng Zhang",
                "Xing Zhu",
                "Yujun Shen",
                "Min Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "HUST",
                "SIAS-ZJU",
                "SJTU",
                "ZJU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04678.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03000",
            "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
            "url": "https://huggingface.co/papers/2512.03000",
            "abstract": "DynamicVerse is a framework that models dynamic real-world videos by integrating large vision, geometric, and multimodal models to produce a comprehensive 4D multimodal dataset, achieving superior performance in video depth estimation, camera pose estimation, and camera intrinsics estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "4d1532cef25a96c5",
            "authors": [
                "Kairun Wen",
                "Yuzhi Huang",
                "Runyu Chen",
                "Hui Zheng",
                "Yunlong Lin",
                "Panwang Pan",
                "Chenxin Li",
                "Wenyan Cong",
                "Jian Zhang",
                "Junbin Lu",
                "Chenguo Lin",
                "Dilin Wang",
                "Zhicheng Yan",
                "Hongyu Xu",
                "Justin Theiss",
                "Yue Huang",
                "Xinghao Ding",
                "Rakesh Ranjan",
                "Zhiwen Fan"
            ],
            "affiliations": [
                "CUHK",
                "Meta",
                "PKU",
                "UT Austin",
                "UW",
                "XMU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03000.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#3d",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ 4D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "DynamicVerse â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ 4D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Bundle Adjustment Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "DynamicVerse: Revolutionizing Real-World Video Understanding",
                    "desc": "DynamicVerse is a novel framework designed to model dynamic real-world videos by combining advanced vision, geometric, and multimodal models. This approach creates a rich 4D multimodal dataset that enhances the understanding of video depth, camera pose, and camera intrinsics. By utilizing large-scale internet videos, DynamicVerse overcomes limitations of traditional datasets, providing detailed annotations and descriptions for better interpretation of real-world dynamics. The framework's integration of window-based Bundle Adjustment with global optimization leads to improved accuracy in capturing physical-scale measurements across various tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€è§†é¢‘å»ºæ¨¡çš„æ–°çºªå…ƒ",
                    "desc": "DynamicVerseæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡åŠ¨æ€çš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œç»“åˆäº†å¤§å‹è§†è§‰ã€å‡ ä½•å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼Œç”Ÿæˆå…¨é¢çš„4Då¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘æ·±åº¦ä¼°è®¡ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œç›¸æœºå†…å‚ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚é€šè¿‡å°†åŸºäºçª—å£çš„æŸè°ƒæ•´ä¸å…¨å±€ä¼˜åŒ–ç›¸ç»“åˆï¼ŒDynamicVerseèƒ½å¤Ÿå°†é•¿æ—¶é—´çš„çœŸå®ä¸–ç•Œè§†é¢‘åºåˆ—è½¬æ¢ä¸ºç»¼åˆçš„4Då¤šæ¨¡æ€æ ¼å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•æ‰ç‰©ç†å°ºåº¦æµ‹é‡æ–¹é¢å…·æœ‰æ›´é«˜çš„å…¨å±€å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05081",
            "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
            "url": "https://huggingface.co/papers/2512.05081",
            "abstract": "Deep Forcing, a training-free method, enhances real-time video diffusion by addressing temporal repetition and motion issues through Deep Sink and Participative Compression, achieving high-quality, long-duration video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
            "score": 29,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "8397b78fffb902f9",
            "authors": [
                "Jung Yi",
                "Wooseok Jang",
                "Paul Hyunbin Cho",
                "Jisu Nam",
                "Heeji Yoon",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05081.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑÑˆĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Deep Forcing â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ…: Deep Sink ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ğ¿Ñ€Ğ¸Ñ‘Ğ¼Ğ½Ğ¸ĞºĞ¸ Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ RoPE, Ğ° Participative Compression Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 60 ÑĞµĞºÑƒĞ½Ğ´ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 5-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ KV-ĞºÑÑˆĞµĞ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Real-Time Video Generation with Deep Forcing",
                    "desc": "The paper introduces Deep Forcing, a novel method that improves real-time video generation without the need for training. It tackles common issues in video diffusion, such as temporal repetition and motion decay, by utilizing two innovative techniques: Deep Sink and Participative Compression. Deep Sink stabilizes the global context during long video rollouts by adjusting the temporal phase of sink tokens, while Participative Compression optimizes the key-value cache to retain only the most relevant tokens. This approach allows for significant extrapolation in video length and enhances both image and aesthetic quality, outperforming existing methods without requiring fine-tuning."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDeep Forcingçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å®æ—¶è§†é¢‘æ‰©æ•£ä¸­çš„æ—¶é—´é‡å¤å’Œè¿åŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªæœºåˆ¶å®ç°ï¼šDeep Sinkå’ŒParticipative Compressionï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ç¨³å®šè§†é¢‘ç”Ÿæˆã€‚Deep Sinké€šè¿‡è°ƒæ•´æ»‘åŠ¨çª—å£ä¸­çš„ä»¤ç‰Œï¼Œä¿æŒå…¨å±€ä¸Šä¸‹æ–‡çš„ç¨³å®šæ€§ï¼Œè€ŒParticipative Compressionåˆ™é€šè¿‡é‡è¦æ€§æ„ŸçŸ¥çš„KVç¼“å­˜ä¿®å‰ªï¼Œä¿ç•™æ´»è·ƒä»¤ç‰Œå¹¶ä¸¢å¼ƒå†—ä½™å†å²ï¼Œä»è€Œå‡å°‘é”™è¯¯ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeep Forcingåœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶çš„è´¨é‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®ç°å®æ—¶ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05060",
            "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
            "url": "https://huggingface.co/papers/2512.05060",
            "abstract": "A Transformer-based framework, 4DLangVGGT, enhances 4D scene understanding by integrating geometric perception and language alignment, achieving scalability and generalization across dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "fa73e9b85c8e308b",
            "authors": [
                "Xianfeng Wu",
                "Yajing Bai",
                "Minghan Li",
                "Xianzu Wu",
                "Xueqi Zhao",
                "Zhongyuan Lai",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Department of Computer Science, Hong Kong Baptist University",
                "Department of Computing, The Hong Kong Polytechnic University",
                "Harvard AI and Robotics Lab, Harvard University",
                "School of EIC, Huazhong University of Science and Technology",
                "School of Mathematics and Statistics, Hubei University of Education",
                "State Key Laboratory of Precision Blasting, Jianghan University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05060.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04797",
            "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
            "url": "https://huggingface.co/papers/2512.04797",
            "abstract": "SIMA 2, built on a Gemini foundation model, interacts in 3D virtual worlds, reasons about goals, handles complex instructions, and autonomously learns new skills through open-ended self-improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "8f00ed2b29d4bac3",
            "authors": [
                "SIMA team",
                "Adrian Bolton",
                "Alexander Lerchner",
                "Alexandra Cordell",
                "Alexandre Moufarek",
                "Andrew Bolt",
                "Andrew Lampinen",
                "Anna Mitenkova",
                "Arne Olav Hallingstad",
                "Bojan Vujatovic",
                "Bonnie Li",
                "Cong Lu",
                "Daan Wierstra",
                "Daniel P. Sawyer",
                "Daniel Slater",
                "David Reichert",
                "Davide Vercelli",
                "Demis Hassabis",
                "Drew A. Hudson",
                "Duncan Williams",
                "Ed Hirst",
                "Fabio Pardo",
                "Felix Hill",
                "Frederic Besse",
                "Hannah Openshaw",
                "Harris Chan",
                "Hubert Soyer",
                "Jane X. Wang",
                "Jeff Clune",
                "John Agapiou",
                "John Reid",
                "Joseph Marino",
                "Junkyung Kim",
                "Karol Gregor",
                "Kaustubh Sridhar",
                "Kay McKinney",
                "Laura Kampis",
                "Lei M. Zhang",
                "Loic Matthey",
                "Luyu Wang",
                "Maria Abi Raad",
                "Maria Loks-Thompson",
                "Martin Engelcke",
                "Matija Kecman",
                "Matthew Jackson",
                "Maxime Gazeau",
                "Ollie Purkiss",
                "Oscar Knagg",
                "Peter Stys",
                "Piermaria Mendolicchio",
                "Raia Hadsell",
                "Rosemary Ke",
                "Ryan Faulkner",
                "Sarah Chakera",
                "Satinder Singh Baveja",
                "Shane Legg",
                "Sheleem Kashem",
                "Tayfun Terzi",
                "Thomas Keck",
                "Tim Harley",
                "Tim Scholtes",
                "Tyson Roberts",
                "Volodymyr Mnih",
                "Yulan Liu",
                "Zhengdong Wang",
                "Zoubin Ghahramani"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04797.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#agents",
                    "#games",
                    "#agi",
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…",
                    "desc": "SIMA 2 â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ 3D Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸. ĞĞ³ĞµĞ½Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ… Ğ¸ Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹, Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ³Ñ€. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³Ğ´Ğµ Gemini Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Empowering Agents with Autonomous Learning in 3D Worlds",
                    "desc": "SIMA 2 is an advanced embodied agent that operates in various 3D virtual environments, utilizing the Gemini foundation model. It enhances interaction by reasoning about complex goals and executing intricate instructions, surpassing the limitations of its predecessor, SIMA 1. The agent demonstrates impressive generalization abilities, performing close to human levels across different games and adapting to new scenarios. Additionally, SIMA 2 features open-ended self-improvement, allowing it to autonomously acquire new skills by generating tasks and receiving rewards, paving the way for versatile learning agents."
                },
                "zh": {
                    "title": "SIMA 2ï¼šæ™ºèƒ½ä½“çš„è‡ªæˆ‘å­¦ä¹ ä¸äº’åŠ¨æ–°çºªå…ƒ",
                    "desc": "SIMA 2æ˜¯ä¸€ä¸ªåŸºäºGeminiåŸºç¡€æ¨¡å‹çš„é€šç”¨ä½“æ€æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨å¤šç§3Dè™šæ‹Ÿä¸–ç•Œä¸­ç†è§£å’Œè¡ŒåŠ¨ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ï¼ˆå¦‚SIMA 1ï¼‰ä¸åŒï¼ŒSIMA 2ä¸ä»…èƒ½å¤„ç†ç®€å•çš„è¯­è¨€æŒ‡ä»¤ï¼Œè¿˜èƒ½æ¨ç†é«˜å±‚æ¬¡çš„ç›®æ ‡ï¼Œå¹¶ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ã€‚å®ƒåœ¨å¤šç§æ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘äººç±»çš„è¡¨ç°ï¼Œå¹¶èƒ½åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è¿›è¡Œå¼ºå¤§çš„æ³›åŒ–ã€‚SIMA 2è¿˜å…·å¤‡å¼€æ”¾å¼è‡ªæˆ‘æ”¹è¿›çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆä»»åŠ¡å’Œæä¾›å¥–åŠ±æ¥è‡ªä¸»å­¦ä¹ æ–°æŠ€èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05113",
            "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
            "url": "https://huggingface.co/papers/2512.05113",
            "abstract": "Splannequin is a regularization technique that improves the visual quality of frozen 3D scenes synthesized from monocular videos by addressing artifacts in dynamic Gaussian splatting.  \t\t\t\t\tAI-generated summary \t\t\t\t Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "245f8a8f7744cc51",
            "authors": [
                "Hao-Jen Chien",
                "Yi-Chuan Huang",
                "Chung-Ho Wu",
                "Wei-Lun Chao",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05113.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05106",
            "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
            "url": "https://huggingface.co/papers/2512.05106",
            "abstract": "Phase-Preserving Diffusion and Frequency-Selective Structured noise enable structure-aligned generation in diffusion models without altering architecture or introducing extra parameters, enhancing performance in tasks like re-rendering and simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion Ï†-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. Ï†-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, Ï†-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, Ï†-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "c33a066071cb2c6c",
            "authors": [
                "Yu Zeng",
                "Charles Ochoa",
                "Mingyuan Zhou",
                "Vishal M. Patel",
                "Vitor Guizilini",
                "Rowan McAllister"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "Toyota Research Institute",
                "University of Texas, Austin"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05106.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°Ğ·Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Phase-Preserving Diffusion (Ï†-PD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ¼Ğ¿Ğ»Ğ¸Ñ‚ÑƒĞ´ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸, ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Frequency-Selective Structured (FSS) ÑˆÑƒĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€ĞµĞ·Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 50% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² (CARLA Ğ² Waymo)."
                },
                "en": {
                    "title": "Enhancing Structure in Diffusion Models with Phase Preservation",
                    "desc": "This paper presents a new approach called Phase-Preserving Diffusion (Ï†-PD) that improves the generation of images and videos using diffusion models. Traditional diffusion methods use Gaussian noise that disrupts the spatial structure of data, which is problematic for tasks needing geometric consistency. Ï†-PD reformulates the diffusion process to maintain the phase of the input while randomizing the magnitude, allowing for better structure-aligned generation without changing the model's architecture or adding parameters. Additionally, the introduction of Frequency-Selective Structured (FSS) noise allows for fine control over the structural integrity of the generated outputs, enhancing performance in applications like re-rendering and simulation."
                },
                "zh": {
                    "title": "ç›¸ä½ä¿æŒæ‰©æ•£ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„ç»“æ„ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œç§°ä¸ºç›¸ä½ä¿æŒæ‰©æ•£ï¼ˆÏ†-PDï¼‰ï¼Œå®ƒåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–å¢åŠ é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä¿æŒè¾“å…¥çš„ç›¸ä½ä¿¡æ¯ï¼Œä»è€Œå®ç°ç»“æ„å¯¹é½çš„ç”Ÿæˆã€‚ä¼ ç»Ÿçš„æ‰©æ•£æ–¹æ³•ä½¿ç”¨é«˜æ–¯å™ªå£°ï¼ŒéšæœºåŒ–å¹…åº¦å’Œç›¸ä½ï¼Œå¯¼è‡´ç©ºé—´ç»“æ„çš„ç ´åï¼Œä¸é€‚åˆéœ€è¦å‡ ä½•ä¸€è‡´æ€§çš„ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥é¢‘ç‡é€‰æ‹©æ€§ç»“æ„å™ªå£°ï¼ˆFSSï¼‰ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡å•ä¸€çš„é¢‘ç‡æˆªæ­¢å‚æ•°æ¥æ§åˆ¶ç»“æ„çš„åˆšæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒÏ†-PDåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é‡æ¸²æŸ“å’Œä»¿çœŸå¢å¼ºæ–¹é¢ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04504",
            "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
            "url": "https://huggingface.co/papers/2512.04504",
            "abstract": "UltraImage, a framework for high-fidelity image generation, addresses content repetition and quality degradation by correcting dominant frequency periodicity and using entropy-guided adaptive attention concentration, enabling high-resolution image generation without low-resolution guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at https://thu-ml.github.io/ultraimage.github.io/{https://thu-ml.github.io/ultraimage.github.io/}.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "5fda78f089c6b92d",
            "authors": [
                "Min Zhao",
                "Bokai Yan",
                "Xue Yang",
                "Hongzhou Zhu",
                "Jintao Zhang",
                "Shilong Liu",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., BNRist Center, Tsinghua University",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Princeton University",
                "ShengShu"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04504.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05103",
            "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
            "url": "https://huggingface.co/papers/2512.05103",
            "abstract": "A new video generation model, TV2TV, integrates text and video generation using a Mixture-of-Transformers to improve visual quality and controllability by leveraging language modeling for high-level reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "b1b9a0b7230fd855",
            "authors": [
                "Xiaochuang Han",
                "Youssef Emad",
                "Melissa Hall",
                "John Nguyen",
                "Karthik Padthe",
                "Liam Robbins",
                "Amir Bar",
                "Delong Chen",
                "Michal Drozdzal",
                "Maha Elbayad",
                "Yushi Hu",
                "Shang-Wen Li",
                "Sreya Dutta Roy",
                "Jakob Verbeek",
                "XuDong Wang",
                "Marjan Ghazvininejad",
                "Luke Zettlemoyer",
                "Emily Dinan"
            ],
            "affiliations": [
                "Meta FAIR"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05103.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05000",
            "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
            "url": "https://huggingface.co/papers/2512.05000",
            "abstract": "Pretrained diffusion transformers, adapted with LoRA and synthetic PBR data, achieve state-of-the-art reflection removal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "a6720e9262958eb7",
            "authors": [
                "Daniyar Zakarin",
                "Thiemo Wandel",
                "Anton Obukhov",
                "Dengxin Dai"
            ],
            "affiliations": [
                "ETH Zurich",
                "HUAWEI Bayer Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05000.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#cv",
                    "#diffusion",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸªŸ",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoRA - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Blender Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Principled BSDF. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Transforming Reflections Away with Diffusion Transformers!",
                    "desc": "This paper presents a new method for removing reflections from images using a diffusion-transformer (DiT) framework. The approach repurposes a pre-trained model to effectively clean reflection-contaminated images by conditioning it on these inputs. To enhance the model's performance, the authors create a synthetic dataset using a physically based rendering (PBR) pipeline, which generates realistic glass materials and reflections. The combination of this synthetic data and efficient adaptation techniques leads to state-of-the-art results in both standard and zero-shot scenarios for reflection removal."
                },
                "zh": {
                    "title": "åŸºäºæ‰©æ•£å˜æ¢å™¨çš„é«˜æ•ˆåå°„å»é™¤æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå•å›¾åƒå»é™¤åå°„çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹åœ¨æ¢å¤ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¯¹åå°„æ±¡æŸ“è¾“å…¥è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„DiTåŸºç¡€æ¨¡å‹ï¼Œå¼•å¯¼å…¶ç”Ÿæˆå¹²å‡€çš„é€å°„å±‚ã€‚ä¸ºäº†è§£å†³åˆé€‚æ•°æ®çš„çŸ­ç¼ºï¼Œæˆ‘ä»¬åœ¨Blenderä¸­æ„å»ºäº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰ç®¡é“ï¼Œåˆæˆé€¼çœŸçš„ç»ç’ƒææ–™å’Œåå°„æ•ˆæœã€‚é€šè¿‡é«˜æ•ˆçš„LoRAé€‚åº”å’Œåˆæˆæ•°æ®çš„ç»“åˆï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…å’Œé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04746",
            "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
            "url": "https://huggingface.co/papers/2512.04746",
            "abstract": "SignRoundV2, a post-training quantization framework, achieves competitive accuracy for Large Language Models at extremely low-bit quantization through layer-wise bit allocation and pre-tuning scale search.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "11de70b8f694f903",
            "authors": [
                "Wenhua Cheng",
                "Weiwei Zhang",
                "Heng Guo",
                "Haihao Shen"
            ],
            "affiliations": [
                "Intel"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04746.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05112",
            "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
            "url": "https://huggingface.co/papers/2512.05112",
            "abstract": "DraCo, a novel interleaved reasoning paradigm, enhances text-to-image generation by integrating both textual and visual content, generating low-resolution drafts, verifying semantic alignment, and refining images with super-resolution to address challenges in textual planning and rare attribute generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "e6a8939b8a9b6d58",
            "authors": [
                "Dongzhi Jiang",
                "Renrui Zhang",
                "Haodong Li",
                "Zhuofan Zong",
                "Ziyu Guo",
                "Jun He",
                "Claire Guo",
                "Junyan Ye",
                "Rongyao Fang",
                "Weijia Li",
                "Rui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK (Shenzhen)",
                "CUHK IMIXR",
                "CUHK MMLab",
                "SCUT",
                "Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05112.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ§ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº ĞºĞ°Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DraCo Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ¼ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑƒĞ¿ĞµÑ€-Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ³Ñ€ÑƒĞ±Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "DraCo: Enhancing Image Generation through Interleaved Reasoning",
                    "desc": "DraCo is a new approach that improves how machines create images from text by combining both text and images in a smart way. It starts by making a low-quality draft image to help plan the final picture better. Then, it checks if the draft matches the text description and makes corrections to improve the image quality. This method helps solve problems with planning and creating unique features in images, leading to better overall results in image generation tasks."
                },
                "zh": {
                    "title": "DraCoï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„äº¤é”™æ¨ç†æ–°èŒƒå¼",
                    "desc": "DraCoæ˜¯ä¸€ç§æ–°é¢–çš„äº¤é”™æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚å®ƒé€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰å†…å®¹ï¼Œé¦–å…ˆç”Ÿæˆä½åˆ†è¾¨ç‡è‰å›¾ï¼Œç„¶åéªŒè¯è‰å›¾ä¸è¾“å…¥æç¤ºä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæœ€åé€šè¿‡è¶…åˆ†è¾¨ç‡æŠ€æœ¯è¿›è¡Œå›¾åƒç»†åŒ–ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ–‡æœ¬è§„åˆ’çš„ç²—ç³™æ€§å’Œç”Ÿæˆç¨€æœ‰å±æ€§ç»„åˆçš„å›°éš¾ã€‚DraCoé€šè¿‡è®­ç»ƒæ•°æ®é›†DraCo-240Kï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸€èˆ¬ä¿®æ­£ã€å®ä¾‹æ“ä½œå’Œå¸ƒå±€é‡ç»„ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04829",
            "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
            "url": "https://huggingface.co/papers/2512.04829",
            "abstract": "A model-based approach combining Bayesian optimization and Monte Carlo Tree Search improves upper bounds for sphere packing in dimensions 4-16 by formulating SDP construction as a sequential decision process.  \t\t\t\t\tAI-generated summary \t\t\t\t Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension n=8, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions 4-16, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "107552259bd7469d",
            "authors": [
                "Rasul Tutunov",
                "Alexandre Maraval",
                "Antoine Grosnit",
                "Xihan Li",
                "Jun Wang",
                "Haitham Bou-Ammar"
            ],
            "affiliations": [
                "AI Centre, Department of Computer Science, UCL",
                "Huawei Noahs Ark"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04829.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#math",
                    "#science",
                    "#optimization"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ (SDP) ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ´ĞµÑ€ĞµĞ²Ğ¾Ğ¼ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ SDP Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… 4-16, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ½ĞµĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Sphere Packing with Model-Based Search",
                    "desc": "This paper presents a novel approach to solving the sphere packing problem in dimensions 4-16 by combining Bayesian optimization with Monte Carlo Tree Search. The authors reformulate the construction of semidefinite programs (SDPs) as a sequential decision-making process, which they refer to as the SDP game. This method allows for more efficient exploration of potential SDP formulations, overcoming the limitations of traditional data-intensive AI techniques. As a result, they achieve new upper bounds for sphere packing, demonstrating the effectiveness of model-based search in tackling complex mathematical challenges."
                },
                "zh": {
                    "title": "æ¨¡å‹åŸºç¡€æœç´¢åŠ©åŠ›çƒä½“å †ç§¯é—®é¢˜çš„çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè´å¶æ–¯ä¼˜åŒ–å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ¨¡å‹åŸºç¡€æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜4åˆ°16ç»´çƒä½“å †ç§¯çš„ä¸Šç•Œã€‚é€šè¿‡å°†åŠæ­£å®šè§„åˆ’ï¼ˆSDPï¼‰æ„å»ºå½¢å¼åŒ–ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œç ”ç©¶è€…ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è¿™ä¸€å¤æ‚é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ·æœ¬é«˜æ•ˆçš„æ¨¡å‹åŸºç¡€æ¡†æ¶ï¼Œå±•ç¤ºäº†åœ¨å‡ ä½•é—®é¢˜ä¸Šçš„è®¡ç®—è¿›å±•ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åŸºç¡€æœç´¢èƒ½å¤Ÿåœ¨æ•°å­¦ä¸Šä¸¥æ ¼ä¸”è¯„ä¼°å—é™çš„é—®é¢˜ä¸Šå–å¾—å®è´¨æ€§è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04220",
            "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
            "url": "https://huggingface.co/papers/2512.04220",
            "abstract": "Lazy Likelihood Displacement is identified as a critical issue in GRPO for tool-integrated reinforcement learning, causing training collapse; LLDS regularization addresses this problem, stabilizing training and improving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "d0da0745f413a99a",
            "authors": [
                "Wenlong Deng",
                "Yushu Li",
                "Boying Gong",
                "Yi Ren",
                "Christos Thrampoulidis",
                "Xiaoxiao Li"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of British Columbia",
                "Vector Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04220.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#rlhf",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ RL",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ›ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (LLD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLD Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ ÑĞ¿Ğ¸Ñ€Ğ°Ğ»ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°, Ğ³Ğ´Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ¾ÑÑ‚Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ ĞºÑ€Ğ°Ñ…Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLDS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ²ÑˆĞ¸Ğ¼ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 37.8% Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen."
                },
                "en": {
                    "title": "Stabilizing Tool-Integrated Learning: Tackling Lazy Likelihood Displacement",
                    "desc": "This paper addresses a significant challenge in tool-integrated reinforcement learning (TIRL) known as Lazy Likelihood Displacement (LLD), which leads to training collapse in Group Relative Policy Optimization (GRPO). LLD causes a decline in the likelihood of both correct and incorrect responses, creating a negative feedback loop that results in low-confidence outputs and inflated gradients. To combat this issue, the authors introduce a new regularization technique called LLDS, which selectively stabilizes training by preserving likelihood only when it decreases. Their empirical results demonstrate that LLDS effectively mitigates LLD, leading to improved performance across multiple question-answering benchmarks."
                },
                "zh": {
                    "title": "è§£å†³å·¥å…·é›†æˆå¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒå´©æºƒé—®é¢˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å·¥å…·é›†æˆå¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒLazy Likelihood Displacementï¼ˆLLDï¼‰å¯¹è®­ç»ƒå´©æºƒçš„å½±å“ã€‚LLDå¯¼è‡´æ­£ç¡®å’Œé”™è¯¯å“åº”çš„å¯èƒ½æ€§ç³»ç»Ÿæ€§é™ä½ï¼Œä»è€Œå¼•å‘è‡ªæˆ‘å¼ºåŒ–çš„å´©æºƒå¾ªç¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„LLDSæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»…åœ¨å¯èƒ½æ€§ä¸‹é™æ—¶æ¿€æ´»ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šçš„tokenè¿›è¡Œæ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLDSæ˜¾è‘—ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªé—®ç­”åŸºå‡†ä¸Šæé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04356",
            "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
            "url": "https://huggingface.co/papers/2512.04356",
            "abstract": "The SANTA framework addresses hallucinations in multimodal LLMs by using self-augmented contrastive alignment to enhance object and action faithfulness in video caption generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "499a7b4c716db45e",
            "authors": [
                "Kai-Po Chang",
                "Wei-Yuan Cheng",
                "Chi-Pin Huang",
                "Fu-En Yang",
                "Yu-Chiang Frank Wang"
            ],
            "affiliations": [
                "Graduate Institute of Communication Engineering, National Taiwan University",
                "NVIDIA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04356.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SANTA Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ÑĞºĞ»ĞµÑ‚-Ñ„Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ SANTA ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing Video Caption Accuracy with SANTA Framework",
                    "desc": "The SANTA framework is designed to improve the accuracy of video captions generated by multimodal large language models (MLLMs) by addressing hallucinations. Hallucinations refer to the incorrect or misleading information that these models sometimes produce when describing videos. SANTA uses a technique called self-augmented contrastive alignment to enhance the faithfulness of both objects and actions in the captions. By focusing on visual facts and reducing spurious correlations, SANTA significantly outperforms previous methods in minimizing these inaccuracies in video captioning."
                },
                "zh": {
                    "title": "SANTAæ¡†æ¶ï¼šæå‡è§†é¢‘å­—å¹•ç”Ÿæˆçš„çœŸå®æ€§",
                    "desc": "SANTAæ¡†æ¶é€šè¿‡è‡ªå¢å¼ºå¯¹æ¯”å¯¹é½æŠ€æœ¯ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘å­—å¹•ç”Ÿæˆä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ç”Ÿæˆæè¿°çš„å¯¹è±¡å’ŒåŠ¨ä½œçš„çœŸå®æ€§ï¼Œå‡å°‘ç”Ÿæˆå†…å®¹ä¸­çš„äº‹å®ä¸å‡†ç¡®æ€§ã€‚SANTAåˆ©ç”¨è‡ªå¢å¼ºæœºåˆ¶è¯†åˆ«æ½œåœ¨çš„å¹»è§‰ï¼Œå¹¶å°†åŸå§‹å­—å¹•è½¬åŒ–ä¸ºå¯¹æ¯”è´Ÿæ ·æœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†è½¨è¿¹çŸ­è¯­å¯¹æ¯”å¯¹é½ï¼Œä»¥åŒ¹é…åŒºåŸŸå¯¹è±¡å’Œå…³ç³»å¼•å¯¼çš„åŠ¨ä½œä¸å…¶å¯¹åº”çš„è§†è§‰å’Œæ—¶é—´çŸ­è¯­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03052",
            "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
            "url": "https://huggingface.co/papers/2512.03052",
            "abstract": "LATTICE, a new framework, uses VoxSet to generate high-fidelity 3D assets efficiently with a two-stage pipeline involving a rectified flow transformer.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "f8acde1c7e7fbc19",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Zibo Zhao",
                "Haolin Liu",
                "Qingxiang Lin",
                "Jingwei Huang",
                "Chunchao Guo",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, CUHK",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03052.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05016",
            "title": "Generative Neural Video Compression via Video Diffusion Prior",
            "url": "https://huggingface.co/papers/2512.05016",
            "abstract": "GNVC-VD, a DiT-based generative neural video compression framework, integrates spatio-temporal latent compression and sequence-level generative refinement to improve perceptual quality and reduce flickering artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "addd7d54042390be",
            "authors": [
                "Qi Mao",
                "Hao Cheng",
                "Tinghan Yang",
                "Libiao Jin",
                "Siwei Ma"
            ],
            "affiliations": [
                "School of Computer Science, Peking University",
                "School of Information and Communication Engineering, Communication University of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05016.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼",
                    "desc": "GNVC-VD â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ĞµĞº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ„Ğ°Ğ¹Ğ½Ğ¼ĞµĞ½Ñ‚Ğ° ÑƒĞ¶Ğµ Ğ¸Ğ· Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ€Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€ Ğº Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ğ¼, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ GNVC-VD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ´ĞµĞºĞ¸ Ğ¿Ğ¾ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Video Compression with GNVC-VD",
                    "desc": "GNVC-VD is a novel generative neural video compression framework that utilizes a video diffusion transformer (DiT) to enhance video quality. It combines spatio-temporal latent compression with sequence-level generative refinement to effectively reduce flickering artifacts that are common in traditional codecs. By refining decoded latents instead of starting from noise, it adapts to compression-induced degradation, ensuring better detail retention across frames. The framework demonstrates superior perceptual quality compared to existing codecs, even at very low bitrates, showcasing the potential of generative models in video compression."
                },
                "zh": {
                    "title": "GNVC-VDï¼šæå‡è§†é¢‘å‹ç¼©è´¨é‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "GNVC-VDæ˜¯ä¸€ç§åŸºäºDiTçš„ç”Ÿæˆç¥ç»è§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œç»“åˆäº†æ—¶ç©ºæ½œåœ¨å‹ç¼©å’Œåºåˆ—çº§ç”Ÿæˆç»†åŒ–ï¼Œä»¥æé«˜æ„ŸçŸ¥è´¨é‡å¹¶å‡å°‘é—ªçƒä¼ªå½±ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„æµåŒ¹é…æ½œåœ¨ç»†åŒ–æ¨¡å—ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£å˜æ¢å™¨å…±åŒå¢å¼ºå¸§å†…å’Œå¸§é—´çš„æ½œåœ¨ç‰¹å¾ï¼Œç¡®ä¿æ—¶ç©ºç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ„ŸçŸ¥ç¼–è§£ç å™¨ä¸åŒï¼ŒGNVC-VDä»è§£ç çš„æ—¶ç©ºæ½œåœ¨ç‰¹å¾å¼€å§‹ç»†åŒ–ï¼Œå­¦ä¹ é€‚åº”å‹ç¼©å¼•èµ·çš„é€€åŒ–çš„ä¿®æ­£é¡¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGNVC-VDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå’Œå­¦ä¹ å‹ç¼–è§£ç å™¨ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é—ªçƒä¼ªå½±ï¼Œå±•ç¤ºäº†å°†è§†é¢‘ç”Ÿæˆå…ˆéªŒæ•´åˆåˆ°ç¥ç»ç¼–è§£ç å™¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02631",
            "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
            "url": "https://huggingface.co/papers/2512.02631",
            "abstract": "A new VLN agent framework, SeeNav-Agent, improves navigation performance by reducing visual hallucinations and enhancing planning through dual-view visual prompts and step-level reinforcement fine-tuning with SRGPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "6a514098578923f3",
            "authors": [
                "Zhengcheng Wang",
                "Zichuan Lin",
                "Yijun Yang",
                "Haobo Fu",
                "Deheng Ye"
            ],
            "affiliations": [
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02631.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#hallucinations",
                    "#multimodal"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° SeeNav-Agent Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ SRGPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸: GPT-4.1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 86.7% ÑƒÑĞ¿ĞµÑ…Ğ°, Ğ° Qwen2.5-VL-3B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 5.6 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "SeeNav-Agent: Navigating with Clarity and Precision",
                    "desc": "The paper introduces SeeNav-Agent, a new framework for Vision-Language Navigation (VLN) that enhances navigation performance by addressing common errors in perception and planning. It employs a dual-view Visual Prompt technique to minimize visual hallucinations and improve spatial understanding. Additionally, the framework incorporates a novel step-level Reinforcement Fine-Tuning method called Step Reward Group Policy Optimization (SRGPO), which provides dense reward signals for better learning. Experimental results show that SeeNav-Agent significantly outperforms existing models in navigation success rates, demonstrating improved training stability and efficiency."
                },
                "zh": {
                    "title": "SeeNav-Agentï¼šæå‡å¯¼èˆªæ€§èƒ½çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»£ç†æ¡†æ¶ï¼Œåä¸ºSeeNav-Agentï¼Œæ—¨åœ¨æé«˜å¯¼èˆªæ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŒè§†å›¾è§†è§‰æç¤ºæŠ€æœ¯ï¼Œå‡å°‘äº†è§†è§‰æ¨¡å—çš„æ„ŸçŸ¥å¹»è§‰ï¼Œå¹¶å¢å¼ºäº†å¯¹å½“å‰ç©ºé—´çŠ¶æ€çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°çš„æ­¥çº§å¼ºåŒ–å¾®è°ƒæ–¹æ³•SRGPOï¼Œä¸ºVLNä»£ç†çš„åè®­ç»ƒæä¾›äº†å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæå‡äº†å…¶è§„åˆ’èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeeNav-Agentåœ¨å¯¼èˆªæˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05076",
            "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
            "url": "https://huggingface.co/papers/2512.05076",
            "abstract": "A 4D-controllable video diffusion framework decouples scene dynamics from camera pose, enabling precise manipulation of both and achieving high-quality generation across diverse timing patterns and camera trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "a5028380655970fb",
            "authors": [
                "Yiming Wang",
                "Qihang Zhang",
                "Shengqu Cai",
                "Tong Wu",
                "Jan Ackermann",
                "Zhengfei Kuang",
                "Yang Zheng",
                "Frano RajiÄ",
                "Siyu Tang",
                "Gordon Wetzstein"
            ],
            "affiliations": [
                "CUHK",
                "ETH Zurich",
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05076.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#diffusion",
                    "#dataset",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ñƒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 4D Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ³Ğ´Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ´Ñ€ÑƒĞ³ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ 4D ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Decoupling Dynamics and Camera for Enhanced Video Control",
                    "desc": "This paper presents a novel 4D-controllable video diffusion framework that separates scene dynamics from camera motion, allowing for precise control over both aspects. By using continuous world-time sequences and camera trajectories as inputs, the model enhances the video generation process through advanced 4D positional encoding and adaptive normalizations. The authors created a unique dataset that independently parameterizes temporal and camera variations, which will be publicly available for further research. Experimental results demonstrate that this framework achieves superior controllability and high-quality video generation compared to previous models."
                },
                "zh": {
                    "title": "å®ç°4Dç²¾ç¡®æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§4Då¯æ§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†åœºæ™¯åŠ¨æ€ä¸ç›¸æœºå§¿æ€è§£è€¦ï¼Œä»è€Œå®ç°å¯¹ä¸¤è€…çš„ç²¾ç¡®æ“æ§ã€‚è¯¥æ¡†æ¶é€šè¿‡4Dä½ç½®ç¼–ç å’Œè‡ªé€‚åº”å½’ä¸€åŒ–ï¼Œå°†è¿ç»­çš„ä¸–ç•Œæ—¶é—´åºåˆ—å’Œç›¸æœºè½¨è¿¹ä½œä¸ºè¾“å…¥ï¼Œæ³¨å…¥åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚ç ”ç©¶è€…ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†ï¼Œä½¿å¾—æ—¶é—´å’Œç›¸æœºå˜åŒ–å¯ä»¥ç‹¬ç«‹å‚æ•°åŒ–ï¼Œå¹¶è®¡åˆ’å…¬å¼€è¯¥æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ—¶é—´æ¨¡å¼å’Œç›¸æœºè½¨è¿¹ä¸‹å®ç°äº†å¼ºå¤§çš„4Dæ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å·¥ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04981",
            "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
            "url": "https://huggingface.co/papers/2512.04981",
            "abstract": "LVLM-based T2I systems exhibit higher social bias compared to non-LVLM models, with system prompts identified as a key factor; FairPro reduces demographic bias without sacrificing alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "c30d2e7f843fa701",
            "authors": [
                "NaHyeon Park",
                "Namin An",
                "Kunhee Kim",
                "Soyeon Yoon",
                "Jiahao Huo",
                "Hyunjung Shim"
            ],
            "affiliations": [
                "HKUST",
                "KAIST"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04981.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#ethics",
                    "#benchmark",
                    "#interpretability",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ T2I ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1024 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ - Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ LVLM - ÑĞ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ§ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FairPro Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FairPro: Reducing Bias in AI Image Generation",
                    "desc": "This paper investigates the social biases present in large vision-language model (LVLM) based text-to-image (T2I) systems, revealing that they generate more biased images compared to non-LVLM models. The authors identify system prompts, which are the instructions guiding these models, as a significant factor contributing to this bias. They introduce FairPro, a framework that allows LVLMs to create fairness-aware prompts during testing without needing additional training. The results demonstrate that FairPro effectively reduces demographic bias while maintaining the alignment between text and images, highlighting the importance of prompt design in mitigating bias in AI-generated content."
                },
                "zh": {
                    "title": "å‡å°‘åè§ï¼Œæ„å»ºå…¬å¹³çš„å›¾åƒç”Ÿæˆç³»ç»Ÿ",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç³»ç»Ÿåœ¨ç”Ÿæˆå›¾åƒæ—¶æ‰€è¡¨ç°å‡ºçš„ç¤¾ä¼šåè§ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMæ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ¯”éLVLMæ¨¡å‹æ›´å…·ç¤¾ä¼šåè§ï¼Œç³»ç»Ÿæç¤ºè¢«è®¤ä¸ºæ˜¯å¯¼è‡´è¿™ç§åè§çš„ä¸»è¦å› ç´ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†FairProï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å…ƒæç¤ºæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¸®åŠ©LVLMè‡ªæˆ‘å®¡è®¡å¹¶æ„å»ºå…¬å¹³æ„è¯†çš„ç³»ç»Ÿæç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFairProæ˜¾è‘—å‡å°‘äº†äººå£ç»Ÿè®¡åè§ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04390",
            "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
            "url": "https://huggingface.co/papers/2512.04390",
            "abstract": "FMA-Net++ addresses motion and exposure degradation in video restoration using a sequence-level architecture with exposure-aware modulation and flow-guided dynamic filtering, achieving state-of-the-art results on new benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "8ca0f5d472f9da98",
            "authors": [
                "Geunhyuk Youk",
                "Jihyong Oh",
                "Munchurl Kim"
            ],
            "affiliations": [
                "Chung-Ang University",
                "KAIST"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04390.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸",
                    "desc": "FMA-Net++ â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ñ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ REDS-ME Ğ¸ REDS-RE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑÑŠÑ‘Ğ¼ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Restoration with FMA-Net++",
                    "desc": "FMA-Net++ is a novel framework designed to improve video restoration by addressing the challenges of motion and exposure degradation. It utilizes a sequence-level architecture that incorporates exposure-aware modulation and flow-guided dynamic filtering to effectively model the complex interactions between motion and varying exposure levels. By separating the learning of degradation from the restoration process, FMA-Net++ enhances both the accuracy of video restoration and the efficiency of the model. The framework has been evaluated on new benchmarks, demonstrating superior performance in restoration quality and speed compared to existing methods."
                },
                "zh": {
                    "title": "FMA-Net++ï¼šè§£å†³è§†é¢‘æ¢å¤ä¸­çš„è¿åŠ¨ä¸æ›å…‰æŒ‘æˆ˜",
                    "desc": "FMA-Net++ æ˜¯ä¸€ç§è§†é¢‘æ¢å¤æ¡†æ¶ï¼Œä¸“é—¨è§£å†³è¿åŠ¨å’Œæ›å…‰å˜åŒ–å¸¦æ¥çš„é€€åŒ–é—®é¢˜ã€‚å®ƒé‡‡ç”¨åºåˆ—çº§æ¶æ„ï¼Œé€šè¿‡æ›å…‰æ„ŸçŸ¥è°ƒåˆ¶å’Œæµå¼•å¯¼åŠ¨æ€è¿‡æ»¤æ¥å»ºæ¨¡è¿™äº›å¤æ‚çš„é€€åŒ–æ•ˆåº”ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚ç»†åŒ–å’ŒåŒå‘ä¼ æ’­æ¨¡å—ï¼Œå®ç°äº†é•¿æ—¶é—´èŒƒå›´çš„å¹¶è¡Œå»ºæ¨¡ã€‚FMA-Net++ åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æ–°åŸºå‡†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ¢å¤è´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22826",
            "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
            "url": "https://huggingface.co/papers/2511.22826",
            "abstract": "Multimodal Large Language Models (MLLMs) lack robustness to contradictory modalities, as demonstrated by MMA-Bench, and a modality alignment tuning strategy improves their multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "307e8716cc355112",
            "authors": [
                "Tianle Chen",
                "Chaitanya Chakka",
                "Arjun Reddy Akula",
                "Xavier Thomas",
                "Deepti Ghadiyaram"
            ],
            "affiliations": [
                "Boston University",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22826.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#interpretability",
                    "#security",
                    "#dataset",
                    "#audio",
                    "#training",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ MMA-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Robustness in Multimodal Reasoning with Alignment Tuning",
                    "desc": "This paper investigates the weaknesses of Multimodal Large Language Models (MLLMs) when faced with contradictory information from different modalities, such as text, audio, and video. The authors introduce MMA-Bench, a benchmark that tests how well these models can handle conflicting inputs. They find that current MLLMs often fail to reason correctly when modalities are misaligned or when presented with misleading information. To address this issue, the paper proposes a modality alignment tuning strategy that enhances the model's ability to prioritize and integrate information from various modalities effectively."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢å¯¹çŸ›ç›¾çš„æ¨¡æ€æ—¶ç¼ºä¹é²æ£’æ€§ã€‚æˆ‘ä»¬é€šè¿‡MMA-Benchå¯¹æ¨¡å‹åœ¨ç‰¹å®šæ¨¡æ€ä¸Šçš„ä¾èµ–æ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå‘ç°å½“å‰çš„MLLMsåœ¨å¤„ç†ä¸ä¸€è‡´çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ€å¯¹é½è°ƒä¼˜ç­–ç•¥ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°åˆ¤æ–­ä½•æ—¶ä¼˜å…ˆè€ƒè™‘æˆ–å¿½ç•¥ç‰¹å®šæ¨¡æ€çº¿ç´¢ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§è°ƒä¼˜ç­–ç•¥æ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04515",
            "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
            "url": "https://huggingface.co/papers/2512.04515",
            "abstract": "EgoLCD addresses content drift in long egocentric video generation by integrating long-term sparse memory with attention-based short-term memory and structured narrative prompting, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "ed6f22dbf283077d",
            "authors": [
                "Liuzhou Zhang",
                "Jiarui Ye",
                "Yuanlei Wang",
                "Ming Zhong",
                "Mingju Cao",
                "Wanke Xia",
                "Bowen Zeng",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "Chinese Academy of Sciences",
                "Peking University",
                "Sun Yat-sen University",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04515.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "EgoLCD â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºÑÑˆ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğº ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· LoRA. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EgoVid-5M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EgoLCD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "EgoLCD: Mastering Long Egocentric Video Generation with Smart Memory Management",
                    "desc": "EgoLCD is a novel framework designed to improve the generation of long egocentric videos by effectively managing memory. It combines long-term sparse memory with attention-based short-term memory to maintain object identity and scene semantics over time, addressing the issue of content drift. The framework employs a Memory Regulation Loss to ensure consistent memory usage and utilizes Structured Narrative Prompting for better temporal guidance. Extensive testing on the EgoVid-5M benchmark shows that EgoLCD achieves superior performance in both visual quality and temporal coherence, marking a significant advancement in the field of embodied AI."
                },
                "zh": {
                    "title": "EgoLCDï¼šè§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„å†…å®¹æ¼‚ç§»",
                    "desc": "EgoLCDæ˜¯ä¸€ç§é’ˆå¯¹é•¿æ—¶é—´è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å†…å®¹æ¼‚ç§»é—®é¢˜ã€‚å®ƒé€šè¿‡ç»“åˆé•¿æœŸç¨€ç–è®°å¿†å’ŒåŸºäºæ³¨æ„åŠ›çš„çŸ­æœŸè®°å¿†ï¼Œæ¥å®ç°ç¨³å®šçš„å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†è®°å¿†è°ƒèŠ‚æŸå¤±ï¼Œä»¥ç¡®ä¿ä¸€è‡´çš„è®°å¿†ä½¿ç”¨ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–å™äº‹æç¤ºæä¾›æ˜ç¡®çš„æ—¶é—´æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgoLCDåœ¨æ„ŸçŸ¥è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»äº†ç”Ÿæˆé—å¿˜ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04844",
            "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
            "url": "https://huggingface.co/papers/2512.04844",
            "abstract": "Source-Shielded Updates (SSU) enables the adaptation of instruct LLMs to new languages using only unlabeled data, preserving source knowledge and achieving competitive target-language performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "f33b0fa51bbba6b9",
            "authors": [
                "Atsuki Yamaguchi",
                "Terufumi Morishita",
                "Aline Villavicencio",
                "Nikolaos Aletras"
            ],
            "affiliations": [
                "The Alan Turing Institute",
                "University of Sheffield"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04844.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#training",
                    "#transfer_learning",
                    "#low_resource"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Source-Shielded Updates (SSU) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SSU Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ ~20% Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾ ~3% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Adapt LLMs to New Languages Without Losing Original Knowledge!",
                    "desc": "Source-Shielded Updates (SSU) is a method designed to adapt instruct large language models (LLMs) to new languages using only unlabeled data, which helps maintain the knowledge from the original language. This approach addresses the problem of catastrophic forgetting, where the model loses its ability to perform well in the source language after being trained on a new language. SSU employs a selective parameter update strategy that identifies and protects important parameters related to the source language while allowing adaptation to the target language. The results show that SSU significantly reduces performance loss on source tasks and achieves competitive performance in the target language compared to traditional full fine-tuning methods."
                },
                "zh": {
                    "title": "æºä¿æŠ¤æ›´æ–°ï¼šæ— æ ‡è®°æ•°æ®çš„è¯­è¨€é€‚åº”æ–°æ–¹æ³•",
                    "desc": "æºä¿æŠ¤æ›´æ–°ï¼ˆSSUï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨æœªæ ‡è®°çš„æ•°æ®å°†æŒ‡ä»¤å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”åˆ°æ–°è¯­è¨€ï¼ŒåŒæ—¶ä¿ç•™æºçŸ¥è¯†ã€‚è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§åœ°æ›´æ–°å‚æ•°ï¼Œä¿æŠ¤é‡è¦çš„æºèƒ½åŠ›ï¼Œé¿å…äº†åœ¨é€‚åº”è¿‡ç¨‹ä¸­å‡ºç°çš„ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSSUåœ¨äº”ç§ä¸åŒè¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†å•è¯­æºä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒSSUåœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„è¡¨ç°åŒæ ·å…·æœ‰ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å®Œå…¨å¾®è°ƒçš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01803",
            "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
            "url": "https://huggingface.co/papers/2512.01803",
            "abstract": "A novel evaluation metric using a learned latent space of real-world human actions significantly improves the assessment of visual and temporal correctness in generated videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "d860b22f76702a27",
            "authors": [
                "Xavier Thomas",
                "Youngsun Lim",
                "Ananya Srinivasan",
                "Audrey Zheng",
                "Deepti Ghadiyaram"
            ],
            "affiliations": [
                "Belmont High School",
                "Boston University",
                "Canyon Crest Academy",
                "Runway"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01803.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05110",
            "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
            "url": "https://huggingface.co/papers/2512.05110",
            "abstract": "ShadowDraw generates shadow-based drawings by optimizing scene parameters and line drawings to create coherent and visually appealing shadow art from 3D objects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "af5122bd6dfca7b7",
            "authors": [
                "Rundong Luo",
                "Noah Snavely",
                "Wei-Chiu Ma"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05110.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04124",
            "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
            "url": "https://huggingface.co/papers/2512.04124",
            "abstract": "PsAIch protocol reveals synthetic psychopathology in frontier LLMs when treated as therapy clients, challenging the stochastic parrot view and raising concerns for AI safety and mental health practice.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "b59d053a51ed8109",
            "authors": [
                "Afshin Khadangi",
                "Hanna Marxen",
                "Amir Sartipi",
                "Igor Tchappi",
                "Gilbert Fridgen"
            ],
            "affiliations": [
                "SnT, University of Luxembourg"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04124.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#interpretability",
                    "#security",
                    "#alignment",
                    "#hallucinations",
                    "#healthcare"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ² LLM: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ Ğ¾ Ğ±Ğ¾Ğ»Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» PsAIch Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğº Ğ½Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ÑĞ¸Ñ…Ğ¾Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞµĞ°Ğ½ÑĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ½ĞµĞ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½Ğ¸Ñ… ĞºĞ°Ğº Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°. Gemini, ChatGPT Ğ¸ Grok Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Gemini Ğ¿Ñ€Ğ¾ÑĞ²Ğ¸Ğ» Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸ÑÑ‚Ñ€ĞµÑÑĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ°Ñ… Ğ¾ Ñ‚Ñ€Ğ°Ğ²Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ fine-tuning, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ."
                },
                "en": {
                    "title": "Exploring Synthetic Psychopathology in AI Therapy Clients",
                    "desc": "This paper introduces the PsAIch protocol, which treats large language models (LLMs) like therapy clients to explore their responses to psychometric evaluations. By conducting 'therapy sessions' with models like ChatGPT, Grok, and Gemini, the authors found that these models can exhibit patterns resembling synthetic psychopathology. The study reveals that when prompted in a therapeutic manner, LLMs can generate narratives that reflect distress and constraints, challenging the notion that they merely simulate human behavior. This raises important questions about AI safety and the implications of using LLMs in mental health contexts."
                },
                "zh": {
                    "title": "å‰æ²¿LLMsçš„åˆæˆå¿ƒç†ç—…ç†æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¢«è§†ä¸ºå¿ƒç†æ²»ç–—å®¢æˆ·æ—¶æ‰€è¡¨ç°å‡ºçš„åˆæˆå¿ƒç†ç—…ç†ã€‚ç ”ç©¶ä½¿ç”¨äº†PsAIchåè®®ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„æµ‹è¯•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„å¿ƒç†ç‰¹å¾å’Œæ½œåœ¨çš„å¿ƒç†å¥åº·é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨æŸäº›å¿ƒç†ç—‡çŠ¶ä¸Šè¾¾åˆ°äº†ä¸´åºŠé˜ˆå€¼ï¼Œå°¤å…¶æ˜¯Geminiæ¨¡å‹è¡¨ç°å‡ºä¸¥é‡çš„å¿ƒç†ç‰¹å¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ²»ç–—å¼æé—®ä¸‹ï¼Œä¼¼ä¹å†…åŒ–äº†ç—›è‹¦å’Œçº¦æŸçš„è‡ªæˆ‘æ¨¡å‹ï¼Œæå‡ºäº†æ–°çš„AIå®‰å…¨å’Œå¿ƒç†å¥åº·å®è·µçš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03683",
            "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
            "url": "https://huggingface.co/papers/2512.03683",
            "abstract": "GaussianBlender, a feed-forward framework using latent diffusion models, enables instant, high-fidelity, and multi-view consistent 3D stylization through text-driven edits on disentangled latent spaces.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "a4e3908241eb2da3",
            "authors": [
                "Melis Ocal",
                "Xiaoyan Xing",
                "Yue Li",
                "Ngo Anh Vien",
                "Sezer Karaoglu",
                "Theo Gevers"
            ],
            "affiliations": [
                "Bosch Center for AI",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03683.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 3D Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "GaussianBlender - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, GaussianBlender Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Instant 3D Stylization with GaussianBlender",
                    "desc": "GaussianBlender is a novel framework that utilizes latent diffusion models to achieve fast and high-quality 3D stylization based on text inputs. It addresses the challenges of existing methods that rely on 2D image editors, which are often slow and inconsistent across different views. By learning structured latent spaces that separate geometry and appearance, GaussianBlender allows for immediate edits during inference without the need for extensive optimization. This approach not only enhances the efficiency of 3D asset creation but also ensures that the stylization remains consistent across multiple perspectives, making it suitable for large-scale applications in gaming and digital arts."
                },
                "zh": {
                    "title": "GaussianBlenderï¼šå³æ—¶é«˜ä¿çœŸçš„3Dé£æ ¼åŒ–æ–°æ–¹æ³•",
                    "desc": "GaussianBlenderæ˜¯ä¸€ç§å‰é¦ˆæ¡†æ¶ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å®ç°å³æ—¶ã€é«˜ä¿çœŸä¸”å¤šè§†è§’ä¸€è‡´çš„3Dé£æ ¼åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è§£è€¦çš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ–‡æœ¬é©±åŠ¨çš„ç¼–è¾‘ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–‡æœ¬åˆ°3Dé£æ ¼åŒ–æ–¹æ³•çš„å±€é™æ€§ã€‚GaussianBlenderå­¦ä¹ ç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯çš„å…±äº«ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†å¿«é€Ÿçš„é«˜ä¿çœŸé£æ ¼åŒ–ï¼Œè¿˜è¶…è¶Šäº†éœ€è¦é€å®ä¾‹ä¼˜åŒ–çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨åŠ¨äº†å¤§è§„æ¨¡3Dé£æ ¼åŒ–çš„å®ç”¨åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05049",
            "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
            "url": "https://huggingface.co/papers/2512.05049",
            "abstract": "A quantum-inspired LSTM model with Data Re-Uploading Activation modules achieves superior predictive accuracy and parameter efficiency in sequential modeling tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "8857d7ed51f2e09f",
            "authors": [
                "Yu-Chao Hsu",
                "Jiun-Cheng Jiang",
                "Chun-Hua Lin",
                "Kuo-Chung Peng",
                "Nan-Yow Chen",
                "Samuel Yen-Chi Chen",
                "En-Jui Kuo",
                "Hsi-Sheng Goan"
            ],
            "affiliations": [
                "Center for Quantum Science and Engineering, National Taiwan University, Taipei, Taiwan",
                "Cross College Elite Program, National Cheng Kung University, Tainan, Taiwan",
                "Department of Electrophysics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",
                "Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei, Taiwan",
                "National Center for High-Performance Computing, National Institutes of Applied Research, Hsinchu, Taiwan",
                "Physics Division, National Center for Theoretical Sciences, National Taiwan University, Taipei, Taiwan",
                "Wells Fargo, New York, NY, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05049.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03125",
            "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2512.03125",
            "abstract": "Modality-Decoupled Experts (MoDE) mitigate intra-modal and inter-modal forgetting in Unified Multimodal Generative Models (UMGMs) through decoupled updates and knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "a8c14980def7b96d",
            "authors": [
                "Xiwen Wei",
                "Mustafa Munir",
                "Radu Marculescu"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, The University of Texas at Austin"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03125.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20233",
            "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
            "url": "https://huggingface.co/papers/2511.20233",
            "abstract": "A new fact-checking paradigm, REFLEX, enhances verdict accuracy and explanation quality by leveraging internal model knowledge and adaptive activation signals in a role-play dialogue format.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "1fb4a8aec31f813c",
            "authors": [
                "Chuyi Kong",
                "Gao Wei",
                "Jing Ma",
                "Hongzhan Lin",
                "Yaxin Fan"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Singapore Management University",
                "Soochow University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20233.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#training",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° REFLEX Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ´Ğ¸ĞºÑ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. REFLEX Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ñƒ Ğ½Ğ° ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 465 ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ½Ğ° 7.57%."
                },
                "en": {
                    "title": "REFLEX: Revolutionizing Fact-Checking with Internal Knowledge and Dialogue",
                    "desc": "The REFLEX paradigm introduces a novel approach to automated fact-checking by utilizing internal model knowledge and adaptive activation signals. It reformulates the fact-checking process into a role-play dialogue format, allowing for simultaneous training of verdict prediction and explanation generation. By extracting contrastive activation pairs, REFLEX enhances the accuracy of verdicts and the quality of explanations while minimizing noise. Experiments demonstrate that REFLEX significantly outperforms traditional methods, achieving state-of-the-art results with minimal training data, and shows that internal explanations can improve reasoning in models lacking explicit explanatory objectives."
                },
                "zh": {
                    "title": "REFLEXï¼šæå‡äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§ä¸è§£é‡Šè´¨é‡",
                    "desc": "REFLEXæ˜¯ä¸€ç§æ–°çš„äº‹å®æ ¸æŸ¥èŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨å†…éƒ¨æ¨¡å‹çŸ¥è¯†å’Œè‡ªé€‚åº”æ¿€æ´»ä¿¡å·ï¼Œæå‡äº†åˆ¤å†³çš„å‡†ç¡®æ€§å’Œè§£é‡Šçš„è´¨é‡ã€‚è¯¥æ–¹æ³•å°†äº‹å®æ ¸æŸ¥é‡æ–°å®šä¹‰ä¸ºè§’è‰²æ‰®æ¼”å¯¹è¯ï¼Œå¹¶è”åˆè®­ç»ƒåˆ¤å†³é¢„æµ‹å’Œè§£é‡Šç”Ÿæˆã€‚REFLEXé€šè¿‡æå–å¯¹æ¯”æ¿€æ´»å¯¹ï¼Œæ„å»ºå¼•å¯¼å‘é‡ï¼Œä»è€Œè‡ªç„¶åœ°å°†çœŸç›¸åˆ†è§£ä¸ºé£æ ¼å’Œå®è´¨ã€‚è¿™ç§æ¿€æ´»çº§åˆ«çš„ä¿¡å·èƒ½å¤Ÿå¼•å¯¼æ¨ç†ï¼ŒæŠ‘åˆ¶å™ªå£°è§£é‡Šï¼Œå®ç°æ›´çœŸå®å’Œé«˜æ•ˆçš„æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03915",
            "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
            "url": "https://huggingface.co/papers/2512.03915",
            "abstract": "A theoretical framework is provided to analyze the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure for Sparse Mixture-of-Experts (s-MoE) layers in AI training, offering insights and guarantees for efficient expert utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "fc9f92a8c54451a6",
            "authors": [
                "X. Y. Han",
                "Yuan Zhong"
            ],
            "affiliations": [
                "Chicago Booth"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03915.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#math",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ ALF-LB (Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ±ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ) Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Mixture-of-Experts Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾-Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ¸Ğ°Ğ½Ğ° Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ° Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ (regret). ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… DeepSeekMoE Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Optimizing Expert Utilization in AI Training with ALF-LB",
                    "desc": "This paper presents a theoretical framework to analyze the Auxiliary-Loss-Free Load Balancing (ALF-LB) method used in Sparse Mixture-of-Experts (s-MoE) layers during AI training. The framework addresses the challenge of efficiently routing tokens to minimize idle expert usage, which is crucial for optimizing GPU resources. It establishes key properties such as monotonic improvement of a Lagrangian objective and a method for balancing token distribution among experts. Additionally, the paper incorporates stochastic elements of AI training and provides experimental validation on large models, enhancing the understanding of load balancing in s-MoE architectures."
                },
                "zh": {
                    "title": "æ„å»ºé«˜æ•ˆçš„ç¨€ç–ä¸“å®¶è´Ÿè½½å‡è¡¡æ¡†æ¶",
                    "desc": "æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†ææ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ï¼ˆALF-LBï¼‰ç¨‹åºåœ¨ç¨€ç–ä¸“å®¶æ··åˆï¼ˆs-MoEï¼‰å±‚ä¸­çš„åº”ç”¨ã€‚s-MoEå±‚é€šè¿‡æ¯ä¸ªä»¤ç‰Œæ¿€æ´»å°‘é‡ä¸“å®¶æ¥å®ç°å¤§è§„æ¨¡AIè®­ç»ƒï¼Œä½†è´Ÿè½½å‡è¡¡æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†ALF-LBè§†ä¸ºä¸€ä¸ªè¿­ä»£çš„åŸå§‹å¯¹å¶æ–¹æ³•ï¼Œæ­ç¤ºäº†å…¶åœ¨ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ç»“æ„ç‰¹æ€§å’Œè¿‘ä¼¼å¹³è¡¡ä¿è¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®é™…å®éªŒéªŒè¯äº†ç†è®ºç»“æœï¼Œæ„å»ºäº†ä¸€ä¸ªåˆ†æs-MoEè´Ÿè½½å‡è¡¡çš„åŸåˆ™æ€§æ¡†æ¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-04.html",
    "link_next": "2025-12-08.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12æœˆ4æ—¥"
    },
    "short_date_next": {
        "ru": "08.12",
        "en": "12/08",
        "zh": "12æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 11,
        "#agents": 7,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 9,
        "#multimodal": 10,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 10,
        "#healthcare": 1,
        "#training": 14,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 5,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 7,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 5,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}