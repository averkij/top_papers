
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 8 papers. July 31.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">31 июля</span> | <span id="title-articles-count">8 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-30.html">⬅️ <span id="prev-date">30.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-01.html">➡️ <span id="next-date">01.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'};
        let feedDateNext = {'ru': '01.08', 'en': '08/01', 'zh': '8月1日'};
        let feedDatePrev = {'ru': '30.07', 'en': '07/30', 'zh': '7月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.22827', 'title': 'ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents', 'url': 'https://huggingface.co/papers/2507.22827', 'abstract': 'A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder.', 'score': 43, 'issue_id': 5103, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': 'a09c860f7fa98aea', 'authors': ['Yilei Jiang', 'Yaozhi Zheng', 'Yuxuan Wan', 'Jiaming Han', 'Qunzhong Wang', 'Michael R. Lyu', 'Xiangyu Yue'], 'affiliations': ['2ARISE Lab', 'CUHK 1MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2507.22827.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#training', '#dataset', '#interpretability', '#multimodal', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'Умное превращение дизайна в код с помощью ИИ', 'desc': 'Статья представляет модульную мультиагентную систему для генерации кода пользовательского интерфейса на основе визуального дизайна. Система использует три этапа: распознавание компонентов с помощью мультимодальной модели, планирование иерархической структуры и генерация HTML/CSS кода. Авторы также создали масштабируемый механизм для автоматического создания обучающих данных. Эксперименты показывают, что подход достигает наилучших результатов по точности макета, структурной согласованности и корректности кода.'}, 'en': {'title': 'Transforming UI Designs into Code with Modular Intelligence', 'desc': 'This paper presents a modular multi-agent framework designed to enhance the process of converting user interface (UI) designs into front-end code. It integrates vision-language models to accurately identify UI components, employs hierarchical layout planning to organize these components, and utilizes adaptive prompt-based synthesis for code generation. By breaking down the UI-to-code generation into three distinct stages—grounding, planning, and generation—the framework improves the robustness and interpretability of the output compared to traditional end-to-end methods. The authors also introduce a scalable data engine that generates large datasets of image-code pairs, which are used to fine-tune a vision-language model, resulting in improved performance in layout accuracy and code quality.'}, 'zh': {'title': '模块化多智能体框架提升UI到代码生成', 'desc': '这篇论文提出了一种模块化的多智能体框架，用于将用户界面（UI）设计转化为前端代码。该框架通过整合视觉-语言模型、层次布局规划和自适应提示合成，分为三个可解释的阶段：基础、规划和生成。基础代理使用视觉-语言模型检测和标记UI组件，规划代理构建层次布局，生成代理则通过自适应提示合成生成HTML/CSS代码。实验结果表明，该方法在布局准确性、结构一致性和代码正确性方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.22448', 'title': 'Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance', 'url': 'https://huggingface.co/papers/2507.22448', 'abstract': 'Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.', 'score': 32, 'issue_id': 5102, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '1023abbaabd95fa0', 'authors': ['Jingwei Zuo', 'Maksim Velikanov', 'Ilyas Chahed', 'Younes Belkada', 'Dhia Eddine Rhayem', 'Guillaume Kunsch', 'Hakim Hacid', 'Hamza Yous', 'Brahim Farhat', 'Ibrahim Khadraoui', 'Mugariya Farooq', 'Giulia Campesan', 'Ruxandra Cojocaru', 'Yasser Djilali', 'Shi Hu', 'Iheb Chaabane', 'Puneesh Khanna', 'Mohamed El Amine Seddik', 'Ngoc Dung Huynh', 'Phuc Le Khac', 'Leen AlQadi', 'Billel Mokeddem', 'Mohamed Chami', 'Abdalgader Abubaker', 'Mikhail Lubinets', 'Kacper Piskorski', 'Slim Frikha'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.22448.jpg', 'data': {'categories': ['#small_models', '#training', '#agi', '#architecture', '#science', '#long_context', '#dataset', '#open_source', '#multilingual'], 'emoji': '🦅', 'ru': {'title': 'Falcon-H1: Гибридная мощь в мире языковых моделей', 'desc': 'Falcon-H1 - это новая серия больших языковых моделей с гибридной архитектурой, сочетающей внимание на основе трансформеров и модели пространства состояний. Модели демонстрируют передовую производительность и эффективность в различных задачах и размерах. Falcon-H1 доступен в нескольких конфигурациях, включая базовые и инструктированные варианты с количеством параметров от 0,5B до 34B. Модели превосходят аналоги с большим количеством параметров и показывают отличные результаты в рассуждениях, математике, многоязычных задачах и научных знаниях.'}, 'en': {'title': 'Falcon-H1: Redefining Efficiency in Language Models', 'desc': 'Falcon-H1 introduces a new series of large language models that utilize a hybrid architecture, merging Transformer-based attention with State Space Models for enhanced performance and efficiency. This innovative design allows the models to handle long-context memory better while maintaining computational efficiency. The models are available in various sizes and configurations, demonstrating state-of-the-art capabilities across multiple tasks, including reasoning and multilingual processing. By outperforming larger models with fewer parameters, Falcon-H1 sets a new standard in the field of AI language models.'}, 'zh': {'title': 'Falcon-H1：高效与性能的完美结合', 'desc': 'Falcon-H1是一系列新的大型语言模型，采用混合架构，结合了基于Transformer的注意力机制和状态空间模型（SSM）。这种设计使得Falcon-H1在多种任务中表现出色，具有高效的计算能力和优越的长时记忆能力。与之前的Falcon模型不同，Falcon-H1提供了多种配置，能够在参数较少的情况下与更大规模的模型竞争。所有模型都以开放源代码的方式发布，体现了我们对可及性和影响力的承诺。'}}}, {'id': 'https://huggingface.co/papers/2507.21493', 'title': 'BANG: Dividing 3D Assets via Generative Exploded Dynamics', 'url': 'https://huggingface.co/papers/2507.21493', 'abstract': 'BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is "Generative Exploded Dynamics", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.', 'score': 30, 'issue_id': 5102, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '28c2cc57bc1db4c8', 'authors': ['Longwen Zhang', 'Qixuan Zhang', 'Haoran Jiang', 'Yinuo Bai', 'Wei Yang', 'Lan Xu', 'Jingyi Yu'], 'affiliations': ['Deemos Technology Co., Ltd., China', 'Huazhong University of Science and Technology, China', 'ShanghaiTech University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.21493.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d', '#reasoning', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'BANG: Интуитивная декомпозиция 3D-объектов для творческого моделирования', 'desc': "BANG - это генеративный подход, использующий латентные диффузионные модели и временное внимание для интуитивной декомпозиции и манипуляции 3D-объектами на уровне частей. Он основан на концепции 'Generative Exploded Dynamics', создающей плавную последовательность разобранных состояний входной геометрии. BANG использует предобученную крупномасштабную латентную диффузионную модель, дообученную для разобранной динамики с помощью легковесного адаптера. Подход позволяет улучшить рабочие процессы 3D-создания, предлагая интуитивно понятный способ трансформации концепций в детализированные 3D-активы."}, 'en': {'title': 'BANG: Intuitive 3D Creation through Generative Decomposition', 'desc': "BANG is a generative approach that enhances 3D creation by allowing users to intuitively decompose and manipulate 3D objects at a part level. It employs latent diffusion models and a temporal attention mechanism to ensure smooth transitions and maintain the coherence of geometric and semantic properties during the decomposition process. The system allows for precise control through spatial prompts, enabling users to specify which parts to manipulate, and can integrate with multimodal models for enhanced creativity. BANG's capabilities support detailed geometry generation and are particularly useful in applications like 3D printing, where it facilitates the creation of separable parts for easy assembly."}, 'zh': {'title': 'BANG：直观的3D对象分解与创作新方法', 'desc': 'BANG是一种生成性方法，利用潜在扩散模型和时间注意力机制，实现3D对象的直观部分级分解和操作，提升3D创作流程。它通过“生成性爆炸动态”技术，创建输入几何体的平滑爆炸状态序列，逐步分离部件，同时保持几何和语义的一致性。BANG使用经过预训练的大规模潜在扩散模型，并通过轻量级的爆炸视图适配器进行微调，确保分解过程的精确控制。该方法还结合了时间注意力模块，确保时间上的平滑过渡和一致性，极大地增强了3D创作的灵活性和直观性。'}}}, {'id': 'https://huggingface.co/papers/2507.22607', 'title': 'VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2507.22607', 'abstract': 'VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.', 'score': 22, 'issue_id': 5106, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': 'ce7a62814ba40250', 'authors': ['Ruifeng Yuan', 'Chenghao Xiao', 'Sicong Leng', 'Jianyu Wang', 'Long Li', 'Weiwen Xu', 'Hou Pong Chan', 'Deli Zhao', 'Tingyang Xu', 'Zhongyu Wei', 'Hao Zhang', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.22607.jpg', 'data': {'categories': ['#multimodal', '#training', '#rlhf', '#reasoning', '#benchmark', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение для улучшения мультимодальных рассуждений ИИ', 'desc': 'VL-Cogito - это мультимодальная модель рассуждений, использующая прогрессивное обучение с подкреплением. Модель динамически адаптирует сложность задач и длину пути рассуждений, что позволяет улучшить производительность на разнообразных задачах. VL-Cogito использует механизм мягкой настройки сложности и динамическое вознаграждение за длину рассуждений. Эксперименты показывают, что модель превосходит существующие аналоги на различных мультимодальных тестах.'}, 'en': {'title': 'Empowering Multimodal Reasoning with Progressive Learning', 'desc': 'VL-Cogito is a multimodal reasoning model that enhances performance on various tasks using a Progressive Curriculum Reinforcement Learning (PCuRL) framework. This framework allows the model to learn by gradually increasing the difficulty of tasks, which helps improve its reasoning skills in complex multimodal scenarios. It introduces two main innovations: an online difficulty soft weighting mechanism that adjusts the training difficulty dynamically, and a dynamic length reward mechanism that helps the model manage its reasoning path length based on task complexity. Experimental results show that VL-Cogito outperforms existing models in multiple reasoning tasks, demonstrating its effectiveness in multimodal reasoning.'}, 'zh': {'title': '渐进式课程强化学习提升多模态推理能力', 'desc': 'VL-Cogito是一种多模态推理模型，采用渐进式课程强化学习框架来提升在不同任务上的表现。该模型通过逐步增加任务难度和推理路径长度，系统性地引导模型进行训练。它引入了在线难度软加权机制和动态长度奖励机制，以适应任务复杂性，从而平衡推理效率和正确性。实验结果表明，VL-Cogito在多个主流多模态基准测试中表现优异，超越了现有的推理模型。'}}}, {'id': 'https://huggingface.co/papers/2507.20976', 'title': 'Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision', 'url': 'https://huggingface.co/papers/2507.20976', 'abstract': 'A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA', 'score': 7, 'issue_id': 5103, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'c48709f829f62b89', 'authors': ['Xiao Fang', 'Minhyek Jeon', 'Zheyang Qin', 'Stanislav Panev', 'Celso de Melo', 'Shuowen Hu', 'Shayok Chakraborty', 'Fernando De la Torre'], 'affiliations': ['Carnegie Mellon University', 'DEVCOM Army Research Laboratory', 'Florida State University'], 'pdf_title_img': 'assets/pdf/title_img/2507.20976.jpg', 'data': {'categories': ['#synthetic', '#transfer_learning', '#dataset', '#cv', '#data', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'Генеративный ИИ повышает точность обнаружения автомобилей на аэрофотоснимках', 'desc': 'Статья представляет новый метод обнаружения транспортных средств на аэрофотоснимках с использованием генеративного ИИ. Авторы разработали многоступенчатую мультимодальную систему передачи знаний на основе тонко настроенных латентных диффузионных моделей (LDM) для преодоления разрыва между исходной и целевой средой. Эксперименты показали значительное улучшение производительности по сравнению с существующими методами обучения и адаптации доменов. Также были представлены два новых аннотированных набора данных аэрофотоснимков для дальнейших исследований в этой области.'}, 'en': {'title': 'Bridging Domain Gaps for Better Vehicle Detection in Aerial Imagery', 'desc': 'This paper presents a new framework for improving vehicle detection in aerial images by using fine-tuned latent diffusion models (LDMs). The challenge addressed is the difficulty of models trained in one area to perform well in different geographic regions due to varying conditions. The proposed method enhances training by generating high-quality synthetic aerial images and their labels, effectively bridging the gap between different domains. Experimental results demonstrate significant performance gains in vehicle detection accuracy compared to existing methods, showcasing the effectiveness of this multi-stage, multi-modal approach.'}, 'zh': {'title': '多模态知识转移提升航空图像车辆检测', 'desc': '本论文提出了一种多阶段、多模态的知识转移框架，利用微调的潜在扩散模型来改善不同领域的航空图像中的车辆检测。通过生成高质量的航空图像及其标签，该方法增强了检测器的训练，解决了模型在不同地理区域间的泛化问题。实验结果表明，与传统的监督学习和其他适应方法相比，该方法在AP50指标上提高了4-23%、6-10%、7-40%及超过50%的性能。我们还引入了来自新西兰和犹他州的两个新标注的航空数据集，以支持该领域的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2507.22886', 'title': 'Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation', 'url': 'https://huggingface.co/papers/2507.22886', 'abstract': 'Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.', 'score': 6, 'issue_id': 5102, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '99373a83d84e0212', 'authors': ['Kaining Ying', 'Henghui Ding', 'Guanquan Jie', 'Yu-Gang Jiang'], 'affiliations': ['Fudan University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.22886.jpg', 'data': {'categories': ['#games', '#cv', '#reasoning', '#dataset', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'Мультимодальная сегментация: новый уровень понимания аудио-визуального контента', 'desc': 'OmniAVS - это новый набор данных для сегментации аудио-визуального контента, включающий 2,098 видео и 59,458 мультимодальных выражений. Датасет отличается гибким сочетанием текста, речи, звука и визуальных подсказок, а также акцентом на понимание аудиоконтента и сложные рассуждения. OISA - это модель, использующая мультимодальную языковую модель (MLLM) для понимания сложных сигналов и выполнения сегментации на основе рассуждений. Эксперименты показывают, что OISA превосходит существующие методы на OmniAVS и достигает конкурентоспособных результатов в других связанных задачах.'}, 'en': {'title': 'Revolutionizing Audio-Visual Segmentation with OmniAVS and OISA', 'desc': 'This paper introduces Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset designed to enhance audio-visual segmentation by incorporating diverse multimodal expressions. It features 2,098 videos and 59,458 multimodal referring expressions, which include text, speech, sound, and visual cues. The study also presents the Omnimodal Instructed Segmentation Assistant (OISA), which utilizes a Multimodal Large Language Model (MLLM) to improve reasoning and understanding of complex audiovisual content. Experimental results demonstrate that OISA significantly outperforms existing segmentation methods on the OmniAVS dataset and shows competitive performance on related tasks.'}, 'zh': {'title': '全模态音视频分割的创新与突破', 'desc': '本文提出了全模态引用音视频分割（OmniAVS）和全模态指令分割助手（OISA），旨在提升音视频分割的能力。OmniAVS是一个新数据集，包含2098个视频和59458个多模态引用表达，具有8种灵活结合文本、语音、声音和视觉线索的多模态表达类型。OISA利用多语言大模型（MLLM）来理解复杂线索并进行推理分割，从而解决多模态推理和音视频内容的细致理解问题。实验结果表明，OISA在OmniAVS上优于现有方法，并在其他相关任务中也取得了竞争性结果。'}}}, {'id': 'https://huggingface.co/papers/2507.22853', 'title': 'Repair-R1: Better Test Before Repair', 'url': 'https://huggingface.co/papers/2507.22853', 'abstract': "Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.  \t\t\t\t\tAI-generated summary \t\t\t\t APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.", 'score': 4, 'issue_id': 5103, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '3f84dd190fcc7d27', 'authors': ['Haichuan Hu', 'Xiaochen Xie', 'Quanjun Zhang'], 'affiliations': ['Alibaba Cloud', 'Nanjing University of Science and Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22853.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#optimization', '#rl'], 'emoji': '🛠️', 'ru': {'title': 'Repair-R1: Революция в автоматическом исправлении программ через интеграцию тестирования', 'desc': 'Repair-R1 - это новый подход к автоматическому исправлению программ (APR), который интегрирует тестовые случаи в фазу обучения и приоритезирует генерацию тестов перед исправлением. Метод использует обучение с подкреплением для совместной оптимизации генерации тестов и исправления ошибок. Эксперименты показали значительное улучшение успешности исправлений, генерации тестов и тестового покрытия по сравнению с традиционными моделями. Repair-R1 может быть реализован с различными базовыми моделями, что демонстрирует его гибкость и эффективность.'}, 'en': {'title': 'Revolutionizing Automated Program Repair with Test-Driven Training', 'desc': 'Repair-R1 is a novel approach to Automated Program Repair (APR) that enhances the repair process by incorporating test cases during the training phase. Unlike traditional methods that only use tests after generating patches, Repair-R1 prioritizes test generation before the repair, allowing the model to create targeted tests that identify defects more effectively. This method leverages reinforcement learning to optimize both test generation and bug repair simultaneously, leading to improved performance metrics. Experimental results show significant increases in repair success rates, test generation success, and overall test coverage compared to standard models.'}, 'zh': {'title': 'Repair-R1：优先生成测试，提升自动修复效果', 'desc': 'Repair-R1 是一种自动程序修复方法，它通过将测试用例整合到训练阶段来增强修复效果。该方法优先生成测试用例，然后再进行修复，从而提高了修复成功率和测试生成成功率。与传统方法不同，Repair-R1 在修复之前生成能够区分缺陷行为的测试用例，使模型更好地定位缺陷并理解其根本原因。实验结果表明，Repair-R1 在多个基准测试中表现优越，显著提高了修复和测试的成功率。'}}}, {'id': 'https://huggingface.co/papers/2507.22565', 'title': 'Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2507.22565', 'abstract': "RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.  \t\t\t\t\tAI-generated summary \t\t\t\t The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same (epsilon, delta)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.", 'score': 2, 'issue_id': 5106, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '0bf987cb582a1073', 'authors': ['Afshin Khadangi', 'Amir Sartipi', 'Igor Tchappi', 'Ramin Bahmani', 'Gilbert Fridgen'], 'affiliations': ['SnT, University of Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2507.22565.jpg', 'data': {'categories': ['#healthcare', '#training', '#security', '#rlhf', '#optimization', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Динамическая оптимизация приватности в обучении языковых моделей', 'desc': 'RLDP - это фреймворк глубокого обучения с подкреплением, который оптимизирует дифференциально приватное обучение путем динамической настройки ограничения градиентов и шума. Он использует агент мягкого актор-критика (SAC) для выбора параметров ограничения градиентов и уровня шума во время обучения языковой модели. RLDP позволяет достичь лучшего баланса между полезностью модели и конфиденциальностью данных по сравнению с базовыми методами. Эксперименты показали значительное улучшение перплексии и downstream-метрик при сохранении тех же гарантий дифференциальной приватности.'}, 'en': {'title': 'Optimizing Privacy and Performance with RLDP', 'desc': 'RLDP is a novel deep reinforcement learning framework designed to optimize differentially private training for machine learning models. It addresses the challenge of balancing data privacy with model performance by dynamically adjusting gradient clipping and noise levels based on real-time learning statistics. This approach allows for more efficient use of the privacy budget, leading to improved model accuracy and faster convergence during training. Through extensive experiments, RLDP demonstrates significant reductions in perplexity and enhanced utility across various language models while maintaining strong privacy guarantees.'}, 'zh': {'title': '动态优化隐私与模型效用的平衡', 'desc': 'RLDP是一个深度强化学习框架，旨在优化差分隐私训练。它通过动态调整梯度裁剪和噪声的大小，提升模型的实用性和训练速度，同时保持数据隐私。传统的差分隐私随机梯度下降方法在保证隐私的同时，往往会降低样本效率和最终准确性。RLDP通过实时感知学习动态，智能选择每个参数的梯度裁剪阈值和噪声大小，从而有效分配隐私预算。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi (1)', '#alignment', '#architecture (1)', '#audio', '#benchmark (2)', '#cv (2)', '#data (1)', '#dataset (5)', '#diffusion (1)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (5)', '#open_source (3)', '#optimization (2)', '#plp', '#rag', '#reasoning (3)', '#rl (3)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-31 10:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-31 10:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-31 10:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    