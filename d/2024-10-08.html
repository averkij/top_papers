
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xcc4c09dfca59a8d4 { background: url("https://hfday.ru/img/20241003/cc4c09dfca59a8d4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xcc4c09dfca59a8d4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xcc4c09dfca59a8d4 { background: url("https://hfday.ru/img/20241003/cc4c09dfca59a8d4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xcc4c09dfca59a8d4:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1f883a68716278f { background: url("https://hfday.ru/img/20241007/e1f883a68716278f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1f883a68716278f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1f883a68716278f { background: url("https://hfday.ru/img/20241007/e1f883a68716278f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1f883a68716278f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x4bfc87467c16c280 { background: url("https://hfday.ru/img/20241006/4bfc87467c16c280.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x4bfc87467c16c280:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x4bfc87467c16c280 { background: url("https://hfday.ru/img/20241006/4bfc87467c16c280.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x4bfc87467c16c280:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x96f3985414b99bf9 { background: url("https://hfday.ru/img/20241007/96f3985414b99bf9.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x96f3985414b99bf9:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x96f3985414b99bf9 { background: url("https://hfday.ru/img/20241007/96f3985414b99bf9.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x96f3985414b99bf9:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x13152fb25949544e { background: url("https://hfday.ru/img/20241007/13152fb25949544e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x13152fb25949544e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x13152fb25949544e { background: url("https://hfday.ru/img/20241007/13152fb25949544e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x13152fb25949544e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xc91b020a4bdfc21e { background: url("https://hfday.ru/img/20241007/c91b020a4bdfc21e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc91b020a4bdfc21e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc91b020a4bdfc21e { background: url("https://hfday.ru/img/20241007/c91b020a4bdfc21e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc91b020a4bdfc21e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0327c1225649bc76 { background: url("https://hfday.ru/img/20241004/0327c1225649bc76.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0327c1225649bc76:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0327c1225649bc76 { background: url("https://hfday.ru/img/20241004/0327c1225649bc76.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0327c1225649bc76:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8c4b74044ea31d8d { background: url("https://hfday.ru/img/20241007/8c4b74044ea31d8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8c4b74044ea31d8d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8c4b74044ea31d8d { background: url("https://hfday.ru/img/20241007/8c4b74044ea31d8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8c4b74044ea31d8d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xf332523f23a7857e { background: url("https://hfday.ru/img/20241007/f332523f23a7857e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xf332523f23a7857e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xf332523f23a7857e { background: url("https://hfday.ru/img/20241007/f332523f23a7857e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xf332523f23a7857e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x1e15804dd57fc363 { background: url("https://hfday.ru/img/20241007/1e15804dd57fc363.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x1e15804dd57fc363:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x1e15804dd57fc363 { background: url("https://hfday.ru/img/20241007/1e15804dd57fc363.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x1e15804dd57fc363:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 –æ–∫—Ç—è–±—Ä—è</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-07.html">‚¨ÖÔ∏è <span id="prev-date">07.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-09.html">‚û°Ô∏è <span id="next-date">09.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">üìà <span id='top-month-label'>–¢–æ–ø –∑–∞ –º–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 8', 'zh': '10Êúà8Êó•'};
        let feedDateNext = {'ru': '09.10', 'en': '10/09', 'zh': '10Êúà9Êó•'};
        let feedDatePrev = {'ru': '07.10', 'en': '10/07', 'zh': '10Êúà7Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '–°—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'Published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–¢–æ–ø –∑–∞ –º–µ—Å—è—Ü', 'en': 'Top by Month', 'zh': 'ÊúàÂ∫¶ÁÉ≠Èó®ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05258', 'title': 'Differential Transformer', 'url': 'https://huggingface.co/papers/2410.05258', 'abstract': 'Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.', 'score': 165, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#hallucinations', '#inference', '#long_context'], 'emoji': 'üîç', 'ru': {'title': 'Diff Transformer: —Ç–æ—á–Ω–µ–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –≤–∞–∂–Ω–æ–º, –æ—Ç—Å–µ–∫–∞—è —à—É–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Diff Transformer, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏ –ø–æ–¥–∞–≤–ª—è–µ—Ç —à—É–º. –ú–µ—Ö–∞–Ω–∏–∑–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –¥–≤—É–º—è –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è softmax. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Diff Transformer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω—ã–π Transformer –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–Ω–∏–∂–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ –æ–±—É—á–µ–Ω–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.'}, 'en': {'title': 'Diff Transformer: Sharpening Focus, Reducing Noise in Language Models', 'desc': 'The paper introduces Diff Transformer, a new model that improves attention mechanisms by focusing more on relevant information and reducing noise. It uses a differential attention mechanism that calculates attention scores by subtracting two softmax attention maps, leading to clearer and more focused attention patterns. This approach enhances performance in tasks like long-context modeling and information retrieval, and it reduces issues like hallucination in text generation. Diff Transformer also shows improved robustness in in-context learning, making it a promising advancement for large language models.'}, 'zh': {'title': 'Diff TransformerÔºöÊõ¥‰∏ìÊ≥®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂', 'desc': 'TransformerÊ®°ÂûãÊúâÊó∂‰ºöËøáÂ§öÂÖ≥Ê≥®‰∏çÁõ∏ÂÖ≥ÁöÑ‰∏ä‰∏ãÊñá„ÄÇDiff TransformerÈÄöËøáÂ∑ÆÂàÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ¢ûÂº∫ÂØπÁõ∏ÂÖ≥‰∏ä‰∏ãÊñáÁöÑÂÖ≥Ê≥®ÔºåÂêåÊó∂Ê∂àÈô§Âô™Â£∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåDiff TransformerÂú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüTransformerÔºåÂ∞§ÂÖ∂Âú®Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°Âíå‰ø°ÊÅØÊ£ÄÁ¥¢Á≠âÂ∫îÁî®‰∏≠ÊïàÊûúÊòæËëó„ÄÇÂÆÉËøòËÉΩÂáèÂ∞ëÂπªËßâÁé∞Ë±°ÔºåÊèêÈ´ò‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ'}}, 'hash': 'fae9004e0f12e4fc', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.02884', 'title': 'LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2410.02884', 'abstract': 'This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.', 'score': 48, 'issue_id': 16, 'pub_date': '2024-10-03', 'pub_date_ru': '3 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#math', '#rlhf'], 'emoji': 'üßÆ', 'ru': {'title': 'LLaMA-Berry: –ü—Ä–æ—Ä—ã–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': 'LLaMA-Berry - —ç—Ç–æ –ø–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É (MCTS) —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å –ø–∞—Ä–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—É—Ç–µ–π —Ä–µ—à–µ–Ω–∏—è. LLaMA-Berry –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'LLaMA-Berry: Revolutionizing Math Reasoning in AI', 'desc': 'The paper introduces LLaMA-Berry, a framework designed to improve the mathematical reasoning skills of Large Language Models by integrating Monte Carlo Tree Search with a method called Self-Refine. This combination allows for a more efficient exploration of possible solutions by overcoming the limitations of traditional search algorithms. The framework also uses a Pairwise Preference Reward Model to evaluate and rank different solution paths, inspired by Reinforcement Learning from Human Feedback. Tested on various benchmarks, LLaMA-Berry demonstrates superior performance in solving complex mathematical problems compared to existing methods.'}, 'zh': {'title': 'LLaMA-BerryÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊï∞Â≠¶Êé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫LLaMA-BerryÁöÑÈ´òÁ∫ßÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥Ê°ÜÊû∂ÔºåÁî®‰∫éÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÂíåËø≠‰ª£Ëá™Êàë‰ºòÂåñÔºå‰ª•‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÔºåÂπ∂Âà©Áî®ÊàêÂØπÂ•ñÂä±Ê®°ÂûãÂØπ‰∏çÂêåË∑ØÂæÑËøõË°åÂÖ®Â±ÄËØÑ‰º∞„ÄÇÈÄöËøáÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™ÊàëÊâπËØÑÂíåÈáçÂÜôËÉΩÂäõÔºåSR-MCTSÂÖãÊúç‰∫Ü‰º†ÁªüÈÄêÊ≠•ÂíåË¥™Â©™ÊêúÁ¥¢ÁÆóÊ≥ïÁöÑ‰ΩéÊïàÂíåÂ±ÄÈôêÊÄß„ÄÇÊàêÂØπÂÅèÂ•ΩÂ•ñÂä±Ê®°ÂûãÔºàPPRMÔºâÂàôÈÄöËøáÂ¢ûÂº∫ÁöÑÂçöÂ∞îËææËÆ°Êï∞ÊñπÊ≥ïÔºåÂ∞ÜËß£ÂÜ≥ÊñπÊ°àÁöÑÊàêÂØπÂÅèÂ•ΩÂêàÊàê‰∏∫ÂÖ®Â±ÄÊéíÂêçÂàÜÊï∞Ôºå‰ª•ÊâæÂà∞Êõ¥Â•ΩÁöÑÁ≠îÊ°à„ÄÇ'}}, 'hash': 'cc4c09dfca59a8d4', 'pub_date_card': {'ru': '3 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 3', 'zh': '10Êúà3Êó•'}}, {'id': 'https://huggingface.co/papers/2410.02707', 'title': 'LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2410.02707', 'abstract': 'Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs\' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs\' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model\'s internal perspective, which can guide future research on enhancing error analysis and mitigation.', 'score': 47, 'issue_id': 15, 'pub_date': '2024-10-03', 'pub_date_ru': '3 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#hallucinations', '#interpretability'], 'emoji': 'üîç', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–æ–¥–µ—Ä–∂–∞—Ç –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á–µ–º —Å—á–∏—Ç–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –û–¥–Ω–∞–∫–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –æ—à–∏–±–æ–∫ –Ω–µ –æ–±–æ–±—â–∞—é—Ç—Å—è –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ LLM –º–æ–≥—É—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–π.'}, 'en': {'title': 'Unlocking the Hidden Truth: Enhancing Error Detection in LLMs', 'desc': "This paper explores how large language models (LLMs) encode information about the truthfulness of their outputs, which can be used to detect errors like hallucinations. The study finds that truthfulness information is concentrated in specific tokens, improving error detection, but these detectors don't generalize well across different datasets. It also shows that internal representations can predict the types of errors a model might make, helping to create specific strategies to reduce these errors. Additionally, the research highlights a gap between what LLMs internally know and what they output, as they might encode the correct answer but still produce an incorrect one."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂÜÖÈÉ®ÁöÑÁúüÂÆûÊÄßÁºñÁ†Å', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ∏∏Â∏∏‰ºö‰∫ßÁîüÈîôËØØÔºåÂåÖÊã¨‰∫ãÂÆû‰∏çÂáÜÁ°Æ„ÄÅÂÅèËßÅÂíåÊé®ÁêÜÂ§±Ë¥•ÔºåËøô‰∫õÁªüÁß∞‰∏∫‚ÄúÂπªËßâ‚Äù„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLLMsÁöÑÂÜÖÈÉ®Áä∂ÊÄÅÂåÖÂê´ÂÖ≥‰∫éÂÖ∂ËæìÂá∫ÁúüÂÆûÊÄßÁöÑ‰ø°ÊÅØÔºåËøô‰∫õ‰ø°ÊÅØÂèØ‰ª•Áî®Êù•Ê£ÄÊµãÈîôËØØ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁúüÂÆûÊÄß‰ø°ÊÅØÈõÜ‰∏≠Âú®ÁâπÂÆöÁöÑÊ†áËÆ∞‰∏äÔºåÂà©Áî®Ëøô‰∏ÄÁâπÊÄßÂèØ‰ª•ÊòæËëóÊèêÈ´òÈîôËØØÊ£ÄÊµãÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÈîôËØØÊ£ÄÊµãÂô®Âú®‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÊó†Ê≥ïÊ≥õÂåñÔºåËØ¥ÊòéÁúüÂÆûÊÄßÁºñÁ†Å‰∏çÊòØÊôÆÈÅçÁöÑÔºåËÄåÊòØÂ§öÊñπÈù¢ÁöÑ„ÄÇ'}}, 'hash': 'd3da276d1028f171', 'pub_date_card': {'ru': '3 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 3', 'zh': '10Êúà3Êó•'}}, {'id': 'https://huggingface.co/papers/2410.04364', 'title': "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide", 'url': 'https://huggingface.co/papers/2410.04364', 'abstract': "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: http://videoguide2025.github.io/", 'score': 26, 'issue_id': 13, 'pub_date': '2024-10-06', 'pub_date_ru': '6 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'üé¨', 'ru': {'title': 'VideoGuide: –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': 'VideoGuide - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–∏–¥–∞ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –≤—ã–≤–æ–¥–∞, –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É—è –µ–µ –æ–±—Ä–∞–∑—Ü—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—è —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –ø—Ä–∏–æ—Ä–∞, –ø–æ–∑–≤–æ–ª—è—è –±–∞–∑–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —É–ª—É—á—à–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'VideoGuide: Elevating T2V Consistency Without Compromise', 'desc': 'The paper introduces VideoGuide, a framework designed to improve the temporal consistency of text-to-video (T2V) generation models without additional training. VideoGuide uses a guiding model to enhance the denoising process of a sampling model, leading to better temporal quality and image fidelity. This approach effectively balances the trade-offs between consistency and computational efficiency. Additionally, the method allows base models to enhance text coherence by leveraging the data prior of the guiding model.'}, 'zh': {'title': 'VideoGuideÔºöÊèêÂçáÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÁöÑ‰∏ÄËá¥ÊÄß‰∏éË¥®Èáè', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VideoGuideÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁî®‰∫éÊèêÈ´òÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÊèêÈ´ò‰∏ÄËá¥ÊÄßÊó∂Â∏∏Â∏∏ÂØºËá¥ÂõæÂÉèË¥®Èáè‰∏ãÈôçÊàñËÆ°ÁÆóÊó∂Èó¥ËøáÈïøÔºåËÄåVideoGuideÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÊàñÂæÆË∞ÉÂç≥ÂèØËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáÂú®Êé®ÁêÜÁöÑÊó©ÊúüÈò∂ÊÆµÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰Ωú‰∏∫ÊåáÂØºÔºåÊîπÂñÑÊó∂Èó¥Ë¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÂõæÂÉè‰øùÁúüÂ∫¶ÔºåÊòØ‰∏ÄÁßçÁªèÊµéÂÆûÁî®ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}, 'hash': 'fd005deb7206d4fe', 'pub_date_card': {'ru': '6 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 6', 'zh': '10Êúà6Êó•'}}, {'id': 'https://huggingface.co/papers/2410.02675', 'title': 'FAN: Fourier Analysis Networks', 'url': 'https://huggingface.co/papers/2410.02675', 'abstract': 'Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.', 'score': 24, 'issue_id': 12, 'pub_date': '2024-10-03', 'pub_date_ru': '3 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#math'], 'emoji': 'üîÑ', 'ru': {'title': 'FAN: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FAN, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –§—É—Ä—å–µ. FAN —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —è–≤–ª–µ–Ω–∏—è, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ FAN –Ω–∞–¥ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–º (MLP) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. FAN –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–µ—Ç–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.'}, 'en': {'title': 'FAN: Revolutionizing Periodic Pattern Recognition with Fourier Analysis', 'desc': 'The paper identifies a limitation in traditional neural networks like MLPs and Transformers, which tend to memorize rather than understand periodic data. To address this, the authors introduce FAN, a new network architecture that uses Fourier Analysis to better model periodic phenomena. FAN integrates Fourier Series into its structure, allowing it to predict periodic patterns more accurately with fewer parameters. Experiments show that FAN outperforms traditional models in tasks like time series forecasting and language modeling.'}, 'zh': {'title': 'FANÔºöÁî®ÂÇÖÈáåÂè∂ÂàÜÊûêÈáçÂ°ëÂë®ÊúüÊÄßÂª∫Ê®°', 'desc': 'ËøôÁØáËÆ∫ÊñáÊåáÂá∫ÔºåÂ∞ΩÁÆ°Á•ûÁªèÁΩëÁªúÂ¶ÇMLPÂíåTransformerÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂú®Â§ÑÁêÜÂë®ÊúüÊÄßÊï∞ÊçÆÊó∂Â≠òÂú®ÊΩúÂú®Áº∫Èô∑ÔºåÂÄæÂêë‰∫éËÆ∞ÂøÜËÄåÈùûÁêÜËß£Âë®ÊúüÊÄßÂéüÁêÜ„ÄÇÂë®ÊúüÊÄßÂú®Êé®ÁêÜÂíåÊ≥õÂåñ‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂΩ±ÂìçËá™ÁÑ∂ÂíåÂ∑•Á®ãÁ≥ªÁªüÁöÑÂèØÈ¢ÑÊµãÊÄß„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂÇÖÈáåÂè∂ÂàÜÊûêÁöÑÊñ∞ÂûãÁΩëÁªúÊû∂ÊûÑFANÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âª∫Ê®°ÂíåÊé®ÁêÜÂë®ÊúüÁé∞Ë±°„ÄÇÂÆûÈ™åË°®ÊòéÔºåFANÂú®Â§öÁßçÂÆûÈôÖ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâÊõ¥Â∞ëÁöÑÂèÇÊï∞ÂíåËÆ°ÁÆóÈáè„ÄÇ'}}, 'hash': '9138501c89f38809', 'pub_date_card': {'ru': '3 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 3', 'zh': '10Êúà3Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05080', 'title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'url': 'https://huggingface.co/papers/2410.05080', 'abstract': 'The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.', 'score': 19, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': 'üß™', 'ru': {'title': '–†–µ–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É–∫–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ScienceAgentBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –∏–∑–≤–ª–µ–∫–ª–∏ 102 –∑–∞–¥–∞—á–∏ –∏–∑ 44 —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ —á–µ—Ç—ã—Ä–µ—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ö –∏ –ø—Ä–∏–≤–ª–µ–∫–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∏—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Python-–∫–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–π –∞–≥–µ–Ω—Ç —Ä–µ—à–∞–µ—Ç —Ç–æ–ª—å–∫–æ 32.4% –∑–∞–¥–∞—á —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –Ω–∞—É—á–Ω–æ–π —Å—Ñ–µ—Ä–µ.'}, 'en': {'title': 'Benchmarking the Future: Evaluating Language Agents in Scientific Discovery', 'desc': "The paper discusses the development of language model-based agents aimed at automating scientific discovery, highlighting the need for these agents to handle all tasks in a scientific workflow. To evaluate these agents, the authors introduce ScienceAgentBench, a benchmark derived from 102 tasks across four scientific disciplines, validated by experts. The benchmark assesses the agents' ability to generate Python programs for these tasks, using various evaluation metrics to ensure scientific accuracy and relevance. Results show that even the best-performing language agents can only solve a limited percentage of tasks, indicating their current limitations in fully automating scientific research."}, 'zh': {'title': 'ÁßëÂ≠¶ÂèëÁé∞Ëá™Âä®ÂåñÔºöËØ≠Ë®Ä‰ª£ÁêÜÁöÑÊåëÊàò‰∏éÊú∫ÈÅá', 'desc': 'ËøôÁØáËÆ∫ÊñáËÆ®ËÆ∫‰∫ÜÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËØ≠Ë®Ä‰ª£ÁêÜÂú®ÁßëÂ≠¶ÂèëÁé∞‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåË¶ÅÂÆûÁé∞ÁßëÂ≠¶ÂèëÁé∞ÁöÑÂÖ®Ëá™Âä®ÂåñÔºå‰ª£ÁêÜÂøÖÈ°ªËÉΩÂ§üÂÆåÊàêÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÊâÄÊúâÂÖ≥ÈîÆ‰ªªÂä°„ÄÇ‰∏∫Ê≠§Ôºå‰ªñ‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ScienceAgentBenchÁöÑÊñ∞Âü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËØ≠Ë®Ä‰ª£ÁêÜÂú®Êï∞ÊçÆÈ©±Âä®ÁöÑÁßëÂ≠¶ÂèëÁé∞‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÁõÆÂâçÁöÑËØ≠Ë®Ä‰ª£ÁêÜÂú®ÁîüÊàê‰ª£Á†ÅÊñπÈù¢ËÉΩÂäõÊúâÈôêÔºåÂ∞öÊó†Ê≥ïÂÆûÁé∞ÁßëÂ≠¶Á†îÁ©∂ÁöÑÁ´ØÂà∞Á´ØËá™Âä®Âåñ„ÄÇ'}}, 'hash': 'e1f883a68716278f', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.04534', 'title': 'UniMuMo: Unified Text, Music and Motion Generation', 'url': 'https://huggingface.co/papers/2410.04534', 'abstract': 'We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the https://hanyangclarence.github.io/unimumo_demo/{project page}.', 'score': 18, 'issue_id': 14, 'pub_date': '2024-10-06', 'pub_date_ru': '6 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#multimodal'], 'emoji': 'üé≠', 'ru': {'title': 'UniMuMo: –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –º—É–∑—ã–∫–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏–π', 'desc': 'UniMuMo - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º—É–∑—ã–∫—É –∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–µ—Å–ø–∞—Ä–µ–Ω–Ω—ã—Ö –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –∏ –¥–≤–∏–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ UniMuMo –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ —Ç–∏–ø–∞ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä —Å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º—É–∑—ã–∫–∏, –¥–≤–∏–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'UniMuMo: Harmonizing Text, Music, and Motion in One Model', 'desc': 'The paper introduces UniMuMo, a model that can generate text, music, and motion from any of these inputs. It solves the problem of unpaired data by aligning music and motion based on rhythm, using large datasets. The model uses a unified transformer architecture to convert these inputs into tokens, allowing them to be processed together. By fine-tuning existing models, it efficiently handles multiple tasks, showing strong performance across different generation benchmarks.'}, 'zh': {'title': 'UniMuMoÔºöË∑®Ë∂äÊñáÊú¨„ÄÅÈü≥‰πê‰∏éÂä®‰ΩúÁöÑÂ§öÊ®°ÊÄÅÁîüÊàê', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫UniMuMoÁöÑÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÂèØ‰ª•Ê†πÊçÆ‰ªªÊÑèÁöÑÊñáÊú¨„ÄÅÈü≥‰πêÂíåÂä®‰ΩúÊï∞ÊçÆÁîüÊàêÁõ∏Â∫îÁöÑËæìÂá∫„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÊó∂Èó¥ÂêåÊ≠•Êï∞ÊçÆÁöÑÁº∫‰πèÔºåÁ†îÁ©∂‰∫∫ÂëòÈÄöËøáËäÇÂ•èÊ®°ÂºèÂØπÊú™ÈÖçÂØπÁöÑÈü≥‰πêÂíåÂä®‰ΩúÊï∞ÊçÆËøõË°åÂØπÈΩê„ÄÇÊ®°ÂûãÈÄöËøáÂ∞ÜÈü≥‰πê„ÄÅÂä®‰ΩúÂíåÊñáÊú¨ËΩ¨Êç¢‰∏∫Âü∫‰∫étokenÁöÑË°®Á§∫Ôºå‰ΩøÁî®Áªü‰∏ÄÁöÑÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®TransformerÊû∂ÊûÑËøûÊé•Ëøô‰∫õÊ®°ÊÄÅ„ÄÇÈÄöËøáÂØπÁé∞ÊúâÁöÑÂçïÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÈúÄÊ±ÇÔºåÂπ∂Âú®Â§öÁßçÁîüÊàê‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÁ´û‰∫âÊÄßÁªìÊûú„ÄÇ'}}, 'hash': '4bfc87467c16c280', 'pub_date_card': {'ru': '6 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 6', 'zh': '10Êúà6Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05229', 'title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2410.05229', 'abstract': "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.", 'score': 17, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#interpretability', '#math', '#reasoning'], 'emoji': 'üßÆ', 'ru': {'title': '–†–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ –∏–ª–ª—é–∑–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GSM-Symbolic, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É—Å–ª–æ–≤–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∫ –ø–æ–¥–ª–∏–Ω–Ω—ã–º –ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º, –∞ –ª–∏—à—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Unveiling the Limits of LLMs in Math Reasoning', 'desc': "This paper explores the mathematical reasoning abilities of Large Language Models (LLMs) using a new benchmark called GSM-Symbolic. The study reveals that LLMs struggle with genuine logical reasoning, as their performance drops significantly when questions are slightly altered. The research highlights that LLMs often rely on patterns from their training data rather than true reasoning, especially when questions become more complex. The findings suggest that current metrics may overestimate LLMs' reasoning capabilities, emphasizing the need for more reliable evaluation methods."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊï∞Â≠¶Êé®ÁêÜÁöÑÂ±ÄÈôêÊÄß', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁâπÂà´ÊòØ‰ΩøÁî®GSM8KÂü∫ÂáÜËøõË°åËØÑ‰º∞„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËØÑ‰º∞Âü∫ÂáÜGSM-SymbolicÔºåÈÄöËøáÁ¨¶Âè∑Ê®°ÊùøÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈóÆÈ¢òÈõÜÔºå‰ª•Êõ¥Â•ΩÂú∞ËØÑ‰º∞Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ®°ÂûãÂú®Èù¢ÂØπÂêå‰∏ÄÈóÆÈ¢òÁöÑ‰∏çÂêåÂÆû‰æãÊó∂Ë°®Áé∞Âá∫ÊòæËëóÁöÑÂ∑ÆÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈóÆÈ¢ò‰∏≠‰ªÖÊîπÂèòÊï∞ÂÄºÊó∂ÔºåÊÄßËÉΩ‰ºö‰∏ãÈôç„ÄÇËÆ∫ÊñáÊåáÂá∫ÔºåÂΩìÂâçÁöÑÊ®°ÂûãÂèØËÉΩÊó†Ê≥ïËøõË°åÁúüÊ≠£ÁöÑÈÄªËæëÊé®ÁêÜÔºåËÄåÊòØ‰æùËµñ‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÊé®ÁêÜÊ≠•È™§„ÄÇ'}}, 'hash': '96f3985414b99bf9', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05046', 'title': 'Named Clinical Entity Recognition Benchmark', 'url': 'https://huggingface.co/papers/2410.05046', 'abstract': 'This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.', 'score': 17, 'issue_id': 17, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#medicine'], 'emoji': 'üè•', 'ru': {'title': '–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º NLP', 'desc': '–≠—Ç–æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —è–≤–ª—è–µ—Ç—Å—è F1-–º–µ—Ä–∞, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –¥—Ä—É–≥–∏–º–∏ —Ä–µ–∂–∏–º–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –¶–µ–ª—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.'}, 'en': {'title': 'Benchmarking Healthcare NLP: Setting the Standard for Clinical Entity Recognition', 'desc': 'This paper introduces a benchmark for evaluating language models in healthcare, focusing on the task of extracting structured information from clinical narratives. It provides a standardized platform to assess various language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities like diseases and medications. The benchmark uses datasets standardized by the OMOP Common Data Model to ensure consistency across healthcare systems. Model performance is primarily measured using the F1-score, with additional assessment modes to offer comprehensive insights.'}, 'zh': {'title': 'Êé®Âä®ÂåªÁñóÈ¢ÜÂüüËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞‰∏éÈÄèÊòé', 'desc': 'ËøôÁØáÊäÄÊúØÊä•Âëä‰ªãÁªç‰∫Ü‰∏ÄÁßçÁî®‰∫éËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÈ¢ÜÂüüË°®Áé∞ÁöÑÂëΩÂêç‰∏¥Â∫äÂÆû‰ΩìËØÜÂà´Âü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÊ†áÂáÜÂåñÂπ≥Âè∞ËØÑ‰º∞‰∏çÂêåËØ≠Ë®ÄÊ®°ÂûãÂú®ËØÜÂà´ÂíåÂàÜÁ±ªÂ§öÁßçÂåªÂ≠¶È¢ÜÂüü‰∏¥Â∫äÂÆû‰ΩìÁöÑËÉΩÂäõ„ÄÇ‰ΩøÁî®ÁöÑ‰∏¥Â∫äÊï∞ÊçÆÈõÜÂåÖÊã¨ÁñæÁóÖ„ÄÅÁóáÁä∂„ÄÅËçØÁâ©„ÄÅÁ®ãÂ∫èÂíåÂÆûÈ™åÂÆ§ÊµãÈáèÁ≠âÂÆû‰ΩìÔºåÂπ∂Ê†πÊçÆOMOPÈÄöÁî®Êï∞ÊçÆÊ®°ÂûãËøõË°åÊ†áÂáÜÂåñ„ÄÇÈÄöËøáËøôÁßçÂü∫ÂáÜÊ°ÜÊû∂ÔºåÊéíË°åÊ¶úÊó®Âú®‰øÉËøõÈÄèÊòéÂ∫¶„ÄÅ‰æø‰∫éÊØîËæÉÂàÜÊûêÔºåÂπ∂Êé®Âä®‰∏¥Â∫äÂÆû‰ΩìËØÜÂà´‰ªªÂä°‰∏≠ÁöÑÂàõÊñ∞„ÄÇ'}}, 'hash': '13152fb25949544e', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05243', 'title': 'Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2410.05243', 'abstract': 'Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.', 'score': 16, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#agents', '#cv', '#dataset', '#multimodal', '#synthetic', '#training'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º GUI-–∞–≥–µ–Ω—Ç–∞–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (GUI) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å UGround, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UGround –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –¥–ª—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 20%. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –Ω–∞–≤–∏–≥–∏—Ä—É—é—â–∏—Ö –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–º –º–∏—Ä–µ –ø–æ–¥–æ–±–Ω–æ –ª—é–¥—è–º.'}, 'en': {'title': 'Seeing is Believing: Revolutionizing GUI Agents with Visual Grounding', 'desc': "This paper explores the development of GUI agents that operate using visual perception, similar to how humans interact with digital interfaces. By focusing on visual grounding models, the authors aim to improve the accuracy of mapping GUI elements to their coordinates, enhancing the agents' effectiveness. They introduce UGround, a model trained on a large dataset of GUI elements, which significantly outperforms existing models by relying solely on visual data. The study demonstrates that GUI agents can achieve superior performance without the need for text-based inputs, suggesting a promising future for visually-grounded digital navigation."}, 'zh': {'title': 'ÂÉè‰∫∫Á±ª‰∏ÄÊ†∑ÂØºËà™Êï∞Â≠ó‰∏ñÁïåÁöÑGUI‰ª£ÁêÜ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÊñπÊ≥ïÔºåÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊù•Â¢ûÂº∫ÂÖ∂Âú®ÁúüÂÆû‰∏ñÁïåÂ∫îÁî®‰∏≠ÁöÑËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑGUI‰ª£ÁêÜ‰∏ªË¶Å‰æùËµñ‰∫éÊñáÊú¨Ë°®Á§∫Ôºå‰ΩÜËøôÁßçÊñπÊ≥ïÂ∏∏Â∏∏Â∏¶Êù•Âô™Â£∞Âíå‰∏çÂÆåÊï¥ÊÄß„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ±ª‰ºº‰∫∫Á±ªÁöÑGUI‰ª£ÁêÜÊñπÊ≥ïÔºåÂÆåÂÖ®ÈÄöËøáËßÜËßâÊÑüÁü•ÁéØÂ¢ÉÔºåÂπ∂Âú®ÂÉèÁ¥†Á∫ßÂà´ËøõË°åÊìç‰Ωú„ÄÇÈÄöËøá‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÂíåLLaVAÊû∂ÊûÑÁöÑËΩªÂæÆË∞ÉÊï¥Ôºå‰ªñ‰ª¨ËÆ≠ÁªÉÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫UGroundÁöÑËßÜËßâÂÆö‰ΩçÊ®°ÂûãÔºåÊòæËëóÊèêÂçá‰∫ÜGUI‰ª£ÁêÜÁöÑÊÄßËÉΩ„ÄÇ'}}, 'hash': 'c91b020a4bdfc21e', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03825', 'title': 'MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion', 'url': 'https://huggingface.co/papers/2410.03825', 'abstract': "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.", 'score': 16, 'issue_id': 14, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#3d', '#cv', '#video'], 'emoji': 'üé•', 'ru': {'title': 'MonST3R: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω', 'desc': 'MonST3R - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –í–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö —Å–∏—Å—Ç–µ–º, –æ–Ω –Ω–∞–ø—Ä—è–º—É—é –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞, –∞–¥–∞–ø—Ç–∏—Ä—É—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ DUST3R –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Ö–≤–∞—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∞–≤—Ç–æ—Ä—ã —Å–º–æ–≥–ª–∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. MonST3R –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –∏ –ø–æ–∑—ã –∫–∞–º–µ—Ä—ã, –∞ —Ç–∞–∫–∂–µ –æ–±–µ—â–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.'}, 'en': {'title': 'Simplifying Dynamic Scene Geometry with MonST3R', 'desc': 'The paper introduces Motion DUSt3R (MonST3R), a new method for estimating geometry in dynamic scenes, where objects move and change shape over time. Unlike traditional methods that break the problem into smaller tasks, MonST3R directly estimates geometry for each moment, simplifying the process and reducing errors. The authors tackle the challenge of limited training data by fine-tuning the model on selected datasets, allowing it to handle dynamic scenes effectively. MonST3R not only improves video depth and camera pose estimation but also shows potential for efficient 4D reconstruction.'}, 'zh': {'title': 'Âä®ÊÄÅÂú∫ÊôØÂá†‰Ωï‰º∞ËÆ°ÁöÑÊñ∞Á™ÅÁ†¥ÔºöMonST3R', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Motion DUSt3RÔºàMonST3RÔºâÔºåÁî®‰∫é‰ªéÂä®ÊÄÅÂú∫ÊôØ‰∏≠Áõ¥Êé•‰º∞ËÆ°ÊØè‰∏™Êó∂Èó¥Ê≠•ÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏Â∞ÜÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠ê‰ªªÂä°ÔºåÂØºËá¥Á≥ªÁªüÂ§çÊùÇ‰∏îÂÆπÊòìÂá∫ÈîôÔºåËÄåMonST3RÈÄöËøáÁÆÄÂçï‰º∞ËÆ°ÊØè‰∏™Êó∂Èó¥Ê≠•ÁöÑÁÇπÂõæÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°Áº∫‰πèÂêàÈÄÇÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊòØ‰∏Ä‰∏™ÊåëÊàòÔºå‰ΩÜÈÄöËøáÂ∞ÜÈóÆÈ¢òËßÜ‰∏∫ÂæÆË∞É‰ªªÂä°Âπ∂ÈÄâÊã©ÂêàÈÄÇÁöÑÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊ®°ÂûãËÉΩÂ§üÂú®Ê≤°ÊúâÊòéÁ°ÆËøêÂä®Ë°®Á§∫ÁöÑÊÉÖÂÜµ‰∏ãÂ§ÑÁêÜÂä®ÊÄÅÂú∫ÊôØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMonST3RÂú®ËßÜÈ¢ëÊ∑±Â∫¶ÂíåÁõ∏Êú∫ÂßøÊÄÅ‰º∞ËÆ°Á≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ºò‰∫é‰ª•ÂæÄÁöÑÊñπÊ≥ï„ÄÇ'}}, 'hash': '0327c1225649bc76', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}, {'id': 'https://huggingface.co/papers/2410.04734', 'title': 'TLDR: Token-Level Detective Reward Model for Large Vision Language Models', 'url': 'https://huggingface.co/papers/2410.04734', 'abstract': 'Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.', 'score': 16, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#hallucinations', '#interpretability', '#multimodal', '#rag', '#synthetic', '#training'], 'emoji': 'üîç', 'ru': {'title': '–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º', 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ (TLDR) –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. TLDR –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—É—é, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ '—Å–ª–æ–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã' –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. TLDR –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π –∏ –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ 3 —Ä–∞–∑–∞."}, 'en': {'title': '"Token-Level Precision: Elevating Multimodal Models with TLDR"', 'desc': "The paper introduces a Token-Level Detective Reward Model (TLDR) to enhance the feedback mechanism in multimodal large language models. Unlike traditional reward models that provide binary feedback, TLDR offers detailed annotations for each text token, improving the model's understanding of both text and images. The authors use a perturbation-based method to create challenging examples and train the TLDR model, which helps models self-correct and evaluate hallucinations. Additionally, TLDR models can accelerate human annotation processes, making it three times faster to gather high-quality data."}, 'zh': {'title': 'ÁªÜÁ≤íÂ∫¶Â•ñÂä±Ê®°ÂûãÔºöÊèêÂçáÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ≥ÈîÆ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ•ñÂä±Ê®°ÂûãÔºåÁß∞‰∏∫Token-Level Detective Reward ModelÔºàTLDRÔºâÔºåÁî®‰∫éÊîπËøõÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÂè™ÁªôÊñáÊú¨Êï¥‰Ωì‰∏Ä‰∏™‰∫åÂÖÉÂèçÈ¶àÔºåËÄåTLDRÊ®°ÂûãÂàôÂØπÊØè‰∏™ÊñáÊú¨Ê†áËÆ∞Êèê‰æõÁªÜÁ≤íÂ∫¶ÁöÑÊ≥®Èáä„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÊâ∞Âä®ÁöÑÊñπÊ≥ïÔºåÁîüÊàêÂêàÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ÂèäÂÖ∂Ê†áËÆ∞ÔºåÊù•ËÆ≠ÁªÉTLDRÊ®°Âûã„ÄÇÂÆûÈ™åË°®ÊòéÔºåTLDRÊ®°Âûã‰∏ç‰ªÖËÉΩÂ∏ÆÂä©Áé∞ÊúâÊ®°ÂûãËá™ÊàëÁ∫†Ê≠£ÁîüÊàêÂÜÖÂÆπÔºåËøòËÉΩ‰Ωú‰∏∫ÂπªËßâËØÑ‰º∞Â∑•ÂÖ∑ÔºåÂπ∂ËÉΩÂ∞Ü‰∫∫Â∑•Ê≥®ÈáäÈÄüÂ∫¶ÊèêÈ´ò‰∏âÂÄç„ÄÇ'}}, 'hash': '8c4b74044ea31d8d', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05167', 'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation', 'url': 'https://huggingface.co/papers/2410.05167', 'abstract': 'Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.', 'score': 15, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#audio', '#diffusion', '#inference'], 'emoji': 'üéµ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –ø–æ–º–æ—â—å—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Presto! - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (DMD) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∏–ª–∏ –º–µ—Ç–æ–¥ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –º—É–∑—ã–∫—É –≤ 10-18 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Presto! - Fast-Track Your Text-to-Music Creations', 'desc': 'The paper introduces Presto!, a method to speed up text-to-music generation using diffusion transformers by reducing both the number of sampling steps and the cost per step. It employs a novel score-based distribution matching distillation (DMD) method, which is the first GAN-based distillation approach for text-to-music models. Additionally, it enhances a recent layer distillation method to better preserve hidden state variance, improving learning efficiency. The combined approach significantly accelerates the model, achieving high-quality outputs with greater diversity and up to 18 times faster than previous state-of-the-art methods.'}, 'zh': {'title': 'Presto!ÔºöÂø´ÈÄüÈ´òË¥®ÈáèÁöÑÊñáÊú¨Âà∞Èü≥‰πêÁîüÊàê', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Presto!ÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂä†ÈÄüÂü∫‰∫éÊâ©Êï£ÁöÑÊñáÊú¨Âà∞Èü≥‰πêÁîüÊàê„ÄÇÈÄöËøáÂáèÂ∞ëÈááÊ†∑Ê≠•È™§ÂíåÊØèÊ≠•ÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåPresto!ÊèêÈ´ò‰∫ÜÁîüÊàêÊïàÁéá„ÄÇÁ†îÁ©∂ËÄÖÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫‰∫éÂæóÂàÜÁöÑÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÊñπÊ≥ïÂíåÊîπËøõÁöÑÂ±ÇËí∏È¶èÊñπÊ≥ïÔºå‰ª•Êõ¥Â•ΩÂú∞‰øùÊåÅÈöêËóèÁä∂ÊÄÅÁöÑÊñπÂ∑Æ„ÄÇÊúÄÁªàÔºåÁªìÂêàËøô‰∏§ÁßçËí∏È¶èÊñπÊ≥ïÔºåPresto!ÂÆûÁé∞‰∫ÜÈ´òË¥®ÈáèËæìÂá∫ÁöÑÂø´ÈÄüÁîüÊàêÔºåÊØîÁé∞ÊúâÊäÄÊúØÂø´15ÂÄç„ÄÇ'}}, 'hash': 'f332523f23a7857e', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.04698', 'title': 'MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2410.04698', 'abstract': "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.", 'score': 13, 'issue_id': 15, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#long_context', '#math'], 'emoji': 'üßÆ', 'ru': {'title': 'MathHay: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MathHay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, MathHay —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ –∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –≤–æ—Å–µ–º—å—é –≤–µ–¥—É—â–∏–º–∏ LLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å Gemini-1.5-Pro-002 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 51.26% –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –¥–ª–∏–Ω–æ–π 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –æ–±–ª–∞—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Pushing the Limits: Math Reasoning in Long Contexts', 'desc': 'The paper introduces MathHay, a new benchmark specifically designed to evaluate the mathematical reasoning abilities of large language models (LLMs) over long contexts. Unlike previous benchmarks, MathHay requires models to not only retrieve information but also perform complex mathematical reasoning. Experiments conducted on eight leading LLMs reveal that even the best model, Gemini-1.5-Pro-002, achieves only 51.26% accuracy at 128K tokens, indicating challenges in this area. This study highlights the need for further advancements in LLMs to improve their mathematical reasoning capabilities in long-context scenarios.'}, 'zh': {'title': 'MathHayÔºöÊåëÊàòÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜÊûÅÈôê', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫MathHayÁöÑËá™Âä®ÂåñÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈïøÊñáÊú¨‰∏≠ÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇ‰∏é‰πãÂâçÁöÑÂü∫ÂáÜ‰∏çÂêåÔºåMathHay‰∏ç‰ªÖË¶ÅÊ±ÇÊ®°ÂûãÂÖ∑Â§á‰ø°ÊÅØÊ£ÄÁ¥¢ËÉΩÂäõÔºåËøòÈúÄË¶ÅÂ§çÊùÇÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØË°®Áé∞ÊúÄÂ•ΩÁöÑÊ®°ÂûãÂú®ÈïøÊñáÊú¨Êï∞Â≠¶Êé®ÁêÜ‰∏≠‰πü‰ªÖËÉΩËææÂà∞51.26%ÁöÑÂáÜÁ°ÆÁéá„ÄÇËøôË°®ÊòéÂú®MathHayÂü∫ÂáÜ‰∏äËøòÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇ'}}, 'hash': '1e15804dd57fc363', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.04932', 'title': 'OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction', 'url': 'https://huggingface.co/papers/2410.04932', 'abstract': 'We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets. Project page: https://len-li.github.io/omnibooth-web/', 'score': 9, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': '–£–ø—Ä–∞–≤–ª—è–µ–º–æ–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'OmniBooth - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏ –∞—Ç—Ä–∏–±—É—Ç—ã –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π-–æ–±—Ä–∞–∑—Ü–æ–≤. –ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è - –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è. OmniBooth —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≥–∏–±–∫–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'OmniBooth: Mastering Image Generation with Precision and Flexibility', 'desc': 'OmniBooth is a framework for generating images where users can control the placement and attributes of objects using text prompts or image references. It introduces latent control signals, which are high-dimensional features that integrate spatial, textual, and image conditions for seamless image generation. This method enhances the flexibility of text-to-image generation by allowing instance-level customization and open-vocabulary generation. Experiments show that OmniBooth improves image synthesis quality and alignment across various tasks and datasets.'}, 'zh': {'title': 'OmniBoothÔºöÂÆûÁé∞ÂõæÂÉèÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅÁ©∫Èó¥ÊéßÂà∂', 'desc': 'OmniBooth ÊòØ‰∏Ä‰∏™ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂ÔºåÂÖÅËÆ∏Áî®Êà∑ÈÄöËøáÊñáÊú¨ÊèêÁ§∫ÊàñÂõæÂÉèÂèÇËÄÉÊù•ËøõË°åÁ©∫Èó¥ÊéßÂà∂ÂíåÂÆû‰æãÁ∫ßÂ§öÊ®°ÊÄÅÂÆöÂà∂„ÄÇÁî®Êà∑ÂèØ‰ª•ÂÆö‰πâÈÅÆÁΩ©ÂíåÁõ∏ÂÖ≥ÁöÑÊñáÊú¨ÊàñÂõæÂÉèÊåáÂØºÔºå‰ª•ÁîüÊàêÂú®ÊåáÂÆöÂùêÊ†á‰∏äÊîæÁΩÆÂ§ö‰∏™ÂØπË±°ÁöÑÂõæÂÉèÔºåÂπ∂Á≤æÁ°ÆÂØπÈΩêÂÖ∂Â±ûÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊèêÂá∫ÊΩúÂú®ÊéßÂà∂‰ø°Âè∑ÔºåÂ∞ÜÁ©∫Èó¥„ÄÅÊñáÊú¨ÂíåÂõæÂÉèÊù°‰ª∂Êó†ÁºùÊï¥ÂêàÔºåÊâ©Â±ï‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑËåÉÂõ¥„ÄÇÂÆûÈ™åË°®ÊòéÔºåOmniBooth Âú®ÂõæÂÉèÂêàÊàêÁöÑ‰øùÁúüÂ∫¶ÂíåÂØπÈΩêÊÄß‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}, 'hash': '20ada6e6ba364412', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05262', 'title': 'TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles', 'url': 'https://huggingface.co/papers/2410.05262', 'abstract': 'As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model\'s logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."', 'score': 9, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning'], 'emoji': 'üê¢', 'ru': {'title': 'TurtleBench: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–æ–≥–∞–¥–æ–∫', 'desc': 'TurtleBench - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–≥–∞–¥–∫–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∏–≥—Ä–µ Turtle Soup Puzzle. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏, —Å–Ω–∏–∂–∞—è —Ä–∏—Å–∫ –æ–±–º–∞–Ω–∞ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π –∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏—Å—å –¥–µ–≤—è—Ç—å –ø–µ—Ä–µ–¥–æ–≤—ã—Ö LLM, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª–∏ —Å–µ—Ä–∏–∏ OpenAI o1 –Ω–µ –ø–æ–∫–∞–∑–∞–ª–∏ –ª–∏–¥–∏—Ä—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–≤–∏–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö o1.'}, 'en': {'title': 'TurtleBench: Revolutionizing LLM Evaluation with Real User Interactions', 'desc': "The paper introduces TurtleBench, a novel evaluation framework for Large Language Models (LLMs) that uses real user interactions from an online puzzle platform to create dynamic datasets. This method addresses the limitations of static benchmarks and reduces biases and costs associated with manual evaluations. TurtleBench's dataset includes over 1,500 user guesses, providing a more authentic measure of a model's reasoning capabilities. The study reveals that some advanced models, like OpenAI's o1 series, may not perform as well as expected, suggesting areas for further research into reasoning techniques and their trade-offs."}, 'zh': {'title': 'TurtleBenchÔºöÂä®ÊÄÅËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ∫îÁî®Êâ©Â§ßÔºåÂØπÂèØÈù†ËØÑ‰º∞ÁöÑÈúÄÊ±Ç‰πüÂú®Â¢ûÂä†„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞Âü∫ÂáÜ‰∏ªË¶Å‰æùËµñ‰∫éÈùôÊÄÅÊï∞ÊçÆÈõÜÔºåÈöæ‰ª•ËØÑ‰º∞Ê®°ÂûãÂú®‰∏éÁî®Êà∑Âä®ÊÄÅ‰∫§‰∫í‰∏≠ÁöÑË°®Áé∞„ÄÇTurtleBenchÈÄöËøáÊî∂ÈõÜÁúüÂÆûÁî®Êà∑Âú®Turtle Soup PuzzleÂπ≥Âè∞‰∏äÁöÑÁåúÊµãÔºåÂä®ÊÄÅÁîüÊàêËØÑ‰º∞Êï∞ÊçÆÈõÜÔºåÈôç‰Ωé‰∫ÜÊ®°Âûã‰ΩúÂºäÁöÑÈ£éÈô©„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊàë‰ª¨ÂØπ‰πù‰∏™ÂÖàËøõÁöÑLLMsËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèëÁé∞OpenAI o1Á≥ªÂàóÊ®°ÂûãÊú™ËÉΩÂèñÂæóÈ¢ÜÂÖàÁªìÊûú„ÄÇ'}}, 'hash': '6d3f3633d606dec9', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03617', 'title': 'What Matters for Model Merging at Scale?', 'url': 'https://huggingface.co/papers/2410.03617', 'abstract': "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.", 'score': 8, 'issue_id': 18, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#optimization'], 'emoji': 'üîÄ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –º–∞—Å—à—Ç–∞–±–µ: –±–æ–ª—å—à–µ, –ª—É—á—à–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –∫–∞—á–µ—Å—Ç–≤–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 1 –¥–æ 64 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —á–µ—Ç—ã—Ä–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–µ—Ç–æ–¥–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∏–ª—å–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –æ —Å–≤–æ–π—Å—Ç–≤–∞—Ö –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Unlocking the Power of Model Merging at Scale', 'desc': 'This paper explores the concept of model merging, which combines multiple expert models into a single, more capable model, focusing on large-scale applications. The study evaluates the effectiveness of merging models of varying sizes, from 1 billion to 64 billion parameters, using four different methods. Key findings include that merging is more successful with strong base models, larger models facilitate easier merging, and merging improves generalization capabilities. The research provides insights into the benefits and limitations of model merging, aiming to guide future studies in this area.'}, 'zh': {'title': 'Â§ßËßÑÊ®°Ê®°ÂûãÂêàÂπ∂ÔºöÊèêÂçáÊ≥õÂåñËÉΩÂäõÁöÑÊñ∞ÈÄîÂæÑ', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÂ§ö‰∏™‰∏ìÂÆ∂Ê®°ÂûãÂêàÂπ∂Êàê‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÂçï‰∏ÄÊ®°Âûã„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂêàÂπ∂ÊïàÊûúÂú®‰∏ìÂÆ∂Ê®°ÂûãÊù•Ëá™Âº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÊó∂Êõ¥Â•ΩÔºåÂ∞§ÂÖ∂ÊòØËøô‰∫õÂü∫Á°ÄÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•Ω„ÄÇÂ§ßËßÑÊ®°Ê®°ÂûãÊõ¥ÂÆπÊòìËøõË°åÂêàÂπ∂ÔºåÂπ∂‰∏îÂêàÂπ∂ÂêéÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÈÄöÂ∏∏‰ºò‰∫éÂ§ö‰ªªÂä°ËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ‰∏çÂêåÁöÑÂêàÂπ∂ÊñπÊ≥ïÂú®Â§ßËßÑÊ®°‰∏ãË°®Áé∞Áõ∏‰ººÔºåËøô‰∏∫Êú™Êù•ÁöÑÂ§ßËßÑÊ®°Ê®°ÂûãÂêàÂπ∂Á†îÁ©∂Êèê‰æõ‰∫ÜÂèÇËÄÉ„ÄÇ'}}, 'hash': '1890d06170bcbcb8', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05057', 'title': 'SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification', 'url': 'https://huggingface.co/papers/2410.05057', 'abstract': 'Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.   In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.   Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.', 'score': 7, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#cv', '#dataset', '#synthetic', '#training'], 'emoji': 'üîç', 'ru': {'title': '–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç SELECT –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ImageNet++, –≤–∫–ª—é—á–∞—é—â–∏–π 5 –Ω–æ–≤—ã—Ö —Å–¥–≤–∏–≥–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è ImageNet-1K –æ—Å—Ç–∞–µ—Ç—Å—è –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º.'}, 'en': {'title': 'SELECT: Benchmarking the Future of Data Curation', 'desc': 'The paper addresses the challenge of data curation, which involves collecting and organizing samples into datasets that enhance machine learning efficiency. It introduces SELECT, a benchmark for evaluating different data curation strategies, using a new dataset called ImageNet++. This dataset expands on ImageNet-1K with five new data shifts, each created using a unique curation method. The study finds that while new curation methods like synthetic data generation show promise, the original ImageNet-1K curation strategy remains the most effective.'}, 'zh': {'title': 'Êï∞ÊçÆÊï¥ÁêÜÊñ∞Âü∫ÂáÜÔºöSELECTÁöÑËØûÁîü', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÊî∂ÈõÜÂíåÁªÑÁªáÊ†∑Êú¨‰ª•ÊîØÊåÅÈ´òÊïàÂ≠¶‰π†ÁöÑÊï∞ÊçÆÊï¥ÁêÜÈóÆÈ¢ò„ÄÇ‰ΩúËÄÖÂºïÂÖ•‰∫ÜSELECTÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂõæÂÉèÂàÜÁ±ªÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•Âü∫ÂáÜ„ÄÇÈÄöËøáÂàõÂª∫Êñ∞ÁöÑÊï∞ÊçÆÈõÜImageNet++Ôºå‰ªñ‰ª¨ËØÑ‰º∞‰∫Ü‰∏çÂêåÁöÑÊï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Êñ∞ÊñπÊ≥ïÂú®Êüê‰∫õ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂéüÂßãImageNet-1KÁöÑÊï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•‰ªçÁÑ∂ÊòØÊ†áÂáÜ„ÄÇ'}}, 'hash': '830db899ae55d1d4', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03187', 'title': 'Autonomous Character-Scene Interaction Synthesis from Text Instruction', 'url': 'https://huggingface.co/papers/2410.03187', 'abstract': 'Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.', 'score': 6, 'issue_id': 15, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#3d', '#dataset', '#diffusion'], 'emoji': 'ü§ñ', 'ru': {'title': '–ò–ò –æ–∂–∏–≤–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ 3D –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∫–æ–º–∞–Ω–¥–∞–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ 3D-—Å—Ä–µ–¥–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏–π. –î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã, —É—á–∏—Ç—ã–≤–∞—é—â–µ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ. –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω –æ–±—à–∏—Ä–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 16 —á–∞—Å–æ–≤ –∑–∞—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≤ 120 indoor-—Å—Ü–µ–Ω–∞—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Animating Life: From Text to Motion in 3D Worlds', 'desc': "This paper introduces a new framework for creating 3D human motions in complex environments using a single text instruction and goal location. It uses an auto-regressive diffusion model to generate motion segments and an autonomous scheduler to manage transitions between action stages. The approach includes a scene representation that accounts for local perception at both the start and goal locations, enhancing motion integration. A large motion-captured dataset with language annotations supports the model's training, showing effective results in producing realistic, multi-stage motions."}, 'zh': {'title': '‰ªéÊñáÊú¨Âà∞Âä®‰ΩúÔºöËá™Âä®ÂåñÂ§öÈò∂ÊÆµÂú∫ÊôØÊÑüÁü•ËøêÂä®ÂêàÊàê', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéÂçï‰∏ÄÁöÑÊñáÊú¨Êåá‰ª§ÂíåÁõÆÊ†á‰ΩçÁΩÆÂêàÊàêÂ§öÈò∂ÊÆµÁöÑÂú∫ÊôØÊÑüÁü•‰∫§‰∫íÂä®‰Ωú„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®Ëá™ÂõûÂΩíÊâ©Êï£Ê®°ÂûãÊù•ÂêàÊàê‰∏ã‰∏Ä‰∏™Âä®‰ΩúÁâáÊÆµÔºåÂπ∂ÈÄöËøáËá™‰∏ªË∞ÉÂ∫¶Âô®È¢ÑÊµãÊØè‰∏™Âä®‰ΩúÈò∂ÊÆµÁöÑËøáÊ∏°„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÂêàÊàêÁöÑÂä®‰Ωú‰∏éÁéØÂ¢ÉÊó†ÁºùÈõÜÊàêÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú∫ÊôØË°®Á§∫ÊñπÊ≥ïÔºåËÄÉËôë‰∫ÜËµ∑ÂßãÂíåÁõÆÊ†á‰ΩçÁΩÆÁöÑÂ±ÄÈÉ®ÊÑüÁü•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑÂ§öÈò∂ÊÆµÂä®‰ΩúÔºå‰∏éÁéØÂ¢ÉÂíåÊñáÊú¨Êù°‰ª∂È´òÂ∫¶‰∏ÄËá¥„ÄÇ'}}, 'hash': '233eb6997f48b10d', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03160', 'title': 'Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach', 'url': 'https://huggingface.co/papers/2410.03160', 'abstract': "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications.", 'score': 4, 'issue_id': 22, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –ø–æ–∫–∞–¥—Ä–æ–≤–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å FVDM (Frame-aware Video Diffusion Model), –∫–æ—Ç–æ—Ä–∞—è –≤–≤–æ–¥–∏—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞ (VTV) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –≤–∏–¥–µ–æ. FVDM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Frame-Aware Diffusion: Revolutionizing Video Generation', 'desc': 'The paper introduces a new approach to video generation using diffusion models, called the frame-aware video diffusion model (FVDM). Unlike traditional models that use a single timestep for an entire video clip, FVDM uses a vectorized timestep variable, allowing each frame to have its own noise schedule. This innovation enables the model to better capture complex temporal dependencies, improving the quality of generated videos across various tasks. The results show that FVDM outperforms existing methods, offering a more flexible and robust framework for video synthesis.'}, 'zh': {'title': 'Â∏ßÊÑüÁü•Êâ©Êï£Ê®°ÂûãÔºöËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ËåÉÂºè', 'desc': 'Êâ©Êï£Ê®°ÂûãÂú®ÂõæÂÉèÁîüÊàê‰∏≠ÂèñÂæó‰∫ÜÈù©ÂëΩÊÄßËøõÂ±ïÔºåÂπ∂Âú®ËßÜÈ¢ëÁîüÊàê‰∏≠Â±ïÁé∞‰∫ÜÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÊó∂Èó¥‰æùËµñÊÄßÊó∂Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∏ßÊÑüÁü•ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÈÄöËøáÂºïÂÖ•Áü¢ÈáèÂåñÊó∂Èó¥Ê≠•ÂèòÈáèÊù•Â¢ûÂº∫Ê®°ÂûãÁöÑÊó∂Èó¥‰æùËµñÊÄßÊçïÊçâËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®ËßÜÈ¢ëÁîüÊàêË¥®Èáè‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®Â§öÈ°π‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}, 'hash': '9b55e7feeb00d83b', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}, {'id': 'https://huggingface.co/papers/2410.05255', 'title': 'SePPO: Semi-Policy Preference Optimization for Diffusion Alignment', 'url': 'https://huggingface.co/papers/2410.05255', 'abstract': 'Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.', 'score': 4, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#cv', '#diffusion', '#rlhf', '#video'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ SePPO (Semi-Policy Preference Optimization) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∑–∞–º–µ–Ω—ã '–ø—Ä–æ–∏–≥—Ä—ã—à–Ω—ã—Ö' –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞—Ä–∞—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∫–æ—Ä–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. SePPO –ø—Ä–µ–≤–∑–æ—à–µ–ª —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö text-to-image –∏ text-to-video."}, 'en': {'title': 'Revolutionizing Visual Generation with SePPO: Smarter Learning from Human Preferences', 'desc': 'The paper introduces a new method called Semi-Policy Preference Optimization (SePPO) to improve diffusion models for visual generation using reinforcement learning from human feedback. Unlike traditional methods that rely heavily on reward models or large datasets of human-annotated images, SePPO uses previous model checkpoints to generate reference samples, optimizing only with "winning images." This approach allows the model to learn more effectively by using an anchor-based criterion to evaluate the quality of reference samples, avoiding performance issues from uncertain data. The method shows superior results in both text-to-image and text-to-video generation tasks, outperforming existing techniques.'}, 'zh': {'title': 'SePPOÔºöÊó†ÈúÄÂ•ñÂä±Ê®°ÂûãÁöÑËßÜËßâÁîüÊàê‰ºòÂåñ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÊù•‰ºòÂåñËßÜËßâÁîüÊàêÊ®°ÂûãÔºåÁß∞‰∏∫ÂçäÁ≠ñÁï•ÂÅèÂ•Ω‰ºòÂåñÔºàSePPOÔºâ„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Á±ªÊ†áÊ≥®Êï∞ÊçÆÔºåËÄåSePPOÈÄöËøá‰ΩøÁî®‰πãÂâçÁöÑÊ®°ÂûãÊ£ÄÊü•ÁÇπ‰Ωú‰∏∫ÂèÇËÄÉÔºåÂáèÂ∞ë‰∫ÜÂØπËøô‰∫õÊï∞ÊçÆÁöÑ‰æùËµñ„ÄÇSePPOÈÄöËøáÈÄâÊã©ÊÄßÂú∞Â≠¶‰π†ÁîüÊàêÁöÑÂèÇËÄÉÊ†∑Êú¨ÔºåÈÅøÂÖç‰∫ÜÂõ†Ê†∑Êú¨Ë¥®Èáè‰∏çÁ°ÆÂÆöÊÄßÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSePPOÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÂíåÊñáÊú¨Âà∞ËßÜÈ¢ëÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}, 'hash': '1202d13e6d3d4583', 'pub_date_card': {'ru': '7 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 7', 'zh': '10Êúà7Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03959', 'title': 'Grounding Language in Multi-Perspective Referential Communication', 'url': 'https://huggingface.co/papers/2410.03959', 'abstract': "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.", 'score': 3, 'issue_id': 12, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#agents', '#dataset', '#multimodal'], 'emoji': 'üë•', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —É—á–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –î–≤–∞ –∞–≥–µ–Ω—Ç–∞ –≤ –æ–±—â–µ–π —Å—Ü–µ–Ω–µ –¥–æ–ª–∂–Ω—ã —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—É –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä–µ–∫—Ç—ã –∏ –∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2970 —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø–∞—Ä, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ–± —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —É–ª—É—á—à–∏–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.'}, 'en': {'title': 'Bridging Perspectives: Enhancing AI Communication in Shared Spaces', 'desc': "This paper introduces a new task and dataset for generating and understanding referring expressions in environments where two agents must consider each other's visual perspectives. The dataset includes 2,970 human-written expressions and human comprehension judgments, used to evaluate automated models in both generating and understanding references. The study finds that current models perform worse than human pairs in these tasks. However, training an open-weight speaker model with feedback on communicative success improves its performance significantly, even surpassing the best proprietary model."}, 'zh': {'title': 'Â§öÊô∫ËÉΩ‰ΩìÁéØÂ¢É‰∏≠ÁöÑÊåá‰ª£Ë°®ËææÁîüÊàê‰∏éÁêÜËß£', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÈ°π‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜÔºåÁî®‰∫éÂú®Â§öÊô∫ËÉΩ‰ΩìÁéØÂ¢É‰∏≠ÁîüÊàêÂíåÁêÜËß£Êåá‰ª£Ë°®Ëææ„ÄÇÂú®Ëøô‰∏™‰ªªÂä°‰∏≠Ôºå‰∏§‰∏™Êô∫ËÉΩ‰ΩìÈúÄË¶ÅËÄÉËôëÂΩºÊ≠§‰∏çÂêåÁöÑËßÜËßâËßÜËßíÔºå‰ª•ÁîüÊàêÂíåÁêÜËß£Âú∫ÊôØ‰∏≠Áâ©‰ΩìÁöÑÊåá‰ª£ÂíåÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÁ†îÁ©∂Êî∂ÈõÜ‰∫Ü2970‰∏™‰∫∫Á±ªÁºñÂÜôÁöÑÊåá‰ª£Ë°®ËææÔºåÂπ∂ËØÑ‰º∞‰∫ÜËá™Âä®ÂåñÊ®°ÂûãÂú®‰∏é‰∫∫Á±ªÊê≠Ê°£Êó∂‰Ωú‰∏∫ËØ¥ËØùËÄÖÂíåÂê¨ËÄÖÁöÑË°®Áé∞ÔºåÂèëÁé∞Ê®°ÂûãÂú®ÁîüÊàêÂíåÁêÜËß£Êåá‰ª£ÊñπÈù¢ÁöÑË°®Áé∞ËêΩÂêé‰∫é‰∫∫Á±ª„ÄÇÊúÄÂêéÔºåÈÄöËøáËÆ≠ÁªÉ‰∏Ä‰∏™ÂºÄÊîæÊùÉÈáçÁöÑËØ¥ËØùËÄÖÊ®°ÂûãÔºåÁªìÂêàÂê¨ËÄÖÁöÑÊ≤üÈÄöÊàêÂäüËØÅÊçÆÔºåÊ®°ÂûãÁöÑÊ≤üÈÄöÊàêÂäüÁéá‰ªé58.9%ÊèêÈ´òÂà∞69.3%ÔºåÁîöËá≥Ë∂ÖËøá‰∫ÜÊúÄÂº∫ÁöÑ‰∏ìÊúâÊ®°Âûã„ÄÇ'}}, 'hash': '7cc5107d8cc84062', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}, {'id': 'https://huggingface.co/papers/2410.03960', 'title': 'SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation', 'url': 'https://huggingface.co/papers/2410.03960', 'abstract': "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.", 'score': 1, 'issue_id': 21, 'pub_date': '2024-10-04', 'pub_date_ru': '4 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#inference', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'SwiftKV: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM', 'desc': 'SwiftKV - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ –≤ LLM. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: SingleInputKV, AcrossKV –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π. SwiftKV —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ 50% –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ KV-–∫—ç—à–∞ –Ω–∞ 62,5% –¥–ª—è –º–æ–¥–µ–ª–µ–π Llama-3.1. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—â–µ–π –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–æ–∫–µ–Ω –Ω–∞ 60%.'}, 'en': {'title': 'SwiftKV: Speeding Up LLMs Without Sacrificing Quality', 'desc': 'The paper introduces SwiftKV, a method to make large language models (LLMs) more efficient by reducing the time and cost of processing prompt tokens. It uses three main techniques: SingleInputKV, which allows skipping some model computations; AcrossKV, which reduces memory use by merging caches; and a distillation process that adapts existing models with minimal accuracy loss. SwiftKV significantly cuts down on computational and memory needs while maintaining output quality, achieving faster processing speeds and higher throughput. This approach is particularly effective for models like Llama-3.1, enhancing their performance in enterprise applications.'}, 'zh': {'title': 'SwiftKVÔºöÈ´òÊïàÂ§ÑÁêÜÊèêÁ§∫‰ª§ÁâåÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'SwiftKV ÊòØ‰∏ÄÁßçÊñ∞ÂûãÊ®°ÂûãËΩ¨Êç¢ÂíåËí∏È¶èÊñπÊ≥ïÔºåÊó®Âú®Èôç‰ΩéÂ§ÑÁêÜÊèêÁ§∫‰ª§ÁâåÁöÑÊó∂Èó¥ÂíåÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÁîüÊàê‰ª§ÁâåÁöÑÈ´òË¥®Èáè„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™ÂÖ≥ÈîÆÊú∫Âà∂ÂÆûÁé∞ÔºöSingleInputKV ÂÖÅËÆ∏ÊèêÁ§∫‰ª§ÁâåË∑≥ËøáÂ§ßÈÉ®ÂàÜÊ®°ÂûãËÆ°ÁÆóÔºåAcrossKV ÂêàÂπ∂Áõ∏ÈÇªÂ±ÇÁöÑ KV ÁºìÂ≠ò‰ª•ÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®ÔºåÂπ∂ÈÄöËøáÁü•ËØÜ‰øùÁïôËí∏È¶èÁ®ãÂ∫èÈÄÇÂ∫îÁé∞Êúâ LLM„ÄÇSwiftKV Âú® Llama-3.1-8B Âíå 70B ‰∏äÂáèÂ∞ë‰∫Ü 50% ÁöÑÈ¢ÑÂ°´ÂÖÖËÆ°ÁÆóÈúÄÊ±ÇÂíå 62.5% ÁöÑ KV ÁºìÂ≠òÂÜÖÂ≠òÈúÄÊ±Ç„ÄÇÊúÄÁªàÔºåSwiftKV Âú®‰ºòÂåñÁöÑ vLLM ÂÆûÁé∞‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ 2 ÂÄçÁöÑÊÄªÂêûÂêêÈáèÂíå 60% ÁöÑÊØè‰∏™ËæìÂá∫‰ª§ÁâåÊó∂Èó¥Èôç‰Ωé„ÄÇ'}}, 'hash': 'f79582d6875d464c', 'pub_date_card': {'ru': '4 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 4', 'zh': '10Êúà4Êó•'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (4)', '#cv (3)', '#data (2)', '#dataset (3)', '#diffusion', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference (1)', '#interpretability (2)', '#long_context (3)', '#math', '#medicine', '#multilingual', '#multimodal (1)', '#optimization (1)', '#plp', '#rag', '#reasoning (2)', '#rl', '#rlhf (1)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training', '#transfer_learning', '#translation', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">üìù ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-10-08 20:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];   
            topMonth.innerHTML = topMonthLabel[currentLang];
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-08 20:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-08 20:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    