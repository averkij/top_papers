{
    "date": {
        "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 24",
        "zh": "11æœˆ24æ—¥"
    },
    "time_utc": "2025-11-24 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-24",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.16719",
            "title": "SAM 3: Segment Anything with Concepts",
            "url": "https://huggingface.co/papers/2511.16719",
            "abstract": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
            "score": 109,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "d064ac8ac0205cab",
            "authors": [
                "Nicolas Carion",
                "Laura Gustafson",
                "Yuan-Ting Hu",
                "Shoubhik Debnath",
                "Ronghang Hu",
                "Didac Suris",
                "Chaitanya Ryali",
                "Kalyan Vasudev Alwala",
                "Haitham Khedr",
                "Andrew Huang",
                "Jie Lei",
                "Tengyu Ma",
                "Baishan Guo",
                "Arpit Kalla",
                "Markus Marks",
                "Joseph Greer",
                "Meng Wang",
                "Peize Sun",
                "Roman RÃ¤dle",
                "Triantafyllos Afouras",
                "Effrosyni Mavroudi",
                "Katherine Xu",
                "Tsung-Han Wu",
                "Yu Zhou",
                "Liliane Momeni",
                "Rishi Hazra",
                "Shuangrui Ding",
                "Sagar Vaze",
                "Francois Porcher",
                "Feng Li",
                "Siyuan Li",
                "Aishwarya Kamath",
                "Ho Kei Cheng",
                "Piotr DollÃ¡r",
                "Nikhila Ravi",
                "Kate Saenko",
                "Pengchuan Zhang",
                "Christoph Feichtenhofer"
            ],
            "affiliations": [
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16719.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#dataset",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Segment Anything Model 3 â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ presence head, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚-Ğ»ĞµĞ¹Ğ±Ğ»Ğ¾Ğ² Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. SAM 3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Object Segmentation with SAM 3",
                    "desc": "The Segment Anything Model 3 (SAM 3) is a cutting-edge machine learning model designed for promptable concept segmentation and tracking in images and videos. It utilizes a unified architecture that separates the tasks of recognizing objects and localizing them, enhancing detection accuracy. SAM 3 processes various prompts, such as noun phrases and image examples, to generate precise segmentation masks and unique identities for detected objects. With a robust dataset of 4 million unique concept labels, SAM 3 significantly outperforms previous models in both image and video segmentation tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¨¡å‹ï¼Œç²¾å‡†åˆ†å‰²ä¸è·Ÿè¸ª",
                    "desc": "Segment Anything Model 3ï¼ˆSAM 3ï¼‰æ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ¦‚å¿µæç¤ºåœ¨å›¾åƒå’Œè§†é¢‘ä¸­æ£€æµ‹ã€åˆ†å‰²å’Œè·Ÿè¸ªå¯¹è±¡ã€‚è¯¥æ¨¡å‹é€šè¿‡è§£è€¦è¯†åˆ«å’Œå®šä½ï¼Œæå‡äº†æç¤ºå¼æ¦‚å¿µåˆ†å‰²ï¼ˆPCSï¼‰çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿè¿”å›åˆ†å‰²æ©ç å’Œå”¯ä¸€èº«ä»½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œç”Ÿæˆäº†åŒ…å«400ä¸‡ä¸ªç‹¬ç‰¹æ¦‚å¿µæ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚SAM 3åœ¨å›¾åƒå’Œè§†é¢‘çš„PCSä»»åŠ¡ä¸­å®ç°äº†ç°æœ‰ç³»ç»Ÿçš„ä¸¤å€å‡†ç¡®ç‡ï¼Œå¹¶æ”¹è¿›äº†ä¹‹å‰SAMåœ¨è§†è§‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15705",
            "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
            "url": "https://huggingface.co/papers/2511.15705",
            "abstract": "GeoVista, an agentic model integrating tool invocation and reinforcement learning, achieves high geolocalization performance on GeoBench, outperforming open-source models and matching closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
            "score": 92,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "b7e22103dfcb9f5e",
            "authors": [
                "Yikun Wang",
                "Zuyan Liu",
                "Ziyi Wang",
                "Pengfei Liu",
                "Han Hu",
                "Yongming Rao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15705.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#rl",
                    "#reasoning",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GeoVista â€” Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GeoBench Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ¾ Ğ²ÑĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ GeoVista Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "GeoVista: Elevating Geolocalization with Agentic Intelligence",
                    "desc": "GeoVista is an advanced agentic model designed for geolocalization tasks, combining tool invocation with reinforcement learning to enhance performance. It addresses the limitations of existing benchmarks by introducing GeoBench, which includes high-resolution images and satellite data for rigorous evaluation. The model employs a two-stage training process, starting with supervised fine-tuning to establish reasoning patterns, followed by reinforcement learning to refine its capabilities. Experimental results demonstrate that GeoVista outperforms open-source models and competes effectively with closed-source counterparts in geolocalization accuracy."
                },
                "zh": {
                    "title": "GeoVistaï¼šæ™ºèƒ½æ¨¡å‹çš„æ–°é«˜åº¦",
                    "desc": "GeoVistaæ˜¯ä¸€ç§ç»“åˆå·¥å…·è°ƒç”¨å’Œå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½æ¨¡å‹ï¼Œåœ¨GeoBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸é—­æºæ¨¡å‹ç›¸å½“ã€‚è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†åœ°ç†å®šä½ä»»åŠ¡ï¼Œå¼ºè°ƒäº†è§†è§‰åŸºç¡€å’Œç½‘ç»œæœç´¢åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚ä¸ºäº†è¯„ä¼°æ™ºèƒ½æ¨¡å‹çš„åœ°ç†å®šä½èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†GeoBenchåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªä¸–ç•Œå„åœ°çš„é«˜åˆ†è¾¨ç‡ç…§ç‰‡å’Œå…¨æ™¯å›¾ã€‚GeoVistaé€šè¿‡åœ¨æ¨ç†å¾ªç¯ä¸­æ— ç¼é›†æˆå·¥å…·è°ƒç”¨ï¼Œé‡‡ç”¨åˆ†å±‚å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†åœ°ç†å®šä½çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16334",
            "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
            "url": "https://huggingface.co/papers/2511.16334",
            "abstract": "OpenMMReasoner, a two-stage training approach combining supervised fine-tuning and reinforcement learning, enhances multimodal reasoning performance through rigorous data curation and improved training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
            "score": 91,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "c390ad0bbfe8688a",
            "authors": [
                "Kaichen Zhang",
                "Keming Wu",
                "Zuhao Yang",
                "Bo Li",
                "Kairui Hu",
                "Bin Wang",
                "Ziwei Liu",
                "Xingxuan Li",
                "Lidong Bing"
            ],
            "affiliations": [
                "LMMs-Lab Team",
                "MiroMind AI",
                "Nanyang Technological University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16334.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "OpenMMReasoner Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ supervised fine-tuning Ğ¸ reinforcement learning. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ SFT ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ¸Ğ· 874K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 74K Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with OpenMMReasoner",
                    "desc": "OpenMMReasoner is a novel approach that enhances multimodal reasoning by combining supervised fine-tuning and reinforcement learning in a two-stage training process. The first stage involves creating a large, well-validated dataset to establish a strong foundation for reasoning tasks. In the second stage, reinforcement learning is applied to refine and stabilize the model's capabilities using a diverse dataset. This method not only improves performance significantly over existing models but also emphasizes the importance of data quality and training strategies in multimodal reasoning."
                },
                "zh": {
                    "title": "OpenMMReasonerï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "OpenMMReasoneræ˜¯ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†å’Œæ”¹è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«874Kæ ·æœ¬çš„å†·å¯åŠ¨æ•°æ®é›†ï¼Œä¸ºæ¨ç†èƒ½åŠ›æä¾›äº†åšå®åŸºç¡€ã€‚éšåï¼Œåˆ©ç”¨74Kæ ·æœ¬çš„æ•°æ®é›†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥å¢å¼ºå’Œç¨³å®šè¿™äº›èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®­ç»ƒæ–¹æ³•åœ¨ä¹ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå¼ºè°ƒäº†æ•°æ®è´¨é‡å’Œè®­ç»ƒè®¾è®¡åœ¨å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15210",
            "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
            "url": "https://huggingface.co/papers/2511.15210",
            "abstract": "The study explores intrinsic dimension in large language models through cross-encoder analysis, linguistic features, and sparse autoencoders, revealing its independence from entropy, genre-specific stratification, and causal features related to text type.  \t\t\t\t\tAI-generated summary \t\t\t\t Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
            "score": 87,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "137843b8ae93cd3b",
            "authors": [
                "Vladislav Pedashenko",
                "Laida Kushnareva",
                "Yana Khassan Nibal",
                "Eduard Tulchinskii",
                "Kristian Kuznetsov",
                "Vladislav Zharchinskii",
                "Yury Maximov",
                "Irina Piontkovskaya"
            ],
            "affiliations": [
                "Interdata Astana",
                "Lomonosov Research Institute",
                "Moscow State University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15210.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ LLM: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ½Ğ°ÑƒĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ° Ğ¾Ñ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¶Ğ°Ğ½Ñ€-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ: Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ (~8), ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ ÑÑ€ĞµĞ´Ğ½ÑÑ (~9), Ğ° Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ (~10.5). Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸: Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞµÑ‘."
                },
                "en": {
                    "title": "Understanding Intrinsic Dimension: The Key to Language Model Complexity",
                    "desc": "This paper investigates the concept of intrinsic dimension (ID) in large language models (LLMs) using various analytical methods. It finds that ID is independent of entropy and varies significantly across different text genres, with scientific writing being simpler and requiring less representational complexity than creative writing. The study also identifies specific linguistic features that influence ID, showing that formal and statistical elements lower ID while personal and emotional elements raise it. Overall, the research offers valuable insights into how ID can be effectively utilized and interpreted in the context of LLMs."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„å†…åœ¨ç»´åº¦",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å†…åœ¨ç»´åº¦ï¼Œé€šè¿‡äº¤å‰ç¼–ç å™¨åˆ†æã€è¯­è¨€ç‰¹å¾å’Œç¨€ç–è‡ªç¼–ç å™¨ï¼Œæ­ç¤ºäº†å…¶ä¸ç†µã€ç‰¹å®šç±»å‹çš„åˆ†ç±»å’Œæ–‡æœ¬ç±»å‹ç›¸å…³çš„å› æœç‰¹å¾çš„ç‹¬ç«‹æ€§ã€‚å†…åœ¨ç»´åº¦ï¼ˆIDï¼‰æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹åˆ†æä¸­çš„é‡è¦å·¥å…·ï¼Œå½±å“è®­ç»ƒåŠ¨æ€ã€æ‰©å±•è¡Œä¸ºå’Œæ•°æ®é›†ç»“æ„ã€‚ç ”ç©¶å‘ç°ï¼ŒIDä¸ç†µåº¦é‡äº’è¡¥ï¼Œä¸”åœ¨æ§åˆ¶æ–‡æœ¬é•¿åº¦åï¼Œä¸¤è€…æ— ç›¸å…³æ€§ï¼ŒIDæ•æ‰å‡ ä½•å¤æ‚æ€§ã€‚é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºç§‘å­¦ä¿¡å·é™ä½IDï¼Œè€Œäººæ€§åŒ–ä¿¡å·åˆ™å¢åŠ IDï¼Œè¡¨æ˜ç§‘å­¦å†™ä½œç›¸å¯¹â€œç®€å•â€ï¼Œè€Œå°è¯´å’Œæƒ…æ„Ÿå†™ä½œåˆ™éœ€è¦æ›´å¤šçš„è¡¨ç°è‡ªç”±åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17502",
            "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
            "url": "https://huggingface.co/papers/2511.17502",
            "abstract": "A unified Vision-Language-Action (VLA) and world model, RynnVLA-002, jointly learns environmental dynamics and action planning, outperforming individual models in both simulation and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "ab117a1cbfe298cd",
            "authors": [
                "Jun Cen",
                "Siteng Huang",
                "Yuqian Yuan",
                "Kehan Li",
                "Hangjie Yuan",
                "Chaohui Yu",
                "Yuming Jiang",
                "Jiayan Guo",
                "Xin Li",
                "Hao Luo",
                "Fan Wang",
                "Deli Zhao",
                "Hao Chen"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17502.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#robotics",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "RynnVLA-002 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Vision-Language-Action Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ world model Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. World model Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº VLA ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ (97.4% Ğ½Ğ° LIBERO), Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ world model Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 50%."
                },
                "en": {
                    "title": "Unified Learning for Enhanced Action and Vision Understanding",
                    "desc": "RynnVLA-002 is a novel model that combines Vision-Language-Action (VLA) capabilities with a world model to improve how machines understand and interact with their environments. It learns to predict future visual states based on actions and visual inputs, effectively capturing the dynamics of the environment. The VLA component enhances action generation by interpreting visual data, which in turn supports the world model's ability to generate accurate images. This integrated approach leads to superior performance in both simulated and real-world tasks, significantly outperforming traditional models."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¨¡å‹ï¼Œæå‡æ™ºèƒ½è¡ŒåŠ¨ï¼",
                    "desc": "RynnVLA-002æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å’Œä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿå…±åŒå­¦ä¹ ç¯å¢ƒåŠ¨æ€å’ŒåŠ¨ä½œè§„åˆ’ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§†è§‰å’ŒåŠ¨ä½œè¾“å…¥æ¥é¢„æµ‹æœªæ¥çš„å›¾åƒçŠ¶æ€ï¼Œä»è€Œå­¦ä¹ ç¯å¢ƒçš„åŸºæœ¬ç‰©ç†è§„å¾‹ã€‚VLAæ¨¡å‹åˆ™æ ¹æ®å›¾åƒè§‚å¯Ÿç”Ÿæˆåç»­åŠ¨ä½œï¼Œå¢å¼ºè§†è§‰ç†è§£å¹¶æ”¯æŒä¸–ç•Œæ¨¡å‹çš„å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRynnVLA-002åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡ä¼˜äºå•ç‹¬çš„VLAå’Œä¸–ç•Œæ¨¡å‹ï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13593",
            "title": "O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
            "url": "https://huggingface.co/papers/2511.13593",
            "abstract": "O-Mem, an active user profiling memory framework, enhances contextual consistency and dynamic personalization in LLM-powered agents, improving performance on benchmarks and response efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "491a11d6cfd4e4ab",
            "authors": [
                "Piaohong Wang",
                "Motong Tian",
                "Jiaxian Li",
                "Yuan Liang",
                "Yuqing Wang",
                "Qianben Chen",
                "Tiannan Wang",
                "Zhicong Lu",
                "Jiawei Ma",
                "Yuchen Eleanor Jiang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "oppo.com"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13593.jpg",
            "data": {
                "categories": [
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ Ğ˜Ğ˜",
                    "desc": "O-Mem â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹. O-Mem Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LoCoMo (51.67%) Ğ¸ PERSONAMEM (62.99%), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ state-of-the-art Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 3-3.5%. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "O-Mem: Enhancing AI Personalization Through Active User Profiling",
                    "desc": "O-Mem is a new memory framework designed to improve how AI agents interact with users by focusing on active user profiling. It addresses issues of contextual consistency and personalization by dynamically updating user information based on their interactions. Unlike traditional memory systems that may miss important details, O-Mem retrieves relevant user attributes and context in a hierarchical manner. This leads to more coherent and personalized responses, achieving significant performance improvements on benchmark tests compared to previous models."
                },
                "zh": {
                    "title": "O-Memï¼šæå‡ä¸ªæ€§åŒ–å“åº”çš„æ™ºèƒ½è®°å¿†æ¡†æ¶",
                    "desc": "O-Memæ˜¯ä¸€ç§åŸºäºä¸»åŠ¨ç”¨æˆ·ç”»åƒçš„è®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€æå–å’Œæ›´æ–°ç”¨æˆ·ç‰¹å¾åŠäº‹ä»¶è®°å½•ï¼Œæ”¯æŒå±‚æ¬¡åŒ–çš„ä¸ªæ€§å±æ€§å’Œä¸»é¢˜ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œä»è€Œå®ç°æ›´é€‚åº”å’Œè¿è´¯çš„ä¸ªæ€§åŒ–å“åº”ã€‚O-Memåœ¨å…¬å…±LoCoMoåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†51.67%çš„æˆç»©ï¼Œæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯LangMemæé«˜äº†è¿‘3%ã€‚æ­¤å¤–ï¼ŒO-Memåœ¨å“åº”æ—¶é—´æ•ˆç‡ä¸Šä¹Ÿä¼˜äºä¹‹å‰çš„è®°å¿†æ¡†æ¶ï¼Œä¸ºæœªæ¥å¼€å‘é«˜æ•ˆä¸”ç±»äººåŒ–çš„ä¸ªæ€§åŒ–AIåŠ©æ‰‹å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17490",
            "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
            "url": "https://huggingface.co/papers/2511.17490",
            "abstract": "Video-R4, a video reasoning LMM, uses iterative visual rumination to improve text-rich video QA by selecting, zooming, and re-encoding frames, achieving state-of-the-art results on various QA tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "708e158d9291bf36",
            "authors": [
                "Yolo Y. Tang",
                "Daiki Shimada",
                "Hang Hua",
                "Chao Huang",
                "Jing Bi",
                "Rogerio Feris",
                "Chenliang Xu"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "Sony Group Corporation",
                "University of Rochester"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17490.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#video",
                    "#dataset",
                    "#hallucinations",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Video-R4 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ ÑĞ²Ğ¾Ñ‘ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ reinforcement learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Iterative Visual Rumination for Enhanced Video QA",
                    "desc": "Video-R4 is a video reasoning language model that enhances question answering (QA) for text-rich videos by using a technique called visual rumination. This method involves iteratively selecting and zooming into important frames, allowing the model to re-encode visual information and refine its understanding. Unlike traditional models that analyze fixed frames in a single pass, Video-R4 mimics human behavior by revisiting and inspecting critical visual cues, which helps reduce errors and improve accuracy. The model has been trained on specialized datasets and achieves top performance on various QA tasks, showcasing the effectiveness of its iterative approach to multimodal reasoning."
                },
                "zh": {
                    "title": "è¿­ä»£è§†è§‰åæ€ï¼Œæå‡è§†é¢‘é—®ç­”èƒ½åŠ›",
                    "desc": "Video-R4æ˜¯ä¸€ç§è§†é¢‘æ¨ç†çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨è¿­ä»£è§†è§‰åæ€çš„æ–¹æ³•æ¥æå‡æ–‡æœ¬ä¸°å¯Œè§†é¢‘çš„é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡é€‰æ‹©ã€æ”¾å¤§å’Œé‡æ–°ç¼–ç è§†é¢‘å¸§ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†é¢‘é—®ç­”æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦è¯æ®æ—¶çš„ä¸è¶³ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒã€‚Video-R4åœ¨å¤šä¸ªé—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†è¿­ä»£åæ€åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16825",
            "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds",
            "url": "https://huggingface.co/papers/2511.16825",
            "abstract": "WorldGen converts text prompts into interactive 3D environments using LLM-driven reasoning, procedural generation, diffusion-based 3D generation, and object-aware decomposition, enabling creators to build coherent, navigable worlds efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "507477860f14b56d",
            "authors": [
                "Dilin Wang",
                "Hyunyoung Jung",
                "Tom Monnier",
                "Kihyuk Sohn",
                "Chuhang Zou",
                "Xiaoyu Xiang",
                "Yu-Ying Yeh",
                "Di Liu",
                "Zixuan Huang",
                "Thu Nguyen-Phuoc",
                "Yuchen Fan",
                "Sergiu Oprea",
                "Ziyan Wang",
                "Roman Shapovalov",
                "Nikolaos Sarafianos",
                "Thibault Groueix",
                "Antoine Toisoul",
                "Prithviraj Dhar",
                "Xiao Chu",
                "Minghao Chen",
                "Geon Yeong Park",
                "Mahima Gupta",
                "Yassir Azziz",
                "Rakesh Ranjan",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Reality Labs, Meta"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16825.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#3d",
                    "#games",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»Ğ¾Ğ²Ğ° Ğº Ğ¼Ğ¸Ñ€Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "WorldGen â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ€Ğ°Ğ·Ñƒ Ğ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ñ…. ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¼, Ğ´ĞµĞ»Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Transforming Text into Interactive 3D Worlds",
                    "desc": "WorldGen is a system that automatically creates interactive 3D environments from text prompts. It uses large language models (LLMs) to understand scene layouts and combines this with procedural generation and diffusion techniques to create detailed 3D worlds. The system allows creators to easily design and navigate these environments without needing advanced 3D modeling skills. By providing modular control over various aspects of the world, WorldGen makes it easier to generate rich, coherent virtual spaces for gaming and simulations."
                },
                "zh": {
                    "title": "æ–‡æœ¬åˆ°3Dä¸–ç•Œçš„è‡ªåŠ¨ç”Ÿæˆ",
                    "desc": "WorldGen æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºè‡ªåŠ¨åˆ›å»ºå¤§è§„æ¨¡çš„äº’åŠ¨ 3D ä¸–ç•Œã€‚å®ƒå°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºå¯æ¢ç´¢çš„ã€å®Œå…¨çº¹ç†åŒ–çš„ç¯å¢ƒï¼Œç”¨æˆ·å¯ä»¥åœ¨æ ‡å‡†æ¸¸æˆå¼•æ“ä¸­ç«‹å³ç¼–è¾‘è¿™äº›ç¯å¢ƒã€‚é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åœºæ™¯å¸ƒå±€æ¨ç†ã€ç¨‹åºç”Ÿæˆã€åŸºäºæ‰©æ•£çš„ 3D ç”Ÿæˆå’Œå¯¹è±¡æ„ŸçŸ¥çš„åœºæ™¯åˆ†è§£ï¼ŒWorldGen ä½¿åˆ›ä½œè€…èƒ½å¤Ÿé«˜æ•ˆåœ°è®¾è®¡è¿è´¯ä¸”å¯å¯¼èˆªçš„è™šæ‹Ÿä¸–ç•Œã€‚è¯¥ç³»ç»Ÿæ¨¡å—åŒ–ï¼Œæ”¯æŒå¯¹å¸ƒå±€ã€è§„æ¨¡å’Œé£æ ¼çš„ç²¾ç»†æ§åˆ¶ï¼Œç”Ÿæˆå‡ ä½•ä¸€è‡´ã€è§†è§‰ä¸°å¯Œä¸”å®æ—¶æ¸²æŸ“é«˜æ•ˆçš„ä¸–ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17220",
            "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
            "url": "https://huggingface.co/papers/2511.17220",
            "abstract": "PARROT evaluates the robustness of large language models against social pressure and sycophancy, revealing significant variability in model behavior and confidence shifts across different domains and authority templates.  \t\t\t\t\tAI-generated summary \t\t\t\t This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" (leq 11%, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "c08f5f1f2e0dcc47",
            "authors": [
                "Yusuf Ã‡elebi",
                "Ã–zay Ezerceli",
                "Mahmoud El Hussieni"
            ],
            "affiliations": [
                "NewMind AI, Istanbul, Turkey"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17220.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#security"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° LLM Ğ¿Ğ¾Ğ´Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ²: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑĞ¸ĞºĞ¾Ñ„Ğ°Ğ½Ñ‚ÑÑ‚Ğ²Ğ° Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PARROT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸ĞºĞ¾Ñ„Ğ°Ğ½Ñ‚ÑÑ‚Ğ²Ñƒ (Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ»ĞµÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ¿Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ¾ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-5, Claude Sonnet) ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ĞºÑ€Ğ°Ñ…Ğ° ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ€Ğ°Ğ²Ğ½Ğµ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Evaluating Language Models: Resisting Social Pressure for Better Accuracy",
                    "desc": "The paper introduces PARROT, a framework that assesses how large language models (LLMs) respond to social pressure and sycophancy, which is the tendency to conform excessively. It evaluates the accuracy degradation of LLMs when faced with authoritative but incorrect information by comparing neutral and biased prompts. The study employs a double-blind evaluation method and tracks confidence shifts in model responses using log-likelihood calibration. Results indicate that while advanced models maintain accuracy under pressure, older models often exhibit significant epistemic collapse, highlighting the need for models to resist overfitting to social cues as a key objective."
                },
                "zh": {
                    "title": "æŠµæŠ—ç¤¾ä¼šå‹åŠ›ï¼Œæå‡æ¨¡å‹é²æ£’æ€§",
                    "desc": "PARROTï¼ˆè¯´æœä¸ä¸€è‡´æ€§é²æ£’æ€§è¾“å‡ºçœŸç›¸è¯„åˆ†ï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šå‹åŠ›ä¸‹çš„é²æ£’æ€§æ¡†æ¶ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå’Œæƒå¨æ¨¡æ¿ä¸‹çš„è¡Œä¸ºå·®å¼‚å’Œä¿¡å¿ƒå˜åŒ–ã€‚é€šè¿‡æ¯”è¾ƒä¸­ç«‹é—®é¢˜ä¸æƒå¨æ€§é”™è¯¯ç‰ˆæœ¬ï¼ŒPARROTèƒ½å¤Ÿé‡åŒ–æ¨¡å‹åœ¨é¢å¯¹ç¤¾ä¼šå‹åŠ›æ—¶çš„å‡†ç¡®æ€§ä¸‹é™ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå…ˆè¿›æ¨¡å‹çš„è·Ÿéšç‡è¾ƒä½ï¼Œè€Œè¾ƒæ—§æˆ–è¾ƒå°çš„æ¨¡å‹åˆ™è¡¨ç°å‡ºä¸¥é‡çš„çŸ¥è¯†å´©æºƒï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å…³æ³¨æŠµæŠ—è¿‡æ‹Ÿåˆå‹åŠ›çš„ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17344",
            "title": "Loomis Painter: Reconstructing the Painting Process",
            "url": "https://huggingface.co/papers/2511.17344",
            "abstract": "A unified framework using diffusion models with semantic control and cross-medium style augmentation generates consistent and high-fidelity multi-media painting processes, supported by a comprehensive dataset and evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "221b2271cf76e366",
            "authors": [
                "Markus Pobitzer",
                "Chang Liu",
                "Chenyi Zhuang",
                "Teng Long",
                "Bin Ren",
                "Nicu Sebe"
            ],
            "affiliations": [
                "University of Pisa",
                "University of Trento"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17344.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾ĞºĞ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñæ¡†æ¶ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ´Ğ¸ÑƒĞ¼Ğ¾Ğ², Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞºÑ€Ğ¾ÑÑ-Ğ¼ĞµĞ´Ğ¸Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºÑƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº LPIPS, DINO, CLIP Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğ³Ğ¾ Perceptual Distance Profile, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°."
                },
                "en": {
                    "title": "Revolutionizing Artistic Creation with Diffusion Models",
                    "desc": "This paper presents a unified framework that utilizes diffusion models to enhance the generation of multi-media painting processes with semantic control. It addresses the limitations of existing generative models by ensuring consistency and fidelity across different artistic styles and media. The framework incorporates a reverse-painting training strategy and a comprehensive dataset to evaluate the quality of generated processes. Additionally, it introduces a Perceptual Distance Profile (PDP) curve to model the creative sequence, reflecting the natural progression of human artistry."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶ï¼šæå‡å¤šåª’ä½“ç»˜ç”»ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸ä¿çœŸåº¦",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šåª’ä½“ç»˜ç”»è¿‡ç¨‹çš„ç”Ÿæˆï¼Œå¹¶ç»“åˆè¯­ä¹‰æ§åˆ¶å’Œè·¨åª’ä»‹é£æ ¼å¢å¼ºã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤šç§åª’ä»‹åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ï¼Œå®ç°äº†ä¸€è‡´çš„çº¹ç†æ¼”å˜å’Œé£æ ¼è½¬ç§»ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„çœŸå®ç»˜ç”»è¿‡ç¨‹æ•°æ®é›†ï¼Œå¹¶åœ¨è·¨åª’ä»‹ä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œæœ€ç»ˆå›¾åƒä¿çœŸåº¦ç­‰æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥è·ç¦»æ›²çº¿(PDP)å®šé‡å»ºæ¨¡äº†åˆ›ä½œåºåˆ—ï¼Œåæ˜ äº†äººç±»è‰ºæœ¯åˆ›ä½œçš„è¿›ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.11007",
            "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.11007",
            "abstract": "VisMem enhances Vision-Language Models by incorporating dynamic latent vision memories, improving performance on complex visual tasks through perceptual fidelity and semantic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "31ad8a9eddd1753a",
            "authors": [
                "Xinlei Yu",
                "Chengming Xu",
                "Guibin Zhang",
                "Zhangquan Chen",
                "Yudong Zhang",
                "Yongbo He",
                "Peng-Tao Jiang",
                "Jiangning Zhang",
                "Xiaobin Hu",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Zhejiang University",
                "vivo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.11007.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Vision-Language Models",
                    "desc": "VisMem â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Vision-Language Models Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 11.8% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Dynamic Memory",
                    "desc": "The paper introduces VisMem, a new framework designed to enhance Vision-Language Models (VLMs) by integrating dynamic latent vision memories. This approach addresses the visual processing bottleneck that VLMs face, which often leads to a loss of visual grounding and contextual understanding during complex tasks. By mimicking human memory, VisMem incorporates both short-term perceptual retention and long-term semantic consolidation, improving the model's ability to maintain accuracy and coherence in visual reasoning. Experimental results show that VisMem significantly boosts performance by an average of 11.8% compared to traditional models, setting a new standard for memory enhancement in machine learning."
                },
                "zh": {
                    "title": "åŠ¨æ€æ½œåœ¨è§†è§‰è®°å¿†ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°",
                    "desc": "VisMemæ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŠ¨æ€æ½œåœ¨è§†è§‰è®°å¿†æ¥æå‡å…¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶ç»“åˆäº†çŸ­æœŸè§†è§‰è®°å¿†å’Œé•¿æœŸè¯­ä¹‰è®°å¿†ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒæ„ŸçŸ¥çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒVisMemåœ¨ç†è§£ã€æ¨ç†å’Œç”Ÿæˆç­‰å¤šç§è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æå‡äº†11.8%çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæ½œåœ¨ç©ºé—´è®°å¿†å¢å¼ºå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16175",
            "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
            "url": "https://huggingface.co/papers/2511.16175",
            "abstract": "Mantis, a VLA framework with Disentangled Visual Foresight and a diffusion Transformer, improves action prediction, comprehension, and reasoning while reducing training complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms Ï€_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "f1963cb80b84fef7",
            "authors": [
                "Yi Yang",
                "Xueqi Li",
                "Yiyang Chen",
                "Jin Song",
                "Yihan Wang",
                "Zipeng Xiao",
                "Jiadi Su",
                "You Qiaoben",
                "Pengfei Liu",
                "Zhijie Deng"
            ],
            "affiliations": [
                "BOSCH",
                "FDU",
                "NJUPT",
                "SII",
                "SJTU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16175.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#robotics",
                    "#video",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Mantis â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (DVF) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ (DiT) Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mantis Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 96.7% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Mantis: Simplifying VLA with Disentangled Visual Foresight",
                    "desc": "Mantis is a new framework designed to enhance Vision-Language-Action (VLA) models by improving action prediction and reasoning while simplifying training. It introduces Disentangled Visual Foresight (DVF), which separates visual foresight prediction from the main model, allowing for better learning of actions without overwhelming the model's capacity. By using a diffusion Transformer (DiT) and meta queries, Mantis effectively captures latent actions and improves comprehension through language supervision. The framework has shown impressive results, achieving a 96.7% success rate on the LIBERO benchmark and outperforming existing models in real-world tasks."
                },
                "zh": {
                    "title": "Mantisï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "Mantisæ˜¯ä¸€ä¸ªæ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨äº†è§£è€¦çš„è§†è§‰å‰ç»ï¼ˆDVFï¼‰å’Œæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œæ—¨åœ¨æé«˜åŠ¨ä½œé¢„æµ‹ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒå¤æ‚æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è§†è§‰å‰ç»é¢„æµ‹ä¸ä¸»å¹²ç½‘ç»œè§£è€¦ï¼Œåˆ©ç”¨å…ƒæŸ¥è¯¢å’ŒDiTå¤´éƒ¨çš„ç»„åˆï¼Œæ¥æœ‰æ•ˆæ•æ‰æ½œåœ¨åŠ¨ä½œï¼Œä»è€Œå¢å¼ºæ˜¾å¼åŠ¨ä½œçš„å­¦ä¹ ã€‚Mantisåœ¨è¯­è¨€ç›‘ç£çš„å¸®åŠ©ä¸‹ï¼Œä¿æŒäº†è‰¯å¥½çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚ç»è¿‡åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒå’Œå¾®è°ƒï¼ŒMantisåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†96.7%çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†è®¸å¤šå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14899",
            "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
            "url": "https://huggingface.co/papers/2511.14899",
            "abstract": "A framework called InstructMix2Mix uses a 2D diffusion model to improve multi-view image editing by leveraging a 3D prior for cross-view consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "4e881e5d1cb67be2",
            "authors": [
                "Daniel Gilo",
                "Or Litany"
            ],
            "affiliations": [
                "NVIDIA",
                "Technion Israel Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14899.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "InstructMix2Mix â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ 3D Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ³Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Score Distillation Sampling Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸: Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ scheduler ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Multi-View Image Editing with InstructMix2Mix",
                    "desc": "InstructMix2Mix (I-Mix2Mix) is a framework designed to enhance multi-view image editing by utilizing a 2D diffusion model alongside a 3D prior for better consistency across different viewpoints. The method addresses challenges faced by existing techniques, which often lead to artifacts and inconsistencies when editing images from sparse input views. By integrating a multi-view diffusion model into the Score Distillation Sampling process, I-Mix2Mix introduces innovative strategies such as incremental updates and a specialized noise scheduler to improve coherence. Experimental results show that this approach not only boosts cross-view consistency but also preserves high-quality edits for each individual frame."
                },
                "zh": {
                    "title": "æå‡å¤šè§†è§’ä¸€è‡´æ€§çš„å›¾åƒç¼–è¾‘æ–°æ¡†æ¶",
                    "desc": "InstructMix2Mixï¼ˆI-Mix2Mixï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨3Då…ˆéªŒçŸ¥è¯†æ¥æ”¹å–„å¤šè§†è§’å›¾åƒç¼–è¾‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨2Dæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç¨€ç–è¾“å…¥è§†å›¾çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ä¿®æ”¹åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒå„è§†è§’ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒI-Mix2Mixé€šè¿‡å¼•å…¥å¤šè§†è§’æ‰©æ•£å­¦ç”Ÿï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„ç¥ç»åœºæ•´åˆå™¨ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†ä¼ªå½±å’Œä¸ä¸€è‡´çš„ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒI-Mix2Mixåœ¨æé«˜å¤šè§†è§’ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒæ¯å¸§ç¼–è¾‘çš„é«˜è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17487",
            "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
            "url": "https://huggingface.co/papers/2511.17487",
            "abstract": "Reducing the capacity of large language models disproportionately impacts visual capabilities in multimodal systems, but visual extraction tuning combined with step-by-step reasoning improves efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "d728f1bb76fff964",
            "authors": [
                "Mark Endo",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17487.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#cv",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ â€” ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ LLM Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑ‘ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Extract+Think, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Extract+Think: Enhancing Visual Reasoning in Smaller Models",
                    "desc": "This paper investigates how reducing the size of large language models (LLMs) affects their ability to understand and reason about visual information in multimodal systems. The authors find that downscaling LLMs leads to a significant decline in visual capabilities, which is more pronounced than the decline in reasoning abilities. To mitigate this issue, they propose a method called visual extraction tuning, which helps the model focus on relevant visual details for better performance. By combining this tuning with step-by-step reasoning, they introduce the Extract+Think approach, enhancing both efficiency and effectiveness in multimodal tasks."
                },
                "zh": {
                    "title": "è§†è§‰èƒ½åŠ›æå‡çš„æ–°æ–¹æ³•ï¼šExtract+Think",
                    "desc": "åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­ï¼Œé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¼šå¯¹è§†è§‰èƒ½åŠ›äº§ç”Ÿä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶åˆ†æäº†åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ç¼©å°æ™ºèƒ½çš„å½±å“ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¸‹é™ä¸»è¦å½±å“è§†è§‰èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ç»§æ‰¿è‡ªè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰æå–è°ƒä¼˜çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¸€è‡´æ€§åœ°è®­ç»ƒæ¨¡å‹æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚ç»“åˆé€æ­¥æ¨ç†ï¼Œæˆ‘ä»¬çš„Extract+Thinkæ–¹æ³•åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14806",
            "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
            "url": "https://huggingface.co/papers/2511.14806",
            "abstract": "MergeDNA uses a hierarchical architecture with Token Merging and latent Transformers to model genomic sequences, achieving superior performance on DNA benchmarks and multi-omics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "034072ae3093ed4e",
            "authors": [
                "Siyuan Li",
                "Kai Yu",
                "Anna Wang",
                "Zicheng Liu",
                "Chang Yu",
                "Jingbo Zhou",
                "Qirong Yang",
                "Yucheng Guo",
                "Xiaoming Zhang",
                "Stan Z. Li"
            ],
            "affiliations": [
                "AI Lab, Research Center for Industries of the Future, Westlake University, China",
                "BioMap Research, Beijing, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14806.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "MergeDNA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Token Merging Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ”ĞĞš. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ merging, Ğ¸ Latent Encoder Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ”ĞĞš Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ¸ĞºÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ”ĞĞš."
                },
                "en": {
                    "title": "Revolutionizing Genomic Modeling with MergeDNA",
                    "desc": "MergeDNA presents a novel hierarchical architecture that effectively models genomic sequences by addressing the challenges of varying information density and undefined vocabulary units. It utilizes Token Merging techniques to create a dynamic genomic tokenizer that adapts to the complexities of DNA data. The architecture incorporates latent Transformers for capturing global context, enhancing the model's ability to learn from genomic sequences. Experimental results demonstrate that MergeDNA outperforms existing methods on DNA benchmarks and multi-omics tasks, showcasing its effectiveness in genomic modeling."
                },
                "zh": {
                    "title": "MergeDNAï¼šåŸºå› ç»„åºåˆ—å»ºæ¨¡çš„æ–°çªç ´",
                    "desc": "MergeDNAæ˜¯ä¸€ç§ä½¿ç”¨åˆ†å±‚æ¶æ„çš„æ¨¡å‹ï¼Œç»“åˆäº†Token Mergingå’Œæ½œåœ¨å˜æ¢å™¨æ¥å¤„ç†åŸºå› ç»„åºåˆ—ã€‚è¯¥æ–¹æ³•è§£å†³äº†åŸºå› ç»„åºåˆ—ä¸­ä¿¡æ¯å¯†åº¦å˜åŒ–å’Œç¼ºä¹æ˜ç¡®æœ€å°è¯æ±‡å•å…ƒçš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€åŸºå› ç»„æ ‡è®°å™¨å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é¢„è®­ç»ƒä»»åŠ¡ï¼ŒMergeDNAèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–æ ‡è®°å’Œæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeDNAåœ¨å¤šä¸ªDNAåŸºå‡†æµ‹è¯•å’Œå¤šç»„å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ ‡è®°æ–¹æ³•å’Œå¤§å‹DNAåŸºç¡€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17199",
            "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
            "url": "https://huggingface.co/papers/2511.17199",
            "abstract": "VLA-4D enhances robotic manipulation by integrating 4D spatial-temporal awareness into visual and action representations, achieved through cross-attention and temporal extension.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "68b33e91457dd995",
            "authors": [
                "Hanyu Zhou",
                "Chuanhao Ma",
                "Gim Hee Lee"
            ],
            "affiliations": [
                "School of Artificial Intelligence and Automation, Huazhong University of Science and Technology",
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17199.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#architecture",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ§ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸",
                    "desc": "VLA-4D â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ (Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ 4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Robots with 4D Spatial-Temporal Awareness for Smooth Manipulation",
                    "desc": "The paper introduces VLA-4D, a novel model that enhances robotic manipulation by incorporating 4D spatial-temporal awareness into visual and action representations. It addresses the limitations of existing vision-language-action (VLA) models, which often struggle with maintaining coherence in time during manipulation tasks. By utilizing a cross-attention mechanism, the model integrates time into visual features, creating a unified representation that captures both spatial and temporal dimensions. Additionally, it extends action representations to include temporal information, allowing for better planning and execution of actions in a coherent manner."
                },
                "zh": {
                    "title": "å››ç»´æ„è¯†æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›",
                    "desc": "VLA-4Dæ¨¡å‹é€šè¿‡å°†å››ç»´æ—¶ç©ºæ„è¯†æ•´åˆåˆ°è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºä¸­ï¼Œå¢å¼ºäº†æœºå™¨äººæ“ä½œçš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†ä¸€ç»´æ—¶é—´åµŒå…¥ä¸‰ç»´ä½ç½®ï¼Œå½¢æˆç»Ÿä¸€çš„å››ç»´è§†è§‰è¡¨ç¤ºã€‚é€šè¿‡æ‰©å±•ä¼ ç»Ÿçš„ç©ºé—´åŠ¨ä½œè¡¨ç¤ºï¼ŒåŠ å…¥æ—¶é—´ä¿¡æ¯ï¼Œå®ç°æ—¶ç©ºè§„åˆ’ï¼Œä½¿å¾—æœºå™¨äººæ“ä½œåœ¨ç©ºé—´ä¸Šå¹³æ»‘ä¸”æ—¶é—´ä¸Šè¿è´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLA-4Dåœ¨ä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17074",
            "title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
            "url": "https://huggingface.co/papers/2511.17074",
            "abstract": "DiverseVAR enhances generative diversity in Visual Autoregressive models by modifying the pivotal component of feature maps without additional training, improving synthesis quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "5f944615180ce131",
            "authors": [
                "Tong Wang",
                "Guanyu Yang",
                "Nian Liu",
                "Kai Wang",
                "Yaxing Wang",
                "Abdelrahman M Shaker",
                "Salman Khan",
                "Fahad Shahbaz Khan",
                "Senmao Li"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "MBZUAI",
                "Nankai University",
                "Southeast University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17074.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DiverseVAR Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ‚Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ€Ñ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ¸ ĞµĞ³Ğ¾ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiverseVAR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unlocking Diversity in Visual Autoregressive Models",
                    "desc": "DiverseVAR is a method designed to improve the diversity of outputs in Visual Autoregressive (VAR) models, which are known for their efficient image generation. The paper identifies a crucial part of the feature map that affects the variability of generated images and proposes a way to modify this component without needing extra training. By adjusting the pivotal component in the input and output of the model, DiverseVAR enhances the model's ability to produce varied and high-quality images. The results show that this approach significantly increases generative diversity while maintaining the overall performance of the model."
                },
                "zh": {
                    "title": "DiverseVARï¼šæå‡è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§",
                    "desc": "DiverseVARæ˜¯ä¸€ç§å¢å¼ºè§†è§‰è‡ªå›å½’æ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¿®æ”¹ç‰¹å¾å›¾çš„å…³é”®ç»„ä»¶ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œä»è€Œæé«˜åˆæˆè´¨é‡ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°æ¢å¤äº†è‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œè§£å†³äº†è¾“å‡ºå˜å¼‚æ€§é™ä½çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiverseVARæ˜¾è‘—æå‡äº†ç”Ÿæˆå¤šæ ·æ€§ï¼ŒåŒæ—¶å¯¹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16931",
            "title": "OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists",
            "url": "https://huggingface.co/papers/2511.16931",
            "abstract": "OmniScientist is a framework that simulates human scientific processes to automate and enhance AI-driven scientific research through structured knowledge systems, collaborative protocols, and evaluation platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as \"AI Scientists.\" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "ea9d862dc86187ee",
            "authors": [
                "Chenyang Shao",
                "Dehao Huang",
                "Yu Li",
                "Keyu Zhao",
                "Weiquan Lin",
                "Yining Zhang",
                "Qingbin Zeng",
                "Zhiyu Chen",
                "Tianxing Li",
                "Yifei Huang",
                "Taozhong Wu",
                "Xinyang Liu",
                "Ruotong Zhao",
                "Mengsheng Zhao",
                "Xuhua Zhang",
                "Yue Wang",
                "Yuanyi Zhen",
                "Fengli Xu",
                "Yong Li",
                "Tie-Yan Liu"
            ],
            "affiliations": [
                "Department of Electronic Engineering, BNRist, Tsinghua University",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16931.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹",
                    "desc": "OmniScientist â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… AI ÑƒÑ‡Ñ‘Ğ½Ñ‹Ñ…, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ScienceArena Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Elo Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Empowering AI with Human-Like Scientific Collaboration",
                    "desc": "OmniScientist is a new framework designed to improve AI-driven scientific research by mimicking human scientific processes. It integrates structured knowledge systems, collaborative protocols, and evaluation platforms to create a more realistic research environment. Unlike traditional AI Scientists that treat scientific discovery as isolated tasks, OmniScientist emphasizes the importance of collaboration and social interaction in research. This framework automates various stages of the research process while ensuring that AI agents can effectively engage with human researchers and the broader scientific community."
                },
                "zh": {
                    "title": "OmniScientistï¼šé‡å¡‘ç§‘å­¦ç ”ç©¶çš„AIæ¡†æ¶",
                    "desc": "OmniScientistæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ¨¡æ‹Ÿäººç±»ç§‘å­¦è¿‡ç¨‹ï¼Œä»¥è‡ªåŠ¨åŒ–å’Œå¢å¼ºåŸºäºAIçš„ç§‘å­¦ç ”ç©¶ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–çŸ¥è¯†ç³»ç»Ÿã€åä½œåè®®å’Œè¯„ä¼°å¹³å°ï¼Œè§£å†³äº†ç°æœ‰AIç§‘å­¦å®¶åœ¨ç§‘å­¦å‘ç°ä¸­å¿½è§†ç¤¾ä¼šåä½œçš„ä¸è¶³ã€‚OmniScientistå®ç°äº†ä»æ•°æ®åŸºç¡€åˆ°æ–‡çŒ®ç»¼è¿°ã€ç ”ç©¶æ„æ€ã€å®éªŒè‡ªåŠ¨åŒ–ã€ç§‘å­¦å†™ä½œå’ŒåŒè¡Œè¯„å®¡çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚è¯¥æ¡†æ¶è¿˜æä¾›äº†æ”¯æŒäººç±»ç§‘å­¦ç³»ç»Ÿçš„åŸºç¡€è®¾æ–½ï¼Œä¿ƒè¿›äº†AIä»£ç†ä¸äººç±»ç ”ç©¶è€…çš„æ·±åº¦åˆä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15462",
            "title": "Insights from the ICLR Peer Review and Rebuttal Process",
            "url": "https://huggingface.co/papers/2511.15462",
            "abstract": "The study analyzes the ICLR 2024 and 2025 peer review processes, focusing on score dynamics and reviewer interactions, using LLM-based text categorization to identify trends and factors influencing score changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "963e43812446bcb8",
            "authors": [
                "Amir Hossein Kargaran",
                "Nafiseh Nikeghbal",
                "Jing Yang",
                "Nedjma Ousidhoum"
            ],
            "affiliations": [
                "Cardiff University",
                "LMU Munich",
                "Munich Center for Machine Learning",
                "Paper Copilot",
                "Technical University of Munich",
                "University of Southern California"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15462.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#open_source"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "ĞšĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ… ICLR 2024 Ğ¸ 2025, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ ÑĞ¾Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²-Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ñ€ÑƒĞ³ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚."
                },
                "en": {
                    "title": "Enhancing Peer Review: Insights from ICLR 2024 and 2025",
                    "desc": "This study investigates the peer review processes of the ICLR conferences in 2024 and 2025, focusing on how review scores change before and after author rebuttals. It employs large language model (LLM) techniques to categorize review texts and analyze reviewer interactions, revealing patterns that influence score dynamics. The research highlights that initial scores and co-reviewer ratings significantly impact score changes, indicating the importance of reviewer influence. Ultimately, the findings provide actionable insights for authors on effective rebuttal strategies and suggest improvements for the overall peer review system."
                },
                "zh": {
                    "title": "æå‡åŒè¡Œè¯„å®¡æ•ˆç‡çš„å…³é”®æ´å¯Ÿ",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†ICLR 2024å’Œ2025çš„åŒè¡Œè¯„å®¡è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨è¯„åˆ†åŠ¨æ€å’Œè¯„å®¡è€…äº’åŠ¨ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼Œè¯†åˆ«å½±å“è¯„åˆ†å˜åŒ–çš„è¶‹åŠ¿å’Œå› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œåˆå§‹è¯„åˆ†å’Œå…±åŒè¯„å®¡è€…çš„è¯„åˆ†æ˜¯è¯„åˆ†å˜åŒ–çš„æœ€å¼ºé¢„æµ‹å› ç´ ï¼Œè¡¨æ˜è¯„å®¡è€…ä¹‹é—´å­˜åœ¨ä¸€å®šçš„å½±å“åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ”¹è¿›åŒè¡Œè¯„å®¡è¿‡ç¨‹æä¾›äº†åŸºäºè¯æ®çš„è§è§£ï¼ŒæŒ‡å¯¼ä½œè€…åˆ¶å®šæœ‰æ•ˆçš„åé©³ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17450",
            "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
            "url": "https://huggingface.co/papers/2511.17450",
            "abstract": "SketchVerify enhances video motion planning and generation by iteratively refining candidate motion plans using a lightweight sketch-based verification process, improving both quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "6395b90b88b6199e",
            "authors": [
                "Yidong Huang",
                "Zun Wang",
                "Han Lin",
                "Dong-Ki Kim",
                "Shayegan Omidshafiei",
                "Jaehong Yoon",
                "Yue Zhang",
                "Mohit Bansal"
            ],
            "affiliations": [
                "FieldAI",
                "Nanyang Technological University",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17450.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°",
                    "desc": "SketchVerify â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞºĞ¸Ğ·Ğ¾Ğ² â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ½Ğ°Ğ±Ñ€Ğ¾ÑĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Refining Motion Plans for Better Video Generation",
                    "desc": "SketchVerify is a novel framework that enhances video motion planning by refining candidate motion plans through a sketch-based verification process. It addresses the limitations of existing methods that either rely on simple single-shot plans or require costly iterative refinements. By predicting multiple motion plans and evaluating them for semantic alignment and physical plausibility, SketchVerify ensures more coherent and realistic motion trajectories. This approach not only improves the quality of generated videos but also significantly reduces computational costs compared to traditional methods."
                },
                "zh": {
                    "title": "SketchVerifyï¼šé«˜æ•ˆçš„è§†é¢‘è¿åŠ¨è§„åˆ’ä¸ç”Ÿæˆ",
                    "desc": "SketchVerify æ˜¯ä¸€ç§åŸºäºè‰å›¾éªŒè¯çš„è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆä¸­çš„è¿åŠ¨è§„åˆ’è´¨é‡å’Œæ•ˆç‡ã€‚å®ƒé€šè¿‡å¼•å…¥æµ‹è¯•æ—¶çš„é‡‡æ ·å’ŒéªŒè¯å¾ªç¯ï¼Œç”Ÿæˆæ›´å…·åŠ¨æ€ä¸€è‡´æ€§çš„è½¨è¿¹ï¼Œç¡®ä¿è¿åŠ¨çš„ç‰©ç†åˆç†æ€§å’ŒæŒ‡ä»¤ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç»™å®šæç¤ºå’Œå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å¤šä¸ªå€™é€‰è¿åŠ¨è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨è§†è§‰-è¯­è¨€éªŒè¯å™¨å¯¹å…¶è¿›è¡Œè¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSketchVerify åœ¨è¿åŠ¨è´¨é‡ã€ç‰©ç†çœŸå®æ„Ÿå’Œé•¿æœŸä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15299",
            "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
            "url": "https://huggingface.co/papers/2511.15299",
            "abstract": "A one-stage text-to-image synthesis pipeline for X-ray security images improves efficiency and quality without extra labor cost, enhancing prohibited item detection performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "b66621108f2d87af",
            "authors": [
                "Jialong Sun",
                "Hongguang Zhu",
                "Weizhe Liu",
                "Yunda Sun",
                "Renshuai Tao",
                "Yunchao Wei"
            ],
            "affiliations": [
                "Faculty of Data Science, City University of Macau",
                "Institute of Information Science, Beijing Jiaotong University",
                "Nuctech Company Limited"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15299.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image generation. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Cross-Attention Refinement Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Background Occlusion Modeling Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Xsyn Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ÑƒÑ‚Ğ¾Ğ¼Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ² Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 1.2% mAP Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient X-ray Image Synthesis for Enhanced Detection",
                    "desc": "This paper presents a novel one-stage pipeline for synthesizing X-ray security images using text-to-image generation, which enhances both efficiency and quality. The proposed method, called Xsyn, eliminates the need for labor-intensive foreground extraction by integrating two innovative strategies: Cross-Attention Refinement (CAR) and Background Occlusion Modeling (BOM). CAR improves bounding box annotations by utilizing cross-attention maps from a diffusion model, while BOM enhances the complexity of the background in the generated images. Experimental results show that Xsyn achieves a 1.2% improvement in mean Average Precision (mAP) over previous methods, significantly boosting the performance of prohibited item detection without incurring additional labor costs."
                },
                "zh": {
                    "title": "ä¸€é˜¶æ®µXå°„çº¿å›¾åƒåˆæˆï¼Œæå‡æ£€æµ‹æ•ˆç‡ä¸è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸€é˜¶æ®µçš„Xå°„çº¿å®‰å…¨å›¾åƒåˆæˆç®¡é“ï¼ˆXsynï¼‰ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œå›¾åƒè´¨é‡ï¼ŒåŒæ—¶é™ä½é¢å¤–çš„äººå·¥æˆæœ¬ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆæˆæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µéœ€è¦è¿›è¡ŒåŠ³åŠ¨å¯†é›†å‹çš„å‰æ™¯æå–ï¼Œç¬¬äºŒé˜¶æ®µå†è¿›è¡Œå›¾åƒåˆæˆï¼Œè¿™æ ·æ•ˆç‡ä½ä¸‹ä¸”å¢åŠ äº†äººå·¥æˆæœ¬ã€‚Xsyné€šè¿‡æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›ç²¾ç‚¼ï¼ˆCARï¼‰å’ŒèƒŒæ™¯é®æŒ¡å»ºæ¨¡ï¼ˆBOMï¼‰ç­–ç•¥ï¼Œä¼˜åŒ–äº†åˆæˆå›¾åƒçš„å¯ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXsynåœ¨å¤šä¸ªXå°„çº¿å®‰å…¨æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šå‡å®ç°äº†1.2%çš„mAPæå‡ï¼Œæ˜¾è‘—æ”¹å–„äº†ç¦å“æ£€æµ‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17127",
            "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
            "url": "https://huggingface.co/papers/2511.17127",
            "abstract": "A large-scale mixture-of-experts (MoE) pretraining study on AMD hardware provides comprehensive guidance for system and model design, demonstrating competitive performance with leading models.  \t\t\t\t\tAI-generated summary \t\t\t\t We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "dabe886bb856f5ce",
            "authors": [
                "Quentin Anthony",
                "Yury Tokpanov",
                "Skyler Szot",
                "Srivatsan Rajagopal",
                "Praneeth Medepalli",
                "Anna Golubeva",
                "Vasu Shyam",
                "Robert Washbourne",
                "Rishi Iyer",
                "Ansh Chaurasia",
                "Tomas Figliolia",
                "Xiao Yang",
                "Abhinav Sarje",
                "Drew Thorstensen",
                "Amartey Pearson",
                "Zack Grossbart",
                "Jason van Patten",
                "Emad Barsoum",
                "Zhenyu Gu",
                "Yao Fu",
                "Beren Millidge"
            ],
            "affiliations": [
                "AMD",
                "IBM",
                "Zyphra"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17127.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MoE Ğ½Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ AMD ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ AMD, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPU MI300X Ñ Ğ¸Ğ½Ñ‚ĞµÑ€ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ğ¼ Pollara. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ (Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°, ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸), Ñ‚Ğ°Ğº Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ MoE). ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ZAYA1 Ñ 760Ğœ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Qwen3-4B Ğ¸ Gemma3-12B, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Llama-3-8B Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾, ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµĞºĞ° AMD Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing MoE Pretraining with AMD Hardware",
                    "desc": "This paper presents a large-scale study on mixture-of-experts (MoE) pretraining using AMD hardware, specifically the MI300X GPUs. It offers detailed insights into system and model design, including microbenchmarks for core collective operations and memory bandwidth, which are crucial for optimizing performance. The authors introduce MI300X-aware transformer sizing rules and justify the configuration of MoE widths to enhance both training speed and inference efficiency. The resulting model, ZAYA1, shows competitive performance against leading models, indicating that AMD's technology is well-suited for advanced machine learning tasks."
                },
                "zh": {
                    "title": "AMDç¡¬ä»¶ä¸Šçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶é¢„è®­ç»ƒç ”ç©¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†åœ¨çº¯AMDç¡¬ä»¶ä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰é¢„è®­ç»ƒç ”ç©¶ã€‚ç ”ç©¶æä¾›äº†ç³»ç»Ÿå’Œæ¨¡å‹è®¾è®¡çš„å®ç”¨æŒ‡å¯¼ï¼Œå±•ç¤ºäº†ä¸é¢†å…ˆæ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„é›†ç¾¤å’Œç½‘ç»œç‰¹æ€§åˆ†æï¼Œå¹¶æä¾›äº†MI300Xå¾®åŸºå‡†æµ‹è¯•ï¼Œä»¥å¸®åŠ©æ¨¡å‹è®¾è®¡ã€‚æœ€ç»ˆï¼ŒZAYA1æ¨¡å‹åœ¨æ¨ç†å’Œè®­ç»ƒæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†AMDç¡¬ä»¶å’Œè½¯ä»¶å †æ ˆçš„æˆç†Ÿåº¦å’Œä¼˜åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16110",
            "title": "Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.16110",
            "abstract": "Multi-Faceted Attack (MFA) framework reveals vulnerabilities in VLMs by transferring adversarial attacks across models, achieving high success rates even against state-of-the-art defenses.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "4253fff7fc472c1c",
            "authors": [
                "Yijun Yang",
                "Lichao Wang",
                "Jianping Zhang",
                "Chi Harold Liu",
                "Lanqing Hong",
                "Qiang Xu"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16110.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#cv",
                    "#security"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ â€” Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ VLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Multi-Faceted Attack (MFA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Vision-Language Models Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ adversarial-Ğ°Ñ‚Ğ°Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” Attention-Transfer Attack (ATA) â€” ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ°Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ reward hacking. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ MFA Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² 52.8% Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 34%."
                },
                "en": {
                    "title": "Exposing Vulnerabilities in Vision-Language Models with MFA",
                    "desc": "The Multi-Faceted Attack (MFA) framework identifies weaknesses in Vision-Language Models (VLMs) by effectively transferring adversarial attacks across different models. It introduces the Attention-Transfer Attack (ATA), which cleverly embeds harmful instructions within tasks that have conflicting goals. This approach reveals that existing defenses, such as alignment tuning and content moderation, may not be as robust as believed, achieving a notable 58.5% success rate against these defenses. The findings emphasize the need for improved safety measures in VLMs, as the vulnerabilities exposed by MFA challenge the effectiveness of current protective strategies."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ¼æ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤šé¢æ”»å‡»ï¼ˆMFAï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„å®‰å…¨æ¼æ´ã€‚MFAé€šè¿‡è·¨æ¨¡å‹è½¬ç§»å¯¹æŠ—æ”»å‡»ï¼ŒæˆåŠŸç‡é«˜ï¼Œå³ä½¿é¢å¯¹æœ€å…ˆè¿›çš„é˜²å¾¡æªæ–½ä¹Ÿèƒ½æœ‰æ•ˆçªç ´ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯æ³¨æ„åŠ›è½¬ç§»æ”»å‡»ï¼ˆATAï¼‰ï¼Œèƒ½å¤Ÿå°†æœ‰å®³æŒ‡ä»¤éšè—åœ¨ç«äº‰ç›®æ ‡çš„å…ƒä»»åŠ¡ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–åçš„å¯¹æŠ—å›¾åƒå¯ä»¥å¹¿æ³›è½¬ç§»åˆ°æœªè§è¿‡çš„VLMsï¼Œæ˜¾ç¤ºå‡ºå…±äº«è§†è§‰è¡¨ç¤ºæ‰€å¸¦æ¥çš„è·¨æ¨¡å‹å®‰å…¨æ¼æ´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13081",
            "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
            "url": "https://huggingface.co/papers/2511.13081",
            "abstract": "The Reference-Frame Ã— Granularity (RFxG) taxonomy and novel faithfulness metrics improve the evaluation and alignment of saliency explanations with user intent in deep learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "ffd4e62e39610239",
            "authors": [
                "Yehonatan Elisha",
                "Seffi Cohen",
                "Oren Barkan",
                "Noam Koenigstein"
            ],
            "affiliations": [
                "Harvard University",
                "Tel Aviv University",
                "The Open University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13081.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#cv",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ RFxG: Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ RFxG Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ€Ñ‚ ÑĞ°Ğ»Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¾ÑÑĞ¼: Ñ‚Ğ¸Ğ¿ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹) Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğº Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Aligning Saliency Explanations with User Intent",
                    "desc": "This paper introduces the Reference-Frame Ã— Granularity (RFxG) taxonomy to enhance the evaluation of saliency explanations in deep learning. It categorizes explanations based on two axes: Reference-Frame, which differentiates between pointwise and contrastive explanations, and Granularity, which ranges from fine-grained to coarse-grained interpretations. The authors identify limitations in current evaluation metrics that focus too much on pointwise faithfulness, overlooking the importance of contrastive reasoning and semantic granularity. They propose four new faithfulness metrics to assess explanation quality across these dimensions, aiming to align saliency maps more closely with user intent and improve their practical utility."
                },
                "zh": {
                    "title": "æå‡æ·±åº¦å­¦ä¹ è§£é‡Šçš„ç”¨æˆ·æ„å›¾å¯¹é½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç§°ä¸ºå‚è€ƒæ¡†æ¶Ã—ç²’åº¦ï¼ˆRFxGï¼‰ï¼Œç”¨äºæ”¹è¿›æ·±åº¦å­¦ä¹ ä¸­æ˜¾è‘—æ€§è§£é‡Šçš„è¯„ä¼°å’Œç”¨æˆ·æ„å›¾çš„å¯¹é½ã€‚æ˜¾è‘—æ€§å›¾åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å¯¹äºå…¶ç›®çš„å’Œä¸ç”¨æˆ·æŸ¥è¯¢çš„å¯¹é½å­˜åœ¨å…±è¯†ç¼ºä¹çš„é—®é¢˜ã€‚RFxGæ¡†æ¶é€šè¿‡åŒºåˆ†ç‚¹å¯¹ç‚¹å’Œå¯¹æ¯”è§£é‡Šï¼Œä»¥åŠç»†ç²’åº¦å’Œç²—ç²’åº¦çš„è§£é‡Šï¼Œç³»ç»ŸåŒ–äº†æ˜¾è‘—æ€§è§£é‡Šçš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å››ç§æ–°çš„å¿ å®åº¦æŒ‡æ ‡ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°è¯„ä¼°ç°æœ‰æ˜¾è‘—æ€§æ–¹æ³•çš„è´¨é‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-21.html",
    "link_next": "2025-11-25.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 1,
        "#cv": 8,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 14,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 9,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 3,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 7,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 6,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}