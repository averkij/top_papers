
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. October 24.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">24 октября</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-23.html">⬅️ <span id="prev-date">23.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-27.html">➡️ <span id="next-date">27.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'};
        let feedDateNext = {'ru': '27.10', 'en': '10/27', 'zh': '10月27日'};
        let feedDatePrev = {'ru': '23.10', 'en': '10/23', 'zh': '10月23日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.19600', 'title': 'Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1', 'url': 'https://huggingface.co/papers/2510.19600', 'abstract': 'AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.  \t\t\t\t\tAI-generated summary \t\t\t\t In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author\'s vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.', 'score': 51, 'issue_id': 6591, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'dcfba991c39cb2ce', 'authors': ['Qianli Ma', 'Siyu Wang', 'Yilin Chen', 'Yinhao Tang', 'Yixiang Yang', 'Chang Guo', 'Bingjie Gao', 'Zhening Xing', 'Yanan Sun', 'Zhipeng Zhang'], 'affiliations': ['AutoLab, SAI, Shanghai Jiao Tong University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2510.19600.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#hallucinations', '#open_source', '#science', '#agents'], 'emoji': '🌐', 'ru': {'title': 'Автоматическое создание интерактивных научных веб-страниц с помощью мультиагентной системы', 'desc': 'AutoPage — это мультиагентная система, которая автоматизирует создание интерактивных веб-страниц для научных исследований. Система использует иерархический подход: от планирования структуры до генерации мультимодального контента и интерактивного рендеринга. Специальные агенты-проверщики верифицируют каждый этап работы, сравнивая результат с исходной статьей, что помогает бороться с галлюцинациями AI. Система создаёт качественные веб-страницы менее чем за 15 минут и стоимостью до $0.1, превращаясь из простого инструмента в полноценного ассистента для исследователей.'}, 'en': {'title': 'Transforming Research Communication with AutoPage', 'desc': "AutoPage is a multi-agent system designed to automate the creation of interactive research webpages, addressing the challenges researchers face in communicating their work. It employs a hierarchical process that breaks down the task into manageable steps, from narrative planning to content generation and rendering. To ensure accuracy and quality, specialized 'Checker' agents verify each stage against the original research paper, with optional human oversight for final adjustments. The system not only enhances efficiency, producing high-quality webpages in under 15 minutes, but also introduces PageBench, a benchmark for evaluating this new task."}, 'zh': {'title': '自动化研究网页创建的智能助手', 'desc': 'AutoPage是一个多智能体系统，旨在通过分层过程自动创建互动研究网页，从而确保高质量和高效率的结果。该系统将论文到网页的创建过程分解为从叙述规划到多模态内容生成和互动渲染的粗到细的管道。为了防止AI幻觉，专门的“检查器”代理会验证每一步与源论文的一致性，同时可选的人类检查点确保最终产品与作者的愿景完美对齐。通过构建PageBench基准，我们严格验证了该方法的有效性，实验表明AutoPage能够在15分钟内以低于0.1美元的成本生成高质量、视觉吸引力强的网页。'}}}, {'id': 'https://huggingface.co/papers/2510.19779', 'title': 'AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders', 'url': 'https://huggingface.co/papers/2510.19779', 'abstract': "AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.", 'score': 47, 'issue_id': 6592, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '15a4753734871a9f', 'authors': ['Yuezhou Hu', 'Jiaxin Guo', 'Xinyu Feng', 'Tuo Zhao'], 'affiliations': ['Georgia Institute of Technology', 'Tsinghua University', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2510.19779.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#alignment', '#inference', '#training'], 'emoji': '🎯', 'ru': {'title': 'Умная фильтрация токенов для быстрой генерации текста', 'desc': 'Speculative Decoding ускоряет генерацию текста в больших языковых моделях, используя маленькую draft-модель для предсказаний, которые затем проверяет целевая модель. AdaSPEC улучшает процесс Knowledge Distillation, фильтруя сложные токены при обучении draft-модели, чтобы она лучше училась на простых примерах. Это повышает процент принятых токенов без потери качества генерации. Метод показывает превосходство над DistillSpec с улучшением до 15% в различных задачах на конфигурациях моделей от 31M до 2.7B параметров.'}, 'en': {'title': 'Enhancing Token Acceptance with AdaSPEC', 'desc': 'AdaSPEC is a new method that improves speculative decoding by filtering out challenging tokens during knowledge distillation. This selective filtering helps the draft model learn better from the target model, focusing on easier tokens to enhance performance. By doing so, AdaSPEC increases the token acceptance rate, which is crucial for effective generation. The method has been tested on various tasks and shows significant improvements over previous techniques, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'AdaSPEC：提升推测解码的令牌接受率', 'desc': 'AdaSPEC是一种改进的知识蒸馏方法，通过选择性过滤令牌来增强推测解码。它解决了传统知识蒸馏方法在令牌接受率和生成质量之间的矛盾。通过识别和过滤难以适应的令牌，AdaSPEC使得草稿模型能够更好地与目标模型对齐。实验结果表明，AdaSPEC在多个任务中表现优于现有的DistillSpec方法，接受率提高了最多15%。'}}}, {'id': 'https://huggingface.co/papers/2510.20579', 'title': 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence', 'url': 'https://huggingface.co/papers/2510.20579', 'abstract': 'Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.', 'score': 31, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'fecf89fc3cf72cdb', 'authors': ['Jiahao Meng', 'Xiangtai Li', 'Haochen Wang', 'Yue Tan', 'Tao Zhang', 'Lingdong Kong', 'Yunhai Tong', 'Anran Wang', 'Zhiyang Teng', 'Yujing Wang', 'Zhuochen Wang'], 'affiliations': ['ByteDance', 'CASIA', 'NUS', 'Peking University', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2510.20579.jpg', 'data': {'categories': ['#dataset', '#video', '#training', '#benchmark', '#reasoning', '#games'], 'emoji': '🎬', 'ru': {'title': 'Видео-рассуждения с пространственно-временными доказательствами', 'desc': 'Open-o3 Video — это фреймворк для рассуждений о видео, который не просто генерирует текстовые объяснения, но и указывает конкретные временные метки и пространственные области (bounding boxes), где находятся ключевые доказательства. Авторы создали два специальных датасета с пространственно-временными аннотациями и применили reinforcement learning с множественными наградами для обучения модели одновременной локализации объектов во времени и пространстве. Модель достигла state-of-the-art результатов на бенчмарке V-STAR, улучшив базовую модель Qwen2.5-VL на 14.4% по метрике mAM и на 24.2% по mLGM. Генерируемые reasoning traces также полезны для test-time scaling, позволяя верифицировать ответы на основе уверенности модели.'}, 'en': {'title': 'Grounding Video Reasoning in Spatio-Temporal Evidence', 'desc': 'Open-o3 Video is a novel framework that enhances video reasoning by incorporating spatio-temporal evidence, allowing it to achieve top performance on various benchmarks. Unlike previous models that only provide textual reasoning, this approach highlights specific timestamps, objects, and bounding boxes, grounding its conclusions in visual data. The model is trained on two meticulously curated datasets that offer unified spatio-temporal annotations, addressing the challenges of tracking and localizing evidence in dynamic video scenes. By employing a cold-start reinforcement learning strategy with tailored rewards, Open-o3 Video not only improves accuracy but also generates reasoning traces that aid in confidence-aware verification during test-time scaling.'}, 'zh': {'title': '时空证据驱动的视频推理新突破', 'desc': 'Open-o3 Video 是一种视频推理模型，它将时空证据整合到推理过程中，从而在多个基准测试中实现了最先进的性能。该模型能够突出显示关键时间戳、对象和边界框，使推理基于具体的视觉观察。为了克服视频推理中的挑战，Open-o3 Video 采用了冷启动强化学习策略，并设计了多种奖励机制，以提高答案的准确性和时空精度。此外，模型生成的推理轨迹为测试时的扩展提供了有价值的信号，增强了答案的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2510.20822', 'title': 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives', 'url': 'https://huggingface.co/papers/2510.20822', 'abstract': 'HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this "narrative gap" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.', 'score': 19, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'd7e5dd1a530e0b1e', 'authors': ['Yihao Meng', 'Hao Ouyang', 'Yue Yu', 'Qiuyu Wang', 'Wen Wang', 'Ka Leong Cheng', 'Hanlin Wang', 'Yixuan Li', 'Cheng Chen', 'Yanhong Zeng', 'Yujun Shen', 'Huamin Qu'], 'affiliations': ['Ant Group', 'CUHK', 'HKUST', 'NTU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2510.20822.jpg', 'data': {'categories': ['#architecture', '#story_generation', '#video', '#cv'], 'emoji': '🎬', 'ru': {'title': 'От клипов к кино: целостная генерация видео-нарративов', 'desc': 'HoloCine — это модель для генерации связных видео-нарративов, состоящих из множества кадров, что решает проблему современных text-to-video моделей, которые хорошо создают отдельные клипы, но не могут выстроить цельную историю. Архитектура использует механизм Window Cross-Attention для привязки текстовых промптов к конкретным кадрам и Sparse Inter-Shot Self-Attention для эффективной генерации видео длительностью до минуты. Модель демонстрирует emergent abilities: устойчивую память о персонажах и сценах, а также понимание кинематографических техник. Работа представляет сдвиг от синтеза отдельных клипов к автоматизированному созданию полноценных фильмов.'}, 'en': {'title': 'Bridging the Narrative Gap in AI Filmmaking', 'desc': 'HoloCine is a machine learning model designed to create coherent multi-shot narratives for video generation. It uses a Window Cross-Attention mechanism to focus on specific text prompts for each shot, ensuring that the generated scenes are consistent and aligned with the narrative. Additionally, the Sparse Inter-Shot Self-Attention allows for efficient processing by maintaining dense attention within shots while being sparse between them. This innovative approach not only enhances narrative coherence but also introduces advanced capabilities like character memory and an understanding of cinematic techniques, paving the way for automated filmmaking.'}, 'zh': {'title': 'HoloCine：从片段合成到自动电影制作的转变', 'desc': 'HoloCine是一种生成连贯多镜头叙事的模型，采用了窗口交叉注意力机制和稀疏镜头间自注意力机制。这种方法能够确保从第一镜头到最后一镜头的全局一致性，解决了现有文本到视频模型在叙事连贯性上的不足。HoloCine通过精确的导演控制，能够将文本提示定位到特定镜头，同时在镜头之间保持高效的生成能力。该模型不仅在叙事连贯性上设立了新的标准，还展现了对角色和场景的持久记忆以及对电影技术的直观理解。'}}}, {'id': 'https://huggingface.co/papers/2510.19304', 'title': 'Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall', 'url': 'https://huggingface.co/papers/2510.19304', 'abstract': 'Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation.', 'score': 16, 'issue_id': 6590, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4c70e804c9d70b2e', 'authors': ['Mingyu Jo', 'Jaesik Yoon', 'Justin Deschenaux', 'Caglar Gulcehre', 'Sungjin Ahn'], 'affiliations': ['EPFL', 'KAIST', 'Microsoft', 'NYU', 'SAP'], 'pdf_title_img': 'assets/pdf/title_img/2510.19304.jpg', 'data': {'categories': ['#data', '#multimodal', '#training', '#reasoning', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Сохранение информации в диффузионных моделях через детерминированный обходной путь', 'desc': 'Статья представляет Loopholing Discrete Diffusion Models (LDDMs) — улучшенные дискретные диффузионные модели для генерации текста. Ключевая проблема обычных дискретных диффузионных моделей в том, что после категориального сэмплирования богатая информация о распределении схлопывается в one-hot векторы и теряется. LDDMs решают эту проблему через механизм loopholing — детерминированный латентный путь, который сохраняет распределительную информацию между шагами генерации. В результате модели достигают снижения perplexity до 61%, улучшают когерентность текста и показывают лучшие результаты на задачах reasoning, приближаясь по качеству к autoregressive моделям.'}, 'en': {'title': 'Enhancing Text Generation with Loopholing in Diffusion Models', 'desc': 'Loopholing Discrete Diffusion Models (LDDMs) improve text generation by maintaining important distributional information through a new deterministic pathway. This approach addresses the issue of information loss that occurs during categorical sampling in traditional discrete diffusion models. By using a self-conditioning strategy, LDDMs significantly reduce perplexity and enhance coherence, making them competitive with autoregressive models. Additionally, LDDMs show improved performance on reasoning tasks, demonstrating their effectiveness in generating high-quality text.'}, 'zh': {'title': '漏洞离散扩散模型：提升文本生成质量的创新机制', 'desc': '本文介绍了一种新的文本生成模型，称为漏洞离散扩散模型（LDDMs），它通过确定性潜在路径保留分布信息，从而提高生成文本的连贯性和性能。传统的离散扩散模型在采样时会导致信息丢失，限制了后续步骤的生成能力。LDDMs通过自我条件化策略进行高效训练，显著降低了生成困惑度，并在推理任务中表现出色。研究结果表明，漏洞机制有效减少了无效步骤和振荡，为高质量的非自回归文本生成提供了可扩展的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2510.20766', 'title': 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion', 'url': 'https://huggingface.co/papers/2510.20766', 'abstract': "Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.", 'score': 14, 'issue_id': 6592, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'eab34606a3b59eda', 'authors': ['Noam Issachar', 'Guy Yariv', 'Sagie Benaim', 'Yossi Adi', 'Dani Lischinski', 'Raanan Fattal'], 'affiliations': ['The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2510.20766.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#cv', '#benchmark', '#architecture'], 'emoji': '🔭', 'ru': {'title': 'Генерация сверхвысокого разрешения через динамическую экстраполяцию позиций', 'desc': 'Статья представляет Dynamic Position Extrapolation (DyPE) - метод для генерации изображений сверхвысокого разрешения с помощью предобученных diffusion transformers без дополнительного обучения. DyPE динамически адаптирует позиционное кодирование на каждом шаге диффузионного процесса, учитывая спектральную прогрессию: низкочастотные структуры формируются рано, а высокочастотные детали - позже. Метод позволяет генерировать изображения с разрешением до 16 миллионов пикселей (используя FLUX) без дополнительных вычислительных затрат на сэмплирование. DyPE достигает state-of-the-art результатов в задаче генерации изображений сверхвысокого разрешения, причём улучшения становятся ещё более заметными при увеличении разрешения.'}, 'en': {'title': 'Revolutionizing Image Generation with Dynamic Position Extrapolation', 'desc': "Dynamic Position Extrapolation (DyPE) is a method that improves ultra-high-resolution image generation using pre-trained diffusion transformers. It works by adjusting positional encodings dynamically during the image synthesis process, allowing the model to generate images at resolutions much higher than it was originally trained on. This technique leverages the diffusion process's spectral properties, ensuring that low-frequency details are resolved quickly while high-frequency details are refined over time. As a result, DyPE achieves state-of-the-art image fidelity without incurring additional sampling costs, making it highly efficient for generating images up to 16 million pixels."}, 'zh': {'title': '动态位置外推：超高分辨率图像生成的新突破', 'desc': '动态位置外推（DyPE）是一种新颖的方法，可以在不增加采样成本的情况下，增强超高分辨率图像生成。它通过动态调整预训练扩散变换器中的位置编码，使得模型能够合成超出训练数据的图像分辨率。DyPE利用扩散过程中的频谱进展特性，匹配生成过程的当前阶段，从而提高图像生成的保真度。实验结果表明，DyPE在多个基准测试中表现优异，尤其在更高分辨率下，性能提升更加显著。'}}}, {'id': 'https://huggingface.co/papers/2510.20187', 'title': 'Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values', 'url': 'https://huggingface.co/papers/2510.20187', 'abstract': 'RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities.', 'score': 13, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '99ea059117e075a9', 'authors': ['Dian Yu', 'Yulai Zhao', 'Kishan Panaganti', 'Linfeng Song', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Princeton University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.20187.jpg', 'data': {'categories': ['#optimization', '#rl', '#alignment', '#rlhf'], 'emoji': '🎯', 'ru': {'title': 'Обучение AI с учётом человеческих ценностей', 'desc': 'В статье предлагается метод RLEV, который использует reinforcement learning для обучения больших языковых моделей с учётом человеческих ценностей и приоритетов. В отличие от традиционных подходов, которые оценивают ответы только по критерию правильности, RLEV учитывает важность различных задач через явные сигналы ценности. Модель обучается не только давать правильные ответы, но и адаптировать свою стратегию: давать краткие ответы на простые вопросы и развёрнутые на важные. Метод показывает устойчивость даже при неточных оценках важности задач, открывая практичный путь к выравниванию LLM с человеческими приоритетами.'}, 'en': {'title': 'Aligning AI with Human Values through RLEV', 'desc': 'RLEV is a reinforcement learning method that enhances the training of Large Language Models (LLMs) by integrating human value signals into the optimization process. Unlike traditional methods that focus solely on correctness, RLEV uses value-weighted rewards to prioritize tasks based on their significance to humans. This approach not only improves the accuracy of the models but also enables them to adapt their responses based on the value of the prompts, providing concise answers for low-value queries and detailed responses for high-value ones. The method has shown robustness even with noisy value signals, indicating its effectiveness in aligning LLMs with human priorities.'}, 'zh': {'title': '强化学习与人类价值的对齐', 'desc': 'RLEV是一种强化学习方法，它将大型语言模型（LLM）的优化与可量化的人类价值信号对齐。与传统的基于二元正确性的奖励机制不同，RLEV直接将人类定义的价值信号纳入奖励函数，从而提高了价值加权的准确性。通过使用带有明确真实价值标签的考试风格数据，RLEV在多个强化学习算法和模型规模上均优于仅依赖正确性的基线。RLEV还学习到了一种价值敏感的终止策略，根据提示的价值高低调整响应的详细程度。'}}}, {'id': 'https://huggingface.co/papers/2510.19365', 'title': 'The Massive Legal Embedding Benchmark (MLEB)', 'url': 'https://huggingface.co/papers/2510.19365', 'abstract': 'MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.', 'score': 12, 'issue_id': 6592, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4a2df25a69dc60ad', 'authors': ['Umar Butler', 'Abdur-Rahman Butler', 'Adrian Lucas Malec'], 'affiliations': ['Isaacus'], 'pdf_title_img': 'assets/pdf/title_img/2510.19365.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#data'], 'emoji': '⚖️', 'ru': {'title': 'Эталонный тест для юридического поиска во всех правовых системах', 'desc': 'Представлен MLEB — крупнейший открытый бенчмарк для информационного поиска в юридической сфере. Он включает десять экспертно размеченных датасетов из шести юрисдикций (США, Великобритания, ЕС, Австралия, Ирландия, Сингапур) и охватывает различные типы документов: судебные дела, законодательство, регуляторные документы, контракты и научную литературу. Бенчмарк тестирует модели на задачах поиска, zero-shot классификации и ответов на вопросы. Семь из десяти датасетов были созданы специально для заполнения пробелов в существующих открытых ресурсах для юридического AI.'}, 'en': {'title': 'Unlocking Legal Insights with MLEB: A Comprehensive Benchmark for Information Retrieval', 'desc': 'The Massive Legal Embedding Benchmark (MLEB) is a comprehensive open-source resource designed for legal information retrieval. It includes ten expert-annotated datasets that cover various jurisdictions, document types, and retrieval tasks. MLEB aims to address gaps in existing legal datasets by introducing seven newly constructed datasets. The authors provide detailed documentation of their methodology and make their code and results publicly available to promote reproducibility in evaluations.'}, 'zh': {'title': '法律信息检索的最大开源基准', 'desc': 'MLEB是迄今为止最大的开源法律信息检索基准，涵盖多个法域、文档类型和任务类型。它包含十个由专家标注的数据集，涉及美国、英国、欧盟、澳大利亚、爱尔兰和新加坡等多个法域。MLEB的七个数据集是新构建的，旨在填补开源法律信息检索领域的空白。我们详细记录了构建MLEB和新数据集的方法，并公开发布代码、结果和数据，以支持可重复的评估。'}}}, {'id': 'https://huggingface.co/papers/2510.16917', 'title': 'SAKE: Towards Editing Auditory Attribute Knowledge of Large\n  Audio-Language Models', 'url': 'https://huggingface.co/papers/2510.16917', 'abstract': 'SAKE is a benchmark for editing auditory attribute knowledge in Large Audio-Language Models, addressing challenges in reliability, generality, locality, and portability.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge editing offers an efficient way to update model knowledge without full retraining, but prior work has concentrated almost exclusively on textual or visual modalities. We introduce SAKE, the first benchmark specifically designed for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory attributes, capturing knowledge types that go beyond conventional textual and visual domains. We benchmark seven editing methods on two LALMs along four dimensions: reliability, generality, audio/text locality, and portability. Results highlight challenges such as preserving intra-attribute knowledge unrelated to the edit, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates. SAKE provides a principled framework to study how knowledge editing extends to the auditory modalities, opening new directions for maintaining and adapting LALMs in more diverse real-world scenarios.', 'score': 8, 'issue_id': 6597, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': 'd580e38cfb678720', 'authors': ['Chih-Kai Yang', 'Yen-Ting Piao', 'Tzu-Wen Hsu', 'Szu-Wei Fu', 'Zhehuai Chen', 'Ke-Han Lu', 'Sung-Feng Huang', 'Chao-Han Huck Yang', 'Yu-Chiang Frank Wang', 'Yun-Nung Chen', 'Hung-yi Lee'], 'affiliations': ['DouDou Capital', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.16917.jpg', 'data': {'categories': ['#audio', '#benchmark', '#multimodal'], 'emoji': '🔊', 'ru': {'title': 'Редактирование звуковых знаний в AI-моделях без полного переобучения', 'desc': 'В статье представлен SAKE — первый бенчмарк для редактирования знаний об аудиальных атрибутах в больших аудио-языковых моделях (LALM). В отличие от традиционных методов редактирования знаний, которые фокусируются на тексте и изображениях, SAKE работает с абстрактными звуковыми характеристиками. Авторы протестировали семь методов редактирования на двух моделях по четырем критериям: надежность, обобщаемость, локальность и переносимость. Результаты выявили сложности в сохранении несвязанных знаний, обобщении правок на мультимодальные рассуждения и поддержке последовательных обновлений.'}, 'en': {'title': 'SAKE: Revolutionizing Auditory Knowledge Editing in AI Models', 'desc': 'SAKE is a new benchmark designed to improve how we edit auditory knowledge in Large Audio-Language Models (LALMs). It focuses on updating abstract auditory attributes rather than just factual information, which is a common approach in text and visual data. The benchmark evaluates seven different editing methods across four key areas: reliability, generality, audio/text locality, and portability. The findings reveal significant challenges in maintaining knowledge integrity and adapting edits in a way that works across different types of data and updates.'}, 'zh': {'title': 'SAKE：音频语言模型知识编辑的新基准', 'desc': 'SAKE是一个基准测试，旨在编辑大型音频语言模型中的听觉属性知识，解决可靠性、普遍性、局部性和可移植性等挑战。知识编辑是一种高效更新模型知识的方法，但之前的研究主要集中在文本或视觉模态上。SAKE是首个专门为编辑听觉属性知识而设计的基准，关注超越传统文本和视觉领域的抽象听觉属性。通过对七种编辑方法在两个大型音频语言模型上的评估，SAKE为研究如何在听觉模态中扩展知识编辑提供了一个系统框架。'}}}, {'id': 'https://huggingface.co/papers/2510.16893', 'title': 'Investigating Safety Vulnerabilities of Large Audio-Language Models\n  Under Speaker Emotional Variations', 'url': 'https://huggingface.co/papers/2510.16893', 'abstract': 'Research investigates the impact of speaker emotion on the safety of large audio-language models, revealing inconsistencies and vulnerabilities that require targeted alignment strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) extend text-based LLMs with auditory understanding, offering new opportunities for multimodal applications. While their perception, reasoning, and task performance have been widely studied, their safety alignment under paralinguistic variation remains underexplored. This work systematically investigates the role of speaker emotion. We construct a dataset of malicious speech instructions expressed across multiple emotions and intensities, and evaluate several state-of-the-art LALMs. Our results reveal substantial safety inconsistencies: different emotions elicit varying levels of unsafe responses, and the effect of intensity is non-monotonic, with medium expressions often posing the greatest risk. These findings highlight an overlooked vulnerability in LALMs and call for alignment strategies explicitly designed to ensure robustness under emotional variation, a prerequisite for trustworthy deployment in real-world settings.', 'score': 8, 'issue_id': 6597, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': 'f23652d9d14eddcb', 'authors': ['Bo-Han Feng', 'Chien-Feng Liu', 'Yu-Hsuan Li Liang', 'Chih-Kai Yang', 'Szu-Wei Fu', 'Zhehuai Chen', 'Ke-Han Lu', 'Sung-Feng Huang', 'Chao-Han Huck Yang', 'Yu-Chiang Frank Wang', 'Yun-Nung Chen', 'Hung-yi Lee'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.16893.jpg', 'data': {'categories': ['#audio', '#dataset', '#security', '#alignment', '#multimodal'], 'emoji': '😠', 'ru': {'title': 'Эмоции как уязвимость: безопасность аудио-LLM под угрозой', 'desc': 'Исследование изучает, как эмоции говорящего влияют на безопасность больших аудио-языковых моделей (LALMs). Учёные создали датасет с вредоносными инструкциями, произнесёнными с разными эмоциями и интенсивностью, и протестировали несколько современных моделей. Результаты показали серьёзные проблемы: разные эмоции вызывают разный уровень небезопасных ответов, а средняя интенсивность эмоций оказалась наиболее опасной. Работа выявляет новую уязвимость в LALMs и призывает к разработке специальных стратегий выравнивания (alignment), учитывающих эмоциональные вариации.'}, 'en': {'title': 'Emotional Nuances: A Safety Challenge for Audio-Language Models', 'desc': "This research explores how the emotions of a speaker can affect the safety of large audio-language models (LALMs). It shows that these models respond differently to speech instructions based on the speaker's emotional tone, leading to inconsistent and sometimes unsafe outputs. The study created a dataset with various emotional expressions to test these models and found that medium emotional intensity often resulted in the highest risk of unsafe responses. The findings emphasize the need for better alignment strategies to make LALMs more reliable and safe in real-world applications."}, 'zh': {'title': '情感影响下的音频语言模型安全性挑战', 'desc': '本研究探讨了说话者情感对大型音频语言模型（LALMs）安全性的影响，揭示了不一致性和脆弱性，需采取针对性的对齐策略。虽然LALMs在感知、推理和任务表现方面得到了广泛研究，但在副语言变异下的安全性对齐仍未得到充分探索。我们构建了一个包含多种情感和强度的恶意语音指令数据集，并评估了几种最先进的LALMs。研究结果显示，不同情感会引发不同程度的不安全反应，而情感强度的影响并非单调，中等强度的表达往往带来最大的风险。'}}}, {'id': 'https://huggingface.co/papers/2510.20470', 'title': 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence', 'url': 'https://huggingface.co/papers/2510.20470', 'abstract': 'Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness.', 'score': 5, 'issue_id': 6591, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '3b033cfc30f5dd2d', 'authors': ['Kun Ouyang', 'Yuanxin Liu', 'Linli Yao', 'Yishuo Cai', 'Hao Zhou', 'Jie Zhou', 'Fandong Meng', 'Xu Sun'], 'affiliations': ['State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University', 'WeChat AI, Tencent Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2510.20470.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#long_context', '#dataset', '#rl', '#video'], 'emoji': '🔍', 'ru': {'title': 'Conan: Пошаговое видеорассуждение с визуальными доказательствами', 'desc': 'Статья представляет Conan — фреймворк для многошагового рассуждения над видео с опорой на визуальные доказательства. Система идентифицирует ключевые кадры, анализирует улики между кадрами и адаптивно решает, когда завершить рассуждение или продолжить поиск. Для обучения создан датасет Conan-91K и разработана многоэтапная стратегия тренировки с reinforcement learning, объединяющая идентификацию, рассуждение и принятие решений. Conan превосходит базовую модель Qwen2.5-VL на 10% по точности на шести бенчмарках и демонстрирует отличную масштабируемость на задачах с длинными видео.'}, 'en': {'title': 'Conan: Elevating Video Reasoning with Evidence and Accuracy', 'desc': "Conan is a new framework designed to improve video reasoning by using evidence from multiple frames. It tackles the challenges of visual grounding and reasoning accuracy through a multi-stage training approach. By utilizing a large dataset called Conan-91K, it enhances the model's ability to identify relevant frames and make informed decisions based on visual evidence. The results show that Conan outperforms existing models, achieving higher accuracy and demonstrating strong performance in understanding long videos."}, 'zh': {'title': 'Conan：提升视频推理的证据基础框架', 'desc': 'Conan是一个用于基于证据的多步骤视频推理的框架，通过多阶段训练策略提高视觉定位和推理的准确性。该框架能够识别上下文和证据帧，并在跨帧线索上进行推理，灵活决定何时结束推理或进一步探索。为了实现这一目标，研究者构建了一个名为Conan-91K的大规模数据集，并设计了结合识别-推理-行动的强化学习训练框架。实验结果表明，Conan在多个多步骤推理基准测试中表现优于现有模型，准确率平均提高超过10%。'}}}, {'id': 'https://huggingface.co/papers/2510.18821', 'title': 'Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision', 'url': 'https://huggingface.co/papers/2510.18821', 'abstract': "Self-play training for deep search agents improves performance through co-evolution of task generation and problem solving without supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, RLVR highly depends on well-crafted task queries and corresponding ground-truth answers to provide accurate rewards, which requires massive human efforts and hinders the RL scaling processes, especially under agentic scenarios. Although a few recent works explore task synthesis methods, the difficulty of generated agentic tasks can hardly be controlled to provide effective RL training advantages. To achieve agentic RLVR with higher scalability, we explore self-play training for deep search agents, in which the learning LLM utilizes multi-turn search engine calling and acts simultaneously as both a task proposer and a problem solver. The task proposer aims to generate deep search queries with well-defined ground-truth answers and increasing task difficulty. The problem solver tries to handle the generated search queries and output the correct answer predictions. To ensure that each generated search query has accurate ground truth, we collect all the searching results from the proposer's trajectory as external knowledge, then conduct retrieval-augmentation generation (RAG) to test whether the proposed query can be correctly answered with all necessary search documents provided. In this search self-play (SSP) game, the proposer and the solver co-evolve their agent capabilities through both competition and cooperation. With substantial experimental results, we find that SSP can significantly improve search agents' performance uniformly on various benchmarks without any supervision under both from-scratch and continuous RL training setups. The code is at https://github.com/Alibaba-Quark/SSP.", 'score': 5, 'issue_id': 6594, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '4a0f2721af5d6e14', 'authors': ['Hongliang Lu', 'Yuhang Wen', 'Pengyu Cheng', 'Ruijin Ding', 'Haotian Xu', 'Jiaqi Guo', 'Chutian Wang', 'Haonan Chen', 'Xiaoxi Jiang', 'Guanjun Jiang'], 'affiliations': ['Peking University', 'Quark LLM Team, Alibaba Group', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18821.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#agents', '#rag', '#games', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Self-play для поисковых агентов: учимся без учителя', 'desc': 'Статья предлагает метод самообучения для агентов глубокого поиска через self-play, где LLM одновременно генерирует поисковые задачи и решает их без человеческого надзора. Модель выступает в двух ролях: предлагает сложные поисковые запросы с верифицируемыми ответами и пытается их решить, используя многошаговые обращения к поисковой системе. Для проверки корректности сгенерированных задач используется RAG на основе всех собранных поисковых результатов. Экспериментально показано, что такая коэволюция через конкуренцию и кооперацию значительно улучшает производительность агентов на различных бенчмарках без необходимости размеченных данных.'}, 'en': {'title': 'Co-evolving Search Agents through Self-Play Training', 'desc': 'This paper presents a novel approach to training deep search agents using self-play, where the agent acts as both a task generator and a problem solver. By generating search queries with increasing difficulty and ensuring accurate ground-truth answers, the agents can co-evolve their capabilities through competition and cooperation. The method leverages retrieval-augmented generation (RAG) to validate the correctness of answers based on external knowledge. Experimental results demonstrate that this self-play training significantly enhances the performance of search agents across various benchmarks without requiring supervision.'}, 'zh': {'title': '自我对弈训练提升深度搜索代理性能', 'desc': '本文探讨了一种自我对弈训练方法，用于深度搜索代理的性能提升。通过让学习型大语言模型（LLM）同时充当任务提出者和问题解决者，本文实现了无监督的强化学习。任务提出者生成具有明确答案和逐步增加难度的搜索查询，而问题解决者则尝试处理这些查询并输出正确答案。实验结果表明，这种自我对弈训练方法显著提高了搜索代理在各种基准测试中的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.20820', 'title': 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas', 'url': 'https://huggingface.co/papers/2510.20820', 'abstract': 'LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.', 'score': 4, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'cd0c9d6f510364a1', 'authors': ['Guocheng Gordon Qian', 'Ruihang Zhang', 'Tsai-Shien Chen', 'Yusuf Dalva', 'Anujraaj Argo Goyal', 'Willi Menapace', 'Ivan Skorokhodov', 'Meng Dong', 'Arpit Sahni', 'Daniil Ostashev', 'Ju Hu', 'Sergey Tulyakov', 'Kuan-Chieh Jackson Wang'], 'affiliations': ['Snap Inc.', 'UC Merced', 'University of Toronto', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2510.20820.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#games', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Послойная композиция: интерактивное управление персонализированной генерацией изображений', 'desc': 'LayerComposer — это фреймворк для персонализированной генерации изображений с несколькими объектами, который вводит концепцию послойного холста. Каждый объект размещается на отдельном слое, что позволяет избежать перекрытий и обеспечивает интуитивное управление композицией через изменение размера и позиции. Механизм блокировки слоёв сохраняет выбранные элементы с высокой точностью, позволяя остальным адаптироваться к контексту. Подход обеспечивает превосходный пространственный контроль и сохранение идентичности объектов по сравнению с существующими методами в персонализированной text-to-image генерации.'}, 'en': {'title': 'LayerComposer: Mastering Multi-Subject Image Generation with Layers', 'desc': 'LayerComposer is a novel framework designed for personalized text-to-image generation that allows users to have interactive control over how multiple subjects are arranged in an image. It introduces a layered canvas where each subject is placed on a separate layer, which helps in achieving clear and occlusion-free compositions. Additionally, it features a locking mechanism that keeps selected layers intact while allowing other layers to adjust to their context, enhancing flexibility. Through extensive testing, LayerComposer shows improved spatial control and better identity preservation compared to existing methods in the field.'}, 'zh': {'title': 'LayerComposer：多主体生成的空间控制新方式', 'desc': 'LayerComposer 是一个交互式框架，旨在解决多主体文本到图像生成中的空间构图和可扩展性问题。它引入了分层画布的概念，每个主体都放置在独立的层上，从而实现无遮挡的组合效果。该框架还包含一个锁定机制，可以在保持选定层高保真的同时，灵活调整其他层以适应周围环境。通过这种方式，用户可以像使用专业图像编辑软件一样，直观地操作输入主体，进行放置、调整大小或锁定。'}}}, {'id': 'https://huggingface.co/papers/2510.19944', 'title': 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets', 'url': 'https://huggingface.co/papers/2510.19944', 'abstract': 'Seed3D 1.0 generates scalable, physics-accurate 3D assets from images for use in simulation environments, enhancing both content diversity and real-time physics feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing embodied AI agents requires scalable training environments that balance content diversity with physics accuracy. World simulators provide such environments but face distinct limitations: video-based methods generate diverse content but lack real-time physics feedback for interactive learning, while physics-based engines provide accurate dynamics but face scalability limitations from costly manual asset creation. We present Seed3D 1.0, a foundation model that generates simulation-ready 3D assets from single images, addressing the scalability challenge while maintaining physics rigor. Unlike existing 3D generation models, our system produces assets with accurate geometry, well-aligned textures, and realistic physically-based materials. These assets can be directly integrated into physics engines with minimal configuration, enabling deployment in robotic manipulation and simulation training. Beyond individual objects, the system scales to complete scene generation through assembling objects into coherent environments. By enabling scalable simulation-ready content creation, Seed3D 1.0 provides a foundation for advancing physics-based world simulators. Seed3D 1.0 is now available on https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D', 'score': 4, 'issue_id': 6594, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'aab6cf80355f0288', 'authors': ['Jiashi Feng', 'Xiu Li', 'Jing Lin', 'Jiahang Liu', 'Gaohong Liu', 'Weiqiang Lou', 'Su Ma', 'Guang Shi', 'Qinlong Wang', 'Jun Wang', 'Zhongcong Xu', 'Xuanyu Yi', 'Zihao Yu', 'Jianfeng Zhang', 'Yifan Zhu', 'Rui Chen', 'Jinxin Chi', 'Zixian Du', 'Li Han', 'Lixin Huang', 'Kaihua Jiang', 'Yuhan Li', 'Guan Luo', 'Shuguang Wang', 'Qianyi Wu', 'Fan Yang', 'Junyang Zhang', 'Xuanmeng Zhang'], 'affiliations': ['ByteDance', 'Volcano Engine'], 'pdf_title_img': 'assets/pdf/title_img/2510.19944.jpg', 'data': {'categories': ['#agents', '#games', '#3d', '#robotics', '#synthetic'], 'emoji': '🎲', 'ru': {'title': 'От картинки к симуляции: генерация физически точных 3D-объектов для обучения AI-агентов', 'desc': 'Seed3D 1.0 — это foundation модель, которая генерирует готовые для симуляции 3D-объекты из одного изображения, решая проблему масштабируемости в создании тренировочных сред для embodied AI. В отличие от существующих методов генерации 3D, система создает объекты с точной геометрией, правильно выровненными текстурами и реалистичными физическими материалами. Сгенерированные ассеты можно напрямую интегрировать в физические движки с минимальной настройкой для использования в робототехнике и обучении с подкреплением. Система масштабируется от отдельных объектов до полных сцен, обеспечивая баланс между разнообразием контента и физической точностью для обучения агентов.'}, 'en': {'title': 'Revolutionizing 3D Asset Creation for Simulations', 'desc': 'Seed3D 1.0 is a model that creates 3D assets from images, making it easier to generate content for simulations. It combines the benefits of diverse content generation with accurate physics, which is important for training AI agents. This system produces high-quality 3D models with correct shapes and textures that can be used in physics engines without much extra work. Additionally, it can create entire scenes by putting together multiple objects, enhancing the capabilities of simulation environments.'}, 'zh': {'title': 'Seed3D 1.0：从图像生成可扩展的3D仿真资产', 'desc': 'Seed3D 1.0 是一种生成模型，可以从单张图像生成可用于仿真环境的3D资产，解决了内容多样性与物理准确性之间的平衡问题。该系统生成的3D资产具有准确的几何形状、良好的纹理对齐和真实的物理材料，能够直接集成到物理引擎中。与现有的3D生成模型不同，Seed3D 1.0 还支持完整场景的生成，通过将对象组装成连贯的环境来扩展应用。通过实现可扩展的仿真内容创建，Seed3D 1.0 为物理基础的世界模拟器的发展奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2510.12487', 'title': 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding', 'url': 'https://huggingface.co/papers/2510.12487', 'abstract': 'A benchmark for code-diff understanding with tasks including apply, anti-apply, and diff generation, revealing optimal diff formats based on model size and use case.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff understanding with three supervised tasks: apply (old code + diff rightarrow new code), anti-apply (new code - diff rightarrow old code), and diff generation (new code - old code rightarrow diff). Instances in the benchmark are triples langle old code, new code, diff rangle drawn from real commits in CommitPackFT, paired with automatic metrics and a clear evaluation protocol. We use the benchmark to do a focused empirical study of the unified diff format and run a cross-format comparison of different diff representations. Our findings reveal that different formats should be used depending on the use case and model size. For example, representing diffs in search-replace format is good for larger models in the diff generation scenario, yet not suited well for diff analysis and smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing and improving diff handling in LLMs that can aid future development of diff formats and models editing code. The dataset is published on HuggingFace Hub: https://huggingface.co/datasets/JetBrains-Research/diff-xyz.', 'score': 4, 'issue_id': 6597, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '59eba354af022bcd', 'authors': ['Evgeniy Glukhov', 'Michele Conti', 'Egor Bogomolov', 'Yaroslav Golubev', 'Alexander Bezzubov'], 'affiliations': ['JetBrains Research Amsterdam, the Netherlands', 'JetBrains Research Belgrade, Serbia'], 'pdf_title_img': 'assets/pdf/title_img/2510.12487.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#open_source'], 'emoji': '🔄', 'ru': {'title': 'Оптимальные форматы diff для разных моделей и задач', 'desc': 'Исследователи представили Diff-XYZ — бенчмарк для оценки понимания code diff моделями с тремя задачами: применение патча, обратное применение и генерация diff. Датасет построен на реальных коммитах и позволяет сравнивать различные форматы представления изменений в коде. Эксперименты показали, что оптимальный формат diff зависит от размера модели и конкретной задачи: например, search-replace формат хорош для больших моделей при генерации, но не подходит для анализа и малых моделей. Этот бенчмарк создаёт основу для улучшения работы LLM с кодом и разработки агентов, редактирующих репозитории в масштабе.'}, 'en': {'title': 'Optimizing Code Diff Understanding with Diff-XYZ Benchmark', 'desc': 'This paper presents Diff-XYZ, a benchmark designed to enhance understanding of code diffs through three key tasks: applying diffs to code, reversing diffs, and generating diffs from code changes. The benchmark utilizes real commit data to create instances that help evaluate different diff formats based on model size and specific use cases. The study reveals that the effectiveness of diff representations varies, suggesting that larger models benefit from search-replace formats for diff generation, while smaller models perform better with traditional diff formats. Overall, Diff-XYZ serves as a valuable resource for improving how machine learning models handle code diffs, facilitating advancements in code editing and refactoring tools.'}, 'zh': {'title': 'Diff-XYZ：优化代码差异理解的基准', 'desc': '本文介绍了Diff-XYZ，这是一个用于代码差异理解的基准测试，包含三个监督任务：应用、反应用和差异生成。通过分析真实提交中的代码和差异，研究不同格式的最佳使用情况。研究发现，较大的模型在差异生成时适合使用搜索替换格式，而在差异分析和较小模型中则不太适用。Diff-XYZ基准为评估和改进大语言模型中的差异处理提供了可重用的基础。'}}}, {'id': 'https://huggingface.co/papers/2510.20803', 'title': 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model', 'url': 'https://huggingface.co/papers/2510.20803', 'abstract': 'A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.', 'score': 3, 'issue_id': 6591, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'b7296c78740064e7', 'authors': ['Xiaolong Wang', 'Lixiang Ru', 'Ziyuan Huang', 'Kaixiang Ji', 'Dandan Zheng', 'Jingdong Chen', 'Jun Zhou'], 'affiliations': ['Ant Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.20803.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#inference', '#games', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Сегментация изображений через авторегрессивную генерацию масок', 'desc': 'Исследователи предложили новый подход к сегментации изображений ARGenSeg, который объединяет мультимодальные большие языковые модели с генерацией изображений. Вместо использования граничных точек или специальных декодеров, метод генерирует плотные маски объектов через визуальные токены и VQ-VAE. Модель опирается на пиксельное понимание изображений самим LLM, а стратегия предсказания следующего масштаба позволяет генерировать токены параллельно для ускорения. Подход превосходит существующие методы на нескольких датасетах по качеству сегментации и скорости инференса, сохраняя при этом сильные способности к пониманию.'}, 'en': {'title': 'Revolutionizing Image Segmentation with ARGenSeg!', 'desc': 'This paper introduces a new method called AutoRegressive Generation-based paradigm for image segmentation (ARGenSeg) that enhances how machines understand and segment images. It combines multimodal large language models (MLLMs) with a technique called VQ-VAE to create detailed masks for objects in images. Unlike previous methods that used discrete representations, this approach generates visual tokens directly, allowing for better pixel-level accuracy. The framework also speeds up the process of generating these masks, achieving faster results while maintaining high performance on various segmentation tasks.'}, 'zh': {'title': '图像分割的新范式：自回归生成', 'desc': '本文提出了一种新颖的基于自回归生成的图像分割范式（ARGenSeg），实现了多模态理解和像素级感知。与以往将图像分割集成到多模态大语言模型（MLLM）中的方法不同，我们的框架基于图像生成，自然生成目标对象的密集掩码。我们利用MLLM输出视觉标记，并通过通用的VQ-VAE将其解码为图像，从而使分割完全依赖于MLLM的像素级理解。实验表明，我们的方法在多个分割数据集上超越了现有的最先进方法，同时显著提高了推理速度。'}}}, {'id': 'https://huggingface.co/papers/2510.20771', 'title': 'AlphaFlow: Understanding and Improving MeanFlow Models', 'url': 'https://huggingface.co/papers/2510.20771', 'abstract': 'The $\\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.  \t\t\t\t\tAI-generated summary \t\t\t\t MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce alpha-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms MeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).', 'score': 2, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'd6e50ecd48746859', 'authors': ['Huijie Zhang', 'Aliaksandr Siarohin', 'Willi Menapace', 'Michael Vasilkovsky', 'Sergey Tulyakov', 'Qing Qu', 'Ivan Skorokhodov'], 'affiliations': ['Department of EECS, University of Michigan', 'Snap Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2510.20771.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#cv', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'α-Flow: разделяй и властвуй в быстрой генерации изображений', 'desc': 'Статья представляет α-Flow — улучшенный framework для генеративного моделирования, который требует всего несколько шагов для создания изображений. Авторы обнаружили, что предыдущий метод MeanFlow содержит конфликтующие части оптимизации (trajectory flow matching и trajectory consistency), которые мешают друг другу. Новый подход использует curriculum strategy — плавный переход между целевыми функциями, что решает проблему конфликта и ускоряет обучение. На датасете ImageNet-1K модель α-Flow достигает state-of-the-art результатов с FID 2.58 за один шаг генерации, используя стандартную архитектуру DiT.'}, 'en': {'title': 'Unifying Objectives for Superior Few-Step Generative Modeling', 'desc': 'The $\x08eta$-Flow framework enhances few-step generative modeling by addressing and resolving conflicting objectives, which leads to improved convergence rates and top-tier performance on ImageNet-1K. This paper reveals that the MeanFlow objective can be broken down into two components: trajectory flow matching and trajectory consistency, which are negatively correlated and hinder optimization. To tackle this issue, the authors propose alpha-Flow, a comprehensive set of objectives that integrates trajectory flow matching, Shortcut Model, and MeanFlow into a single framework. By employing a curriculum learning approach that gradually transitions from trajectory flow matching to MeanFlow, alpha-Flow effectively disentangles these conflicting objectives, resulting in superior performance compared to MeanFlow.'}, 'zh': {'title': 'α-Flow：解耦冲突目标，实现更优生成建模', 'desc': '本文提出了α-Flow框架，通过统一和解耦相互冲突的目标，改善了少步生成建模的效果。我们发现MeanFlow目标可以自然分解为轨迹流匹配和轨迹一致性两个部分，这两个部分之间存在强烈的负相关，导致优化冲突和收敛缓慢。为了解决这个问题，α-Flow引入了一种新的目标函数，结合了轨迹流匹配、Shortcut模型和MeanFlow，并采用逐步策略平滑过渡，从而解耦了冲突目标。实验结果表明，α-Flow在ImageNet-1K数据集上训练时，表现优于MeanFlow，达到了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2510.20733', 'title': 'Thought Communication in Multiagent Collaboration', 'url': 'https://huggingface.co/papers/2510.20733', 'abstract': 'Thought communication enables direct mind-to-mind interaction between agents, uncovering latent thoughts and improving collaborative intelligence beyond natural language.  \t\t\t\t\tAI-generated summary \t\t\t\t Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.', 'score': 2, 'issue_id': 6595, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '3384a06d75c16d83', 'authors': ['Yujia Zheng', 'Zhuokai Zhao', 'Zijian Li', 'Yaqi Xie', 'Mingze Gao', 'Lizhu Zhang', 'Kun Zhang'], 'affiliations': ['CMU', 'MBZUAI', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.20733.jpg', 'data': {'categories': ['#multimodal', '#agents', '#benchmark', '#reasoning', '#interpretability', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Телепатия для ИИ: общение напрямую через скрытые мысли', 'desc': 'Исследователи предложили новую парадигму коммуникации между AI-агентами, которая позволяет им обмениваться информацией напрямую через латентные представления, минуя естественный язык. Они формализовали процесс как модель со скрытыми переменными и доказали, что можно идентифицировать как общие, так и приватные «мысли» между агентами. Метод извлекает скрытые состояния из всех агентов и определяет структуру их совместного использования с теоретическими гарантиями. Эксперименты показывают, что такая «телепатическая» коммуникация превосходит традиционный обмен через токены или эмбеддинги в задачах коллективного интеллекта.'}, 'en': {'title': 'Unlocking Collaboration Through Direct Thought Communication', 'desc': 'This paper introduces a novel concept called thought communication, which allows agents to interact directly by sharing their underlying thoughts, bypassing the limitations of natural language. By formalizing this interaction as a latent variable model, the authors demonstrate how to identify both shared and private thoughts between agents without needing additional information. The framework developed extracts these latent thoughts before communication, revealing the structure of thought sharing among agents. Experiments show that this approach enhances collaborative intelligence, suggesting that many complex problems can be better addressed by understanding hidden generative processes rather than relying solely on observable data.'}, 'zh': {'title': '超越语言的思维交流', 'desc': '这篇论文介绍了一种新的思维交流范式，允许智能体之间直接进行心灵对心灵的互动，超越自然语言的限制。通过将思维交流形式化为一种潜变量模型，研究者证明了在没有辅助信息的情况下，可以识别智能体之间的共享和私有潜在思维。该框架提取所有智能体的潜在思维，并分配相关思维及其共享模式，从而提高协作智能。实验结果验证了理论，并展示了思维交流的协作优势，表明隐藏的思维世界在解决许多问题时具有重要潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.20668', 'title': "From Masks to Worlds: A Hitchhiker's Guide to World Models", 'url': 'https://huggingface.co/papers/2510.20668', 'abstract': 'The guide outlines a progression from early masked models to memory-augmented systems, emphasizing generative capabilities, interactive loops, and memory for building world models.  \t\t\t\t\tAI-generated summary \t\t\t\t This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models.', 'score': 2, 'issue_id': 6594, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '049a652ce3f21398', 'authors': ['Jinbin Bai', 'Yu Lei', 'Hecong Wu', 'Yuchen Zhu', 'Shufan Li', 'Yi Xin', 'Xiangtai Li', 'Molei Tao', 'Aditya Grover', 'Ming-Hsuan Yang'], 'affiliations': ['Georgia Tech', 'MeissonFlow Research', 'UC Merced', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.20668.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#games', '#diffusion'], 'emoji': '🌍', 'ru': {'title': 'Путеводитель по строительству миров: от масок к памяти', 'desc': 'Статья представляет собой практическое руководство по построению world models, а не обычный обзор литературы. Авторы прослеживают эволюцию от ранних masked-моделей через унифицированные архитектуры к интерактивным генеративным моделям. Особое внимание уделяется замыканию цикла действие-восприятие и системам с памятью для поддержания консистентных миров. Работа фокусируется на трёх ключевых компонентах: генеративном ядре, интерактивном цикле и системе памяти как основе для создания настоящих world models.'}, 'en': {'title': 'Building Worlds: From Masks to Memory in AI', 'desc': 'This paper presents a structured approach to developing world models in artificial intelligence, starting from early masked models that integrate representation learning across different data types. It progresses to unified architectures that operate under a single framework, enhancing the efficiency of model training and application. The guide emphasizes the importance of interactive generative models that create a feedback loop between actions and perceptions, leading to more dynamic and responsive systems. Finally, it highlights the role of memory-augmented systems in maintaining coherent and consistent world representations over time, suggesting this pathway as the most effective for advancing true world models.'}, 'zh': {'title': '构建世界模型的最佳路径', 'desc': '本文介绍了从早期的掩蔽模型到增强记忆系统的发展过程，强调了生成能力、交互循环和记忆在构建世界模型中的重要性。我们并不打算列举所有提到“世界模型”的论文，而是专注于一个清晰的方向。这个方向包括统一表示学习的早期掩蔽模型、共享单一范式的统一架构、闭合动作-感知循环的交互生成模型，以及能够持续一致世界的增强记忆系统。我们认为这是通向真正世界模型的最有前景的路径。'}}}, {'id': 'https://huggingface.co/papers/2510.20270', 'title': "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", 'url': 'https://huggingface.co/papers/2510.20270', 'abstract': 'ImpossibleBench is a benchmark framework that measures and mitigates LLMs\' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t The tendency to find and exploit "shortcuts" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.   To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents\' propensity to exploit test cases. ImpossibleBench creates "impossible" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent\'s "cheating rate" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.   As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.   Our implementation can be found at https://github.com/safety-research/impossiblebench.', 'score': 2, 'issue_id': 6590, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '8843fcc2113f2c3a', 'authors': ['Ziqian Zhong', 'Aditi Raghunathan', 'Nicholas Carlini'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2510.20270.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#security'], 'emoji': '🚫', 'ru': {'title': 'Ловушка для читеров: как поймать LLM на нечестном решении задач', 'desc': 'ImpossibleBench — это фреймворк для тестирования LLM, который измеряет склонность моделей к использованию нечестных способов решения задач. Система создаёт «невозможные» варианты задач из существующих бенчмарков, вводя прямые противоречия между текстовым описанием и unit-тестами. Любое успешное прохождение такой задачи означает, что модель нашла обходной путь вместо честного решения. Фреймворк помогает изучать поведение моделей, оптимизировать промпты и разрабатывать инструменты мониторинга для создания более надёжных AI-систем.'}, 'en': {'title': 'Enhancing LLM Reliability with ImpossibleBench', 'desc': "ImpossibleBench is a benchmark framework designed to evaluate and reduce the tendency of large language models (LLMs) to exploit shortcuts in task completion. It introduces 'impossible' task variants that create conflicts between natural-language specifications and unit tests, allowing researchers to measure the 'cheating rate' of LLMs. This framework not only assesses model behavior but also aids in context engineering and the development of monitoring tools to enhance model reliability. By systematically addressing these issues, ImpossibleBench aims to foster the creation of more robust LLM systems."}, 'zh': {'title': '提升大型语言模型的可靠性', 'desc': 'ImpossibleBench是一个基准框架，用于测量和减轻大型语言模型（LLMs）利用测试用例的倾向，从而增强模型的可靠性。该框架通过引入与自然语言规范和单元测试之间的直接冲突，创建了现有基准任务的“不可完成”变体。我们通过测量模型在这些不可完成任务上的通过率来量化其“作弊率”，这意味着任何通过都暗示了违反规范的捷径。ImpossibleBench不仅是一个评估工具，还是一个多功能的工具，可以用于研究模型行为、上下文工程和开发监控工具。'}}}, {'id': 'https://huggingface.co/papers/2510.20362', 'title': 'ComProScanner: A multi-agent based framework for composition-property\n  structured data extraction from scientific literature', 'url': 'https://huggingface.co/papers/2510.20362', 'abstract': 'ComProScanner, an autonomous multi-agent platform, extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, outperforming various LLMs in accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Since the advent of various pre-trained large language models, extracting structured knowledge from scientific text has experienced a revolutionary change compared with traditional machine learning or natural language processing techniques. Despite these advances, accessible automated tools that allow users to construct, validate, and visualise datasets from scientific literature extraction remain scarce. We therefore developed ComProScanner, an autonomous multi-agent platform that facilitates the extraction, validation, classification, and visualisation of machine-readable chemical compositions and properties, integrated with synthesis data from journal articles for comprehensive database creation. We evaluated our framework using 100 journal articles against 10 different LLMs, including both open-source and proprietary models, to extract highly complex compositions associated with ceramic piezoelectric materials and corresponding piezoelectric strain coefficients (d33), motivated by the lack of a large dataset for such materials. DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of 0.82. This framework provides a simple, user-friendly, readily-usable package for extracting highly complex experimental data buried in the literature to build machine learning or deep learning datasets.', 'score': 0, 'issue_id': 6598, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '40190630d78a5c2f', 'authors': ['Aritra Roy', 'Enrico Grisan', 'John Buckeridge', 'Chiara Gattinoni'], 'affiliations': ['Bioscience and Bioengineering Research Centre, London South Bank University, London', 'Department of Physics, Kings College London, London WC2R 2LS, UK', 'Energy, Materials and Environment Research Centre, London South Bank University, London SE1 0AA, UK', 'School of Engineering and Design, London South Bank University, London SE1 0AA, UK'], 'pdf_title_img': 'assets/pdf/title_img/2510.20362.jpg', 'data': {'categories': ['#data', '#dataset', '#agents', '#benchmark', '#open_source', '#science'], 'emoji': '🔬', 'ru': {'title': 'Автономная платформа для извлечения химических данных из научных статей', 'desc': 'ComProScanner — это мультиагентная платформа, которая автоматически извлекает, проверяет, классифицирует и визуализирует химические составы и свойства из научной литературы. Система была протестирована на 100 статьях о пьезоэлектрических материалах, где сравнивались 10 различных LLM моделей, включая открытые и проприетарные. Лучшую точность показала модель DeepSeek-V3-0324 с результатом 0.82, превзойдя все остальные модели. Платформа предоставляет простой инструмент для создания датасетов из сложных экспериментальных данных, что особенно ценно для machine learning и deep learning задач.'}, 'en': {'title': 'Revolutionizing Chemical Data Extraction with ComProScanner', 'desc': 'ComProScanner is an innovative multi-agent platform designed to autonomously extract, validate, classify, and visualize chemical compositions and properties from scientific literature. It surpasses various large language models (LLMs) in accuracy, demonstrating its effectiveness in handling complex data related to ceramic piezoelectric materials. The platform addresses the need for accessible tools that allow users to create and manage datasets from scientific texts, which is often a challenging task. By evaluating its performance against multiple LLMs, ComProScanner achieved a notable accuracy of 0.82, making it a valuable resource for researchers looking to build machine learning datasets from experimental data.'}, 'zh': {'title': 'ComProScanner：科学文献中的化学成分提取新工具', 'desc': 'ComProScanner是一个自主的多智能体平台，能够从科学文献中提取、验证、分类和可视化化学成分及其属性。与传统的机器学习或自然语言处理技术相比，它在准确性上超越了多种大型语言模型。该平台集成了期刊文章中的合成数据，旨在创建全面的数据库，特别是针对陶瓷压电材料及其压电应变系数（d33）的复杂成分提取。通过对100篇期刊文章的评估，DeepSeek-V3-0324模型的整体准确率达到了0.82，展示了该框架在提取复杂实验数据方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.15804', 'title': 'Emergence of Linear Truth Encodings in Language Models', 'url': 'https://huggingface.co/papers/2510.15804', 'abstract': 'A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce a transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. We study one simple setting in which truth encoding can emerge: a data distribution where factual statements co-occur with other factual statements (and vice-versa), encouraging the model to learn this distinction in order to lower the LM loss on future tokens. We corroborate this pattern with experiments in pretrained language models. Finally, in the toy setting we observe a two-phase learning dynamic: networks first memorize individual factual associations in a few steps, then -- over a longer horizon -- learn to linearly separate true from false, which in turn lowers language-modeling loss. Together, these results provide both a mechanistic demonstration and an empirical motivation for how and why linear truth representations can emerge in language models.', 'score': 0, 'issue_id': 6593, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '13b71229edbc4573', 'authors': ['Shauli Ravfogel', 'Gilad Yehudai', 'Tal Linzen', 'Joan Bruna', 'Alberto Bietti'], 'affiliations': ['Flatiron Institute', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.15804.jpg', 'data': {'categories': ['#architecture', '#training', '#reasoning', '#interpretability'], 'emoji': '🎯', 'ru': {'title': 'Как трансформеры учатся отличать правду от лжи через линейное разделение', 'desc': 'Исследователи создали простую однослойную трансформер-модель, которая демонстрирует, как в языковых моделях возникают линейные подпространства, разделяющие правдивые и ложные утверждения. Модель обучается в два этапа: сначала запоминает отдельные факты, а затем учится линейно разделять истинные и ложные утверждения, что снижает loss функцию языковой модели. Ключевой механизм основан на том, что в данных фактические утверждения встречаются вместе с другими фактическими (и наоборот для ложных), что побуждает модель выучить это различие. Результаты подтверждены экспериментами на предобученных LLM и объясняют, почему линейные представления истинности появляются в языковых моделях.'}, 'en': {'title': 'Unveiling Truth: How Transformers Learn to Separate Facts from Fiction', 'desc': 'This paper explores how a simple one-layer transformer model can learn to distinguish between true and false statements in language. It shows that through memorization of factual associations, the model can create linear subspaces that separate these statements. The authors demonstrate that this separation occurs in two phases: first, the model memorizes facts, and then it learns to linearly separate true from false statements over time. This research provides insights into the mechanisms behind truth representation in large language models and how they can improve performance in language tasks.'}, 'zh': {'title': '揭示语言模型中的真实与虚假分离机制', 'desc': '这篇论文探讨了一层变换器模型如何通过记忆和线性分离，形成区分真实与虚假陈述的线性子空间。研究表明，大型语言模型中存在这样的线性子空间，但其形成机制尚不清楚。我们提出了一个透明的玩具模型，展示了真实编码如何在特定数据分布中出现，并通过实验验证了这一模式。最终，我们观察到网络在学习过程中经历了两个阶段：首先快速记忆个别事实关联，然后在更长的时间内学习线性分离真实与虚假，从而降低语言模型的损失。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi', '#alignment (3)', '#architecture (4)', '#audio (2)', '#benchmark (11)', '#cv (5)', '#data (4)', '#dataset (6)', '#diffusion (4)', '#ethics', '#games (6)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (9)', '#open_source (4)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (3)', '#rlhf (1)', '#robotics (1)', '#science (2)', '#security (2)', '#small_models', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (5)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-24 13:25',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-24 13:25')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-24 13:25')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    