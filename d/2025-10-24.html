
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. October 24.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">24 октября</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-23.html">⬅️ <span id="prev-date">23.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-27.html">➡️ <span id="next-date">27.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'};
        let feedDateNext = {'ru': '27.10', 'en': '10/27', 'zh': '10月27日'};
        let feedDatePrev = {'ru': '23.10', 'en': '10/23', 'zh': '10月23日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.19338', 'title': 'Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning', 'url': 'https://huggingface.co/papers/2510.19338', 'abstract': 'The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.', 'score': 77, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'bf307f4447578ac2', 'authors': ['Ling Team', 'Bin Han', 'Caizhi Tang', 'Chen Liang', 'Donghao Zhang', 'Fan Yuan', 'Feng Zhu', 'Jie Gao', 'Jingyu Hu', 'Longfei Li', 'Meng Li', 'Mingyang Zhang', 'Peijie Jiang', 'Peng Jiao', 'Qian Zhao', 'Qingyuan Yang', 'Wenbo Shen', 'Xinxing Yang', 'Yalin Zhang', 'Yankun Ren', 'Yao Zhao', 'Yibo Cao', 'Yixuan Sun', 'Yue Zhang', 'Yuchen Fang', 'Zibin Lin', 'Zixuan Cheng', 'Jun Zhou'], 'affiliations': ['Ling Team'], 'pdf_title_img': 'assets/pdf/title_img/2510.19338.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#benchmark', '#inference', '#optimization'], 'emoji': '💍', 'ru': {'title': 'Гибридное внимание для эффективного вывода на длинных контекстах', 'desc': 'Серия моделей Ring-linear использует гибридную архитектуру, комбинирующую линейное и softmax внимание для снижения вычислительных затрат. Модели Ring-mini-linear-2.0 (16B параметров) и Ring-flash-linear-2.0 (104B параметров) уменьшают стоимость inference в 10 раз по сравнению с dense моделями. Благодаря оптимальному соотношению механизмов внимания и использованию FP8 операторов, эффективность обучения выросла на 50%. Модели демонстрируют SOTA результаты на сложных бенчмарках reasoning при стабильной оптимизации в фазе reinforcement learning.'}, 'en': {'title': 'Efficient Inference with Hybrid Attention Models', 'desc': 'The Ring-linear model series introduces two advanced models, Ring-mini-linear-2.0 and Ring-flash-linear-2.0, which utilize a hybrid architecture that combines linear and softmax attention mechanisms. This innovative approach significantly reduces inference costs and enhances training efficiency, making it suitable for long-context scenarios. With 16B and 104B parameters respectively, these models achieve a remarkable reduction in computational overhead, cutting inference costs to one-tenth of a comparable dense model. Additionally, the integration of a high-performance FP8 operator library has led to a 50% improvement in training efficiency, ensuring that the models maintain state-of-the-art performance across various complex reasoning tasks.'}, 'zh': {'title': '高效推理与训练的混合架构', 'desc': '本论文介绍了Ring-linear模型系列，包括Ring-mini-linear-2.0和Ring-flash-linear-2.0。这些模型采用混合架构，结合了线性注意力和softmax注意力，显著降低了推理成本并提高了训练效率。Ring-mini-linear-2.0拥有160亿参数，而Ring-flash-linear-2.0则有1040亿参数，二者在长上下文推理场景中表现出色。通过优化不同注意力机制的比例，我们找到了当前最佳的模型结构，并利用自研的高性能FP8运算库提高了训练效率。'}}}, {'id': 'https://huggingface.co/papers/2510.18927', 'title': 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping', 'url': 'https://huggingface.co/papers/2510.18927', 'abstract': 'BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking.', 'score': 67, 'issue_id': 6569, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '968dccb2af4a1217', 'authors': ['Zhiheng Xi', 'Xin Guo', 'Yang Nan', 'Enyu Zhou', 'Junrui Shen', 'Wenxiang Chen', 'Jiaqi Liu', 'Jixuan Huang', 'Zhihao Zhang', 'Honglin Guo', 'Xun Deng', 'Zhikai Lei', 'Miao Zheng', 'Guoteng Wang', 'Shuo Zhang', 'Peng Sun', 'Rui Zheng', 'Hang Yan', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Shanghai Qiji Zhifeng Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2510.18927.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Балансировка градиентов для стабильного обучения LLM через RL', 'desc': 'Статья представляет метод BAPO для обучения больших языковых моделей с помощью reinforcement learning в off-policy режиме. Авторы выявили две ключевые проблемы: дисбаланс в оптимизации из-за доминирования отрицательных градиентов и фиксированный clipping механизм в PPO, который подавляет entropy и ограничивает exploration. BAPO решает эти проблемы через динамическую адаптацию clipping bounds, что балансирует положительные и отрицательные вклады в обновление политики. Метод показал впечатляющие результаты на бенчмарках AIME 2024/2025, превзойдя как open-source модели, так и проприетарные системы вроде o3-mini.'}, 'en': {'title': 'Dynamic Clipping for Enhanced Reinforcement Learning Stability', 'desc': 'BAlanced Policy Optimization with Adaptive Clipping (BAPO) enhances off-policy reinforcement learning by dynamically modifying clipping bounds to improve training efficiency and stability. The paper identifies issues like negative-advantage samples dominating the policy gradient, which can lead to poor performance and instability. It introduces the Entropy-Clip Rule, highlighting how fixed clipping mechanisms can hinder exploration by favoring over-exploitation. BAPO effectively addresses these challenges, resulting in faster and more efficient training across various scenarios, outperforming existing models on benchmark tests.'}, 'zh': {'title': '动态裁剪，平衡优化，提升强化学习效率', 'desc': 'BAPO（平衡策略优化与自适应裁剪）解决了离线强化学习中的挑战，通过动态调整裁剪边界来提高样本效率、稳定性和性能。该方法识别出优化中的不平衡现象，负优势样本主导策略梯度，抑制有用行为并可能导致梯度爆炸。同时，BAPO引入了熵裁剪规则，揭示了固定裁剪机制如何阻碍熵增加的更新，导致策略过度开发而忽视探索。通过在多种离线场景中应用，BAPO实现了快速、稳定和数据高效的训练，超越了多个开源和商业模型。'}}}, {'id': 'https://huggingface.co/papers/2510.19363', 'title': 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts', 'url': 'https://huggingface.co/papers/2510.19363', 'abstract': 'LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing "Aha" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.', 'score': 42, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'c9f40dde34631067', 'authors': ['Siyuan Wang', 'Gaokai Zhang', 'Li Lyna Zhang', 'Ning Shang', 'Fan Yang', 'Dongyao Chen', 'Mao Yang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19363.jpg', 'data': {'categories': ['#training', '#rl', '#long_context', '#reasoning'], 'emoji': '🔗', 'ru': {'title': 'Обучение LLM длинному рассуждению через цепочки UUID', 'desc': 'LoongRL — это метод reinforcement learning для улучшения рассуждений над длинными контекстами в больших языковых моделях. Ключевая идея — превращать короткие multi-hop вопросы в сложные задачи длиной 128K токенов, вставляя цепочки UUID среди отвлекающих документов. Обучение на таких данных формирует у модели паттерн «планируй-ищи-рассуждай-перепроверяй», который обобщается далеко за пределы длины обучающих примеров. На моделях Qwen2.5 метод даёт прирост точности +23.5% и позволяет 14B модели достичь результатов, сравнимых с гораздо большими frontier моделями вроде o3-mini и DeepSeek-R1.'}, 'en': {'title': 'Unlocking Long-Context Reasoning with LoongRL', 'desc': 'LoongRL is a novel reinforcement learning method designed to enhance long-context reasoning in large language models. It transforms short multi-hop question-answering tasks into more complex challenges by using UUID chains to obscure the actual question among distracting information. This approach encourages models to develop a systematic reasoning pattern that involves planning, retrieving, and verifying information, which significantly improves their performance on long-context tasks. The results show that models trained with LoongRL achieve substantial accuracy gains and can handle a much larger set of tasks than previously possible, while still maintaining their short-context reasoning abilities.'}, 'zh': {'title': 'LoongRL：提升长上下文推理的强化学习新方法', 'desc': 'LoongRL是一种数据驱动的强化学习方法，旨在增强长上下文推理能力。它通过将短多跳问答转化为高难度任务，提升了大型语言模型的准确性和泛化能力。LoongRL的核心是KeyChain方法，它通过插入UUID链来隐藏真实问题，从而在大量干扰文档中生成长上下文任务。经过KeyChain数据的强化学习训练，模型能够形成有效的推理模式，显著提高了多跳问答的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.15511', 'title': 'Language Models are Injective and Hence Invertible', 'url': 'https://huggingface.co/papers/2510.15511', 'abstract': "Transformer language models are proven to be injective, allowing exact input reconstruction from hidden activations, which has implications for transparency and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model's representations. In this paper, we challenge this view. First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, our work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment.", 'score': 42, 'issue_id': 6578, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '290696f608551ff7', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#math', '#architecture', '#training', '#security'], 'emoji': '🔄', 'ru': {'title': 'Трансформеры обратимы: восстановление входных данных из скрытых состояний', 'desc': 'Исследователи доказали, что transformer language models являются инъективными функциями, то есть разные входные последовательности всегда дают разные представления в скрытых слоях. Это свойство позволяет точно восстановить исходный текст из внутренних активаций модели, что подтверждено как математически, так и эмпирически на шести современных LLM. Авторы представили алгоритм SipIt, который эффективно реконструирует входной текст из скрытых состояний за линейное время. Открытие имеет важные последствия для прозрачности, интерпретируемости и безопасного использования языковых моделей.'}, 'en': {'title': 'Unlocking Transparency: Proving Injectivity in Transformer Models', 'desc': "This paper demonstrates that transformer language models are injective, meaning they can perfectly reconstruct input data from their hidden activations. The authors provide a mathematical proof that this injectivity is maintained throughout the training process, countering the belief that non-linear components could lead to information loss. They also conduct extensive empirical tests on multiple state-of-the-art models, confirming that no input collisions occur. Additionally, they introduce SipIt, a novel algorithm that efficiently reconstructs the original input from the model's representations, enhancing transparency and safety in AI applications."}, 'zh': {'title': '变换器模型的单射特性：透明性与安全性的基础', 'desc': '本论文探讨了变换器语言模型的单射特性，证明了从隐藏激活中可以精确重建输入。这一特性在模型初始化时就已确立，并在训练过程中得以保持。我们通过对六种最先进语言模型进行数十亿次碰撞测试，实验证明没有发生碰撞。我们还提出了SipIt算法，能够高效地从隐藏激活中重建精确的输入文本，确保了线性时间的可保证性和实际的可逆性。'}}}, {'id': 'https://huggingface.co/papers/2510.15731', 'title': 'Attention Sinks in Diffusion Language Models', 'url': 'https://huggingface.co/papers/2510.15731', 'abstract': 'Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  \t\t\t\t\tAI-generated summary \t\t\t\t Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.', 'score': 35, 'issue_id': 6571, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '97abd15d347fface', 'authors': ['Maximo Eduardo Rulli', 'Simone Petruzzi', 'Edoardo Michielon', 'Fabrizio Silvestri', 'Simone Scardapane', 'Alessio Devoto'], 'affiliations': ['Fastweb', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2510.15731.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#interpretability'], 'emoji': '🌊', 'ru': {'title': 'Динамические паттерны внимания в диффузионных языковых моделях', 'desc': 'Исследователи изучили механизмы внимания в Masked Diffusion Language Models (DLM) — новом классе языковых моделей, которые генерируют токены параллельно, в отличие от традиционных autoregressive моделей. Оказалось, что DLM также демонстрируют эффект attention sinking (концентрации внимания на определённых позициях), но с уникальными особенностями: позиции «стоков» внимания динамически меняются в процессе генерации. В отличие от autoregressive моделей, DLM остаются устойчивыми при удалении этих attention sinks — производительность падает незначительно. Эти результаты раскрывают фундаментальные различия в том, как диффузионные и autoregressive модели распределяют и используют механизмы внимания.'}, 'en': {'title': 'Unveiling the Dynamic Attention of Masked Diffusion Models', 'desc': 'This paper investigates Masked Diffusion Language Models (DLMs) and their attention mechanisms compared to Autoregressive Models (ARMs). It identifies a phenomenon called attention sinking, where certain tokens receive less attention during generation. The study finds that DLMs exhibit dynamic attention sinks that shift over time, unlike the static sinks in ARMs. Additionally, DLMs show robustness to the removal of these sinks, indicating a more resilient attention allocation strategy.'}, 'zh': {'title': '掩蔽扩散模型的注意力机制新发现', 'desc': '本研究分析了掩蔽扩散语言模型（DLMs）与自回归模型（ARMs）在注意力机制上的差异。DLMs使用双向注意力的变换器编码器，能够并行生成标记，并且在性能上与ARMs相当。我们发现DLMs存在注意力沉没现象，但其特征与ARMs不同，DLMs的沉没位置在生成过程中会动态变化。尽管ARMs对注意力沉没的去除非常敏感，但DLMs表现出较强的鲁棒性，掩蔽沉没对性能的影响较小。'}}}, {'id': 'https://huggingface.co/papers/2510.19430', 'title': 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model', 'url': 'https://huggingface.co/papers/2510.19430', 'abstract': 'GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.', 'score': 33, 'issue_id': 6570, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '3079040be10dd49e', 'authors': ['GigaBrain Team', 'Angen Ye', 'Boyuan Wang', 'Chaojun Ni', 'Guan Huang', 'Guosheng Zhao', 'Haoyun Li', 'Jie Li', 'Jiagang Zhu', 'Lv Feng', 'Peng Li', 'Qiuping Deng', 'Runqi Ouyang', 'Wenkang Qin', 'Xinze Chen', 'Xiaofeng Wang', 'Yang Wang', 'Yifan Li', 'Yilong Li', 'Yiran Ding', 'Yuan Xu', 'Yun Ye', 'Yukun Zhou', 'Zhehao Dong', 'Zhenan Wang', 'Zhichao Liu', 'Zheng Zhu'], 'affiliations': ['GigaAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.19430.jpg', 'data': {'categories': ['#small_models', '#agi', '#transfer_learning', '#agents', '#optimization', '#3d', '#training', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов на синтетических данных из world model', 'desc': 'GigaBrain-0 - это VLA-модель (Vision-Language-Action) для обучения роботов, которая использует синтетические данные, сгенерированные world model, вместо дорогих реальных данных с роботов. Модель применяет различные типы генерации данных: видео, переносы между средами (sim2real, real2real) и переносы от человека к роботу, что значительно улучшает обобщающую способность. Для повышения робастности политики используется RGBD-вход и embodied Chain-of-Thought reasoning, позволяющий модели рассуждать о пространственной геометрии и состояниях объектов. Эксперименты показывают превосходную генерализацию на сложных задачах манипуляции с вариациями внешнего вида, расположения объектов и точек обзора камеры.'}, 'en': {'title': 'GigaBrain-0: Revolutionizing Robot Learning with World Model Data', 'desc': "GigaBrain-0 is a new foundation model for Vision-Language-Action (VLA) that enhances the performance of robots in complex tasks by using data generated from world models. This approach reduces the need for expensive real-world data collection, allowing for better scalability and generalization across different tasks. By incorporating techniques like RGBD input modeling and Chain-of-Thought supervision, GigaBrain-0 improves the model's ability to understand spatial relationships and long-term dependencies. The model has shown significant improvements in real-world manipulation tasks, demonstrating its effectiveness in various scenarios and conditions."}, 'zh': {'title': 'GigaBrain-0：提升机器人任务泛化与鲁棒性的创新模型', 'desc': 'GigaBrain-0是一种新的视觉-语言-动作（VLA）基础模型，利用世界模型生成的数据来增强跨任务的泛化能力和策略的鲁棒性，从而提高复杂操作任务的实际表现。该模型通过生成多样化的数据，显著减少了对真实机器人数据的依赖，同时改善了模型在不同任务间的泛化能力。GigaBrain-0还通过RGBD输入建模和具身的思维链（CoT）监督，提升了策略的鲁棒性，使模型能够在任务执行中推理空间几何、物体状态和长期依赖关系。实验结果表明，GigaBrain-0在外观变化、物体放置和相机视角等方面表现出色，具有更强的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2510.19307', 'title': 'Unified Reinforcement and Imitation Learning for Vision-Language Models', 'url': 'https://huggingface.co/papers/2510.19307', 'abstract': 'A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.', 'score': 19, 'issue_id': 6568, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '9ddac8d5d67d2842', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yong Man Ro', 'Yu-Chiang Frank Wang', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19307.jpg', 'data': {'categories': ['#games', '#optimization', '#rlhf', '#cv', '#training', '#rl'], 'emoji': '🎓', 'ru': {'title': 'Компактные VLM учатся у гигантов через reinforcement и imitation learning', 'desc': 'Статья представляет алгоритм Unified Reinforcement and Imitation Learning (RIL) для создания эффективных и компактных vision-language моделей. Метод объединяет reinforcement learning с adversarial imitation learning, позволяя небольшим студенческим VLM имитировать генерацию текста крупных учительских моделей и улучшать свои возможности через сигналы подкрепления. В основе подхода лежит LLM-дискриминатор, который различает выходы студента и учителя, а также использует руководство от нескольких крупных VLM для разнообразного обучения. Эксперименты показывают, что компактные модели, обученные с помощью RIL, достигают производительности ведущих open-source и closed-source VLM, а в некоторых случаях превосходят их.'}, 'en': {'title': 'Efficient Learning for Powerful Vision-Language Models', 'desc': 'This paper presents a new training algorithm called Unified Reinforcement and Imitation Learning (RIL) for creating efficient vision-language models (VLMs). RIL combines reinforcement learning and adversarial imitation learning to enable smaller models to learn from larger, more complex teacher models. By using a discriminator based on large language models (LLMs), the student models can effectively mimic and improve upon the text generation capabilities of their teachers. The results show that these lightweight models can perform comparably to, or even better than, existing state-of-the-art VLMs, making them suitable for environments with limited resources.'}, 'zh': {'title': '统一强化与模仿学习，提升视觉语言模型性能', 'desc': '本文提出了一种统一的强化学习与模仿学习算法（RIL），旨在创建高效且轻量的视觉语言模型（VLMs）。该算法结合了强化学习和对抗模仿学习的优势，使得小型学生模型能够模仿大型教师模型的文本生成能力，并通过强化信号系统性地提升其生成能力。RIL的核心是一个基于大型语言模型（LLM）的判别器，能够有效区分学生和教师的输出，并通过多个大型教师VLM的指导确保多样化学习。实验结果表明，RIL显著缩小了与最先进的开源和闭源VLMs之间的性能差距，并在某些情况下超越了它们。'}}}, {'id': 'https://huggingface.co/papers/2510.19488', 'title': 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos', 'url': 'https://huggingface.co/papers/2510.19488', 'abstract': 'VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.', 'score': 17, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4237f40e97c8b930', 'authors': ['Dunjie Lu', 'Yiheng Xu', 'Junli Wang', 'Haoyuan Wu', 'Xinyuan Wang', 'Zekun Wang', 'Junlin Yang', 'Hongjin Su', 'Jixuan Chen', 'Junda Chen', 'Yuchen Mao', 'Jingren Zhou', 'Junyang Lin', 'Binyuan Hui', 'Tao Yu'], 'affiliations': ['Qwen Team, Alibaba Group', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.19488.jpg', 'data': {'categories': ['#training', '#dataset', '#video', '#data', '#agents', '#optimization', '#transfer_learning'], 'emoji': '🎥', 'ru': {'title': 'Обучение AI-агентов на YouTube видео вместо ручной разметки', 'desc': 'Исследователи создали VideoAgentTrek — систему, которая автоматически извлекает данные о взаимодействии с GUI из обычных YouTube-видео, устраняя необходимость дорогостоящей ручной разметки. В основе лежит модуль Video2Action, который определяет действия пользователя на видео и извлекает параметры вроде координат кликов и введённого текста. Обработав 39 тысяч обучающих видео, система сгенерировала 1.52 миллиона шагов взаимодействия для обучения AI-агентов. Результаты показали улучшение успешности выполнения задач на 70% по сравнению с базовым подходом, доказывая, что обычные видео из интернета могут стать качественным источником данных для обучения.'}, 'en': {'title': 'Transforming YouTube Videos into Training Gold for AI Agents', 'desc': 'VideoAgentTrek is a novel system that automatically extracts graphical user interface (GUI) interaction data from YouTube videos, addressing the challenge of obtaining large-scale training data for computer-use agents without manual annotation. It utilizes an inverse dynamics module called Video2Action, which includes a video grounding model to identify and time-stamp GUI actions, and an action-content recognizer to capture detailed parameters like click locations and text input. By applying this method to 39,000 tutorial videos, the system generated over 1.5 million interaction steps, significantly enhancing the training dataset. The results show a marked improvement in task success rates and step accuracy, demonstrating the effectiveness of using passive video content for training AI agents.'}, 'zh': {'title': '自动化提取视频交互数据，提升计算机代理性能', 'desc': 'VideoAgentTrek 是一个自动从 YouTube 视频中提取 GUI 交互数据的系统，使用了逆动态模块 Video2Action，显著提高了计算机使用代理的任务成功率和步骤准确性。该系统解决了手动标注大量交互数据的高成本问题，通过从公开的屏幕录制视频中自动挖掘训练数据。Video2Action 包含两个主要组件：视频定位模型和动作内容识别器，能够精确检测和提取 GUI 操作的时间边界和结构化参数。通过对 39,000 个 YouTube 教程视频的应用，我们的管道自动生成了 152 万个交互步骤，展示了被动互联网视频可以转化为高质量的计算机使用代理监督数据。'}}}, {'id': 'https://huggingface.co/papers/2510.19336', 'title': 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents', 'url': 'https://huggingface.co/papers/2510.19336', 'abstract': "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git", 'score': 16, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4d330d1d597c2031', 'authors': ['Kai Shi', 'Jun Yang', 'Ni Yang', 'Binqiang Pan', 'Qingsong Xie', 'Chao Zhang', 'Zhenyu Yang', 'Tianhuang Su', 'Haonan Lu'], 'affiliations': ['OPPO AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2510.19336.jpg', 'data': {'categories': ['#survey', '#training', '#architecture', '#dataset', '#data', '#benchmark', '#multimodal', '#optimization'], 'emoji': '📱', 'ru': {'title': 'DaMo: умная оптимизация данных для AI-агентов на смартфонах', 'desc': 'DaMo - это обучаемая нейросеть, которая оптимизирует состав обучающих данных для мультимодальных LLM, работающих в качестве агентов на мобильных телефонах. Метод предсказывает оптимальные пропорции различных датасетов для multitask обучения, достигая коэффициента детерминации R²=0.81 в экспериментах. Авторы представили PhoneAgentBench - первый специализированный бенчмарк для оценки мультимодальных задач на мобильных устройствах с 1235 вопросами из реальных приложений. DaMo показывает улучшение на 3.38% на PhoneAgentBench и на 2.57% в среднем на других бенчмарках по сравнению с альтернативными методами композиции данных.'}, 'en': {'title': 'Optimizing Data Mixtures for Enhanced Mobile Task Performance', 'desc': 'DaMo is a novel trainable network designed to optimize data mixtures for Multimodal Large Language Models (MLLMs), specifically enhancing their performance on mobile phone tasks. It addresses the challenge of determining the best training data compositions for multitask learning, which traditional methods struggle with. By predicting optimal data mixtures based on expected task performance, DaMo demonstrates significant improvements in various benchmarks, including a 3.38% increase on PhoneAgentBench. Additionally, it shows strong generalization capabilities across established benchmarks, outperforming existing methods and maintaining effectiveness across different model architectures.'}, 'zh': {'title': 'DaMo：优化多模态任务的数据组合', 'desc': 'DaMo是一种可训练的网络，旨在优化多模态大语言模型（MLLMs）中的数据组合，从而提升在手机任务和基准测试中的表现。该方法通过预测下游任务性能，确定最佳的数据混合比例，解决了多任务学习中数据组合选择的难题。我们还推出了PhoneAgentBench，这是第一个专门评估多模态手机任务的基准，包含1235个问答对，覆盖多种真实工业应用场景。实验结果表明，DaMo在多个基准测试中表现优越，尤其在BFCL-v3任务上提升了12.47%的指标。'}}}, {'id': 'https://huggingface.co/papers/2510.19808', 'title': 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2510.19808', 'abstract': "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.", 'score': 15, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '19f1485a76e3707f', 'authors': ['Yusu Qian', 'Eli Bocek-Rivele', 'Liangchen Song', 'Jialing Tong', 'Yinfei Yang', 'Jiasen Lu', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2510.19808.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#synthetic', '#alignment', '#reasoning'], 'emoji': '🍌', 'ru': {'title': 'Pico-Banana-400K: масштабный датасет для обучения редактированию изображений по текстовым инструкциям', 'desc': 'Статья представляет Pico-Banana-400K — датасет из 400 тысяч изображений для обучения моделей редактированию картинок по текстовым командам. Датасет создан на основе реальных фотографий из OpenImages с использованием системы Nano-Banana и включает разнообразные типы редактирования с контролем качества через MLLM. Помимо базовых примеров, датасет содержит три специализированных подмножества: для последовательного многошагового редактирования (72K примеров), для обучения с предпочтениями и reward-моделей (56K примеров), а также пары длинных и коротких инструкций. Этот ресурс создаёт основу для обучения и бенчмаркинга следующего поколения моделей редактирования изображений по тексту.'}, 'en': {'title': 'Unlocking Advanced Image Editing with Pico-Banana-400K', 'desc': 'Pico-Banana-400K is a new dataset designed for instruction-based image editing, containing 400,000 high-quality images. It features diverse edit pairs and supports complex editing tasks through multi-turn editing and preference subsets. The dataset is built from real images, ensuring high quality and relevance, and includes specialized subsets for various research needs. This resource aims to enhance the development and evaluation of advanced text-guided image editing models.'}, 'zh': {'title': 'Pico-Banana-400K：图像编辑的新基石', 'desc': 'Pico-Banana-400K是一个大规模、高质量的图像编辑数据集，专注于基于指令的图像编辑。该数据集包含多样的编辑对、多轮编辑和偏好子集，支持复杂的编辑场景研究。通过利用Nano-Banana生成真实照片的编辑对，Pico-Banana-400K确保了编辑类型的全面覆盖和内容的精确保留。这个数据集为下一代文本引导的图像编辑模型的训练和基准测试提供了坚实的基础。'}}}, {'id': 'https://huggingface.co/papers/2510.19817', 'title': 'olmOCR 2: Unit Test Rewards for Document OCR', 'url': 'https://huggingface.co/papers/2510.19817', 'abstract': 'olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.', 'score': 8, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'f414d75457c4f9ee', 'authors': ['Jake Poznanski', 'Luca Soldaini', 'Kyle Lo'], 'affiliations': ['Allen Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.19817.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#open_source', '#synthetic', '#optimization', '#cv'], 'emoji': '📄', 'ru': {'title': 'OCR нового поколения через обучение с подкреплением на unit-тестах', 'desc': "Представлена модель olmOCR 2 на базе vision language model с 7 миллиардами параметров для преобразования PDF-документов в текст. Модель обучена с помощью reinforcement learning с проверяемыми наградами (RLVR), где наградами служат бинарные unit-тесты на синтетических документах с известной разметкой. Система показывает state-of-the-art результаты на бенчмарке olmOCR-Bench, особенно в распознавании математических формул, парсинге таблиц и многоколоночных layout'ов. Модель, данные и код выпущены под открытыми лицензиями."}, 'en': {'title': 'Revolutionizing OCR with Reinforcement Learning!', 'desc': 'olmOCR 2 is an advanced optical character recognition (OCR) system that excels in converting printed documents into structured text. It utilizes a vision language model (VLM) with 7 billion parameters, trained through reinforcement learning with verifiable rewards to ensure high accuracy. The model is particularly effective in handling complex tasks such as math formula conversion, table parsing, and multi-column layouts. By generating synthetic documents for training, olmOCR 2 achieves state-of-the-art results on the olmOCR-Bench benchmark, demonstrating significant improvements over its predecessors.'}, 'zh': {'title': 'olmOCR 2：OCR任务的最优解', 'desc': 'olmOCR 2 是一种新型的视觉语言模型，专门用于光学字符识别（OCR）任务，特别是在数学公式转换、表格解析和多列布局方面表现出色。该模型使用强化学习和可验证奖励进行训练，确保了其在处理复杂文档时的高效性。我们开发了一种生成合成文档的管道，以创建多样化和具有挑战性的布局，并通过二元单元测试来评估模型性能。最终，olmOCR 2 在我们的英语OCR基准测试中达到了最先进的性能，显著提升了数学公式和表格的处理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.19592', 'title': 'Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.19592', 'abstract': 'Decomposed Attention Fusion (DecAF) enhances video object segmentation by refining attention maps from multimodal large language models without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF.', 'score': 7, 'issue_id': 6578, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '08731f24dee80293', 'authors': ['Su Ho Han', 'Jeongseok Hyun', 'Pilhyeon Lee', 'Minho Shim', 'Dongyoon Wee', 'Seon Joo Kim'], 'affiliations': ['Inha University', 'NAVER Cloud', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19592.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#video', '#games', '#reasoning', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Сегментация видео через внимание без переобучения', 'desc': 'Исследователи предложили метод DecAF для видео сегментации объектов, который использует карты внимания из мультимодальных LLM без необходимости дообучения моделей. Метод преобразует задачу рассуждений о видео в формат вопрос-ответ и извлекает карты внимания, которые затем улучшаются через два механизма: контрастное слияние объекта и фона, а также комплементарное слияние видео и кадров. Полученные карты внимания преобразуются в грубые маски сегментации, которые затем уточняются с помощью SAM2 под управлением внимания. DecAF превосходит другие методы без обучения и достигает результатов, сопоставимых с методами, требующими тренировки, на бенчмарках видео сегментации.'}, 'en': {'title': 'Refining Attention for Better Video Segmentation Without Retraining', 'desc': 'Decomposed Attention Fusion (DecAF) improves video object segmentation by refining attention maps generated from multimodal large language models (MLLMs) without the need for retraining. The approach treats video reasoning segmentation as a video question-answering task, utilizing a rollout mechanism to extract attention maps. To enhance the quality of these maps, DecAF employs two techniques: contrastive object-background fusion and complementary video-frame fusion, which help to reduce noise and improve alignment with object regions. This method not only produces coarse segmentation masks from attention maps but also introduces attention-guided prompting for finer masks, achieving results that rival traditional training-based methods.'}, 'zh': {'title': '无训练视频目标分割的新方法', 'desc': '本文提出了一种名为Decomposed Attention Fusion（DecAF）的方法，旨在通过多模态大语言模型（MLLMs）增强视频目标分割。该方法通过提取注意力图并进行精炼，避免了重新训练的过程。DecAF利用对比对象-背景融合和互补视频帧融合机制，减少了无关激活并增强了对象相关线索，从而将注意力图直接转换为粗略分割掩码。此外，本文还引入了基于注意力的SAM2提示，以获取更精细的掩码。'}}}, {'id': 'https://huggingface.co/papers/2510.16844', 'title': 'FinSight: Towards Real-World Financial Deep Research', 'url': 'https://huggingface.co/papers/2510.16844', 'abstract': 'FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.', 'score': 7, 'issue_id': 6567, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': '4316f8c0ef12ddf6', 'authors': ['Jiajie Jin', 'Yuyao Zhang', 'Yimeng Xu', 'Hongjin Qian', 'Yutao Zhu', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.16844.jpg', 'data': {'categories': ['#multimodal', '#agents', '#architecture'], 'emoji': '📊', 'ru': {'title': 'AI-аналитик создаёт финансовые отчёты профессионального уровня', 'desc': 'FinSight — это мультиагентный фреймворк для автоматической генерации профессиональных финансовых отчётов с графиками и аналитикой. Система использует архитектуру CAVM, которая объединяет данные, инструменты и агенты в программируемое пространство переменных для гибкого анализа через исполняемый код. Итеративный механизм улучшения визуализации последовательно превращает черновые графики в качественные финансовые диаграммы. Двухэтапный процесс написания расширяет краткие аналитические цепочки в связные мультимодальные отчёты со ссылками, превосходя существующие системы по точности и качеству презентации.'}, 'en': {'title': 'FinSight: Revolutionizing Financial Reporting with AI', 'desc': 'FinSight is a multi-agent framework designed to automate the generation of high-quality financial reports. It utilizes the Code Agent with Variable Memory (CAVM) architecture, which allows for flexible data integration and analysis through programmable code. The framework also includes an Iterative Vision-Enhanced Mechanism that improves visual outputs into professional-grade financial charts. Overall, FinSight outperforms existing AI systems in accuracy and presentation, making strides towards achieving human-expert quality in financial reporting.'}, 'zh': {'title': 'FinSight：智能生成高质量金融报告的未来', 'desc': 'FinSight是一个多智能体框架，采用可变内存的代码代理架构（CAVM），能够生成高质量的多模态金融报告。该系统通过可执行代码灵活地收集和分析数据，确保报告的准确性和专业性。为了提升可视化效果，FinSight引入了迭代视觉增强机制，逐步优化原始视觉输出。实验结果表明，FinSight在准确性、分析深度和展示质量上显著优于现有系统，接近人类专家的水平。'}}}, {'id': 'https://huggingface.co/papers/2510.15050', 'title': 'Directional Reasoning Injection for Fine-Tuning MLLMs', 'url': 'https://huggingface.co/papers/2510.15050', 'abstract': 'DRIFT, a lightweight method, enhances multimodal large language models\' reasoning ability by transferring knowledge in gradient space, outperforming naive merging and supervised fine-tuning with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are rapidly advancing, yet their reasoning ability often lags behind that of strong text-only counterparts. Existing methods to bridge this gap rely on supervised fine-tuning over large-scale multimodal reasoning data or reinforcement learning, both of which are resource-intensive. A promising alternative is model merging, which interpolates parameters between reasoning-enhanced LLMs and multimodal variants. However, our analysis shows that naive merging is not always a "free lunch": its effectiveness varies drastically across model families, with some (e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance degradation. To address this, we propose Directional Reasoning Injection for Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning knowledge in the gradient space, without destabilizing multimodal alignment. DRIFT precomputes a reasoning prior as the parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning. This approach preserves the simplicity of standard supervised fine-tuning pipelines while enabling efficient reasoning transfer. Extensive experiments on multimodal reasoning benchmarks, including MathVista and MathVerse, demonstrate that DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning, while matching or surpassing training-heavy methods at a fraction of the cost.', 'score': 7, 'issue_id': 6574, 'pub_date': '2025-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '9c13d4d86aef12dc', 'authors': ['Chao Huang', 'Zeliang Zhang', 'Jiang Liu', 'Ximeng Sun', 'Jialian Wu', 'Xiaodong Yu', 'Ze Wang', 'Chenliang Xu', 'Emad Barsoum', 'Zicheng Liu'], 'affiliations': ['Advanced Micro Devices, Inc.', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2510.15050.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#reasoning', '#multimodal', '#transfer_learning'], 'emoji': '🧭', 'ru': {'title': 'Перенос знаний в пространстве градиентов для улучшения рассуждений', 'desc': 'DRIFT — это легковесный метод для улучшения способности к рассуждениям у мультимодальных LLM. Вместо ресурсоёмкого supervised fine-tuning на больших датасетах, метод переносит знания в пространстве градиентов. DRIFT предварительно вычисляет разницу параметров между моделями с улучшенными рассуждениями и мультимодальными версиями, затем использует её для коррекции градиентов при обучении. Эксперименты показывают, что DRIFT превосходит наивное слияние моделей и стандартный fine-tuning при значительно меньших вычислительных затратах.'}, 'en': {'title': 'DRIFT: Efficient Reasoning Enhancement for Multimodal Models', 'desc': 'The paper introduces DRIFT, a novel method designed to enhance the reasoning capabilities of multimodal large language models (MLLMs) by transferring knowledge in the gradient space. Unlike traditional approaches that require extensive resources for supervised fine-tuning or reinforcement learning, DRIFT offers a lightweight alternative that maintains multimodal alignment. It precomputes a reasoning prior, which is the difference in parameters between reasoning-enhanced and multimodal models, and uses this to adjust gradients during fine-tuning. Experimental results show that DRIFT outperforms naive merging and standard fine-tuning methods, achieving better reasoning performance with significantly lower computational costs.'}, 'zh': {'title': 'DRIFT：高效推理能力的轻量级解决方案', 'desc': 'DRIFT是一种轻量级的方法，通过在梯度空间中转移知识，增强多模态大语言模型的推理能力。与传统的简单合并和监督微调方法相比，DRIFT在计算成本上显著降低，同时提升了推理性能。该方法通过预计算推理先验，利用参数空间的差异来偏置多模态微调过程中的梯度，从而保持多模态对齐的稳定性。实验结果表明，DRIFT在多个多模态推理基准上表现优于简单合并和监督微调，且在成本上远低于训练密集型的方法。'}}}, {'id': 'https://huggingface.co/papers/2510.19386', 'title': 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent', 'url': 'https://huggingface.co/papers/2510.19386', 'abstract': "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.", 'score': 6, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '671d46e29b93bd8f', 'authors': ['Ning Li', 'Qiqiang Lin', 'Zheng Wu', 'Xiaoyun Mo', 'Weiming Zhang', 'Yin Zhao', 'Xiangmou Qu', 'Jiamu Zhou', 'Jun Wang', 'Congmin Zheng', 'Yuanyi Song', 'Hongjiang Chen', 'Heyuan Huang', 'Jihong Wang', 'Jiaxin Yin', 'Jingwei Yu', 'Junwei Liao', 'Qiuying Peng', 'Xingyu Lou', 'Jun Wang', 'Weiwen Liu', 'Zhuosheng Zhang', 'Weinan Zhang'], 'affiliations': ['OPPO Research Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19386.jpg', 'data': {'categories': ['#rl', '#games', '#benchmark', '#security', '#agents', '#open_source', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'ColorAgent: умный OS-агент с обучением через взаимодействие', 'desc': 'ColorAgent — это AI-агент для операционной системы Android, способный выполнять сложные многошаговые задачи и персонализированно взаимодействовать с пользователем. Система использует пошаговое reinforcement learning и самообучение для долгосрочных взаимодействий со средой, а также multi-agent фреймворк для обеспечения надёжности и согласованности. На бенчмарках AndroidWorld и AndroidLab агент достиг результатов 77.2% и 50.7% соответственно, установив новый state-of-the-art. Авторы позиционируют ColorAgent не просто как инструмент автоматизации, а как персонального помощника, способного распознавать намерения пользователя и проактивно взаимодействовать с ним.'}, 'en': {'title': 'ColorAgent: Your Personalized OS Partner for Intelligent Interaction', 'desc': 'ColorAgent is an operating system agent that utilizes step-wise reinforcement learning within a multi-agent framework to enhance user interactions over long periods. It focuses on personalized user engagement, allowing the agent to understand and anticipate user needs, making it more than just an automation tool. The agent has been tested on Android benchmarks, achieving impressive success rates of 77.2% and 50.7%, setting a new standard in the field. The paper also highlights the need for improved evaluation methods for OS agents and suggests future research directions in collaboration and security.'}, 'zh': {'title': 'ColorAgent：智能操作系统的个性化交互新纪元', 'desc': 'ColorAgent是一种操作系统代理，采用逐步强化学习和多智能体框架，能够在长时间交互中实现高成功率，并提供个性化的用户参与体验。该代理通过增强模型能力，支持与环境的长时间交互，并进行自我进化训练。ColorAgent不仅是一个自动化工具，更是一个温暖的合作伙伴，能够识别用户意图并主动参与。我们在AndroidWorld和AndroidLab基准测试中评估了ColorAgent，分别取得了77.2%和50.7%的成功率，创造了新的技术标准。'}}}, {'id': 'https://huggingface.co/papers/2510.19316', 'title': 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints', 'url': 'https://huggingface.co/papers/2510.19316', 'abstract': "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.", 'score': 6, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '97796b9efcce99e8', 'authors': ['Kailin Jiang', 'Hongbo Jiang', 'Ning Jiang', 'Zhi Gao', 'Jinhe Bi', 'Yuchen Ren', 'Bin Li', 'Yuntao Du', 'Lei Liu', 'Qing Li'], 'affiliations': ['Beijing Institute of Technology', 'Ludwig Maximilian University of Munich', 'Northeast Forestry University', 'Shandong University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'University of Science and Technology of China', 'University of Sydney', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19316.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'KORE: инъекция знаний без забывания старого', 'desc': 'KORE — это метод для внедрения новых знаний в большие мультимодальные модели с сохранением ранее изученной информации. Метод использует структурированные аугментации данных, автоматически преобразуя отдельные факты в комплексные знания для точного обучения. Для предотвращения катастрофического забывания KORE сохраняет предыдущие знания в ковариационной матрице активаций линейных слоев LLM и инициализирует адаптер проекцией в нулевое пространство этой матрицы. Эксперименты на моделях LLaVA и Qwen2.5-VL показали превосходную производительность в инъекции знаний и эффективное сохранение старой информации.'}, 'en': {'title': 'KORE: Retain the Old, Embrace the New in Multimodal Learning!', 'desc': 'KORE is a novel method designed to enhance large multimodal models by injecting new knowledge while ensuring that previously learned information is retained. It utilizes structured augmentations and covariance matrix constraints to minimize the risk of catastrophic forgetting, which is when a model loses old knowledge while learning new information. By converting knowledge items into structured formats, KORE allows models to accurately learn and adapt to new data. Experimental results demonstrate that KORE significantly improves the performance of knowledge injection in various multimodal models without compromising their existing knowledge.'}, 'zh': {'title': 'KORE：知识注入与保留的完美平衡', 'desc': 'KORE是一种向大型多模态模型注入新知识的方法，同时保留旧知识。它通过结构化增强和协方差矩阵约束来最小化灾难性遗忘。KORE能够有效地将新知识适应到模型中，同时确保旧知识的保留。实验表明，KORE在多种大型多模态模型上表现出色，显著提高了新知识注入的效果。'}}}, {'id': 'https://huggingface.co/papers/2510.19028', 'title': "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues", 'url': 'https://huggingface.co/papers/2510.19028', 'abstract': "Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) are increasingly used in human-AI interactions, their social reasoning capabilities in interpersonal contexts are critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean, sourced from movie scripts. The task involves evaluating models' social reasoning capability to infer the interpersonal relationships (e.g., friends, sisters, lovers) between speakers in each dialogue. Each dialogue is annotated with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by native (or equivalent) Korean and English speakers from Korea and the U.S. Evaluating nine models on our task, current proprietary LLMs achieve around 75-80% on the English dataset, whereas their performance on Korean drops to 58-69%. More strikingly, models select Unlikely relationships in 10-25% of their responses. Furthermore, we find that thinking models and chain-of-thought prompting, effective for general reasoning, provide minimal benefits for social reasoning and occasionally amplify social biases. Our findings reveal significant limitations in current LLMs' social reasoning capabilities, highlighting the need for efforts to develop socially-aware language models.", 'score': 6, 'issue_id': 6570, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '8920c08609cd558d', 'authors': ['Eunsu Kim', 'Junyeong Park', 'Juhyun Oh', 'Kiwoong Park', 'Seyoung Song', 'A. Seza Dogruoz', 'Najoung Kim', 'Alice Oh'], 'affiliations': ['Boston University', 'KAIST', 'University of Ghent'], 'pdf_title_img': 'assets/pdf/title_img/2510.19028.jpg', 'data': {'categories': ['#low_resource', '#alignment', '#dataset', '#multilingual', '#reasoning'], 'emoji': '👥', 'ru': {'title': 'LLM не понимают человеческие отношения так хорошо, как мы думали', 'desc': 'Исследователи представили датасет SCRIPTS для оценки способности языковых моделей определять межличностные отношения между людьми в диалогах на английском и корейском языках. Современные LLM показывают приличные результаты на английском (75-80%), но их точность на корейском падает до 58-69%, что указывает на языковой разрыв. Удивительно, но thinking models и chain-of-thought prompting практически не улучшают качество социального reasoning и иногда даже усиливают предвзятость. Результаты демонстрируют серьёзные ограничения текущих LLM в понимании социального контекста и подчёркивают необходимость разработки более социально-осведомлённых моделей.'}, 'en': {'title': 'Enhancing Social Reasoning in Language Models', 'desc': 'This paper discusses the limitations of current large language models (LLMs) in social reasoning, especially in understanding interpersonal relationships across languages. The authors introduce a new dataset called SCRIPTS, which consists of dialogues in English and Korean, to evaluate how well models can infer relationships like friends or lovers between speakers. They found that while LLMs perform reasonably well in English, their accuracy drops significantly in Korean, and they often misclassify relationships. The study highlights that traditional reasoning techniques, such as chain-of-thought prompting, do not improve social reasoning and may even exacerbate biases, indicating a need for more socially-aware AI models.'}, 'zh': {'title': '提升语言模型的社会推理能力', 'desc': '当前的大型语言模型在社会推理方面存在显著局限，尤其是在跨语言推断人际关系方面。我们提出了SCRIPTS，这是一个包含英语和韩语的对话数据集，旨在评估模型在推断对话中说话者之间人际关系的能力。通过对九个模型的评估，发现当前的专有大型语言模型在英语数据集上的表现为75-80%，而在韩语数据集上的表现下降至58-69%。我们的研究表明，现有模型在社会推理能力上存在重大不足，亟需开发更具社会意识的语言模型。'}}}, {'id': 'https://huggingface.co/papers/2510.18313', 'title': 'OmniNWM: Omniscient Driving Navigation World Models', 'url': 'https://huggingface.co/papers/2510.18313', 'abstract': 'OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.', 'score': 6, 'issue_id': 6567, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '234f9481e9fb3cf2', 'authors': ['Bohan Li', 'Zhuang Ma', 'Dalong Du', 'Baorui Peng', 'Zhujin Liang', 'Zhenqiang Liu', 'Chao Ma', 'Yueming Jin', 'Hao Zhao', 'Wenjun Zeng', 'Xin Jin'], 'affiliations': ['Eastern Institute of Technology, Ningbo', 'National University of Singapore', 'PhiGent', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18313.jpg', 'data': {'categories': ['#training', '#video', '#games', '#agents', '#3d', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Всевидящая world model для автономного вождения с панорамным видео и 3D-наградами', 'desc': 'OmniNWM — это универсальная world model для автономного вождения, которая одновременно генерирует панорамные видео с RGB, семантикой, глубиной и 3D occupancy. Модель использует представление траекторий через нормализованные Plucker ray-maps на уровне пикселей, что обеспечивает точный контроль над генерацией видео. Вместо обучения функции reward через внешние модели, система определяет плотные награды напрямую из сгенерированной 3D occupancy на основе правил безопасности вождения. Эксперименты показывают state-of-the-art результаты в генерации видео, точности управления и стабильности на длинных горизонтах предсказания.'}, 'en': {'title': 'OmniNWM: The Future of Autonomous Driving World Models', 'desc': 'OmniNWM is a comprehensive world model designed for autonomous driving that excels in generating panoramic videos and encoding actions with Plucker ray-maps. It effectively integrates state, action, and reward dimensions into a single framework, overcoming limitations of previous models. The model generates high-quality panoramic videos that include RGB, semantics, metric depth, and 3D occupancy, while also providing precise control through pixel-level action encoding. By utilizing 3D occupancy for defining dense rewards, OmniNWM ensures compliance and safety in driving, achieving top performance in video generation and control stability.'}, 'zh': {'title': 'OmniNWM：自动驾驶的全景导航新模型', 'desc': 'OmniNWM是一种统一的世界模型，专为自动驾驶设计，能够生成全景视频，并使用Plucker光线图编码动作，同时基于3D占用定义密集奖励。该模型在状态、动作和奖励三个核心维度上表现出色，克服了现有模型的局限性。OmniNWM生成RGB、语义、度量深度和3D占用的全景视频，采用灵活的强制策略实现高质量的长时间自回归生成。通过直接利用生成的3D占用定义基于规则的密集奖励，OmniNWM在视频生成、控制精度和长期稳定性方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.19286', 'title': 'TheMCPCompany: Creating General-purpose Agents with Task-specific Tools', 'url': 'https://huggingface.co/papers/2510.19286', 'abstract': "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.", 'score': 5, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'd7d7013ab05a9068', 'authors': ['Reza Esfandiarpoor', 'Vishwas Suryanarayanan', 'Stephen H. Bach', 'Vishal Chowdhary', 'Anthony Aue'], 'affiliations': ['Brown University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.19286.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning'], 'emoji': '🔧', 'ru': {'title': 'Тысячи инструментов — испытание для AI-агентов', 'desc': 'Исследователи представили TheMCPCompany — бенчмарк для оценки AI-агентов, использующих инструменты через REST API для взаимодействия с реальными сервисами. Бенчмарк включает более 18,000 инструментов, созданных на основе Model Context Protocol (MCP), и позволяет сравнивать performance агентов с инструментами и браузерных агентов. Эксперименты показали, что продвинутые модели типа GPT-5 хорошо справляются с простыми окружениями, но испытывают серьёзные трудности в сложных корпоративных средах с тысячами инструментов. Результаты демонстрируют, что навигация по большому количеству инструментов и их комбинирование для решения сложных задач остаётся вызовом для современных LLM и требует улучшения как reasoning, так и retrieval моделей.'}, 'en': {'title': 'Navigating Complexity: Evaluating Tool-Calling Agents in Real-World Environments', 'desc': 'The paper introduces TheMCPCompany, a benchmark designed to evaluate tool-calling agents that interact with real-world services through REST APIs. It highlights the performance of advanced models in simpler environments, where they excel, but reveals their limitations in complex enterprise settings. The study demonstrates that while tool retrieval enhances agent performance, smaller models struggle to utilize the available tools effectively. Ultimately, the findings suggest that improving reasoning and retrieval capabilities is essential for navigating intricate tasks involving numerous tools.'}, 'zh': {'title': '评估工具调用代理的挑战与机遇', 'desc': 'TheMCPCompany是一个评估工具调用代理的基准，使用REST API与真实世界服务进行交互。研究表明，尽管先进的模型在简单环境中表现良好，但在复杂的企业环境中却面临挑战。我们创建了一个包含超过18,000个工具的MCP服务器，并提供了每个任务的手动标注真实工具。实验结果显示，虽然所有模型在工具检索方面的表现优于基于浏览器的代理，但较小的模型无法充分利用可用工具。'}}}, {'id': 'https://huggingface.co/papers/2510.19127', 'title': 'Steering Autoregressive Music Generation with Recursive Feature Machines', 'url': 'https://huggingface.co/papers/2510.19127', 'abstract': 'MusicRFM uses Recursive Feature Machines to enable real-time, fine-grained control over pre-trained music models by steering their internal activations, improving musical note accuracy with minimal impact on prompt fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model\'s internal gradients to produce interpretable "concept directions", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen\'s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain.', 'score': 5, 'issue_id': 6585, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '4e0373104d475f82', 'authors': ['Daniel Zhao', 'Daniel Beaglehole', 'Taylor Berg-Kirkpatrick', 'Julian McAuley', 'Zachary Novack'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.19127.jpg', 'data': {'categories': ['#audio', '#interpretability', '#open_source', '#data', '#optimization', '#inference'], 'emoji': '🎹', 'ru': {'title': 'Точное управление музыкальными моделями через внутренние активации', 'desc': 'MusicRFM использует Recursive Feature Machines для управления предобученными музыкальными моделями без необходимости их дообучения. Метод анализирует внутренние градиенты модели, чтобы найти "направления концептов" в пространстве активаций, соответствующие музыкальным атрибутам вроде нот и аккордов. Во время генерации эти направления внедряются обратно в модель MusicGen, что позволяет управлять процессом в реальном времени. Точность генерации целевой ноты повышается с 0.23 до 0.82 при минимальном влиянии на соответствие текстовому промпту.'}, 'en': {'title': 'Fine-Grained Control in Music Generation with MusicRFM', 'desc': "MusicRFM is a novel framework that utilizes Recursive Feature Machines (RFMs) to enhance the control of pre-trained music generation models. By analyzing internal gradients, RFMs identify specific 'concept directions' that correspond to musical features, allowing for precise manipulation of the model's outputs. This approach enables real-time adjustments to the generated music without the need for retraining or introducing artifacts. The results show a significant improvement in musical note accuracy while maintaining high fidelity to the original prompts, demonstrating a successful balance between control and quality in music generation."}, 'zh': {'title': '实时精细控制音乐生成的创新方法', 'desc': 'MusicRFM 是一种使用递归特征机器（RFM）的框架，旨在实现对预训练音乐模型的实时、精细控制。通过直接引导模型的内部激活，MusicRFM 能够提高音乐音符的准确性，同时对提示的保真度影响最小。该方法通过分析模型的内部梯度，生成可解释的“概念方向”，对应于音乐属性如音符或和弦。我们的方法在控制和生成质量之间成功平衡，显著提高了目标音乐音符的生成准确性，同时保持了与未引导基线的提示一致性。'}}}, {'id': 'https://huggingface.co/papers/2510.18941', 'title': 'ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge', 'url': 'https://huggingface.co/papers/2510.18941', 'abstract': 'ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench', 'score': 5, 'issue_id': 6569, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '5e5633a00c2a8ce8', 'authors': ['Zhilin Wang', 'Jaehun Jung', 'Ximing Lu', 'Shizhe Diao', 'Ellie Evans', 'Jiaqi Zeng', 'Pavlo Molchanov', 'Yejin Choi', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2510.18941.jpg', 'data': {'categories': ['#data', '#science', '#open_source', '#dataset', '#benchmark'], 'emoji': '🎓', 'ru': {'title': 'ProfBench: даже лучшие LLM справляются только на 66% с профессиональными задачами', 'desc': 'Исследователи представили ProfBench - бенчмарк для оценки больших языковых моделей в профессиональных областях, содержащий более 7000 пар ответов и критериев оценки от экспертов с PhD по физике и химии, MBA в финансах и консалтинге. Для доступной оценки созданы специальные LLM-судьи, которые снижают стоимость тестирования в сотни раз и устраняют проблему завышения моделями собственных оценок. Результаты показывают, что даже лучшие модели вроде GPT-5-high достигают только 65.9% производительности на этом сложном бенчмарке. Обнаружен значительный разрыв между проприетарными и открытыми моделями в решении комплексных профессиональных задач.'}, 'en': {'title': 'ProfBench: Bridging the Gap in LLM Evaluation for Professional Domains', 'desc': 'ProfBench is a benchmark designed to evaluate large language models (LLMs) specifically in professional domains using criteria set by human experts. It includes over 7000 response-criterion pairs evaluated by professionals in fields like Physics, Chemistry, Finance, and Consulting. The study highlights the limitations of current LLMs, revealing that even the best models struggle with complex tasks, achieving only 65.9% performance on average. Additionally, it uncovers significant performance differences between proprietary models and those with open weights, emphasizing the importance of extended reasoning in professional applications.'}, 'zh': {'title': 'ProfBench：评估专业领域大型语言模型的挑战与差异', 'desc': 'ProfBench是一个评估大型语言模型在专业领域表现的工具，使用人类专家的标准进行评估。它包含超过7000对响应标准，由物理、化学、金融和咨询等领域的专家进行评估。研究发现，即使是最先进的模型，如GPT-5-high，在整体表现上也仅达到65.9%。此外，研究还揭示了专有模型与开放权重模型之间的显著性能差异，以及扩展思维在处理复杂专业任务中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.18940', 'title': "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning", 'url': 'https://huggingface.co/papers/2510.18940', 'abstract': 'NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as leq 0.02% trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: https://github.com/FightingFighting/NeuroAda.git.', 'score': 5, 'issue_id': 6568, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'fcfd58e5ced254c9', 'authors': ['Zhi Zhang', 'Yixian Shen', 'Congfeng Cao', 'Ekaterina Shutova'], 'affiliations': ['ILLC, University of Amsterdam', 'PCS, University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2510.18940.jpg', 'data': {'categories': ['#open_source', '#small_models', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'NeuroAda: тонкая настройка нейросетей через обходные пути для важных параметров', 'desc': 'NeuroAda — это метод эффективной тонкой настройки (PEFT), который объединяет селективную адаптацию с обходными соединениями для достижения высокой производительности. Метод сначала определяет важные параметры в сети, а затем создаёт для них bypass-соединения, обновляя только их при обучении и оставляя исходные веса замороженными. Это позволяет достичь детальной адаптации модели как при selective adaptation, но с экономией памяти как у LoRA. Эксперименты на 23+ задачах показали state-of-the-art результаты при обучении всего 0.02% параметров и снижении потребления памяти CUDA до 60%.'}, 'en': {'title': 'Efficient Fine-Tuning with NeuroAda: Less is More!', 'desc': 'NeuroAda is a new method for fine-tuning machine learning models that focuses on being efficient with parameters and memory. It combines two techniques: selective adaptation, which targets important model parameters, and bypass connections, which allow for updates without changing the entire model. This approach enables precise adjustments to the model while keeping most of it unchanged, leading to significant memory savings. Tests show that NeuroAda performs exceptionally well on various tasks, using only a tiny fraction of trainable parameters and reducing memory usage significantly.'}, 'zh': {'title': '高效微调，性能卓越的NeuroAda', 'desc': 'NeuroAda是一种参数高效的微调方法，它结合了选择性适应和旁路连接，以实现高性能和最小的可训练参数。该方法首先识别出重要的参数，然后为这些参数引入旁路连接。在微调过程中，仅更新旁路连接，保持原始模型参数不变。实验结果表明，NeuroAda在23个以上的任务中表现出色，使用的可训练参数少于0.02%，并且CUDA内存使用量减少了60%。'}}}, {'id': 'https://huggingface.co/papers/2510.17932', 'title': 'From Charts to Code: A Hierarchical Benchmark for Multimodal Models', 'url': 'https://huggingface.co/papers/2510.17932', 'abstract': 'Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.', 'score': 5, 'issue_id': 6571, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '515c35b7e783d482', 'authors': ['Jiahao Tang', 'Henry Hengyuan Zhao', 'Lijian Wu', 'Yifei Tao', 'Dongxing Mao', 'Yang Wan', 'Jingru Tan', 'Min Zeng', 'Min Li', 'Alex Jinpeng Wang'], 'affiliations': ['CSU-JPG, Central South University', 'Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.17932.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#multimodal', '#interpretability', '#optimization', '#benchmark', '#games'], 'emoji': '📊', 'ru': {'title': 'Научить AI создавать графики — сложнее, чем кажется', 'desc': 'Chart2Code — это иерархический бенчмарк для оценки способностей больших мультимодальных моделей понимать графики и генерировать код. Бенчмарк состоит из трёх уровней возрастающей сложности: воспроизведение графиков, их редактирование и создание графиков из больших таблиц. В тестировании участвовали 25 современных LLM, включая GPT-5, но даже лучшие модели показали невысокие результаты — GPT-5 набрал в среднем 0.57 по качеству кода и 0.22 по качеству визуализации. Бенчмарк содержит 2023 задачи по 22 типам графиков и призван стимулировать развитие мультимодальных моделей для практических задач работы с визуализацией данных.'}, 'en': {'title': 'Chart2Code: Elevating Chart Understanding in AI', 'desc': 'Chart2Code is a new benchmark designed to evaluate how well large multimodal models (LMMs) can understand charts and generate corresponding code. It features three levels of tasks that increase in complexity, starting from simple chart reproduction to more complex tasks like editing charts and generating charts from detailed tables. The benchmark includes 2,023 tasks across 22 different chart types, with metrics to assess both the accuracy of the generated code and the quality of the charts produced. Initial tests on state-of-the-art models, including GPT-5, show that these tasks are quite challenging, highlighting the need for further advancements in multimodal reasoning capabilities.'}, 'zh': {'title': 'Chart2Code：图表理解与代码生成的新基准', 'desc': 'Chart2Code是一个用于评估大型多模态模型（LMM）在图表理解和代码生成能力的分层基准。它设计了三个复杂度逐渐增加的任务，涵盖了多种真实场景。第一级任务是从参考图形和用户查询中重现图表；第二级任务涉及复杂的修改，如更改图表类型或添加元素；第三级任务则要求模型将信息密集的长表格转换为符合用户指令的图表。通过对25个最先进的LMM进行基准测试，结果显示即使是最先进的模型GPT-5在任务中的表现也相对较低，表明Chart2Code的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2510.19457', 'title': 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models', 'url': 'https://huggingface.co/papers/2510.19457', 'abstract': "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.", 'score': 4, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '016313b1f4c1861b', 'authors': ['Kailin Jiang', 'Ning Jiang', 'Yuchen Ren', 'Yuchen Li', 'Yifan Gao', 'Jinhe Bi', 'Yunpu Ma', 'Qingqing Liu', 'Xianhao Wang', 'Yifan Jia', 'Hongbo Jiang', 'Yaocong Hu', 'Bin Li', 'Lei Liu', 'Yuntao Du'], 'affiliations': ['Anhui Polytechnic University', 'Beijing Institute of Technology', 'Ludwig Maximilian University of Munich', 'Northeast Forestry University', 'Shandong University', 'University of Science and Technology of China', 'University of Sydney', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19457.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#reasoning'], 'emoji': '⏰', 'ru': {'title': 'MINED: учим мультимодальные модели понимать время', 'desc': 'Статья представляет MINED — новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) понимать знания, чувствительные ко времени. Бенчмарк включает 2104 образца по 6 типам знаний и оценивает модели по 11 задачам в 6 измерениях: когнитивность, осведомлённость, надёжность, понимание, рассуждение и робастность. Тестирование 15 популярных LMM показало, что Gemini-2.5-Pro достигает лучшего результата (63.07%), в то время как большинство open-source моделей испытывают трудности с временны́м пониманием. Авторы также исследуют методы knowledge editing для обновления временны́х знаний в LMM и демонстрируют их эффективность в сценариях единичного редактирования.'}, 'en': {'title': 'Enhancing Temporal Awareness in Large Multimodal Models with MINED', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding time-sensitive factual knowledge due to their static representations. The authors introduce MINED, a new benchmark designed to assess LMMs' temporal awareness across six dimensions and eleven tasks, including cognition and reasoning. The benchmark is built from Wikipedia and includes 2,104 samples of time-sensitive knowledge. The evaluation reveals that while some LMMs, like Gemini-2.5-Pro, perform well, many open-source models struggle with temporal understanding, particularly in areas like sports."}, 'zh': {'title': '提升时间敏感知识理解的基准评估', 'desc': '大型多模态模型（LMMs）通过跨模态预训练编码丰富的事实知识，但它们的静态表示在理解时间敏感的事实知识方面存在困难。现有的基准测试由于设计静态，无法充分评估LMMs对时间敏感知识的理解能力。为了解决这个问题，我们提出了MINED，这是一个全面的基准，评估时间意识的六个关键维度和十一项具有挑战性的任务。通过对15个广泛使用的LMMs进行评估，发现Gemini-2.5-Pro在时间敏感知识的理解上表现最佳，而大多数开源LMMs仍然缺乏这种能力。'}}}, {'id': 'https://huggingface.co/papers/2510.18917', 'title': 'RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling', 'url': 'https://huggingface.co/papers/2510.18917', 'abstract': 'RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.  \t\t\t\t\tAI-generated summary \t\t\t\t Room impulse responses are a core resource for dereverberation, robust speech recognition, source localization, and room acoustics estimation. We present RIR-Mega, a large collection of simulated RIRs described by a compact, machine friendly metadata schema and distributed with simple tools for validation and reuse. The dataset ships with a Hugging Face Datasets loader, scripts for metadata checks and checksums, and a reference regression baseline that predicts RT60 like targets from waveforms. On a train and validation split of 36,000 and 4,000 examples, a small Random Forest on lightweight time and spectral features reaches a mean absolute error near 0.013 s and a root mean square error near 0.022 s. We host a subset with 1,000 linear array RIRs and 3,000 circular array RIRs on Hugging Face for streaming and quick tests, and preserve the complete 50,000 RIR archive on Zenodo. The dataset and code are public to support reproducible studies.', 'score': 4, 'issue_id': 6568, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '2355986738e20368', 'authors': ['Mandip Goswami'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2510.18917.jpg', 'data': {'categories': ['#dataset', '#data', '#open_source', '#science'], 'emoji': '🔊', 'ru': {'title': 'Мега-датасет импульсных откликов помещений для акустических исследований', 'desc': 'Представлен RIR-Mega — крупный датасет симулированных импульсных откликов помещений (room impulse responses), который используется для задач дереверберации, распознавания речи и оценки акустики. Датасет содержит 50,000 примеров с компактными метаданными, инструментами валидации и базовой моделью для предсказания времени реверберации RT60 из аудиосигналов. Базовая модель Random Forest на простых временных и спектральных признаках достигает средней абсолютной ошибки около 0.013 секунд на валидационной выборке. Датасет открыто доступен на Hugging Face и Zenodo для воспроизводимых научных исследований.'}, 'en': {'title': 'RIR-Mega: A Comprehensive Dataset for Room Impulse Response Analysis', 'desc': 'RIR-Mega is a comprehensive dataset designed for simulating room impulse responses (RIRs), which are essential for various audio processing tasks like dereverberation and speech recognition. It includes a structured metadata schema and tools for easy validation and reuse, making it accessible for researchers. The dataset features a baseline model that utilizes a Random Forest algorithm to predict RT60 values from audio waveforms, achieving impressive accuracy metrics. Additionally, RIR-Mega is available on platforms like Hugging Face and Zenodo, promoting reproducibility in research.'}, 'zh': {'title': 'RIR-Mega：房间脉冲响应数据集的创新与应用', 'desc': 'RIR-Mega是一个大型的模拟房间脉冲响应数据集，旨在为去混响、稳健的语音识别、声源定位和房间声学估计提供核心资源。该数据集采用紧凑的元数据架构，便于机器处理，并配备简单的验证和重用工具。我们提供了一个基准模型，使用轻量级的时间和频谱特征，通过随机森林算法预测RT60，取得了较低的平均绝对误差和均方根误差。数据集和代码都是公开的，以支持可重复的研究。'}}}, {'id': 'https://huggingface.co/papers/2510.18428', 'title': 'AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library', 'url': 'https://huggingface.co/papers/2510.18428', 'abstract': 'AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.', 'score': 3, 'issue_id': 6568, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '6d876f8d11b4ea83', 'authors': ['Minwei Kong', 'Ao Qu', 'Xiaotong Guo', 'Wenbin Ouyang', 'Chonghe Jiang', 'Han Zheng', 'Yining Ma', 'Dingyi Zhuang', 'Yuhan Tang', 'Junyi Li', 'Hai Wang', 'Cathy Wu', 'Jinhua Zhao'], 'affiliations': ['London School of Economics and Political Science', 'Massachusetts Institute of Technology', 'Singapore Management University', 'Singapore-MIT Alliance for Research and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.18428.jpg', 'data': {'categories': ['#interpretability', '#dataset', '#transfer_learning', '#optimization', '#rlhf', '#training'], 'emoji': '📚', 'ru': {'title': 'Библиотека опыта для самообучения LLM в оптимизационном моделировании', 'desc': 'AlphaOPT — это самообучающаяся библиотека опыта, которая позволяет LLM автоматизировать оптимизационное моделирование без дорогостоящего переобучения. Система работает в два этапа: извлекает проверенные решателем инсайты из неудачных попыток и уточняет условия их применимости для лучшего переноса знаний. AlphaOPT может обучаться даже на одних ответах без размеченных рассуждений, накапливая знания в явном интерпретируемом виде. На датасете OptiBench система показала улучшение с 65% до 72% при увеличении обучающих данных и превзошла лучший baseline на 7.7%.'}, 'en': {'title': 'Empowering LLMs to Optimize Without Costly Retraining', 'desc': 'AlphaOPT is a novel library designed to enhance the capabilities of large language models (LLMs) in optimization modeling by learning from limited examples and solver feedback. It operates through a two-phase cycle: first, it learns from past failures to extract structured insights, and second, it refines these insights to improve their applicability across different tasks. This approach allows AlphaOPT to continuously evolve without the need for expensive retraining, as it updates its knowledge library instead of the model parameters. Experimental results demonstrate that AlphaOPT improves performance significantly with more data and outperforms existing methods on challenging datasets.'}, 'zh': {'title': 'AlphaOPT：自我改进的优化建模库', 'desc': 'AlphaOPT是一个自我改进的库，能够让大型语言模型（LLM）从有限的示范和求解器反馈中学习，从而提高优化建模的能力。它通过两个阶段的循环工作：首先是库学习阶段，从失败的尝试中提取经过求解器验证的结构化见解；其次是库演化阶段，诊断检索不匹配并优化存储见解的适用条件。AlphaOPT的设计使得它能够在没有昂贵重训练的情况下，持续扩展和提高性能，并且使知识对人类可解释和可干预。实验结果表明，AlphaOPT在数据量增加时表现出持续的改进，且在特定数据集上超越了最强基线。'}}}, {'id': 'https://huggingface.co/papers/2510.18909', 'title': 'Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection', 'url': 'https://huggingface.co/papers/2510.18909', 'abstract': 'The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, directly selecting top-scored data often degrades performance, and sampling from a broader range is required to recover results. The above non-monotonicity between dataset scores and downstream benchmark results reveals a fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, which preserves both quality and diversity during data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, a Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs.', 'score': 3, 'issue_id': 6570, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'f33b553748178ffe', 'authors': ['Hongyi He', 'Xiao Liu', 'Zhenghao Lin', 'Mingni Tang', 'Yi Cheng', 'Jintao Wang', 'Wenjie Li', 'Peng Cheng', 'Yeyun Gong'], 'affiliations': ['Microsoft Research', 'The Hong Kong Polytechnic University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18909.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#dataset', '#data', '#training'], 'emoji': '⊥', 'ru': {'title': 'Ортогональность измерений для разнообразия обучающих данных LLM', 'desc': "Алгоритм ODiS улучшает качество обучающих данных для больших языковых моделей путём одновременного обеспечения высокого качества и разнообразия данных. Традиционные методы отбора данных по скорингу страдают от фундamentального недостатка: они схлопывают коррелирующие измерения, что приводит к потере разнообразия при выборе топовых данных. ODiS решает эту проблему через ортогональную декомпозицию метрик оценки с помощью PCA, разделяя их на независимые измерения качества языка, знаний и сложности понимания. Модели, обученные на данных отобранных ODiS, показывают значительно лучшие результаты на benchmark'ах по сравнению с базовыми методами."}, 'en': {'title': 'Enhancing Language Models with Quality and Diversity through ODiS', 'desc': 'The Orthogonal Diversity-Aware Selection (ODiS) algorithm improves the performance of large language models by focusing on both the quality and diversity of training data. It uses orthogonal decomposition to separate evaluation metrics, ensuring that selected data is not only high-quality but also covers a wide range of topics and styles. Traditional methods often fail because they prioritize high scores without considering the diversity of the data, leading to suboptimal model performance. ODiS addresses this issue by evaluating data across multiple dimensions and selecting the best samples from each, resulting in a more effective training dataset.'}, 'zh': {'title': '正交多样性意识选择算法提升语言模型性能', 'desc': 'ODiS算法通过对评估维度的正交分解，提升了大型语言模型的性能，确保训练数据的质量和多样性。高质量的预训练数据不仅要具备事实可靠性和语义价值，还需涵盖广泛的内容和分布异质性。传统方法通常依赖于单一或多维评分选择，但直接选择高分数据往往会降低性能，因此需要从更广泛的范围进行采样。ODiS算法通过多维评估和主成分分析（PCA）去相关化，确保在每个正交维度中选择高分数据，从而实现质量与多样性的平衡。'}}}, {'id': 'https://huggingface.co/papers/2510.18091', 'title': 'Accelerating Vision Transformers with Adaptive Patch Sizes', 'url': 'https://huggingface.co/papers/2510.18091', 'abstract': 'Adaptive Patch Transformers (APT) improve Vision Transformer (ViT) efficiency by using variable patch sizes, enhancing speed without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\\% faster training and inference in visual QA, object detection, and semantic segmentation.', 'score': 3, 'issue_id': 6586, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': 'b2cbb090934b5cb9', 'authors': ['Rohan Choudhury', 'JungEun Kim', 'Jinhyung Park', 'Eunho Yang', 'László A. Jeni', 'Kris M. Kitani'], 'affiliations': ['Carnegie Mellon University', 'General Robotics', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2510.18091.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#inference', '#training'], 'emoji': '🧩', 'ru': {'title': 'Умные патчи: больше скорости, меньше токенов', 'desc': 'Adaptive Patch Transformers (APT) улучшают эффективность Vision Transformer за счёт использования патчей разного размера в одном изображении. Метод выделяет большие патчи для однородных областей и маленькие для сложных, сокращая общее количество токенов. APT ускоряет inference и обучение на 40-50% для больших моделей без потери качества. Технология легко применяется к уже обученным ViT и показывает прирост скорости до 30% в задачах visual QA, детекции объектов и семантической сегментации.'}, 'en': {'title': 'Boosting Vision Transformers with Adaptive Patch Sizes!', 'desc': 'Adaptive Patch Transformers (APT) enhance the efficiency of Vision Transformers (ViTs) by utilizing variable patch sizes tailored to the content of the image. This approach reduces the number of input tokens by applying larger patches in simpler areas and smaller patches in more complex regions, leading to faster processing. APT significantly increases throughput, achieving up to 50% speedup on ViT models while preserving performance on various visual tasks. The method allows for quicker training and inference times, making it suitable for high-resolution dense visual applications without sacrificing accuracy.'}, 'zh': {'title': '自适应补丁，提升视觉变换器效率', 'desc': '自适应补丁变换器（APT）通过使用可变的补丁大小来提高视觉变换器（ViT）的效率，从而在不影响性能的情况下提升速度。APT在同一图像中使用多种不同的补丁大小，解决了传统ViT在处理高分辨率图像时输入序列过长的问题。通过在同质区域分配较大的补丁大小，而在复杂区域使用较小的补丁，APT显著减少了输入标记的总数。APT在ViT推理和训练中实现了显著的加速，提升了40%到50%的吞吐量，同时保持了下游任务的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.19753', 'title': 'When Do Transformers Learn Heuristics for Graph Connectivity?', 'url': 'https://huggingface.co/papers/2510.19753', 'abstract': "Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an L-layer model has capacity to solve for graphs with diameters up to exactly 3^L, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic.", 'score': 2, 'issue_id': 6570, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'fea19a943f96f863', 'authors': ['Qilin Ye', 'Deqing Fu', 'Robin Jia', 'Vatsal Sharan'], 'affiliations': ['Duke University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2510.19753.jpg', 'data': {'categories': ['#architecture', '#graphs', '#optimization', '#training'], 'emoji': '🔗', 'ru': {'title': 'Трансформеры учат алгоритмы только в пределах своих возможностей', 'desc': 'Исследование показывает, что Transformer-модели часто не могут выучить обобщаемые алгоритмы и вместо этого полагаются на простые эвристики. На примере задачи связности графов авторы доказывают, что упрощённый disentangled Transformer с L слоями способен решать графы с диаметром до 3^L, реализуя алгоритм возведения матрицы смежности в степень. Ключевой вывод: если обучающие данные находятся в пределах capacity модели, то Transformer учит правильный алгоритм, а если превышают эти пределы — переключается на простую эвристику, основанную на степенях вершин. Ограничение сложности обучающих данных позволяет моделям выучить точный алгоритм вместо приближённых правил.'}, 'en': {'title': 'Unlocking Algorithm Learning in Transformers', 'desc': "This paper investigates the limitations of Transformers in learning generalizable algorithms, particularly in the context of graph connectivity. It introduces the disentangled Transformer, which can theoretically solve graph problems up to a certain complexity defined by its layers. The study shows that when training data is within the model's capacity, the Transformer learns the correct algorithm, while data beyond this capacity leads to reliance on simpler heuristics. The findings emphasize the importance of aligning training data complexity with model capacity to achieve effective learning outcomes."}, 'zh': {'title': '解耦变换器：从启发式到算法的转变', 'desc': '本文探讨了变换器（Transformers）在学习可推广算法时的困难，通常依赖于脆弱的启发式方法。我们使用图的连通性作为测试平台，理论和实证分析了这一现象。研究表明，解耦变换器（disentangled Transformer）能够在其能力范围内学习图算法，但在超出能力范围时则退化为启发式方法。通过限制训练数据在模型能力范围内，可以使变换器学习到准确的算法，而不是基于节点度的启发式方法。'}}}, {'id': 'https://huggingface.co/papers/2510.18840', 'title': 'See the Text: From Tokenization to Visual Reading', 'url': 'https://huggingface.co/papers/2510.18840', 'abstract': 'SeeTok, a vision-centric method, renders text as images and uses pretrained multimodal LLMs to interpret them, offering efficiency and robustness improvements over traditional subword tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.', 'score': 2, 'issue_id': 6575, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '3216ef93935e1583', 'authors': ['Ling Xing', 'Alex Jinpeng Wang', 'Rui Yan', 'Hongyu Qu', 'Zechao Li', 'Jinhui Tang'], 'affiliations': ['Central South University', 'Nanjing Forestry University', 'Nanjing University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.18840.jpg', 'data': {'categories': ['#multimodal', '#cv', '#optimization', '#low_resource', '#data'], 'emoji': '👁️', 'ru': {'title': 'Чтение текста глазами: когда LLM видит слова как картинки', 'desc': 'SeeTok представляет новый подход к обработке текста в языковых моделях, преобразуя текст в изображения вместо традиционной токенизации на подслова. Метод использует предобученные мультимодальные LLM с их встроенными способностями OCR для интерпретации визуального текста, что имитирует человеческое восприятие. Это позволяет сократить количество токенов в 4.43 раза и уменьшить вычислительные затраты на 70.5%, особенно для языков с низкими ресурсами. Подход демонстрирует лучшую устойчивость к опечаткам и кросс-языковую генерализацию, приближая AI к естественному визуальному чтению человека.'}, 'en': {'title': 'Transforming Text into Images for Smarter Language Processing', 'desc': 'SeeTok is a novel approach that transforms text into images, allowing pretrained multimodal large language models (LLMs) to interpret them more effectively. This method addresses the limitations of traditional subword tokenization, which can lead to inefficiencies, especially in low-resource languages. By leveraging visual-text representations, SeeTok reduces the number of tokens needed and significantly lowers computational costs while improving performance across various language tasks. Ultimately, this approach mimics human reading by focusing on visual recognition, paving the way for more intuitive and robust language models.'}, 'zh': {'title': 'SeeTok：从符号分词到视觉阅读的转变', 'desc': 'SeeTok是一种以视觉为中心的方法，它将文本呈现为图像，并利用预训练的多模态大语言模型（LLM）进行解读。这种方法比传统的子词分词技术更高效、更稳健，尤其在处理低资源语言时表现更佳。SeeTok通过将文本转化为视觉文本，利用强大的光学字符识别（OCR）和文本-视觉对齐能力，显著减少了所需的标记数量和计算量。该方法在多个语言任务中表现出色，标志着从符号分词向更自然的人类视觉阅读的转变。'}}}, {'id': 'https://huggingface.co/papers/2510.18279', 'title': 'Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text\n  Inputs in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2510.18279', 'abstract': 'Rendering text as images reduces token usage for decoder LLMs without compromising performance on tasks like long-context retrieval and document summarization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.', 'score': 2, 'issue_id': 6587, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '42242aa89d01ef4a', 'authors': ['Yanhong Li', 'Zixuan Lan', 'Jiawei Zhou'], 'affiliations': ['Allen Institute for AI', 'Stony Brook University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2510.18279.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#optimization', '#data', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Текст как картинка: новый способ сжатия входных данных для LLM', 'desc': 'Исследователи предложили необычный метод сжатия текстовых входных данных для декодерных LLM: преобразовывать текст в изображения. Мультимодальные языковые модели могут обрабатывать визуальный текст, что позволяет существенно сократить количество токенов на входе. Эксперименты на бенчмарках RULER (поиск в длинном контексте) и CNN/DailyMail (суммаризация документов) показали, что метод сокращает число токенов почти вдвое без потери качества. Это открывает новые возможности для оптимизации работы с длинными текстами в AI-системах.'}, 'en': {'title': 'Compressing Text with Images: A Token-Saving Breakthrough!', 'desc': 'This paper explores a novel approach to reduce token usage in decoder large language models (LLMs) by converting text into images. By rendering long text inputs as single images, the authors demonstrate that it is possible to significantly decrease the number of tokens needed for processing. The method maintains performance on tasks such as long-context retrieval and document summarization, showing that visual representations can effectively compress textual data. Experiments on benchmarks like RULER and CNN/DailyMail confirm that this text-as-image technique can save nearly half the tokens while preserving the quality of the output.'}, 'zh': {'title': '文本图像化：高效压缩输入的创新方法', 'desc': '本论文探讨了将文本渲染为图像以减少解码器大语言模型（LLMs）中的令牌使用量的可能性。我们发现，这种视觉文本表示法是一种有效的输入压缩方式，能够在不影响任务性能的情况下显著减少所需的解码器令牌数量。通过将长文本输入作为单个图像提供给模型，我们实现了新的输入压缩形式。实验结果表明，在长上下文检索和文档摘要等任务中，该方法能够节省大量令牌，且性能保持良好。'}}}, {'id': 'https://huggingface.co/papers/2510.16435', 'title': 'What Questions Should Robots Be Able to Answer? A Dataset of User\n  Questions for Explainable Robotics', 'url': 'https://huggingface.co/papers/2510.16435', 'abstract': "A dataset of user questions for household robots is introduced, providing insights into the types of questions users ask and the information robots need to answer, supporting the development of conversational interfaces and explanation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t With the growing use of large language models and conversational interfaces in human-robot interaction, robots' ability to answer user questions is more important than ever. We therefore introduce a dataset of 1,893 user questions for household robots, collected from 100 participants and organized into 12 categories and 70 subcategories. Most work in explainable robotics focuses on why-questions. In contrast, our dataset provides a wide variety of questions, from questions about simple execution details to questions about how the robot would act in hypothetical scenarios -- thus giving roboticists valuable insights into what questions their robot needs to be able to answer. To collect the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots performing varied household tasks. We then asked participants on Prolific what questions they would want to ask the robot in each portrayed situation. In the final dataset, the most frequent categories are questions about task execution details (22.5%), the robot's capabilities (12.7%), and performance assessments (11.3%). Although questions about how robots would handle potentially difficult scenarios and ensure correct behavior are less frequent, users rank them as the most important for robots to be able to answer. Moreover, we find that users who identify as novices in robotics ask different questions than more experienced users. Novices are more likely to inquire about simple facts, such as what the robot did or the current state of the environment. As robots enter environments shared with humans and language becomes central to giving instructions and interaction, this dataset provides a valuable foundation for (i) identifying the information robots need to log and expose to conversational interfaces, (ii) benchmarking question-answering modules, and (iii) designing explanation strategies that align with user expectations.", 'score': 2, 'issue_id': 6582, 'pub_date': '2025-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': '01413106af764f9f', 'authors': ['Lennart Wachowiak', 'Andrew Coles', 'Gerard Canal', 'Oya Celiktutan'], 'affiliations': ['Kings College London, CDT in Safe and Trusted AI, UK', 'Kings College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2510.16435.jpg', 'data': {'categories': ['#alignment', '#agents', '#multimodal', '#interpretability', '#dataset', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Что люди хотят спросить у домашних роботов', 'desc': 'Исследователи создали датасет из 1893 вопросов, которые люди задают домашним роботам, собранных от 100 участников и организованных в 12 категорий. Большинство вопросов касаются деталей выполнения задач (22.5%), возможностей робота (12.7%) и оценки его работы (11.3%). Новички в робототехнике задают более простые вопросы о действиях робота, тогда как опытные пользователи интересуются гипотетическими сценариями. Этот датасет поможет разработчикам создавать conversational интерфейсы для роботов с использованием LLM и проектировать стратегии объяснений, соответствующие ожиданиям пользователей.'}, 'en': {'title': 'Empowering Robots with User Questions for Better Interaction', 'desc': 'This paper presents a new dataset containing 1,893 user questions directed at household robots, categorized into 12 main types and 70 subtypes. The dataset highlights the diverse inquiries users have, ranging from simple task execution to complex hypothetical scenarios, which is crucial for developing effective conversational interfaces. It reveals that novice users tend to ask more straightforward questions compared to experienced users, who may focus on more intricate details. Overall, this dataset serves as a foundational resource for improving robotic question-answering capabilities and aligning them with user expectations.'}, 'zh': {'title': '家庭机器人问答的关键数据集', 'desc': '本文介绍了一个关于家庭机器人用户提问的数据集，包含1893个问题，帮助理解用户对机器人的提问类型及机器人所需的信息。这些问题分为12个类别和70个子类别，涵盖了从简单执行细节到假设场景的多种提问。研究发现，初学者和有经验的用户在提问时存在差异，初学者更倾向于询问简单事实。该数据集为开发对话界面、评估问答模块和设计符合用户期望的解释策略提供了重要基础。'}}}, {'id': 'https://huggingface.co/papers/2510.15015', 'title': 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage\n  Mitigation in Text-to-Image Models', 'url': 'https://huggingface.co/papers/2510.15015', 'abstract': "DeLeaker mitigates semantic leakage in text-to-image models by dynamically reweighting attention maps during the diffusion process, outperforming existing methods without compromising quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable to semantic leakage, the unintended transfer of semantically related features between distinct entities. Existing mitigation strategies are often optimization-based or dependent on external inputs. We introduce DeLeaker, a lightweight, optimization-free inference-time approach that mitigates leakage by directly intervening on the model's attention maps. Throughout the diffusion process, DeLeaker dynamically reweights attention maps to suppress excessive cross-entity interactions while strengthening the identity of each entity. To support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages), the first dataset dedicated to semantic leakage, comprising 1,130 human-verified samples spanning diverse scenarios, together with a novel automatic evaluation framework. Experiments demonstrate that DeLeaker consistently outperforms all baselines, even when they are provided with external information, achieving effective leakage mitigation without compromising fidelity or quality. These results underscore the value of attention control and pave the way for more semantically precise T2I models.", 'score': 2, 'issue_id': 6577, 'pub_date': '2025-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'b01733ac711d2f30', 'authors': ['Mor Ventura', 'Michael Toker', 'Or Patashnik', 'Yonatan Belinkov', 'Roi Reichart'], 'affiliations': ['Technion', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2510.15015.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference', '#leakage', '#benchmark', '#dataset'], 'emoji': '🔒', 'ru': {'title': 'DeLeaker: устранение семантической утечки в генерации изображений через управление вниманием', 'desc': 'Исследователи представили DeLeaker — метод борьбы с семантической утечкой в text-to-image моделях, когда характеристики одного объекта непреднамеренно переносятся на другой при генерации. Подход работает во время инференса, динамически перевешивая attention maps в процессе диффузии, подавляя избыточные взаимодействия между разными сущностями. Для оценки создан датасет SLIM из 1130 верифицированных примеров с автоматической системой метрик. DeLeaker превосходит существующие методы без ущерба для качества изображений, демонстрируя эффективность прямого контроля внимания в диффузионных моделях.'}, 'en': {'title': 'DeLeaker: Enhancing T2I Models by Controlling Semantic Leakage', 'desc': 'DeLeaker is a new method designed to reduce semantic leakage in text-to-image (T2I) models, which occurs when features from different entities unintentionally mix. Unlike previous methods that rely on optimization or external inputs, DeLeaker operates at inference time by adjusting attention maps during the diffusion process. This approach helps to minimize unwanted interactions between different entities while enhancing their distinct identities. The effectiveness of DeLeaker is validated through experiments and a new dataset called SLIM, which focuses on semantic leakage, showing that it outperforms existing techniques without sacrificing image quality.'}, 'zh': {'title': 'DeLeaker：提升文本到图像模型的语义精确度', 'desc': 'DeLeaker是一种新方法，用于减少文本到图像模型中的语义泄漏问题。它通过在扩散过程中动态调整注意力图，来抑制不同实体之间的过度交互。与现有的优化方法不同，DeLeaker不依赖外部输入，能够在推理时直接干预模型的注意力机制。实验结果表明，DeLeaker在不降低图像质量的情况下，显著优于其他方法，展示了注意力控制在提高语义精确度方面的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.19492', 'title': 'Machine Text Detectors are Membership Inference Attacks', 'url': 'https://huggingface.co/papers/2510.19492', 'abstract': "Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model's probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (rho > 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce MINT, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks.", 'score': 1, 'issue_id': 6570, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'a43f47a5cd4d15c3', 'authors': ['Ryuto Koike', 'Liam Dugan', 'Masahiro Kaneko', 'Chris Callison-Burch', 'Naoaki Okazaki'], 'affiliations': ['Institute of Science Tokyo', 'MBZUAI', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2510.19492.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#benchmark', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Два в одном: атаки на приватность и детекция AI-текста используют одни методы', 'desc': 'Исследование показывает сильную взаимосвязь между атаками на определение членства в обучающей выборке (MIA) и детекцией машинно-генерированного текста, так как оба метода используют похожие сигналы из вероятностного распределения языковой модели. Авторы теоретически доказали, что оптимальная метрика для обеих задач одинакова, и эмпирически подтвердили высокую корреляцию производительности методов при переносе между задачами. Особенно примечательно, что метод Binoculars, изначально разработанный для детекции AI-текста, показал state-of-the-art результаты и в MIA задачах. Для объединения исследований представлен MINT — единый бенчмарк для оценки 15 методов из обеих областей.'}, 'en': {'title': 'Bridging the Gap: Unifying Membership Inference and Text Detection', 'desc': 'This paper explores the connection between membership inference attacks (MIAs) and machine-generated text detection, revealing that methods from one task can effectively apply to the other. The authors demonstrate that both tasks share a common metric that maximizes performance, suggesting that insights from one area can enhance the other. Through extensive experiments, they show a strong correlation in performance across different methods, indicating that techniques like Binoculars can excel in both domains. To support collaboration and evaluation, the paper introduces MINT, a unified framework for assessing both MIAs and machine-generated text detection methods.'}, 'zh': {'title': '跨任务合作，提升模型性能！', 'desc': '本研究探讨了成员推断攻击（MIA）与机器生成文本检测之间的强转移性，强调了跨任务合作的重要性。尽管这两项任务的目标不同，但它们的方法常常基于语言模型的概率分布，利用相似的信号。我们证明了在这两项任务中，达到最高性能的度量是相同的，并且提出了MINT，一个统一的评估工具，以促进这两个领域的交叉发展。我们的实验证明了不同任务之间的性能排名具有很强的相关性，显示了转移性的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.18034', 'title': 'SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection', 'url': 'https://huggingface.co/papers/2510.18034', 'abstract': 'SAVANT, a structured reasoning framework using VLMs, achieves high accuracy and recall in detecting anomalous driving scenarios through layered scene analysis and multi-modal evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.', 'score': 0, 'issue_id': 6585, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': 'f3256a215b58d803', 'authors': ['Roberto Brusnicki', 'David Pop', 'Yuan Gao', 'Mattia Piccinini', 'Johannes Betz'], 'affiliations': ['Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, 85748 Garching, Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI)'], 'pdf_title_img': 'assets/pdf/title_img/2510.18034.jpg', 'data': {'categories': ['#open_source', '#cv', '#dataset', '#reasoning', '#training', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'SAVANT: Структурированный анализ аномалий для автономного вождения', 'desc': 'SAVANT — это фреймворк для обнаружения редких и аномальных дорожных ситуаций с использованием Vision Language Models (VLM). Система использует структурированный подход, анализируя сцену по четырём семантическим слоям: улица, инфраструктура, движущиеся объекты и окружающая среда. В отличие от обычных prompting подходов, двухфазный pipeline SAVANT достигает точности 88% и recall 89.6% на реальных данных. Ключевое достижение — файнтюненная open-source модель на 7B параметров превосходит проприетарные решения, достигая 93.8% точности, что делает систему доступной для локального развёртывания.'}, 'en': {'title': 'SAVANT: Structured Reasoning for Safer Autonomous Driving', 'desc': 'SAVANT is a structured reasoning framework that enhances the detection of unusual driving situations using Vision Language Models (VLMs). It improves performance by analyzing scenes in layers, focusing on key elements like streets and objects, rather than relying on simple prompts. This method allows for high accuracy and recall rates, achieving 90.8% recall and 93.8% accuracy with a fine-tuned open-source model. By automatically labeling a large dataset of images, SAVANT also helps solve the data scarcity issue in anomaly detection for autonomous driving systems.'}, 'zh': {'title': 'SAVANT：高效检测异常驾驶场景的智能框架', 'desc': 'SAVANT是一个使用视觉语言模型（VLMs）的结构化推理框架，能够通过分层场景分析和多模态评估高效检测异常驾驶场景。该框架将VLM的推理过程从简单的提示转变为系统化的分析，涵盖街道、基础设施、可移动物体和环境四个语义层次。SAVANT在真实驾驶场景中实现了89.6%的召回率和88.0%的准确率，显著优于无结构基线。通过自动标注9640多张真实世界图像，SAVANT有效解决了异常检测中的数据稀缺问题，为自主系统提供了可靠的语义监测路径。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (7)', '#agi (1)', '#alignment (3)', '#architecture (7)', '#audio (1)', '#benchmark (16)', '#cv (6)', '#data (9)', '#dataset (12)', '#diffusion (1)', '#ethics', '#games (5)', '#graphs (1)', '#hallucinations', '#healthcare', '#inference (4)', '#interpretability (6)', '#leakage (1)', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (13)', '#open_source (9)', '#optimization (22)', '#plp', '#rag', '#reasoning (10)', '#rl (5)', '#rlhf (3)', '#robotics', '#science (2)', '#security (2)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (17)', '#transfer_learning (7)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-24 00:49',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-24 00:49')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-24 00:49')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    