{
    "date": {
        "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 8",
        "zh": "8æœˆ8æ—¥"
    },
    "time_utc": "2025-08-08 04:37",
    "weekday": 4,
    "issue_id": 5243,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.05004",
            "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data",
            "url": "https://huggingface.co/papers/2508.05004",
            "abstract": "R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.",
            "score": 33,
            "issue_id": 5242,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "4e0838dc787e59cf",
            "authors": [
                "Chengsong Huang",
                "Wenhao Yu",
                "Xiaoyang Wang",
                "Hongming Zhang",
                "Zongxia Li",
                "Ruosen Li",
                "Jiaxin Huang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Seattle Lab",
                "The University of Texas at Dallas",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05004.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·ÑƒĞ¼Ñƒ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "R-Zero - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Challenger Ğ¸ Solver, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸ Ñ€ĞµÑˆĞ°Ñ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. R-Zero Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ R-Zero."
                },
                "en": {
                    "title": "Autonomous Learning for Super-Intelligent AI",
                    "desc": "R-Zero is an innovative framework that enables Large Language Models (LLMs) to autonomously create and learn from their own training data, eliminating the need for human-curated tasks. It consists of two models, a Challenger and a Solver, which interact and evolve together; the Challenger proposes tasks that push the Solver's limits, while the Solver learns to tackle these challenges. This self-improving process generates a focused curriculum that enhances reasoning capabilities without relying on pre-existing labels or tasks. Empirical results show that R-Zero significantly boosts the performance of various LLMs on reasoning benchmarks, demonstrating its potential to advance AI systems beyond human intelligence."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½æ¡†æ¶ï¼Œè¶…è¶Šäººç±»æ™ºèƒ½çš„æœªæ¥",
                    "desc": "R-Zeroæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå’Œå­¦ä¹ è‡ªå·±çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥ç­–åˆ’çš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆå§‹åŒ–ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹â€”â€”æŒ‘æˆ˜è€…å’Œè§£å†³è€…ï¼Œæ¥å®ç°æ¨¡å‹çš„å…±åŒè¿›åŒ–ã€‚æŒ‘æˆ˜è€…è´Ÿè´£æå‡ºæ¥è¿‘è§£å†³è€…èƒ½åŠ›è¾¹ç•Œçš„ä»»åŠ¡ï¼Œè€Œè§£å†³è€…åˆ™ä¸“æ³¨äºè§£å†³è¿™äº›æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§äº’åŠ¨ï¼ŒR-Zeroèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡å’Œæ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆä¸€ä¸ªæœ‰é’ˆå¯¹æ€§çš„è‡ªæˆ‘æå‡è¯¾ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒåŸºç¡€LLMsçš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05405",
            "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning",
            "url": "https://huggingface.co/papers/2508.05405",
            "abstract": "DeepPHY evaluates Vision Language Models' physical reasoning and control through simulated environments with varying difficulty levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.",
            "score": 26,
            "issue_id": 5243,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "3ec0b6b2d584612a",
            "authors": [
                "Xinrun Xu",
                "Pi Bu",
                "Ye Wang",
                "BÃ¶rje F. Karlsson",
                "Ziming Wang",
                "Tengtao Song",
                "Qi Zhu",
                "Jun Song",
                "Zhiming Ding",
                "Bo Zheng"
            ],
            "affiliations": [
                "Informatics Department, PUC-Rio",
                "Institute of Software, Chinese Academy of Science",
                "Renmin University of China",
                "Taobao & Tmall Group of Alibaba",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05405.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DeepPHY: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…",
                    "desc": "DeepPHY - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. DeepPHY Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "DeepPHY: Bridging the Gap in Physical Reasoning for Vision Language Models",
                    "desc": "DeepPHY is a benchmark framework that assesses Vision Language Models (VLMs) on their ability to understand and apply physical reasoning in simulated environments. It highlights the challenges VLMs face in executing precise actions and planning in complex scenarios, which are essential for real-world tasks. The framework includes various environments with different difficulty levels and uses detailed metrics for evaluation. Results show that even advanced VLMs have difficulty converting their understanding of physical concepts into accurate control actions."
                },
                "zh": {
                    "title": "DeepPHYï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›",
                    "desc": "DeepPHYæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰©ç†æ¨ç†å’Œæ§åˆ¶æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰ä¸åŒéš¾åº¦çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç³»ç»Ÿåœ°æµ‹è¯•VLMså¯¹åŸºæœ¬ç‰©ç†åŸç†çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å½“å‰çš„VLMsåœ¨æ„ŸçŸ¥å’Œè§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ï¼Œå®ƒä»¬åœ¨ç»†èŠ‚å…³æ³¨å’Œç²¾ç¡®è¡ŒåŠ¨è§„åˆ’æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„VLMsä¹Ÿéš¾ä»¥å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®çš„é¢„æµ‹æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05629",
            "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
            "url": "https://huggingface.co/papers/2508.05629",
            "abstract": "Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.",
            "score": 20,
            "issue_id": 5242,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "50bf66dc29886a85",
            "authors": [
                "Yongliang Wu",
                "Yizhou Zhou",
                "Zhou Ziheng",
                "Yingzhe Peng",
                "Xinyu Ye",
                "Xinting Hu",
                "Wenbo Zhu",
                "Lu Qi",
                "Ming-Hsuan Yang",
                "Xu Yang"
            ],
            "affiliations": [
                "Independent Researcher",
                "Nanyang Technological University",
                "Shanghai Jiao Tong University",
                "Southeast University",
                "University of California, Berkeley",
                "University of California, Los Angeles",
                "University of California, Merced",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (DFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). DFT Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (SFT). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ SFT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ SFT."
                },
                "en": {
                    "title": "Dynamic Fine-Tuning: Elevating LLM Generalization with Smart Gradients",
                    "desc": "This paper introduces Dynamic Fine-Tuning (DFT), a method that enhances the generalization of Large Language Models (LLMs) by adjusting gradient updates. The authors identify that traditional Supervised Fine-Tuning (SFT) can limit model performance due to its inherent reward structure. By dynamically rescaling the objective function based on token probabilities, DFT stabilizes the training process and leads to better outcomes on various benchmarks. The results indicate that DFT not only surpasses SFT but also performs competitively in offline reinforcement learning scenarios, making it a valuable advancement in model training techniques."
                },
                "zh": {
                    "title": "åŠ¨æ€å¾®è°ƒï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼",
                    "desc": "åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´æ¢¯åº¦çš„ç¼©æ”¾ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼ŒDFTåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜ï¼Œä¸”åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä¹Ÿå±•ç°å‡ºç«äº‰åŠ›ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†SFTçš„æ¢¯åº¦éšå«äº†ä¸€ä¸ªæœ‰é—®é¢˜çš„å¥–åŠ±ç»“æ„ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚DFTé€šè¿‡æ ¹æ®æ¯ä¸ªtokençš„æ¦‚ç‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°ï¼Œç¨³å®šäº†æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05635",
            "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation",
            "url": "https://huggingface.co/papers/2508.05635",
            "abstract": "Genie Envisioner integrates policy learning, evaluation, and simulation using a video diffusion model and neural simulator for instruction-driven robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.",
            "score": 11,
            "issue_id": 5243,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "f4ca777a8500b711",
            "authors": [
                "Yue Liao",
                "Pengfei Zhou",
                "Siyuan Huang",
                "Donglin Yang",
                "Shengcong Chen",
                "Yuxin Jiang",
                "Yue Hu",
                "Jingbin Cai",
                "Si Liu",
                "Jianlan Luo",
                "Liliang Chen",
                "Shuicheng Yan",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "BUAA",
                "LV-Lab",
                "NUS",
                "Unified World Foundation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05635.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#robotics",
                    "#agi",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Genie Envisioner (GE) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GE Ğ»ĞµĞ¶Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. GE-Act Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° GE-Sim ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ EWMBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unified Framework for Instruction-Driven Robotic Manipulation",
                    "desc": "Genie Envisioner (GE) is a comprehensive platform designed for robotic manipulation that combines policy learning, evaluation, and simulation into one framework. It utilizes a video diffusion model to understand and generate realistic robotic interactions based on instructions. The system includes a decoder that translates learned representations into actionable movements, allowing robots to perform tasks with minimal guidance. Additionally, it features a neural simulator for testing and refining policies, along with a benchmark suite to evaluate performance across various criteria."
                },
                "zh": {
                    "title": "Genie Envisionerï¼šæŒ‡ä»¤é©±åŠ¨çš„æœºå™¨äººæ™ºèƒ½æ–°å¹³å°",
                    "desc": "Genie Envisionerï¼ˆGEï¼‰æ˜¯ä¸€ä¸ªé›†æˆäº†ç­–ç•¥å­¦ä¹ ã€è¯„ä¼°å’Œæ¨¡æ‹Ÿçš„æœºå™¨äººæ“ä½œå¹³å°ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªå¤§å‹çš„ã€åŸºäºæŒ‡ä»¤çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰ç°å®ä¸–ç•Œä¸­æœºå™¨äººäº¤äº’çš„ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰åŠ¨æ€ã€‚GE-Acté€šè¿‡è½»é‡çº§çš„è§£ç å™¨å°†æ½œåœ¨è¡¨ç¤ºæ˜ å°„åˆ°å¯æ‰§è¡Œçš„åŠ¨ä½œè½¨è¿¹ï¼Œå®ç°äº†åœ¨ä¸åŒç¯å¢ƒä¸­ç²¾ç¡®ä¸”å¯æ¨å¹¿çš„ç­–ç•¥æ¨æ–­ã€‚GE-Simä½œä¸ºä¸€ä¸ªç¥ç»æ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒé«˜ä¿çœŸåº¦çš„é—­ç¯ç­–ç•¥å¼€å‘ï¼Œæ•´ä¸ªç³»ç»Ÿä¸ºæŒ‡ä»¤é©±åŠ¨çš„é€šç”¨æ™ºèƒ½æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05609",
            "title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity",
            "url": "https://huggingface.co/papers/2508.05609",
            "abstract": "Hi3DEval is a hierarchical evaluation framework for 3D generative content that combines object-level and part-level assessments, including material realism, using a large-scale dataset and hybrid 3D representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.",
            "score": 9,
            "issue_id": 5242,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "97d6454893bee33c",
            "authors": [
                "Yuhan Zhang",
                "Long Zhuo",
                "Ziyang Chu",
                "Tong Wu",
                "Zhibing Li",
                "Liang Pan",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05609.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#optimization",
                    "#games",
                    "#dataset"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ",
                    "desc": "Hi3DEval - ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Hi3DEval Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Content Evaluation with Hi3DEval",
                    "desc": "Hi3DEval is a new framework designed to evaluate 3D generative content by assessing both the overall object and its individual parts. It addresses the limitations of current methods that only use image-based metrics, which often miss important details like spatial coherence and material realism. The framework includes a large dataset called Hi3DBench, which features a variety of 3D assets and detailed annotations to support comprehensive evaluations. By utilizing advanced scoring systems and hybrid 3D representations, Hi3DEval provides a more accurate and scalable way to assess the quality of 3D generated content."
                },
                "zh": {
                    "title": "3Dç”Ÿæˆå†…å®¹çš„åˆ†å±‚è¯„ä¼°æ–°æ¡†æ¶",
                    "desc": "Hi3DEvalæ˜¯ä¸€ä¸ªé’ˆå¯¹3Dç”Ÿæˆå†…å®¹çš„åˆ†å±‚è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†å¯¹è±¡çº§å’Œéƒ¨åˆ†çº§çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ææ–™çœŸå®æ„Ÿã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºå›¾åƒçš„æŒ‡æ ‡ï¼Œä»…åœ¨å¯¹è±¡çº§åˆ«è¿›è¡Œè¯„ä¼°ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ç©ºé—´ä¸€è‡´æ€§å’Œææ–™çœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒHi3DEvalæä¾›äº†å¤šç»´åº¦çš„æ•´ä½“è¯„ä¼°å’Œç»†è‡´çš„è´¨é‡åˆ†æï¼Œå¹¶æ‰©å±•äº†çº¹ç†è¯„ä¼°ï¼Œå…³æ³¨å¦‚åå°„ç‡ã€é¥±å’Œåº¦å’Œé‡‘å±æ„Ÿç­‰å±æ€§ã€‚é€šè¿‡æ„å»ºHi3DBenchæ•°æ®é›†å’Œ3Dæ„ŸçŸ¥çš„è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿï¼ŒHi3DEvalåœ¨å»ºæ¨¡3Dç‰¹æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„å›¾åƒåŸºå‡†ï¼Œæä¾›äº†å¯æ‰©å±•çš„è¯„ä¼°æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03990",
            "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?",
            "url": "https://huggingface.co/papers/2508.03990",
            "abstract": "LLMs can be fine-tuned to generate high-quality, audience-tailored explanations of well-being concepts using Supervised Fine-Tuning and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.",
            "score": 6,
            "issue_id": 5242,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "a742f57af42990c1",
            "authors": [
                "Bohan Jiang",
                "Dawei Li",
                "Zhen Tan",
                "Chengshuai Zhao",
                "Huan Liu"
            ],
            "affiliations": [
                "School of Computing and Augmented Intelligence, Arizona State University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03990.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#alignment",
                    "#open_source",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 43 880 Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ 2 194 ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµÑÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Supervised Fine-Tuning Ğ¸ Direct Preference Optimization, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Tailoring Well-Being Explanations with Fine-Tuned LLMs",
                    "desc": "This paper discusses how Large Language Models (LLMs) can be improved to provide better explanations of well-being concepts tailored to different audiences. It highlights the importance of both factual accuracy and audience-specific needs in generating high-quality explanations. The authors created a large dataset of explanations and developed a unique evaluation framework using LLMs as judges to assess the quality of these explanations. Their findings show that fine-tuning LLMs with Supervised Fine-Tuning and Direct Preference Optimization leads to significant improvements in explanation quality compared to larger, unrefined models."
                },
                "zh": {
                    "title": "æå‡å¹¸ç¦æ„Ÿè§£é‡Šè´¨é‡çš„æ™ºèƒ½æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å…³äºå¹¸ç¦æ„Ÿæ¦‚å¿µçš„è§£é‡Šè´¨é‡ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«43,880ä¸ªè§£é‡Šçš„å¤§å‹æ•°æ®é›†ï¼Œæ¶µç›–2,194ä¸ªå¹¸ç¦æ„Ÿæ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŸºäºåŸåˆ™çš„LLMè¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆè§£é‡Šæ—¶çš„è´¨é‡æ˜¾è‘—æé«˜ï¼Œä¸”ä¸äººç±»è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚ä¸åŒæ¨¡å‹ã€å—ä¼—å’Œç±»åˆ«ä¹‹é—´çš„è§£é‡Šè´¨é‡å·®å¼‚æ˜æ˜¾ï¼Œè¡¨æ˜åå¥½å­¦ä¹ åœ¨ä¸“ä¸šè§£é‡Šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03644",
            "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?",
            "url": "https://huggingface.co/papers/2508.03644",
            "abstract": "Double-Bench is a large-scale, multilingual, and multimodal evaluation system for document Retrieval-Augmented Generation (RAG) systems, addressing limitations in current benchmarks and providing comprehensive assessments of system components.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.",
            "score": 6,
            "issue_id": 5243,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "b8c44e363c76888f",
            "authors": [
                "Wenxuan Shen",
                "Mingjia Wang",
                "Yaochen Wang",
                "Dongping Chen",
                "Junjie Yang",
                "Yao Wan",
                "Weiwei Lin"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "South China University of Technology",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03644.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#survey",
                    "#rag",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Double-Bench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° RAG ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Double-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Retrieval-Augmented Generation (RAG) ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 3276 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ 5168 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° 6 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 4 Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° RAG ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RAG ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Double-Bench: Elevating RAG Evaluation for Real-World Challenges",
                    "desc": "Double-Bench is a new evaluation system designed to improve the assessment of Retrieval-Augmented Generation (RAG) systems, which combine document retrieval and generation. It addresses the shortcomings of existing benchmarks by providing a large-scale, multilingual, and multimodal dataset that includes 3,276 documents and 5,168 queries across multiple languages and document types. The evaluation focuses on fine-grained assessments of each component in RAG systems, ensuring that queries are based on thoroughly verified evidence. Our experiments reveal important insights into the performance of various models and highlight the need for better document retrieval capabilities in the face of over-confidence in current frameworks."
                },
                "zh": {
                    "title": "åŒé‡åŸºå‡†ï¼šæå‡æ–‡æ¡£RAGç³»ç»Ÿè¯„ä¼°çš„å…¨æ–°æ ‡å‡†",
                    "desc": "Double-Benchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€å¤šæ¨¡æ€è¯„ä¼°ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæ–‡æ¡£å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„è¯„ä¼°ã€‚å®ƒè§£å†³äº†å½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿå¯¹RAGç³»ç»Ÿçš„å„ä¸ªç»„ä»¶è¿›è¡Œå…¨é¢çš„è¯„ä¼°ã€‚è¯¥ç³»ç»ŸåŒ…å«3276ä»½æ–‡æ¡£å’Œ5168ä¸ªæŸ¥è¯¢ï¼Œæ¶µç›–6ç§è¯­è¨€å’Œ4ç§æ–‡æ¡£ç±»å‹ï¼Œç¡®ä¿è¯„ä¼°çš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬å’Œè§†è§‰åµŒå…¥æ¨¡å‹ä¹‹é—´çš„å·®è·æ­£åœ¨ç¼©å°ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å½“å‰RAGæ¡†æ¶ä¸­å­˜åœ¨çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04017",
            "title": "Can Large Multimodal Models Actively Recognize Faulty Inputs? A\n  Systematic Evaluation Framework of Their Input Scrutiny Ability",
            "url": "https://huggingface.co/papers/2508.04017",
            "abstract": "ISEval framework evaluates large multimodal models' ability to detect flawed inputs, revealing challenges in identifying certain types of errors and modality-specific biases.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing formidable capabilities in handling intricate multimodal tasks with exceptional performance. Recent research has underscored the inclination of large language models to passively accept defective inputs, often resulting in futile reasoning on invalid prompts. However, the same critical question of whether LMMs can actively detect and scrutinize erroneous inputs still remains unexplored. To address this gap, we introduce the Input Scrutiny Ability Evaluation Framework (ISEval), which encompasses seven categories of flawed premises and three evaluation metrics. Our extensive evaluation of ten advanced LMMs has identified key findings. Most models struggle to actively detect flawed textual premises without guidance, which reflects a strong reliance on explicit prompts for premise error identification. Error type affects performance: models excel at identifying logical fallacies but struggle with surface-level linguistic errors and certain conditional flaws. Modality trust varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info, while aya-vision-8b over-rely on text in conflicts. These insights underscore the urgent need to enhance LMMs' proactive verification of input validity and shed novel insights into mitigating the problem. The code is available at https://github.com/MLGroupJLU/LMM_ISEval.",
            "score": 4,
            "issue_id": 5243,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "37794107e7cbe332",
            "authors": [
                "Haiqi Yang",
                "Jinzhe Li",
                "Gengxu Li",
                "Yi Chang",
                "Yuan Wu"
            ],
            "affiliations": [
                "Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China",
                "International Center of Future Science, Jilin University",
                "School of Artificial Intelligence, Jilin University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04017.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#ethics",
                    "#hallucinations",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ˜Ğ˜ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ISEval Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğ¸ Ğº Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼: Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Input Validation in Large Multimodal Models",
                    "desc": "The ISEval framework assesses the ability of large multimodal models (LMMs) to identify flawed inputs, highlighting their challenges in recognizing specific errors and biases related to different modalities. Despite their impressive performance in multimodal tasks, many LMMs tend to accept defective inputs without questioning them, leading to ineffective reasoning. The framework categorizes seven types of flawed premises and employs three evaluation metrics to analyze ten advanced LMMs, revealing that most struggle to detect errors without explicit prompts. The findings indicate that while models perform well in identifying logical fallacies, they face difficulties with linguistic errors and exhibit varying trust in different modalities, emphasizing the need for improved input validation mechanisms."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¾“å…¥éªŒè¯èƒ½åŠ›",
                    "desc": "ISEvalæ¡†æ¶è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ£€æµ‹ç¼ºé™·è¾“å…¥çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†è¯†åˆ«æŸäº›ç±»å‹é”™è¯¯å’Œç‰¹å®šæ¨¡æ€åè§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å€¾å‘äºè¢«åŠ¨æ¥å—æœ‰ç¼ºé™·çš„è¾“å…¥ï¼Œå¯¼è‡´åœ¨æ— æ•ˆæç¤ºä¸Šè¿›è¡Œæ— æ•ˆæ¨ç†ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒLMMsæ˜¯å¦èƒ½å¤Ÿä¸»åŠ¨æ£€æµ‹å’Œå®¡æŸ¥é”™è¯¯è¾“å…¥çš„é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨æ²¡æœ‰æŒ‡å¯¼çš„æƒ…å†µä¸‹éš¾ä»¥ä¸»åŠ¨è¯†åˆ«æ–‡æœ¬å‰æçš„ç¼ºé™·ï¼Œå¼ºè°ƒäº†å¯¹æ˜ç¡®æç¤ºçš„å¼ºçƒˆä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05630",
            "title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes",
            "url": "https://huggingface.co/papers/2508.05630",
            "abstract": "MOSEv2, a more challenging dataset, highlights the limitations of current VOS methods in real-world scenarios with increased complexity and diverse challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.",
            "score": 2,
            "issue_id": 5242,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "a2a9fbee6a3ffe42",
            "authors": [
                "Henghui Ding",
                "Kaining Ying",
                "Chang Liu",
                "Shuting He",
                "Xudong Jiang",
                "Yu-Gang Jiang",
                "Philip H. S. Torr",
                "Song Bai"
            ],
            "affiliations": [
                "ByteDance Inc",
                "Fudan University, Shanghai, China",
                "Nanyang Technological University, Singapore",
                "Shanghai University of Finance and Economics, China",
                "University of Oxford, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05630.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MOSEv2: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "MOSEv2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (VOS), ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸, Ğ½ĞµĞ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² VOS Ğ½Ğ° MOSEv2 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "MOSEv2: Elevating Video Object Segmentation to Real-World Challenges",
                    "desc": "The paper introduces MOSEv2, a new dataset for video object segmentation (VOS) that presents more complex real-world challenges than previous datasets. While existing methods perform well on simpler benchmarks like DAVIS and YouTube-VOS, they struggle with the increased difficulties found in MOSEv2, which includes diverse scenarios such as occlusions, object disappearance, and adverse weather conditions. The dataset contains over 5,000 videos and nearly 702,000 high-quality masks for a wide variety of objects, making it a significant resource for advancing VOS research. Benchmarking shows that current VOS methods experience substantial performance drops when tested on MOSEv2, indicating a need for improved algorithms that can handle real-world complexities."
                },
                "zh": {
                    "title": "MOSEv2ï¼šåº”å¯¹çœŸå®ä¸–ç•Œå¤æ‚æ€§çš„æŒ‘æˆ˜",
                    "desc": "MOSEv2æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œæ­ç¤ºäº†å½“å‰è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆVOSï¼‰æ–¹æ³•åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨DAVISå’ŒYouTube-VOSç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™äº›æ•°æ®é›†ä¸»è¦åŒ…å«æ˜¾è‘—ã€ä¸»å¯¼å’Œå­¤ç«‹çš„å¯¹è±¡ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚MOSEv2åŒ…å«5024ä¸ªè§†é¢‘å’Œè¶…è¿‡701976ä¸ªé«˜è´¨é‡çš„æ©è†œï¼Œæ¶µç›–200ä¸ªç±»åˆ«çš„10074ä¸ªå¯¹è±¡ï¼Œåœºæ™¯å¤æ‚æ€§æ˜¾è‘—å¢åŠ ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨ç°æœ‰æ•°æ®é›†ä¸Šå‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†å½“å‰çš„VOSæ–¹æ³•åœ¨é¢å¯¹çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04699",
            "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during\n  Multi-Hop Analysis",
            "url": "https://huggingface.co/papers/2508.04699",
            "abstract": "Research investigates reasoning failures in language models for multi-hop question answering, introducing a framework to categorize errors and improve model fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (\"hops\"), completeness in capturing relevant information (\"coverage\"), and cognitive inefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.",
            "score": 2,
            "issue_id": 5242,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "97e95e6b08388e4f",
            "authors": [
                "Anushka Yadav",
                "Isha Nalawade",
                "Srujana Pillarichety",
                "Yashwanth Babu",
                "Reshmi Ghosh",
                "Samyadeep Basu",
                "Wenlong Zhao",
                "Ali Nasaeh",
                "Sriram Balasubramanian",
                "Soundararajan Srinivasan"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Massachusetts, Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04699.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#reasoning",
                    "#hallucinations",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ³Ğ°Ğ´ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ÑƒÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unraveling Reasoning Failures in Language Models",
                    "desc": "This paper investigates the reasoning failures of language models specifically in multi-hop question answering tasks. It introduces a new framework to categorize these errors based on three dimensions: the diversity of source documents, the completeness of relevant information, and cognitive inefficiency. The study uses human annotation and automated metrics to reveal complex error patterns that are often overlooked in traditional accuracy evaluations. The findings aim to enhance the understanding of cognitive limitations in language models and provide guidance for improving their reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤šè·³é—®ç­”ä¸­çš„æ¨ç†å¤±è´¥ï¼Œæå‡ºäº†ä¸€ç§æ¡†æ¶æ¥åˆ†ç±»é”™è¯¯å¹¶æé«˜æ¨¡å‹çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜æ—¶ï¼Œå¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ï¼Œç¼ºä¹å¯¹é”™è¯¯åŸå› çš„å…¨é¢ç†è§£ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é”™è¯¯åˆ†ç±»æ¡†æ¶ï¼Œä»æºæ–‡æ¡£çš„å¤šæ ·æ€§ã€ä¿¡æ¯æ•æ‰çš„å®Œæ•´æ€§å’Œè®¤çŸ¥æ•ˆç‡ä¸‰ä¸ªç»´åº¦è¿›è¡Œåˆ†æã€‚é€šè¿‡ä¸¥æ ¼çš„äººç±»æ ‡æ³¨å’Œè‡ªåŠ¨åŒ–æŒ‡æ ‡çš„æ”¯æŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†éšè—åœ¨å‡†ç¡®æ€§è¯„ä¼°èƒŒåçš„å¤æ‚é”™è¯¯æ¨¡å¼ï¼Œä¸ºæœªæ¥è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†å¯è¡Œçš„æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02120",
            "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models",
            "url": "https://huggingface.co/papers/2508.02120",
            "abstract": "Research on efficient reasoning methods for Large Reasoning Models (LRMs) aims to reduce reasoning path length without sacrificing performance, through single-model optimization and model collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.",
            "score": 2,
            "issue_id": 5243,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "ab1d76b8f9fbefe8",
            "authors": [
                "Linan Yue",
                "Yichao Du",
                "Yizhi Wang",
                "Weibo Gao",
                "Fangzhou Yao",
                "Li Wang",
                "Ye Liu",
                "Ziyu Xu",
                "Qi Liu",
                "Shimin Di",
                "Min-Ling Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education",
                "School of Computer Science and Engineering, Southeast University",
                "University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02120.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LRM: ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LRM Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Streamlining Reasoning: Enhancing Efficiency in Large Reasoning Models",
                    "desc": "This paper discusses the development of efficient reasoning methods for Large Reasoning Models (LRMs), which are designed to improve logical deduction and decision-making. It highlights the challenges posed by overly long reasoning paths that can hinder performance and accuracy. The authors categorize existing research into two main approaches: optimizing single models for better reasoning efficiency and enhancing collaboration between multiple models. Additionally, they provide a public GitHub repository to share ongoing advancements in this area."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼šæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­é«˜æ•ˆæ¨ç†æ–¹æ³•çš„ç ”ç©¶ï¼Œæ—¨åœ¨åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘æ¨ç†è·¯å¾„çš„é•¿åº¦ã€‚ç ”ç©¶ä¸­æåˆ°çš„DeepSeek R1æ¨¡å‹å› å…¶å“è¶Šçš„è¡¨ç°å’Œå¼€æºç‰¹æ€§è€Œå—åˆ°å…³æ³¨ï¼Œæ¨åŠ¨äº†R1é£æ ¼LRMsçš„ç ”ç©¶è¿›å±•ã€‚ä¸ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šè¿‡é•¿é“¾æ¨ç†å’Œè‡ªæˆ‘åæ€ç­‰æœºåˆ¶å¢å¼ºäº†é€»è¾‘æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€åº”ç”¨çš„å¹¿æ³›ï¼Œè¿‡åº¦æ¨ç†çš„é—®é¢˜é€æ¸æ˜¾ç°ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡é™ä½ï¼Œå› æ­¤æå‡ºäº†å¤šç§é«˜æ•ˆæ¨ç†æ–¹æ³•ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04423",
            "title": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation",
            "url": "https://huggingface.co/papers/2508.04423",
            "abstract": "A structured framework and datasets for training customer service agents using well-defined support strategies improve the quality of customer support interactions and problem resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.",
            "score": 1,
            "issue_id": 5242,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "d1f778b32418973c",
            "authors": [
                "Jie Zhu",
                "Huaixia Dou",
                "Junhui Li",
                "Lifan Guo",
                "Feng Chen",
                "Chi Zhang",
                "Fang Kong"
            ],
            "affiliations": [
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04423.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#agents",
                    "#science",
                    "#dataset",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Customer Support Conversation (CSC), Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ¸ Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: CSConv Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ RoleCS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ fine-tuning LLM Ğ½Ğ° RoleCS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Customer Support with Strategic Conversations",
                    "desc": "This paper presents a structured framework for training customer service agents, focusing on effective communication and problem resolution. It introduces the Customer Support Conversation (CSC) task, which utilizes well-defined support strategies based on COPC guidelines. The authors create a dataset called CSConv, consisting of real-world conversations rewritten to reflect strategic communication, and a training dataset called RoleCS that simulates these interactions. Experiments demonstrate that fine-tuning large language models (LLMs) on RoleCS enhances their ability to produce high-quality, strategy-aligned responses, leading to improved customer support outcomes."
                },
                "zh": {
                    "title": "æå‡å®¢æˆ·æ”¯æŒè´¨é‡çš„ç»“æ„åŒ–æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå®¢æˆ·æœåŠ¡ä»£ç†ï¼Œé‡‡ç”¨æ˜ç¡®çš„æ”¯æŒç­–ç•¥ä»¥æé«˜å®¢æˆ·æ”¯æŒäº’åŠ¨çš„è´¨é‡å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†å®¢æˆ·æ”¯æŒå¯¹è¯ï¼ˆCSCï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¸®åŠ©å®¢æœä»£ç†ä½¿ç”¨å®šä¹‰è‰¯å¥½çš„æ”¯æŒç­–ç•¥è¿›è¡Œå“åº”ã€‚åŸºäºCOPCæŒ‡å—ï¼Œæˆ‘ä»¬å®šä¹‰äº†äº”ä¸ªå¯¹è¯é˜¶æ®µå’ŒåäºŒç§ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼é«˜è´¨é‡çš„äº’åŠ¨ã€‚é€šè¿‡æ„å»ºCSConvæ•°æ®é›†å’ŒRoleCSè®­ç»ƒæ•°æ®é›†ï¼Œå®éªŒè¡¨æ˜åœ¨RoleCSä¸Šå¾®è°ƒå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾è‘—æå‡äº†å…¶åœ¨CSConvä¸Šçš„ç­–ç•¥ä¸€è‡´æ€§å“åº”èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01650",
            "title": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance",
            "url": "https://huggingface.co/papers/2508.01650",
            "abstract": "A sketch-based strand generation model using a learnable upsampling strategy and multi-scale adaptive conditioning mechanism outperforms existing methods in realism and precision for hair strand generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).",
            "score": 0,
            "issue_id": 5243,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 3",
                "zh": "8æœˆ3æ—¥"
            },
            "hash": "377e9e9eca3593db",
            "authors": [
                "Na Zhang",
                "Moran Li",
                "Chengming Xu",
                "Han Feng",
                "Xiaobin Hu",
                "Jiangning Zhang",
                "Weijian Cao",
                "Chengjie Wang",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University",
                "Tencent YouTu Lab, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01650.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#cv",
                    "#3d",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ’‡",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ¿Ğ¾ ÑÑĞºĞ¸Ğ·Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞºĞ¸Ğ·Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ¿ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ»Ğ¾Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Hair Strand Generation with Sketch-Based Precision",
                    "desc": "This paper presents a novel sketch-based model for generating realistic hair strands, addressing the limitations of existing methods. The model utilizes a learnable upsampling strategy to effectively encode 3D hair strands into multi-scale latent spaces, enhancing detail and precision. Additionally, it incorporates a multi-scale adaptive conditioning mechanism that employs transformers with diffusion heads to maintain consistency across different levels of detail. Experimental results demonstrate that this approach significantly improves realism and precision in hair strand generation compared to traditional techniques."
                },
                "zh": {
                    "title": "è‰å›¾é©±åŠ¨çš„å‘ä¸ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‰å›¾çš„å‘ä¸ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·ç­–ç•¥å’Œå¤šå°ºåº¦è‡ªé€‚åº”æ¡ä»¶æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†å‘ä¸ç”Ÿæˆçš„çœŸå®æ„Ÿå’Œç²¾ç¡®åº¦ã€‚è¯¥æ¨¡å‹è§£å†³äº†å¤æ‚å‘ä¸äº¤äº’å’Œå¤šæ ·åŒ–è‰å›¾æ¨¡å¼å»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡å°†3Då‘ä¸ç¼–ç åˆ°å¤šå°ºåº¦æ½œåœ¨ç©ºé—´ï¼Œæ¨¡å‹å®ç°äº†æ›´ç»†è‡´çš„æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒç”¨æˆ·å‹å¥½æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-07.html",
    "link_next": "2025-08-11.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "07.08",
        "en": "08/07",
        "zh": "8æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "11.08",
        "en": "08/11",
        "zh": "8æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}