
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. September 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 сентября</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-24.html">⬅️ <span id="prev-date">24.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-26.html">➡️ <span id="next-date">26.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'};
        let feedDateNext = {'ru': '26.09', 'en': '09/26', 'zh': '9月26日'};
        let feedDatePrev = {'ru': '24.09', 'en': '09/24', 'zh': '9月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.16191', 'title': 'HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models', 'url': 'https://huggingface.co/papers/2409.16191', 'abstract': "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (e.g., long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are not well investigated. Therefore, we introduce the Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive, in-the-wild, and open-ended benchmark to evaluate LLMs' performance in generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation. Besides, we propose Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation method that significantly reduces the time and effort required for human evaluation while maintaining a high correlation with human evaluation. We have conducted extensive experiments across around 30 mainstream LLMs and observed that the current LLMs lack long text generation capabilities. Specifically, first, regardless of whether the instructions include explicit or implicit length constraints, we observe that most LLMs cannot generate text that is longer than 4000 words. Second, we observe that while some LLMs can generate longer text, many issues exist (e.g., severe repetition and quality degradation). Third, to demonstrate the effectiveness of HelloEval, we compare HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge methods, which show that HelloEval has the highest correlation with human evaluation. We release our code in https://github.com/Quehry/HelloBench.", 'score': 41, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '6922e8bba7bd05f8', 'data': {'categories': ['#benchmark', '#long_context', '#evaluation'], 'emoji': '📝', 'ru': {'title': 'Новый бенчмарк раскрывает ограничения LLM в генерации длинных текстов', 'desc': 'Исследователи представили новый бенчмарк HelloBench для оценки способностей больших языковых моделей (LLM) генерировать длинные тексты. LLM оцениваются по пяти подзадачам, включая ответы на открытые вопросы и обобщение текста. Авторы также предложили метод оценки HelloEval, который значительно сокращает время и усилия, необходимые для человеческой оценки. Эксперименты показали, что современным LLM не хватает возможностей для генерации длинных текстов, а HelloEval имеет наивысшую корреляцию с человеческой оценкой по сравнению с традиционными метриками.'}, 'en': {'title': 'Evaluating Long Text Generation in LLMs with HelloBench', 'desc': "This paper introduces HelloBench, a new benchmark designed to evaluate the long text generation capabilities of Large Language Models (LLMs). It categorizes tasks into five subtasks based on Bloom's Taxonomy, including open-ended question answering and summarization. The authors also present HelloEval, a novel evaluation method that aligns closely with human judgment while being more efficient. Through experiments with around 30 LLMs, the study reveals that many models struggle with generating coherent long texts, often producing repetitive and low-quality outputs."}, 'zh': {'title': '探索大型语言模型的长文本生成能力', 'desc': '近年来，大型语言模型（LLMs）在各种任务中表现出色，但长文本生成能力尚未得到充分研究。为此，我们提出了层次化长文本生成基准（HelloBench），用于评估LLMs在生成长文本方面的表现。HelloBench根据布鲁姆分类法将长文本生成任务分为五个子任务：开放式问答、摘要、聊天、文本补全和启发式文本生成。此外，我们还提出了层次化长文本评估（HelloEval），这是一种与人类评估高度相关的评估方法，显著减少了人类评估所需的时间和精力。'}}}, {'id': 'https://huggingface.co/papers/2409.16160', 'title': 'MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling', 'url': 'https://huggingface.co/papers/2409.16160', 'abstract': 'Character video synthesis aims to produce realistic videos of animatable characters within lifelike scenes. As a fundamental problem in the computer vision and graphics community, 3D works typically require multi-view captures for per-case training, which severely limits their applicability of modeling arbitrary characters in a short time. Recent 2D methods break this limitation via pre-trained diffusion models, but they struggle for pose generality and scene interaction. To this end, we propose MIMO, a novel framework which can not only synthesize character videos with controllable attributes (i.e., character, motion and scene) provided by simple user inputs, but also simultaneously achieve advanced scalability to arbitrary characters, generality to novel 3D motions, and applicability to interactive real-world scenes in a unified framework. The core idea is to encode the 2D video to compact spatial codes, considering the inherent 3D nature of video occurrence. Concretely, we lift the 2D frame pixels into 3D using monocular depth estimators, and decompose the video clip to three spatial components (i.e., main human, underlying scene, and floating occlusion) in hierarchical layers based on the 3D depth. These components are further encoded to canonical identity code, structured motion code and full scene code, which are utilized as control signals of synthesis process. The design of spatial decomposed modeling enables flexible user control, complex motion expression, as well as 3D-aware synthesis for scene interactions. Experimental results demonstrate effectiveness and robustness of the proposed method.', 'score': 32, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '1fac86521a579432', 'data': {'categories': ['#cv', '#3d', '#video'], 'emoji': '🎭', 'ru': {'title': 'Синтез реалистичных видео с управляемыми 3D-персонажами', 'desc': 'MIMO - это новая система для синтеза видео с управляемыми персонажами в реалистичных сценах. Она позволяет создавать видео с произвольными персонажами, новыми трехмерными движениями и взаимодействием со средой на основе простых пользовательских данных. Ключевая идея заключается в кодировании 2D видео в компактные пространственные коды с учетом трехмерной природы видео. MIMO декомпозирует видеоклип на три пространственных компонента и кодирует их для гибкого управления синтезом.'}, 'en': {'title': 'MIMO: Unleashing Realistic Character Video Synthesis with User Control', 'desc': 'This paper presents MIMO, a new framework for character video synthesis that allows users to create realistic videos of animated characters in various scenes. Unlike traditional 3D methods that require extensive multi-view captures, MIMO leverages pre-trained diffusion models to enhance scalability and adaptability to different characters and motions. The framework encodes 2D video into compact spatial codes by utilizing monocular depth estimators, which helps in understanding the 3D nature of video. By decomposing video clips into hierarchical spatial components, MIMO enables flexible user control and complex motion expressions while ensuring realistic scene interactions.'}, 'zh': {'title': '灵活控制的角色视频合成新框架', 'desc': '角色视频合成旨在生成逼真的可动画角色视频，融入生动的场景中。传统的3D方法需要多视角捕捉进行个性化训练，限制了其在短时间内建模任意角色的能力。我们提出的MIMO框架能够通过简单的用户输入合成具有可控属性的角色视频，同时实现对任意角色的扩展性、对新3D动作的通用性以及对互动现实场景的适用性。该方法通过将2D视频编码为紧凑的空间编码，结合单目深度估计，将视频片段分解为主要人类、底层场景和浮动遮挡等空间组件，从而实现灵活的用户控制和复杂的动作表达。'}}}, {'id': 'https://huggingface.co/papers/2409.15700', 'title': 'Making Text Embedders Few-Shot Learners', 'url': 'https://huggingface.co/papers/2409.15700', 'abstract': 'Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding .', 'score': 29, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '1052f4cb71fa8bf6', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Улучшение текстовых эмбеддингов с помощью обучения в контексте', 'desc': 'Исследователи предложили новый подход к генерации текстовых эмбеддингов, используя возможности обучения в контексте (ICL) больших языковых моделей (LLM). Они разработали модель bge-en-icl, которая использует примеры для создания высококачественных эмбеддингов. Эксперименты показали, что этот метод превосходит современные аналоги на бенчмарках MTEB и AIR-Bench. Исследователи также изучили различные механизмы внимания и методы пулинга, придя к выводу, что простота часто дает лучшие результаты.'}, 'en': {'title': 'Enhancing Text Embeddings with In-Context Learning', 'desc': 'This paper discusses the use of large language models (LLMs) with decoder-only architectures for in-context learning (ICL), which allows them to adapt to new tasks using examples in their input. The authors introduce a new model called bge-en-icl that enhances text embedding generation by incorporating few-shot examples directly into the input queries. They explore various techniques for utilizing LLMs as embedding models, including different attention mechanisms and pooling methods, while emphasizing that maintaining a simple framework often yields the best results. The experimental results show that their approach achieves state-of-the-art performance on benchmark datasets, demonstrating the effectiveness of their method.'}, 'zh': {'title': '利用上下文学习提升文本嵌入生成', 'desc': '本文探讨了大型语言模型（LLMs）在上下文学习（ICL）方面的能力，特别是如何利用这一特性来改进文本嵌入生成。我们提出了一种新模型bge-en-icl，通过少量示例生成高质量的文本嵌入。该方法将与任务相关的示例直接整合到查询中，从而在多个任务上显著提升性能。此外，我们还研究了如何有效利用LLMs作为嵌入模型，包括不同的注意力机制和池化方法。'}}}, {'id': 'https://huggingface.co/papers/2409.15272', 'title': 'OmniBench: Towards The Future of Universal Omni-Language Models', 'url': 'https://huggingface.co/papers/2409.15272', 'abstract': "Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs). OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) the baseline models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities. The codes and live leaderboard could be found at https://m-a-p.ai/OmniBench.", 'score': 25, 'issue_id': 1, 'pub_date': '2024-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '5a3bbcb55b6bff9b', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'OmniBench: новый рубеж в оценке мультимодальных языковых моделей', 'desc': 'В статье представлен новый бенчмарк OmniBench для оценки мультимодальных языковых моделей (MLLM), способных одновременно обрабатывать визуальные, акустические и текстовые данные. Исследование выявило существенные ограничения существующих открытых MLLM в понимании инструкций и рассуждениях в трехмодальном контексте. Результаты показывают, что базовые модели демонстрируют низкую точность (менее 50%) даже при наличии текстовых представлений изображений и аудио. Авторы призывают к разработке более надежных методов интеграции трех модальностей и стратегий обучения для улучшения производительности MLLM.'}, 'en': {'title': 'Enhancing Multimodal Understanding with OmniBench', 'desc': 'This paper presents OmniBench, a new benchmark for evaluating multimodal large language models (MLLMs) that can process visual, acoustic, and textual data simultaneously. The authors define models that can handle this tri-modal processing as omni-language models (OLMs). They found that current open-source OLMs struggle with instruction-following and reasoning tasks, often scoring below 50% accuracy when interpreting data across modalities. The study highlights the need for improved training methods and integration techniques to enhance the performance of these models in understanding complex, multimodal contexts.'}, 'zh': {'title': '全模态模型的评估新基准', 'desc': '最近，多模态大型语言模型（MLLMs）在整合和解释不同类型数据方面取得了进展。然而，这些模型同时处理和推理多种模态的能力仍然没有得到充分探索，部分原因是缺乏全面的模态基准测试。我们提出了OmniBench，这是一个新颖的基准，旨在严格评估模型在视觉、听觉和文本输入之间的识别、解释和推理能力。我们的研究发现，现有的开放源代码的全模态模型（OLMs）在三模态上下文中的指令遵循和推理能力存在显著限制。'}}}, {'id': 'https://huggingface.co/papers/2409.16235', 'title': 'EuroLLM: Multilingual Language Models for Europe', 'url': 'https://huggingface.co/papers/2409.16235', 'abstract': 'The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.', 'score': 24, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '0e8383e791404636', 'data': {'categories': ['#multilingual', '#dataset', '#data', '#training', '#translation'], 'emoji': '🌍', 'ru': {'title': 'EuroLLM: Мультиязычные языковые модели для Европы', 'desc': 'Статья представляет проект EuroLLM, целью которого является разработка набора открытых мультиязычных языковых моделей для всех официальных языков Европейского Союза. Авторы описывают процесс сбора и фильтрации данных, разработку законов масштабирования и создание мультиязычного токенизатора. В работе представлены первые модели проекта: EuroLLM-1.7B и EuroLLM-1.7B-Instruct. Приводятся результаты оценки производительности моделей на мультиязычных тестах и в задаче машинного перевода.'}, 'en': {'title': 'Empowering Multilingual Understanding with EuroLLM', 'desc': 'This paper presents the EuroLLM project, which focuses on creating open-weight multilingual large language models (LLMs) that can understand and generate text in multiple languages, particularly those of the European Union. The authors discuss their methods for data collection and filtering, as well as the development of scaling laws and a multilingual tokenizer to enhance model performance. They also detail the configurations used for data mixing and modeling to optimize the training process. The paper concludes by introducing their initial models, EuroLLM-1.7B and EuroLLM-1.7B-Instruct, and evaluating their performance on multilingual benchmarks and machine translation tasks.'}, 'zh': {'title': '推动多语言处理的EuroLLM项目', 'desc': '本论文介绍了EuroLLM项目，旨在开发一套开放权重的多语言大语言模型（LLM），能够理解和生成所有欧盟官方语言及其他相关语言的文本。我们详细描述了数据收集和过滤过程、扩展法则的开发、多语言分词器的创建以及数据混合和建模配置。我们还发布了初始模型：EuroLLM-1.7B和EuroLLM-1.7B-Instruct，并报告了它们在多语言通用基准和机器翻译上的表现。该项目的目标是提升非英语语言的处理能力，推动多语言自然语言处理的发展。'}}}, {'id': 'https://huggingface.co/papers/2409.14128', 'title': 'Present and Future Generalization of Synthetic Image Detectors', 'url': 'https://huggingface.co/papers/2409.14128', 'abstract': 'The continued release of new and better image generation models increases the demand for synthetic image detectors. In such a dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected in the wild show this task to be more challenging than the one defined by large-scale datasets, pointing to a gap between experimentation and actual practice. Finally, we observe a race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards a perpetually close race between generators and detectors.', 'score': 18, 'issue_id': 1, 'pub_date': '2024-09-21', 'pub_date_card': {'ru': '21 сентября', 'en': 'September 21', 'zh': '9月21日'}, 'hash': '998f15880f8ff97f', 'data': {'categories': ['#synthetic', '#cv', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Вечная гонка: генераторы против детекторов синтетических изображений', 'desc': 'Статья рассматривает проблему обнаружения синтетических изображений в условиях постоянного улучшения генеративных моделей. Авторы исследуют влияние времени, трансформаций изображений и источников данных на обобщающую способность детекторов. Эксперименты показывают, что ни один из оцененных детекторов не является универсальным, но ансамбль детекторов может быть эффективным решением. Наблюдается эффект равновесия гонки, где улучшение генераторов ведет к улучшению детекторов и наоборот.'}, 'en': {'title': 'Balancing the Race: Generators vs. Detectors in Image Synthesis', 'desc': 'This paper discusses the increasing need for synthetic image detectors due to advancements in image generation models. It highlights the challenges of generalization and robustness in detectors when faced with various image transformations and data sources. The study finds that while no single detector is universally effective, an ensemble approach may improve performance. Additionally, it notes a continuous competition between image generators and detectors, suggesting that advancements in one area drive improvements in the other.'}, 'zh': {'title': '生成器与检测器的竞赛平衡', 'desc': '随着新型图像生成模型的不断推出，合成图像检测器的需求也在增加。本文研究了时间、图像变换和数据来源对检测器泛化能力的影响。实验表明，虽然没有一个检测器是通用的，但通过集成方法可能实现更好的性能。此外，我们观察到生成器和检测器之间存在一种竞赛平衡效应，意味着更好的生成器会促使检测器的改进，反之亦然。'}}}, {'id': 'https://huggingface.co/papers/2409.16280', 'title': 'MonoFormer: One Transformer for Both Diffusion and Autoregression', 'url': 'https://huggingface.co/papers/2409.16280', 'abstract': 'Most existing multimodality methods use separate backbones for autoregression-based discrete text generation and diffusion-based continuous visual generation, or the same backbone by discretizing the visual data to use autoregression for both text and visual generation. In this paper, we propose to study a simple idea: share one transformer for both autoregression and diffusion. The feasibility comes from two main aspects: (i) Transformer is successfully applied to diffusion for visual generation, and (ii) transformer training for autoregression and diffusion is very similar, and the difference merely lies in that diffusion uses bidirectional attention mask and autoregression uses causal attention mask. Experimental results show that our approach achieves comparable image generation performance to current state-of-the-art methods as well as maintains the text generation capability. The project is publicly available at https://monoformer.github.io/.', 'score': 17, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'f7d206834c7984b5', 'data': {'categories': ['#multimodal', '#architecture'], 'emoji': '🦄', 'ru': {'title': 'Единый трансформер для мультимодальной генерации', 'desc': 'В статье предлагается использовать единую архитектуру трансформера для генерации как текста, так и изображений. Авторы объединяют авторегрессионный подход для текста и диффузионный для изображений в одной модели. Ключевое отличие заключается в использовании разных масок внимания: двунаправленной для диффузии и причинной для авторегрессии. Эксперименты показывают, что такой подход позволяет достичь качества генерации изображений на уровне современных методов, сохраняя при этом возможность генерации текста.'}, 'en': {'title': 'Unified Transformer for Text and Image Generation', 'desc': 'This paper introduces a novel approach to multimodal generation by utilizing a single transformer model for both autoregressive text generation and diffusion-based image generation. The authors highlight that transformers can effectively handle both tasks due to their similar training processes, differing only in the type of attention masks used. By sharing one transformer, the method simplifies the architecture while achieving competitive performance in image generation alongside maintaining text generation capabilities. The results demonstrate that this unified approach can match the effectiveness of existing state-of-the-art methods in the field.'}, 'zh': {'title': '共享变换器，实现多模态生成新突破', 'desc': '本论文提出了一种新的多模态生成方法，使用同一个变换器（transformer）来处理自回归（autoregression）和扩散（diffusion）生成任务。我们发现，变换器在视觉生成的扩散任务中表现良好，并且自回归和扩散的训练过程非常相似，主要区别在于注意力掩码的使用。实验结果表明，我们的方法在图像生成性能上与当前最先进的方法相当，同时保持了文本生成的能力。该项目的代码和数据集已公开，供研究者使用。'}}}, {'id': 'https://huggingface.co/papers/2409.16211', 'title': 'MaskBit: Embedding-free Image Generation via Bit Tokens', 'url': 'https://huggingface.co/papers/2409.16211', 'abstract': 'Masked transformer models for class-conditional image generation have become a compelling alternative to diffusion models. Typically comprising two stages - an initial VQGAN model for transitioning between latent space and image space, and a subsequent Transformer model for image generation within latent space - these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel embedding-free generation network operating directly on bit tokens - a binary quantized representation of tokens with rich semantics. The first contribution furnishes a transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256 benchmark, with a compact generator model of mere 305M parameters.', 'score': 16, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '517490c283effe33', 'data': {'categories': ['#cv', '#architecture', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений без эмбеддингов', 'desc': 'Статья представляет два основных вклада в область генерации изображений с использованием маскированных трансформеров. Во-первых, проводится систематическое исследование VQGAN, приводящее к его модернизации. Во-вторых, предлагается новая сеть генерации, работающая напрямую с битовыми токенами без использования эмбеддингов. Результаты демонстрируют высокую производительность и достижение нового state-of-the-art FID 1.52 на бенчмарке ImageNet 256x256 при компактном размере модели в 305 миллионов параметров.'}, 'en': {'title': 'Revolutionizing Image Generation with Masked Transformers and VQGANs', 'desc': 'This paper explores the use of masked transformer models for generating images based on specific classes, presenting a new approach that competes with diffusion models. It introduces an updated VQGAN model that improves the transition between latent and image spaces, making it more efficient and accessible. Additionally, the authors propose a novel generation network that operates directly on binary quantized tokens, eliminating the need for embeddings. This method achieves a remarkable FID score of 1.52 on the ImageNet benchmark, showcasing its effectiveness with a compact model.'}, 'zh': {'title': '掩蔽变换器：图像生成的新选择', 'desc': '这篇论文探讨了用于条件图像生成的掩蔽变换器模型，作为扩散模型的有力替代方案。研究中提出了两个主要贡献：首先，对VQGAN模型进行了系统的实证研究，提出了现代化的VQGAN。其次，介绍了一种新的无嵌入生成网络，直接在比特标记上进行操作，这种表示方式具有丰富的语义。该研究的结果显示，使用比特标记的无嵌入图像生成在ImageNet 256x256基准测试中达到了1.52的最新FID，且生成器模型仅有305M参数。'}}}, {'id': 'https://huggingface.co/papers/2409.16143', 'title': 'Seeing Faces in Things: A Model and Dataset for Pareidolia', 'url': 'https://huggingface.co/papers/2409.16143', 'abstract': "The human visual system is well-tuned to detect faces of all shapes and sizes. While this brings obvious survival advantages, such as a better chance of spotting unknown predators in the bush, it also leads to spurious face detections. ``Face pareidolia'' describes the perception of face-like structure among otherwise random stimuli: seeing faces in coffee stains or clouds in the sky. In this paper, we study face pareidolia from a computer vision perspective. We present an image dataset of ``Faces in Things'', consisting of five thousand web images with human-annotated pareidolic faces. Using this dataset, we examine the extent to which a state-of-the-art human face detector exhibits pareidolia, and find a significant behavioral gap between humans and machines. We find that the evolutionary need for humans to detect animal faces, as well as human faces, may explain some of this gap. Finally, we propose a simple statistical model of pareidolia in images. Through studies on human subjects and our pareidolic face detectors we confirm a key prediction of our model regarding what image conditions are most likely to induce pareidolia. Dataset and Website: https://aka.ms/faces-in-things", 'score': 15, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '16f30ee15781a4fd', 'data': {'categories': ['#dataset', '#cv'], 'emoji': '👀', 'ru': {'title': 'Лица в облаках: как машины учатся видеть невидимое', 'desc': 'Статья исследует феномен парейдолии лиц с точки зрения компьютерного зрения. Авторы создали датасет из 5000 изображений с аннотированными парейдолическими лицами. Они обнаружили значительную разницу между способностью людей и алгоритмов распознавать такие лица. Исследователи также предложили статистическую модель парейдолии и подтвердили её ключевые предсказания в экспериментах.'}, 'en': {'title': 'Unveiling Face Pareidolia: Bridging Human and Machine Perception', 'desc': "This paper explores the phenomenon of face pareidolia, where people perceive faces in random objects. It introduces a dataset called 'Faces in Things' containing 5,000 images with human-annotated pareidolic faces. The authors analyze how well a state-of-the-art face detector performs compared to human perception, revealing a significant difference in detection capabilities. They also propose a statistical model to explain the conditions that lead to pareidolia, supported by experiments with both human subjects and face detection algorithms."}, 'zh': {'title': '探索面孔错觉：人类与机器的差距', 'desc': '本论文研究了人类视觉系统如何识别面孔，尤其是面孔错觉现象，即在随机刺激中看到面孔的情况。我们创建了一个名为“事物中的面孔”的图像数据集，包含五千张带有人类标注的面孔错觉图像。通过分析这个数据集，我们发现最先进的人脸检测器在面孔错觉方面与人类存在显著差距。我们还提出了一个简单的统计模型来解释图像中面孔错觉的产生条件，并通过实验验证了模型的关键预测。'}}}, {'id': 'https://huggingface.co/papers/2409.16040', 'title': 'Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts', 'url': 'https://huggingface.co/papers/2409.16040', 'abstract': 'Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '7165c5f934c7d936', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': '📈', 'ru': {'title': 'Time-MoE: Эффективное масштабирование моделей прогнозирования временных рядов', 'desc': 'В статье представлена новая архитектура Time-MoE для предобучения крупномасштабных моделей прогнозирования временных рядов. Модель использует разреженную смесь экспертов (MoE) для повышения вычислительной эффективности, активируя только часть сети для каждого предсказания. Time-MoE обучена на новом наборе данных Time-300B, охватывающем более 300 миллиардов временных точек из 9 доменов. Масштабирование модели до 2.4 миллиардов параметров позволило значительно улучшить точность прогнозирования по сравнению с плотными моделями аналогичного размера.'}, 'en': {'title': 'Time-MoE: Scalable Time Series Forecasting with Sparse Mixture-of-Experts', 'desc': 'This paper presents Time-MoE, a novel architecture for time series forecasting that utilizes a sparse mixture-of-experts (MoE) approach to enhance computational efficiency. By activating only a subset of networks for each prediction, Time-MoE reduces inference costs while maintaining a high model capacity, allowing for scalable forecasting without increased computational load. The model is pre-trained on a large-scale dataset, Time-300B, which includes over 300 billion time points across various domains, enabling it to achieve significant improvements in forecasting precision. Time-MoE demonstrates that scaling laws for training tokens and model size can be effectively applied to time series forecasting, outperforming traditional dense models in both capability and efficiency.'}, 'zh': {'title': 'Time-MoE：高效灵活的时间序列预测新方案', 'desc': '深度学习在时间序列预测方面取得了显著进展，但现有的预训练时间序列模型规模有限且成本高昂。为了解决这个问题，我们提出了Time-MoE，这是一种可扩展的统一架构，旨在预训练更大、更强大的预测基础模型，同时降低推理成本。Time-MoE采用稀疏专家混合（MoE）设计，仅激活部分网络进行每次预测，从而提高计算效率。通过在新的大规模数据集Time-300B上进行预训练，Time-MoE实现了高达24亿参数的时间序列基础模型，显著提高了预测精度。'}}}, {'id': 'https://huggingface.co/papers/2409.15997', 'title': 'Improvements to SDXL in NovelAI Diffusion V3', 'url': 'https://huggingface.co/papers/2409.15997', 'abstract': 'In this technical report, we document the changes we made to SDXL in the process of training NovelAI Diffusion V3, our state of the art anime image generation model.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'd7548813ab80a9ba', 'data': {'categories': ['#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Эволюция SDXL: Рождение передовой модели генерации аниме', 'desc': 'Этот технический отчет описывает изменения, внесенные в модель SDXL при разработке NovelAI Diffusion V3. NovelAI Diffusion V3 представляет собой современную модель генерации аниме-изображений. Авторы подробно документируют процесс обучения и модификации базовой архитектуры SDXL. Целью этих изменений было улучшение качества и специализация модели для создания аниме-стиля.'}, 'en': {'title': 'Advancing Anime Image Generation with NovelAI Diffusion V3', 'desc': "This paper outlines the modifications implemented in the SDXL framework during the training of NovelAI Diffusion V3, which is designed for generating high-quality anime images. The authors detail the enhancements made to the model architecture and training procedures to improve image fidelity and diversity. They also discuss the impact of these changes on the model's performance metrics and its ability to generate visually appealing outputs. Overall, the report serves as a comprehensive guide for understanding the advancements in anime image generation using diffusion models."}, 'zh': {'title': '提升动漫图像生成的技术进步', 'desc': '本文记录了在训练NovelAI Diffusion V3过程中对SDXL所做的改进。这是一种先进的动漫图像生成模型，旨在提高生成图像的质量和多样性。我们通过优化模型架构和训练流程，提升了生成效果。此报告为相关研究人员提供了有价值的技术细节和经验。'}}}, {'id': 'https://huggingface.co/papers/2409.17143', 'title': 'Attention Prompting on Image for Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2409.17143', 'abstract': "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image, which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, Attention Prompting on Image improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively.", 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '448c8c959d632668', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'Улучшение восприятия изображений в AI с помощью текстовых подсказок', 'desc': 'Исследователи предложили новый метод визуального промптинга для улучшения работы больших визуально-языковых моделей (LVLM). Метод, названный Attention Prompting on Image, накладывает на входное изображение карту внимания, сгенерированную на основе текстового запроса. Это позволяет модели лучше фокусироваться на релевантных частях изображения при выполнении задач. Эксперименты показали значительное улучшение производительности LVLM на различных бенчмарках компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Enhancing LVLMs with Text-Driven Attention Heatmaps', 'desc': "This paper introduces a new technique called Attention Prompting on Image to improve Large Vision-Language Models (LVLMs). Unlike previous methods that only process visual inputs, this technique incorporates text queries to enhance the model's understanding of images. By generating a text-query-guided attention heatmap and overlaying it on the original image, the model can better follow text instructions for various tasks. The results show significant performance improvements on vision-language benchmarks, demonstrating the effectiveness of this approach."}, 'zh': {'title': '图像注意力提示：提升视觉语言模型的能力', 'desc': '与大型语言模型（LLMs）相比，大型视觉语言模型（LVLMs）能够接受图像作为输入，展现出更有趣的能力，并在各种视觉语言任务中表现出色。本文提出了一种新的提示技术，称为图像注意力提示（Attention Prompting on Image），旨在通过叠加文本查询引导的注意力热图来增强LVLM的视觉信息感知能力。该方法通过辅助模型（如CLIP）生成与输入图像相关的注意力热图，并将其与原始图像的像素值相乘，从而得到实际输入图像。大量实验表明，该技术在多个视觉语言基准测试中显著提高了模型性能。'}}}, {'id': 'https://huggingface.co/papers/2409.16283', 'title': 'Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation', 'url': 'https://huggingface.co/papers/2409.16283', 'abstract': "How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. Videos are at https://homangab.github.io/gen2act/", 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'd61af2b420bcfc6c', 'data': {'categories': ['#robotics', '#agents', '#video'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов новым навыкам через генерацию видео с людьми', 'desc': 'Статья представляет подход Gen2Act для обобщения политик манипуляции роботов на новые задачи с незнакомыми объектами и движениями. Метод использует генерацию видео с людьми на основе веб-данных для обучения робота. Gen2Act преобразует задачу манипуляции в двухэтапный процесс: генерацию видео по текстовому описанию и выполнение действий на основе сгенерированного видео. Подход требует значительно меньше данных о взаимодействии робота по сравнению с объемом данных для обучения модели генерации видео.'}, 'en': {'title': 'Empowering Robots to Learn from Human Videos for New Tasks', 'desc': 'This paper presents a method called Gen2Act that helps robots learn to manipulate new objects and perform unfamiliar tasks by using human video data. Instead of collecting expensive robot interaction data, the approach utilizes video generation models trained on readily available web videos to predict motion information. The robot policy is conditioned on these generated videos, allowing it to generalize to unseen object types and new motions without needing extensive retraining. The results demonstrate that Gen2Act can effectively enable robots to perform tasks that were not part of their original training data.'}, 'zh': {'title': 'Gen2Act：让机器人轻松应对新任务！', 'desc': '本文探讨了机器人如何在面对新物体和新动作时，能够有效地执行操作。我们提出了一种方法，通过人类视频生成来预测运动信息，并将机器人策略与生成的视频相结合。与传统的机器人数据收集方法相比，我们的方法利用了网络数据训练的视频生成模型，从而显著减少了所需的机器人交互数据。实验结果表明，Gen2Act能够在多样化的真实场景中，成功处理未见过的物体类型和新动作。'}}}, {'id': 'https://huggingface.co/papers/2409.15360', 'title': 'Reward-Robust RLHF in LLMs', 'url': 'https://huggingface.co/papers/2409.15360', 'abstract': 'As Large Language Models (LLMs) continue to progress toward more advanced forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is increasingly seen as a key pathway toward achieving Artificial General Intelligence (AGI). However, the reliance on reward-model-based (RM-based) alignment methods introduces significant challenges due to the inherent instability and imperfections of Reward Models (RMs), which can lead to critical issues such as reward hacking and misalignment with human intentions. In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. Our approach introduces a novel optimization objective that carefully balances performance and robustness by incorporating Bayesian Reward Model Ensembles (BRME) to model the uncertainty set of reward functions. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect reward models. Empirical results demonstrate that our framework consistently outperforms traditional RLHF across diverse benchmarks, showing improved accuracy and long-term stability. We also provide a theoretical analysis, demonstrating that reward-robust RLHF approaches the stability of constant reward settings, which proves to be effective in a stochastic-case analysis. Together, these contributions highlight the framework potential to enhance both the performance and stability of LLM alignment with RLHF.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'decce639f0e50f9e', 'data': {'categories': ['#rlhf', '#agi', '#optimization', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Надежное обучение с подкреплением для больших языковых моделей', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Авторы предлагают устойчивую к ошибкам модель вознаграждения структуру, использующую байесовские ансамбли моделей вознаграждения для учета неопределенности. Эта методика позволяет балансировать между производительностью и устойчивостью, обеспечивая более стабильное обучение. Эмпирические результаты показывают превосходство данного подхода над традиционным RLHF в точности и долгосрочной стабильности.'}, 'en': {'title': 'Enhancing LLM Alignment with Reward-Robust RLHF', 'desc': 'This paper discusses the challenges of aligning Large Language Models (LLMs) with human intentions using Reinforcement Learning from Human Feedback (RLHF). It highlights the issues caused by traditional reward models, such as instability and reward hacking. The authors propose a new framework called reward-robust RLHF, which uses Bayesian Reward Model Ensembles to improve the reliability of learning. Their empirical results show that this approach leads to better performance and stability compared to conventional methods.'}, 'zh': {'title': '提升大型语言模型的学习稳定性与性能', 'desc': '本文探讨了大型语言模型（LLMs）在实现人工通用智能（AGI）过程中，如何通过人类反馈的强化学习（RLHF）来提高学习的可靠性和稳定性。我们提出了一种奖励稳健的RLHF框架，旨在解决传统奖励模型（RMs）带来的不稳定性和不完美性问题。该框架引入了一种新的优化目标，通过贝叶斯奖励模型集成（BRME）来平衡性能和稳健性，从而确保即使在不完美的奖励模型下也能实现稳定学习。实验证明，我们的方法在多个基准测试中优于传统的RLHF方法，显示出更高的准确性和长期稳定性。'}}}, {'id': 'https://huggingface.co/papers/2409.15933', 'title': 'SLIMER-IT: Zero-Shot NER on Italian Language', 'url': 'https://huggingface.co/papers/2409.15933', 'abstract': 'Traditional approaches to Named Entity Recognition (NER) frame the task into a BIO sequence labeling problem. Although these systems often excel in the downstream task at hand, they require extensive annotated data and struggle to generalize to out-of-distribution input domains and unseen entity types. On the contrary, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities. While several works address Zero-Shot NER in English, little has been done in other languages. In this paper, we define an evaluation framework for Zero-Shot NER, applying it to the Italian language. Furthermore, we introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning approach for zero-shot NER leveraging prompts enriched with definition and guidelines. Comparisons with other state-of-the-art models, demonstrate the superiority of SLIMER-IT on never-seen-before entity tags.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '6b71ccb6ba817853', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset'], 'emoji': '🇮🇹', 'ru': {'title': 'Революция в итальянском NER: zero-shot подход с помощью LLM', 'desc': 'В статье описывается новый подход к распознаванию именованных сущностей (NER) на итальянском языке с использованием больших языковых моделей (LLM). Авторы представляют SLIMER-IT - итальянскую версию метода SLIMER, основанного на обучении с инструкциями и обогащенных промптах. В отличие от традиционных методов NER, требующих большого количества размеченных данных, SLIMER-IT демонстрирует превосходные результаты в задачах с нулевым обучением на новых типах сущностей. Исследование включает разработку фреймворка для оценки zero-shot NER на итальянском языке.'}, 'en': {'title': 'Empowering Italian NER with Zero-Shot Learning', 'desc': 'This paper addresses the challenges of Named Entity Recognition (NER) in languages other than English, specifically focusing on Italian. Traditional NER systems rely heavily on annotated data and often fail to recognize new entity types or adapt to different contexts. In contrast, the authors propose SLIMER-IT, a model that utilizes instruction tuning and prompts to enhance zero-shot NER capabilities. The evaluation shows that SLIMER-IT outperforms existing models in identifying previously unseen entity tags, showcasing its effectiveness in a zero-shot setting.'}, 'zh': {'title': 'SLIMER-IT：意大利语的零样本命名实体识别新方法', 'desc': '传统的命名实体识别（NER）方法将任务框定为BIO序列标注问题。这些系统在特定任务上表现出色，但需要大量标注数据，并且在处理未见过的输入领域和实体类型时表现不佳。相比之下，大型语言模型（LLMs）展示了强大的零样本能力。本文定义了一个零样本NER的评估框架，并将其应用于意大利语，同时介绍了SLIMER-IT，这是一个基于指令调优的零样本NER方法，利用丰富定义和指导的提示，显示出在未见过的实体标签上的优越性。'}}}, {'id': 'https://huggingface.co/papers/2409.12192', 'title': 'DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control', 'url': 'https://huggingface.co/papers/2409.12192', 'abstract': 'Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io', 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '15b569ff12bdc0e6', 'data': {'categories': ['#rl', '#agents', '#training', '#cv', '#robots'], 'emoji': '🤖', 'ru': {'title': 'DynaMo: эффективное самообучаемое представление для имитационного обучения роботов', 'desc': 'DynaMo - это новый метод самообучаемого представления для имитационного обучения в области компьютерного зрения и робототехники. Он использует совместное обучение латентной обратной и прямой моделей динамики для предсказания следующего кадра в латентном пространстве, не требуя дополнительных данных. DynaMo значительно улучшает эффективность имитационного обучения по сравнению с существующими методами на различных симулированных и реальных средах. Метод показывает хорошие результаты с разными архитектурами политик, включая трансформеры и диффузионные модели.'}, 'en': {'title': 'DynaMo: Efficient Imitation Learning Through Self-Supervised Visual Representation', 'desc': "This paper introduces DynaMo, a self-supervised method designed to improve the efficiency of imitation learning by learning visual representations directly from expert demonstrations. Unlike traditional methods that rely on large amounts of out-of-domain data, DynaMo operates solely within the domain of the task, using a latent inverse dynamics model and a forward dynamics model to predict future frames in latent space. The results show that DynaMo significantly enhances the performance of various imitation learning policies, outperforming previous self-supervised learning techniques and pretrained models. The study also includes an analysis of DynaMo's components to understand their contributions to policy performance."}, 'zh': {'title': 'DynaMo：提升模仿学习的视觉表示学习效率', 'desc': '模仿学习是一种强大的工具，用于训练复杂的视觉运动策略，但现有方法通常需要大量的专家示范。本文提出了一种新的自监督方法DynaMo，旨在提高视觉表示的学习效率。DynaMo通过联合学习潜在逆动态模型和前向动态模型，能够在没有外部数据的情况下，从专家示范中学习有效的视觉表示。实验结果表明，DynaMo在多个模拟和真实环境中显著提升了下游模仿学习的性能。'}}}, {'id': 'https://huggingface.co/papers/2409.13156', 'title': 'RRM: Robust Reward Model Training Mitigates Reward Hacking', 'url': 'https://huggingface.co/papers/2409.13156', 'abstract': 'Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-20', 'pub_date_card': {'ru': '20 сентября', 'en': 'September 20', 'zh': '9月20日'}, 'hash': '5c20b31a0506a9d7', 'data': {'categories': ['#rlhf', '#alignment', '#training'], 'emoji': '🎯', 'ru': {'title': 'Устранение артефактов для более точных моделей вознаграждения', 'desc': 'Эта статья посвящена проблеме обучения моделей вознаграждения (RMs) для больших языковых моделей (LLMs). Авторы выявили ограничение существующих методов, где RMs не могут эффективно различать контекстные сигналы и нерелевантные артефакты. Для решения этой проблемы предложен каузальный подход и новая техника аугментации данных. Эксперименты показали, что предложенный метод улучшает производительность RM и повышает качество моделей, обученных с помощью DPO.'}, 'en': {'title': 'Enhancing Reward Models for Better Alignment with Human Preferences', 'desc': 'This paper addresses the challenges in training reward models (RMs) for large language models (LLMs) by highlighting their inability to separate prompt-driven preferences from irrelevant artifacts. The authors propose a causal framework that allows RMs to learn preferences without being influenced by these artifacts, thus improving their effectiveness. They also introduce a novel data augmentation technique to further eliminate these artifacts during training. Experimental results demonstrate that their robust reward model (RRM) significantly enhances the performance of pairwise reward models and improves the alignment of decision-making policies in various benchmarks.'}, 'zh': {'title': '提升奖励模型的有效性', 'desc': '奖励模型（RM）在将大型语言模型（LLM）与人类偏好对齐中起着关键作用。然而，传统的RM训练依赖于特定提示的响应对，难以区分提示驱动的偏好与提示无关的伪影，如响应长度和格式。本文揭示了当前RM训练方法的一个基本局限性，即RM在确定偏好时未能有效区分上下文信号和无关伪影。为了解决这个问题，我们引入了一个因果框架，学习独立于这些伪影的偏好，并提出了一种新颖的数据增强技术，旨在消除这些伪影。'}}}, {'id': 'https://huggingface.co/papers/2409.13882', 'title': 'Tabular Data Generation using Binary Diffusion', 'url': 'https://huggingface.co/papers/2409.13882', 'abstract': 'Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-20', 'pub_date_card': {'ru': '20 сентября', 'en': 'September 20', 'zh': '9月20日'}, 'hash': '2df6195df4d45d13', 'data': {'categories': ['#synthetic', '#dataset', '#benchmark'], 'emoji': '🔢', 'ru': {'title': 'Бинарная диффузия: простой и эффективный подход к генерации табличных данных', 'desc': 'Статья представляет новый метод генерации синтетических табличных данных под названием Binary Diffusion. Авторы предлагают преобразовывать табличные данные в бинарное представление фиксированного размера, что позволяет применить диффузионную модель без сложной предобработки. Binary Diffusion использует простые XOR операции для добавления и удаления шума, а также бинарную кросс-энтропию в качестве функции потерь. Эксперименты показывают, что предложенный метод превосходит современные модели на нескольких популярных наборах табличных данных.'}, 'en': {'title': 'Revolutionizing Synthetic Data Generation with Binary Diffusion', 'desc': 'This paper presents a new method for generating synthetic tabular data, which is important when real data is scarce or sensitive. The authors introduce a lossless binary transformation that converts tabular data into fixed-size binary formats, making it easier to handle. They propose a generative model called Binary Diffusion that uses simple XOR operations for adding and removing noise, along with binary cross-entropy loss for effective training. The results show that Binary Diffusion outperforms existing models on several benchmark datasets while requiring less complexity and preprocessing.'}, 'zh': {'title': '无损二进制转换与二进制扩散模型的创新', 'desc': '生成合成表格数据在机器学习中非常重要，尤其是在真实数据有限或敏感的情况下。传统的生成模型在处理表格数据时面临挑战，因为表格数据具有混合数据类型和不同分布的独特特征。本文提出了一种新颖的无损二进制转换方法，将任何表格数据转换为固定大小的二进制表示，并引入了一种新的生成模型——二进制扩散，专门针对二进制数据。我们的模型在多个流行的表格基准数据集上进行了评估，结果表明二进制扩散在旅行、成人收入和糖尿病数据集上优于现有的最先进模型，同时模型体积显著更小。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (8)', '#cv (7)', '#data (1)', '#dataset (7)', '#diffusion (1)', '#edge_computing', '#ethics', '#evaluation', '#games', '#graphs', '#hallucinations', '#inference', '#interpretability', '#long_context (1)', '#math', '#medicine', '#multilingual (2)', '#multimodal (3)', '#optimization (1)', '#plp', '#rag', '#reasoning (1)', '#rl (1)', '#rlhf (2)', '#robotics (1)', '#robots', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning', '#translation (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-25 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-25 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-25 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    