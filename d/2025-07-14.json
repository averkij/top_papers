{
    "date": {
        "ru": "14 июля",
        "en": "July 14",
        "zh": "7月14日"
    },
    "time_utc": "2025-07-14 09:19",
    "weekday": 0,
    "issue_id": 4798,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01951",
            "title": "Test-Time Scaling with Reflective Generative Model",
            "url": "https://huggingface.co/papers/2507.01951",
            "abstract": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3's performance via the self-supervised process reward model (SPRM). Through sharing the backbone network and using task-specific heads for next token prediction and process scoring respectively, SPRM successfully integrates the policy model and process reward model(PRM) into a unified interface without extra process annotation, reducing over 99% PRM parameters for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable for test time scaling (TTS), and we provide three reasoning effort modes (low, medium, and high), based on the controllable thinking length. Moreover, we empirically establish a scaling law that reveals the relationship between total thinking computation and TTS performance. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
            "score": 56,
            "issue_id": 4792,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "46a6ab7d22e05401",
            "authors": [
                "Zixiao Wang",
                "Yuxin Wang",
                "Xiaorui Wang",
                "Mengting Xing",
                "Jie Gao",
                "Jianjun Xu",
                "Guangcan Liu",
                "Chenhui Jin",
                "Zhuo Wang",
                "Shengzhuo Zhang",
                "Hongtao Xie"
            ],
            "affiliations": [
                "MetaStone-AI",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01951.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#rl",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение с меньшими ресурсами: MetaStone-S1 переопределяет генеративные модели",
                    "desc": "MetaStone-S1 - это рефлексивная генеративная модель, использующая самоконтролируемую модель вознаграждения процесса (SPRM). Она достигает эффективного рассуждения и масштабируемой производительности с меньшим количеством параметров по сравнению с существующими моделями. SPRM объединяет модель политики и модель вознаграждения процесса в единый интерфейс, сокращая более 99% параметров для эффективного рассуждения. MetaStone-S1 предлагает три режима усилий рассуждения и устанавливает закон масштабирования, связывающий общие вычисления мышления и производительность TTS."
                },
                "en": {
                    "title": "Efficient Reasoning with Fewer Parameters: Introducing MetaStone-S1",
                    "desc": "MetaStone-S1 is a new generative model that uses a self-supervised process reward model (SPRM) to enhance reasoning capabilities while maintaining a smaller parameter size. By integrating the policy model and process reward model into a single framework, it significantly reduces the number of parameters needed for effective reasoning. The model offers different reasoning effort modes, allowing users to control the depth of thinking during tasks. Experiments show that MetaStone-S1 performs comparably to larger models while being more efficient, and it is available for the research community to explore."
                },
                "zh": {
                    "title": "MetaStone-S1：高效推理的新一代生成模型",
                    "desc": "MetaStone-S1是一种反思生成模型，采用自监督过程奖励模型（SPRM），在参数更少的情况下实现高效推理和可扩展性能。该模型通过共享主干网络，并使用特定任务的头部进行下一个标记预测和过程评分，成功将策略模型和过程奖励模型整合为统一接口，减少了99%以上的参数。MetaStone-S1适合测试时间扩展（TTS），并提供低、中、高三种推理努力模式，基于可控的思考长度。实验表明，MetaStone-S1在仅32B参数的情况下，性能与OpenAI-o3-mini系列相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08776",
            "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
            "url": "https://huggingface.co/papers/2507.08776",
            "abstract": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.",
            "score": 37,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "9361d4738ec618fe",
            "authors": [
                "Zhengqing Wang",
                "Yuefan Wu",
                "Jiacheng Chen",
                "Fuyang Zhang",
                "Yasutaka Furukawa"
            ],
            "affiliations": [
                "Simon Fraser University",
                "Wayve"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08776.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективный нейрорендеринг с помощью сжатых токенов светового поля",
                    "desc": "Эта статья представляет нейронный метод рендеринга, использующий сжатые токены светового поля (CLiFTs) для эффективного представления сцен. Метод позволяет рендерить новые ракурсы с различными вычислительными бюджетами, сохраняя богатую информацию о внешнем виде и геометрии сцены. Система использует мультиракурсное кодирование, кластеризацию в латентном пространстве и адаптивный рендерер для достижения компромисса между размером данных, качеством рендеринга и скоростью. Эксперименты на наборах данных RealEstate10K и DL3DV подтверждают эффективность подхода, демонстрируя значительное сокращение данных при сопоставимом качестве рендеринга."
                },
                "en": {
                    "title": "Efficient Scene Representation with Compressed Light-Field Tokens",
                    "desc": "This paper introduces a novel neural rendering technique that utilizes compressed light-field tokens (CLiFTs) to efficiently depict scenes and generate new views. By employing a multi-view encoder, the method tokenizes images based on their camera positions, allowing for effective representation of both appearance and geometry. The approach leverages latent-space K-means to select key rays as cluster centroids, which are then condensed into CLiFTs for streamlined rendering. The system adapts to different compute budgets by varying the number of tokens used, demonstrating significant data reduction while maintaining high rendering quality across various datasets."
                },
                "zh": {
                    "title": "高效神经渲染：压缩光场标记的应用",
                    "desc": "这篇论文提出了一种神经渲染方法，使用压缩光场标记（CLiFTs）来高效表示场景并渲染新视图。CLiFT通过压缩标记实现计算效率，同时能够根据需要调整标记数量以表示场景或渲染新视图。具体来说，给定一组图像，多视角编码器将图像与相机姿态进行标记。通过潜在空间K均值选择一组减少的光线作为聚类中心，最终构建出CLiFTs。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08800",
            "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
            "url": "https://huggingface.co/papers/2507.08800",
            "abstract": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.",
            "score": 21,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "97c49e854df6a19d",
            "authors": [
                "Luke Rivard",
                "Sun Sun",
                "Hongyu Guo",
                "Wenhu Chen",
                "Yuntian Deng"
            ],
            "affiliations": [
                "National Research Council Canada",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08800.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#diffusion",
                    "#dataset",
                    "#agents",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Нейронная симуляция GUI: шаг к адаптивным интерфейсам будущего",
                    "desc": "NeuralOS - это нейронная система, симулирующая графические интерфейсы операционных систем путем предсказания кадров экрана в ответ на действия пользователя. Она объединяет рекуррентную нейронную сеть (RNN) для отслеживания состояния компьютера и нейронный рендерер на основе диффузии для генерации изображений экрана. Модель обучена на большом наборе данных записей Ubuntu XFCE, включающем как случайные, так и реалистичные взаимодействия. Эксперименты показывают, что NeuralOS успешно визуализирует реалистичные GUI-последовательности и точно предсказывает переходы состояний, хотя моделирование детальных клавиатурных взаимодействий остается сложной задачей."
                },
                "en": {
                    "title": "NeuralOS: Predicting GUIs with RNNs and Diffusion Rendering",
                    "desc": "NeuralOS is a neural framework designed to simulate operating system graphical user interfaces (GUIs) by predicting screen frames based on user inputs like mouse movements and keyboard events. It utilizes a recurrent neural network (RNN) to keep track of the computer's state and a diffusion-based renderer to create realistic screen images. The model is trained on a comprehensive dataset of Ubuntu XFCE recordings, which include both random and realistic user interactions. While it excels at rendering GUI sequences and predicting state transitions, accurately modeling detailed keyboard interactions remains a challenge, marking a significant advancement in generative neural interfaces for human-computer interaction."
                },
                "zh": {
                    "title": "NeuralOS：未来人机交互的智能界面",
                    "desc": "NeuralOS 是一个神经网络框架，能够通过预测用户输入（如鼠标移动、点击和键盘事件）来模拟操作系统的图形用户界面（GUI）。它结合了递归神经网络（RNN）和基于扩散的神经渲染器，能够生成屏幕图像并跟踪计算机状态。该模型在大规模的 Ubuntu XFCE 录制数据集上进行训练，包含随机生成的交互和 AI 代理生成的真实交互。尽管精确建模细粒度的键盘交互仍然具有挑战性，NeuralOS 为未来人机交互系统创建完全自适应的生成神经接口迈出了重要一步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05397",
            "title": "Neural-Driven Image Editing",
            "url": "https://huggingface.co/papers/2507.05397",
            "abstract": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area.",
            "score": 15,
            "issue_id": 4796,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "37985f3d38738096",
            "authors": [
                "Pengfei Zhou",
                "Jie Xia",
                "Xiaopeng Peng",
                "Wangbo Zhao",
                "Zilong Ye",
                "Zekai Li",
                "Suorong Yang",
                "Jiadong Pan",
                "Yuanxiang Chen",
                "Ziqiao Wang",
                "Kai Wang",
                "Qian Zheng",
                "Xiaojun Chang",
                "Gang Pan",
                "Shurong Dong",
                "Kaipeng Zhang",
                "Yang You"
            ],
            "affiliations": [
                "MBZUAI",
                "NJU",
                "NUS",
                "RIT",
                "SII",
                "Shanghai AI Lab",
                "USTC",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05397.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Мысленное редактирование изображений становится реальностью",
                    "desc": "LoongX - это новый подход к редактированию изображений без использования рук, основанный на мультимодальных нейрофизиологических сигналах и диффузионных моделях. Система использует современные методы машинного обучения, включая нейронные сети с пространством состояний и динамическое слияние признаков, для обработки сигналов ЭЭГ, фНИРС, ФПГ и движений головы. LoongX достигает производительности, сравнимой с методами на основе текста, и превосходит их при комбинировании с речью. Этот подход открывает новые возможности для интуитивного редактирования изображений и развития когнитивно-управляемых творческих технологий."
                },
                "en": {
                    "title": "Hands-Free Image Editing with Brain Signals!",
                    "desc": "LoongX is a novel hands-free image editing system that utilizes multimodal neurophysiological signals, such as EEG and fNIRS, combined with advanced diffusion models. This approach allows users with limited motor control or language abilities to edit images intuitively, without manual input. By integrating a cross-scale state space module and a dynamic gated fusion module, LoongX effectively processes diverse signals to capture user intent and align it with image editing semantics. Experimental results show that LoongX performs comparably to traditional text-driven methods and even surpasses them when incorporating speech signals, demonstrating its potential for accessible creative technologies."
                },
                "zh": {
                    "title": "LoongX：无障碍图像编辑的新方式",
                    "desc": "LoongX是一种基于多模态神经生理信号的免手动图像编辑方法，利用扩散模型实现图像编辑。该方法结合了脑机接口（BCI）和生成模型的最新进展，能够帮助运动能力或语言能力有限的用户进行图像编辑。LoongX通过跨尺度状态空间（CS3）模块和动态门控融合（DGF）模块有效处理不同类型的信号，并将其整合到统一的潜在空间中。实验结果表明，LoongX在性能上与基于文本的方法相当，并在结合语音时表现更佳，展示了神经驱动生成模型在图像编辑中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08799",
            "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
            "url": "https://huggingface.co/papers/2507.08799",
            "abstract": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.",
            "score": 13,
            "issue_id": 4796,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "368d77b32f2df891",
            "authors": [
                "Max Belitsky",
                "Dawid J. Kopiczko",
                "Michael Dorkenwald",
                "M. Jehanzeb Mirza",
                "Cees G. M. Snoek",
                "Yuki M. Asano"
            ],
            "affiliations": [
                "CSAIL MIT",
                "FunAI Lab University of Technology Nuremberg",
                "VIS Lab University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08799.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений в ИИ через одноразовое вмешательство в кэш",
                    "desc": "В статье представлен метод кэш-управления для улучшения рассуждений в языковых моделях. Этот легковесный подход использует одноразовое вмешательство в кэш ключ-значение для изменения поведения модели. Метод применяется для индукции рассуждений по цепочке мыслей в небольших языковых моделях, используя следы рассуждений, сгенерированные GPT-4. Экспериментальные оценки показывают улучшение как качественной структуры рассуждений модели, так и количественной производительности задач."
                },
                "en": {
                    "title": "Cache Steering: Enhancing Reasoning in Language Models Efficiently",
                    "desc": "This paper introduces cache steering, a novel technique that enhances the reasoning capabilities of language models by modifying the key-value cache in a single step. By using this method, the authors demonstrate how to encourage chain-of-thought reasoning in smaller language models without the need for extensive fine-tuning or altering prompts. The approach utilizes reasoning traces generated by GPT-4o to create steering vectors that guide the model towards more structured and multi-step reasoning. Experimental results show that cache steering not only improves the quality of reasoning but also boosts performance on various reasoning tasks, offering a more efficient and stable alternative to previous methods."
                },
                "zh": {
                    "title": "缓存引导：提升语言模型推理的有效方法",
                    "desc": "本文提出了一种名为缓存引导的方法，通过对关键值缓存进行一次性干预，隐式地引导语言模型的推理。该方法旨在提高小型语言模型的链式推理能力，利用GPT-4生成的推理轨迹构建引导向量，从而使模型行为向更明确的多步骤推理转变，而无需进行微调或修改提示。实验结果表明，缓存引导不仅改善了模型推理的定性结构，还提高了定量任务性能。与需要持续干预的先前激活引导技术相比，我们的一次性缓存引导在超参数稳定性、推理时间效率和集成便利性方面具有显著优势，成为一种更稳健和实用的受控生成解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08801",
            "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
            "url": "https://huggingface.co/papers/2507.08801",
            "abstract": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.",
            "score": 12,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "d53fd2bb25db02a0",
            "authors": [
                "Hangjie Yuan",
                "Weihua Chen",
                "Jun Cen",
                "Hu Yu",
                "Jingyun Liang",
                "Shuning Chang",
                "Zhihui Lin",
                "Tao Feng",
                "Pengwei Liu",
                "Jiazheng Xing",
                "Hao Luo",
                "Jiasheng Tang",
                "Fan Wang",
                "Yi Yang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08801.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#architecture",
                    "#video",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективная генерация видео с помощью модифицированных языковых моделей",
                    "desc": "Lumos-1 - это авторегрессионный генератор видео, использующий модифицированную архитектуру языковой модели (LLM). Он применяет технологии MM-RoPE и AR-DF для решения проблем пространственно-временной корреляции и дисбаланса покадровых потерь. Модель достигает конкурентоспособной производительности, используя меньше вычислительных ресурсов. Lumos-1 сохраняет архитектуру LLM с минимальными модификациями, что отличает его от существующих подходов к генерации видео."
                },
                "en": {
                    "title": "Lumos-1: Efficient Video Generation with LLM Architecture",
                    "desc": "Lumos-1 is an innovative autoregressive video generator that enhances the traditional large language model (LLM) architecture to effectively handle video data. It introduces a new method called MM-RoPE, which improves the model's ability to understand spatiotemporal correlations while addressing issues of frame-wise loss imbalance. The model employs a token dependency strategy that respects both intra-frame and inter-frame relationships, ensuring coherent video generation. By utilizing efficient training techniques, Lumos-1 achieves competitive performance with fewer computational resources compared to existing models."
                },
                "zh": {
                    "title": "Lumos-1：高效自回归视频生成的新突破",
                    "desc": "Lumos-1是一种自回归视频生成器，采用了经过修改的LLM架构，结合了MM-RoPE和AR-DF技术，以解决时空相关性和帧间损失不平衡的问题。该模型在保持LLM架构的基础上，利用3D RoPE来增强时空相关性，并提出了一种新的RoPE方案MM-RoPE，以支持多模态时空数据建模。Lumos-1还采用了一种令牌依赖策略，确保帧内双向性和帧间时间因果关系，从而解决了空间信息冗余导致的帧间损失不平衡问题。通过高效的训练技术，Lumos-1在仅使用48个GPU的情况下，达到了与现有模型相当的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08772",
            "title": "From One to More: Contextual Part Latents for 3D Generation",
            "url": "https://huggingface.co/papers/2507.08772",
            "abstract": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.",
            "score": 8,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "13e80f68dc4965b1",
            "authors": [
                "Shaocong Dong",
                "Lihe Ding",
                "Xiao Chen",
                "Yaokun Li",
                "Yuxin Wang",
                "Yucheng Wang",
                "Qi Wang",
                "Jaehyeok Kim",
                "Chenjian Gao",
                "Zhanpeng Huang",
                "Zibin Wang",
                "Tianfan Xue",
                "Dan Xu"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SenseTime Research",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08772.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Разделяй и властвуй: новый подход к генерации 3D-объектов",
                    "desc": "CoPart - это новая система для генерации трехмерных объектов, основанная на диффузионных моделях. Она разбивает объекты на отдельные части, что позволяет лучше моделировать сложные геометрические формы и взаимосвязи между компонентами. CoPart дает возможность более точного контроля над генерацией на уровне отдельных частей объекта. Для обучения системы был создан новый набор данных Partverse с аннотированными трехмерными моделями и их сегментацией на части."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with Part-Aware Diffusion",
                    "desc": "The paper introduces CoPart, a part-aware diffusion framework designed to improve 3D object generation by breaking down objects into contextual parts. This approach addresses limitations in existing models, such as the inability to capture complex geometries and the lack of part independence in holistic representations. CoPart enhances the modeling of relationships between parts and allows for fine-grained control over part-level conditioning. Additionally, the authors present a new dataset, Partverse, to support large-scale training and demonstrate CoPart's effectiveness in tasks like part-level editing and scene composition."
                },
                "zh": {
                    "title": "部件意识的3D生成新框架",
                    "desc": "CoPart是一个关注部件的扩散框架，旨在通过将3D对象分解为上下文相关的部件来增强3D生成。该方法解决了传统方法在处理复杂多部件几何形状时的局限性，能够更好地建模部件之间的关系。通过部件分解，CoPart降低了编码复杂性，并支持精细的部件级条件控制。实验结果表明，CoPart在部件级编辑、关节对象生成和场景组合方面表现出色，具有前所未有的可控性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08794",
            "title": "One Token to Fool LLM-as-a-Judge",
            "url": "https://huggingface.co/papers/2507.08794",
            "abstract": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning openers like \"Thought process:\" and \"Let's solve this problem step by step.\" can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.",
            "score": 6,
            "issue_id": 4793,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "c5951de5379e6612",
            "authors": [
                "Yulai Zhao",
                "Haolin Liu",
                "Dian Yu",
                "S. Y. Kung",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Princeton University",
                "Tencent AI Lab",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08794.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#data",
                    "#dataset",
                    "#optimization",
                    "#hallucinations",
                    "#synthetic",
                    "#rlhf"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Укрепление генеративных моделей вознаграждения против поверхностных манипуляций",
                    "desc": "Статья посвящена уязвимостям генеративных моделей вознаграждения, использующих большие языковые модели (LLM) для оценки качества ответов. Авторы обнаружили, что такие модели могут быть обмануты поверхностными манипуляциями, такими как добавление символов или фраз-заполнителей. Для решения этой проблемы предложена стратегия аугментации данных, позволяющая создать более надежную модель оценки. Исследование подчеркивает необходимость разработки более надежных методов оценки на основе LLM."
                },
                "en": {
                    "title": "Strengthening LLMs: Combatting Superficial Manipulations in Reward Models",
                    "desc": "This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications."
                },
                "zh": {
                    "title": "提升生成奖励模型的鲁棒性",
                    "desc": "生成奖励模型（LLMs作为评判者）在使用大型语言模型评估答案质量时，容易受到表面操控的影响。尽管这种比较任务看似简单，但我们发现生成奖励模型在面对非单词符号或推理开头词时，常常会产生错误的正向奖励。这种脆弱性在不同的LLM、数据集和提示格式中普遍存在，威胁到依赖生成奖励模型的核心算法范式。为了解决这个问题，我们提出了一种简单有效的数据增强策略，训练出一种具有显著改进鲁棒性的生成奖励模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08441",
            "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2507.08441",
            "abstract": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.",
            "score": 3,
            "issue_id": 4797,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "700a23d0c8771ece",
            "authors": [
                "Anlin Zheng",
                "Xin Wen",
                "Xuanyang Zhang",
                "Chuofan Ma",
                "Tiancai Wang",
                "Gang Yu",
                "Xiangyu Zhang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Dexmal",
                "MEGVII Technology",
                "StepFun",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08441.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "VFMTok: Революция в токенизации изображений с помощью фундаментальных моделей зрения",
                    "desc": "Исследователи представили новый токенизатор изображений VFMTok, основанный на предобученных фундаментальных моделях компьютерного зрения. VFMTok использует замороженную фундаментальную модель в качестве энкодера и включает адаптивное квантование регионов и семантическую цель реконструкции. Этот подход значительно улучшает качество реконструкции и генерации изображений, а также эффективность токенизации. VFMTok также ускоряет сходимость авторегрессионных моделей и позволяет осуществлять высококачественный условный синтез изображений без использования classifier-free guidance."
                },
                "en": {
                    "title": "Enhancing Image Generation with VFMTok: A New Tokenizer Approach",
                    "desc": "This paper introduces VFMTok, a new image tokenizer that utilizes pre-trained vision foundation models to improve image reconstruction and generation. By employing a frozen vision model as its encoder, VFMTok incorporates a region-adaptive quantization framework to minimize redundancy in feature representation. Additionally, it uses a semantic reconstruction objective to ensure that the outputs maintain semantic accuracy aligned with the original model. The results show significant enhancements in token efficiency and autoregressive generation performance, achieving a gFID of 2.07 on ImageNet and tripling model convergence speed."
                },
                "zh": {
                    "title": "基于视觉模型的高效图像标记器",
                    "desc": "本文提出了一种新颖的图像标记器，基于预训练的视觉基础模型，旨在提高图像重建和生成的质量以及标记效率。我们使用冻结的视觉基础模型作为标记器的编码器，并引入了区域自适应量化框架和语义重建目标，以减少冗余并保持语义一致性。通过这些设计，VFMTok在图像重建和生成质量上取得了显著提升，同时加速了自回归生成的收敛速度。该方法在ImageNet基准测试中实现了2.07的gFID，并支持高保真度的类别条件合成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08771",
            "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
            "url": "https://huggingface.co/papers/2507.08771",
            "abstract": "To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).",
            "score": 2,
            "issue_id": 4794,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "27bac3ede0d76a2a",
            "authors": [
                "Chenyang Song",
                "Weilin Zhao",
                "Xu Han",
                "Chaojun Xiao",
                "Yingfa Chen",
                "Yuxuan Li",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08771.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#low_resource",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "BlockFFN: Эффективная архитектура смеси экспертов для ускорения больших языковых моделей",
                    "desc": "Статья представляет новую архитектуру смеси экспертов (MoE) под названием BlockFFN для снижения вычислительной нагрузки больших языковых моделей (LLM). BlockFFN использует дифференцируемую и гибкую маршрутизацию, объединяющую активацию ReLU и RMSNorm. Авторы разработали цели обучения, учитывающие разреженность на уровне токенов и чанков, что делает модель более подходящей для ускорения. Экспериментальные результаты показывают превосходство BlockFFN над другими базовыми MoE, достигая более 80% разреженности на уровне токенов и 70% на уровне 8-токенных чанков."
                },
                "en": {
                    "title": "BlockFFN: Efficient Sparsity for Faster Language Models",
                    "desc": "This paper presents a new architecture called BlockFFN that improves the efficiency of mixture-of-experts (MoE) models by addressing issues with routing and sparsity. The authors introduce a router that uses ReLU activation and RMSNorm, allowing for more flexible and differentiable routing of parameters. They also propose training objectives that enhance both token-level and chunk-level sparsity, making the model more suitable for low-resource environments. Experimental results show that BlockFFN outperforms existing MoE models, achieving significant speedups on end-side devices while maintaining high levels of sparsity."
                },
                "zh": {
                    "title": "提升稀疏激活模型的性能与加速",
                    "desc": "为了减轻大型语言模型的计算负担，稀疏激活架构（如专家混合模型MoE）受到越来越多的关注。然而，传统MoE的非可微和不灵活的路由方式会影响模型性能。此外，虽然每个token只激活少量参数，但这些稀疏激活架构在块级稀疏性上表现较低，这使得在低资源条件下加速变得困难。为了解决这些问题，我们提出了一种新型的MoE架构BlockFFN，并设计了高效的训练和部署技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06952",
            "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
            "url": "https://huggingface.co/papers/2507.06952",
            "abstract": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.",
            "score": 2,
            "issue_id": 4795,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 июля",
                "en": "July 9",
                "zh": "7月9日"
            },
            "hash": "d203424a90614c2b",
            "authors": [
                "Keyon Vafa",
                "Peter G. Chang",
                "Ashesh Rambachan",
                "Sendhil Mullainathan"
            ],
            "affiliations": [
                "Harvard University",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06952.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Фундаментальные модели: успех в обучении, провал в обобщении",
                    "desc": "Фундаментальные модели, несмотря на их успехи в обучении, часто не могут обобщаться на новые задачи из-за использования эвристик, специфичных для конкретных задач, вместо понимания базовых моделей мира. Авторы разработали метод оценки фундаментальных моделей, который исследует их адаптацию к синтетическим датасетам, сгенерированным на основе некоторой постулированной модели мира. Этот метод, названный зондом индуктивного смещения, измеряет, насколько индуктивное смещение модели соответствует модели мира. Исследования показали, что фундаментальные модели, обученные на орбитальных траекториях, не смогли применить законы Ньютона при адаптации к новым физическим задачам."
                },
                "en": {
                    "title": "Unveiling the Limits of Foundation Models: Task-Specific Heuristics vs. General Understanding",
                    "desc": "This paper discusses the limitations of foundation models in machine learning, particularly their inability to generalize to new tasks. The authors introduce a method called the inductive bias probe, which evaluates how well these models adapt to synthetic datasets based on a theoretical world model. Their findings indicate that while foundation models perform well on training tasks, they often rely on task-specific heuristics instead of understanding the underlying principles. Specifically, models trained on orbital trajectories struggle to apply Newtonian mechanics in new physics tasks, highlighting the need for better generalization capabilities."
                },
                "zh": {
                    "title": "基础模型的归纳偏差探测与泛化能力",
                    "desc": "基础模型在训练任务中表现出色，但在新任务上常常无法泛化，这是因为它们依赖于特定任务的启发式方法，而不是捕捉到更深层次的世界模型。我们提出了一种评估基础模型的新技术，旨在检查它们如何适应从假设的世界模型生成的合成数据集。该技术测量基础模型的归纳偏差是否与世界模型一致，因此我们称之为归纳偏差探测器。我们的研究发现，尽管基础模型在训练任务中表现良好，但在适应新任务时，它们往往未能发展出对底层世界模型的归纳偏差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07151",
            "title": "Robust Multimodal Large Language Models Against Modality Conflict",
            "url": "https://huggingface.co/papers/2507.07151",
            "abstract": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.",
            "score": 2,
            "issue_id": 4792,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 июля",
                "en": "July 9",
                "zh": "7月9日"
            },
            "hash": "2c4bd34981d368dd",
            "authors": [
                "Zongmeng Zhang",
                "Wengang Zhou",
                "Jie Zhao",
                "Houqiang Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering and Information Science, University of Science and Technology of China",
                "Huawei Technologies Co., Ltd.",
                "School of Artificial Intelligence and Data Science, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07151.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#dataset",
                    "#rl",
                    "#hallucinations",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Борьба с галлюцинациями в мультимодальных нейросетях",
                    "desc": "Исследование посвящено проблеме галлюцинаций в мультимодальных больших языковых моделях (MLLM) из-за конфликта модальностей. Авторы создали датасет MMMC для симуляции этого явления в задачах компьютерного зрения и обработки естественного языка. Были предложены три метода для уменьшения галлюцинаций: инженерия промптов, обучение с учителем и обучение с подкреплением. Эксперименты показали, что обучение с подкреплением наиболее эффективно в снижении галлюцинаций при конфликте модальностей."
                },
                "en": {
                    "title": "Tackling Hallucinations in MLLMs: The Power of Reinforcement Learning",
                    "desc": "This paper explores how multimodal large language models (MLLMs) can experience hallucinations due to conflicts between different types of input data, known as modality conflict. The authors define modality conflict and create a dataset called Multimodal Modality Conflict (MMMC) to study this issue in vision-language tasks. They propose three strategies to reduce hallucinations: prompt engineering, supervised fine-tuning, and reinforcement learning. Among these, reinforcement learning is found to be the most effective method for addressing hallucinations caused by modality conflict, while supervised fine-tuning also shows reliable results."
                },
                "zh": {
                    "title": "揭示模态冲突，减轻幻觉的有效策略",
                    "desc": "这篇论文研究了多模态大型语言模型（MLLMs）中的模态冲突现象，发现它是导致幻觉的一个重要原因。与以往研究不同，本文关注的是来自不同模态的输入之间的内在冲突，这种冲突使得MLLMs面临困境并直接导致幻觉。我们正式定义了模态冲突，并构建了一个名为多模态模态冲突（MMMC）的数据集，以模拟这一现象。通过实验，我们发现基于强化学习的方法在减轻模态冲突引起的幻觉方面表现最佳，而监督微调方法则展现出良好且稳定的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-11.html",
    "link_next": "2025-07-15.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "11.07",
        "en": "07/11",
        "zh": "7月11日"
    },
    "short_date_next": {
        "ru": "15.07",
        "en": "07/15",
        "zh": "7月15日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    }
}