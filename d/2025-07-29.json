{
    "date": {
        "ru": "29 июля",
        "en": "July 29",
        "zh": "7月29日"
    },
    "time_utc": "2025-07-29 19:15",
    "weekday": 1,
    "issue_id": 5074,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.19849",
            "title": "Agentic Reinforced Policy Optimization",
            "url": "https://huggingface.co/papers/2507.19849",
            "abstract": "Agentic Reinforced Policy Optimization (ARPO) is a novel RL algorithm that enhances multi-turn LLM-based agents by adaptive uncertainty management and advantage attribution, outperforming trajectory-level RL algorithms with reduced resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO",
            "score": 67,
            "issue_id": 5058,
            "pub_date": "2025-07-26",
            "pub_date_card": {
                "ru": "26 июля",
                "en": "July 26",
                "zh": "7月26日"
            },
            "hash": "a8b71350a642e881",
            "authors": [
                "Guanting Dong",
                "Hangyu Mao",
                "Kai Ma",
                "Licheng Bao",
                "Yifei Chen",
                "Zhongyuan Wang",
                "Zhongxia Chen",
                "Jiazhen Du",
                "Huiyang Wang",
                "Fuzheng Zhang",
                "Guorui Zhou",
                "Yutao Zhu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.19849.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ARPO: Умное обучение ИИ-агентов для эффективного решения сложных задач",
                    "desc": "ARPO - это новый алгоритм обучения с подкреплением для многоходовых агентов на основе больших языковых моделей. Он использует адаптивное управление неопределенностью и оценку преимуществ для улучшения взаимодействия с внешними инструментами. ARPO превосходит существующие алгоритмы RL на уровне траекторий по эффективности и использованию ресурсов. Алгоритм показал высокие результаты на 13 сложных бенчмарках в задачах вычислительного рассуждения, рассуждения на основе знаний и глубокого поиска."
                },
                "en": {
                    "title": "Enhancing Multi-Turn LLMs with Adaptive Uncertainty Management",
                    "desc": "Agentic Reinforced Policy Optimization (ARPO) is a new reinforcement learning (RL) algorithm designed to improve multi-turn interactions of large language models (LLMs) by managing uncertainty and attributing advantages effectively. It addresses the limitations of existing RL methods that struggle with balancing long-term reasoning and tool interactions in realistic scenarios. ARPO uses an adaptive rollout mechanism that adjusts sampling strategies based on the uncertainty observed after using external tools, promoting better exploration. Experimental results show that ARPO outperforms traditional trajectory-level RL algorithms while using significantly fewer resources, making it a more efficient choice for training LLM-based agents in dynamic environments."
                },
                "zh": {
                    "title": "ARPO：提升多轮智能体的强化学习新方法",
                    "desc": "Agentic Reinforced Policy Optimization（ARPO）是一种新颖的强化学习算法，旨在通过自适应不确定性管理和优势归因来增强基于大型语言模型（LLM）的多轮智能体。该算法在资源使用减少的情况下，超越了传统的轨迹级强化学习算法。ARPO通过动态平衡全局轨迹采样和步级采样，促进在工具使用后高不确定性步骤的探索。实验结果表明，ARPO在计算推理、知识推理和深度搜索等13个具有挑战性的基准测试中表现优越，且仅需现有方法一半的工具使用预算。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20939",
            "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
            "url": "https://huggingface.co/papers/2507.20939",
            "abstract": "A multimodal model that processes visual, audio, and textual signals for structured comprehension of real-world short videos improves video search, recommendation, and engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world user-generated short videos, especially those distributed on platforms such as WeChat Channel and TikTok, dominate the mobile internet. However, current large multimodal models lack essential temporally-structured, detailed, and in-depth video comprehension capabilities, which are the cornerstone of effective video search and recommendation, as well as emerging video applications. Understanding real-world shorts is actually challenging due to their complex visual elements, high information density in both visuals and audio, and fast pacing that focuses on emotional expression and viewpoint delivery. This requires advanced reasoning to effectively integrate multimodal information, including visual, audio, and text. In this work, we introduce ARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual signals from raw video inputs end-to-end for structured comprehension. The model is capable of multi-granularity timestamped video captioning and summarization, open-ended video question answering, temporal video grounding, and video reasoning. Leveraging high-quality data from an automated annotation pipeline, our compact 7B-parameter model is trained through a comprehensive regimen: pre-training, instruction fine-tuning, cold start, reinforcement learning (RL) post-training, and final instruction fine-tuning. Quantitative evaluations on our introduced benchmark ShortVid-Bench and qualitative comparisons demonstrate its strong performance in real-world video comprehension, and it supports zero-shot or fine-tuning with a few samples for diverse downstream applications. The real-world production deployment of our model has yielded tangible and measurable improvements in user engagement and satisfaction, a success supported by its remarkable efficiency, with stress tests indicating an inference time of just 10 seconds for a one-minute video on H20 GPU.",
            "score": 44,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "6531c472054cd026",
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Chen Li",
                "Teng Wang",
                "Junfu Pu",
                "Yizhuo Li",
                "Lu Qiu",
                "Jin Ma",
                "Lisheng Duan",
                "Xinyu Zuo",
                "Jinwen Luo",
                "Weibo Gu",
                "Zexuan Li",
                "Xiaojing Zhang",
                "Yangyu Tao",
                "Han Hu",
                "Di Wang",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Big Data Platform Department, Tencent PCG",
                "Search Application Department, Tencent CSIG",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20939.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Мультимодальный ИИ для глубокого понимания коротких видео",
                    "desc": "Модель ARC-Hunyuan-Video представляет собой мультимодальную систему для структурированного понимания коротких видео. Она обрабатывает визуальные, аудио и текстовые сигналы для выполнения задач аннотирования, суммаризации, ответов на вопросы и рассуждений о видео. Модель обучена с использованием предварительной подготовки, дообучения на инструкциях, холодного старта и обучения с подкреплением. Реальное внедрение модели показало улучшение вовлеченности пользователей и эффективность обработки видео."
                },
                "en": {
                    "title": "Revolutionizing Video Comprehension with Multimodal AI",
                    "desc": "This paper presents ARC-Hunyuan-Video, a multimodal model designed to enhance the understanding of short videos by integrating visual, audio, and textual information. The model addresses the challenges of complex video content and fast pacing, enabling structured comprehension through advanced reasoning techniques. It offers capabilities such as timestamped video captioning, summarization, and open-ended question answering, making it suitable for various applications. The model's efficient training process and real-world deployment have shown significant improvements in user engagement and satisfaction."
                },
                "zh": {
                    "title": "多模态模型提升短视频理解与推荐",
                    "desc": "本论文介绍了一种名为ARC-Hunyuan-Video的多模态模型，能够处理视频中的视觉、音频和文本信号，以实现对短视频的结构化理解。该模型具备多粒度的时间戳视频字幕生成、视频问答、时间视频定位和视频推理等功能。通过高质量的数据和多阶段的训练流程，该模型在真实世界视频理解方面表现出色，并在用户参与度和满意度上取得了显著提升。其高效的推理能力使得在H20 GPU上处理一段一分钟的视频仅需10秒。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21049",
            "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for\n  Multi-Task Learning",
            "url": "https://huggingface.co/papers/2507.21049",
            "abstract": "Rep-MTL optimizes multi-task learning by leveraging task saliency in shared representations to promote complementarity and reduce negative transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE.",
            "score": 29,
            "issue_id": 5059,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "c419c0691a4c6b57",
            "authors": [
                "Zedong Wang",
                "Siyuan Li",
                "Dan Xu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21049.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация многозадачного обучения через анализ значимости задач в общих представлениях",
                    "desc": "Rep-MTL - это новый подход к многозадачному обучению, который оптимизирует взаимодействие между задачами на уровне общих представлений. Метод использует анализ значимости задач для количественной оценки взаимодействий между оптимизацией конкретных задач и обучением общим представлениям. Rep-MTL применяет энтропийную пенализацию и выравнивание по образцам между задачами для снижения негативного переноса. Эксперименты на четырех сложных наборах данных показали эффективность Rep-MTL в балансировке обучения конкретным задачам и обмена информацией между ними."
                },
                "en": {
                    "title": "Enhancing Multi-Task Learning through Task Saliency",
                    "desc": "Rep-MTL is a novel approach to multi-task learning (MTL) that focuses on improving task interactions by utilizing task saliency in shared representations. Unlike traditional methods that primarily address conflicts through loss scaling and gradient adjustments, Rep-MTL emphasizes the importance of complementarity between tasks. It employs entropy-based penalization and cross-task alignment to reduce negative transfer while enhancing the learning of individual tasks. Experimental results on various benchmarks indicate that Rep-MTL not only improves performance but also maintains efficiency in training across multiple tasks."
                },
                "zh": {
                    "title": "Rep-MTL：优化多任务学习的互补性",
                    "desc": "Rep-MTL是一种优化多任务学习的方法，通过利用任务显著性来促进任务之间的互补性，减少负迁移。现有的多任务优化技术主要集中在通过损失缩放和梯度操作来解决任务冲突，但效果并不理想。本文提出的Rep-MTL利用共享表示空间中的任务交互信息，量化任务特定优化与共享表示学习之间的关系。通过熵惩罚和样本级跨任务对齐，Rep-MTL旨在有效训练各个任务，同时促进互补信息的共享。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20984",
            "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
            "url": "https://huggingface.co/papers/2507.20984",
            "abstract": "SmallThinker, designed for localdevices with limited resources, uses advanced architectural innovations to achieve high performance without requiring GPU hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
            "score": 26,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "1a02f43842725db8",
            "authors": [
                "Yixin Song",
                "Zhenliang Xue",
                "Dongliang Wei",
                "Feiyang Chen",
                "Jianxiang Gao",
                "Junchen Liu",
                "Hangyu Liang",
                "Guangshuo Qin",
                "Chengrong Tian",
                "Bo Wen",
                "Longyu Zhao",
                "Xinrui Zheng",
                "Zeyu Mi",
                "Haibo Chen"
            ],
            "affiliations": [
                "Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University",
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "Zenergize AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20984.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#low_resource",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "SmallThinker: Мощные языковые модели для слабых устройств",
                    "desc": "SmallThinker - это семейство языковых моделей, разработанных специально для локальных устройств с ограниченными ресурсами. Модель использует инновационную архитектуру, включающую двухуровневую разреженную структуру с комбинацией Mixture-of-Experts и разреженных полносвязных слоев. SmallThinker также применяет предварительный маршрутизатор для эффективной предзагрузки параметров и гибридный механизм разреженного внимания NoPE-RoPE для экономии памяти. Благодаря этим технологиям, модели SmallThinker достигают высокой производительности на обычных CPU без необходимости в GPU."
                },
                "en": {
                    "title": "Empowering Local Devices with Efficient LLMs",
                    "desc": "SmallThinker is a new family of large language models (LLMs) specifically designed for local devices with limited resources, such as low computational power and memory. Unlike traditional models that are adapted for cloud environments, SmallThinker is built from the ground up to operate efficiently within these constraints. It employs a two-level sparse structure that combines Mixture-of-Experts (MoE) with sparse feed-forward networks to minimize computational demands while maintaining model performance. Additionally, it features a pre-attention router to manage slow storage access and a hybrid sparse attention mechanism to reduce memory usage, allowing it to achieve high performance without the need for expensive GPU hardware."
                },
                "zh": {
                    "title": "小设备上的大语言模型革命",
                    "desc": "SmallThinker是一种专为资源有限的本地设备设计的大型语言模型（LLM），它通过先进的架构创新实现高性能，而无需依赖GPU硬件。与传统方法不同，SmallThinker从根本上考虑了本地设备的计算能力、内存和存储限制，采用了两级稀疏结构和混合专家机制，显著降低了计算需求。为了克服慢存储带来的I/O瓶颈，SmallThinker设计了预注意力路由器，使得在计算注意力的同时可以预取专家参数，从而隐藏存储延迟。最终，SmallThinker在普通消费者CPU上实现了超过20个token/s的速度，且内存消耗仅为1GB和8GB，展现了卓越的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21045",
            "title": "Reconstructing 4D Spatial Intelligence: A Survey",
            "url": "https://huggingface.co/papers/2507.21045",
            "abstract": "A survey organizes methods for reconstructing 4D spatial intelligence from visual observations into five progressive levels, offering analysis and identifying future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
            "score": 25,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "47424252dbce5de2",
            "authors": [
                "Yukang Cao",
                "Jiahao Lu",
                "Zhisheng Huang",
                "Zhuowei Shen",
                "Chengfeng Zhao",
                "Fangzhou Hong",
                "Zhaoxi Chen",
                "Xin Li",
                "Wenping Wang",
                "Yuan Liu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Intelligent Graphics Lab, The Hong Kong University of Science and Technology",
                "S-Lab, College of Computing and Data Science, Nanyang Technological University, Singapore 639798",
                "Texas A&M University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21045.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#3d",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "От пикселей к пониманию: путь к 4D пространственному интеллекту",
                    "desc": "Статья представляет обзор методов реконструкции 4D пространственного интеллекта из визуальных наблюдений, организованных в пять прогрессивных уровней. Авторы анализируют существующие подходы, начиная от реконструкции базовых 3D атрибутов и заканчивая моделированием взаимодействий и физических законов. Обзор охватывает широкий спектр применений - от развлечений до воплощенного ИИ. В заключение обсуждаются ключевые проблемы на каждом уровне и перспективные направления для дальнейших исследований."
                },
                "en": {
                    "title": "Progressing Through 4D Spatial Intelligence: A Structured Approach",
                    "desc": "This paper surveys methods for reconstructing 4D spatial intelligence from visual data, categorizing them into five progressive levels. The levels range from basic 3D attribute reconstruction to complex modeling of interactions and physical laws. It highlights the rapid advancements in deep learning and 3D representations that have propelled the field forward. The authors also identify key challenges and future research directions to enhance 4D scene reconstruction."
                },
                "zh": {
                    "title": "重建4D空间智能的五个层次",
                    "desc": "本论文对从视觉观察中重建4D空间智能的方法进行了调查，并将其组织为五个逐步发展的层次。研究涵盖了从低级3D属性重建到动态场景建模的各个方面，强调了在计算机视觉中的重要性和应用。随着3D表示和深度学习架构的快速发展，该领域迅速演变，超越了以往的研究范围。最后，论文讨论了每个层次的关键挑战，并指出了未来研究的有希望方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21046",
            "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
            "url": "https://huggingface.co/papers/2507.21046",
            "abstract": "This survey reviews architectures and methods for self-evolving agents in continual learning environments, examining different components, adaptation stages, and design considerations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.",
            "score": 21,
            "issue_id": 5062,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "b14049ba15e033a3",
            "authors": [
                "Huan-ang Gao",
                "Jiayi Geng",
                "Wenyue Hua",
                "Mengkang Hu",
                "Xinzhe Juan",
                "Hongzhang Liu",
                "Shilong Liu",
                "Jiahao Qiu",
                "Xuan Qi",
                "Yiran Wu",
                "Hongru Wang",
                "Han Xiao",
                "Yuhang Zhou",
                "Shaokun Zhang",
                "Jiayi Zhang",
                "Jinyu Xiang",
                "Yixiong Fang",
                "Qiwen Zhao",
                "Dongrui Liu",
                "Qihan Ren",
                "Cheng Qian",
                "Zhenghailong Wang",
                "Minda Hu",
                "Huazheng Wang",
                "Qingyun Wu",
                "Heng Ji",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Oregon State University",
                "Pennsylvania State University",
                "Princeton AI Lab",
                "Princeton University",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of California San Diego",
                "University of California, Santa Barbara",
                "University of Illinois Urbana-Champaign",
                "University of Michigan",
                "University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21046.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#healthcare",
                    "#benchmark",
                    "#training",
                    "#agi",
                    "#reasoning"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "От статичных моделей к самоэволюционирующим агентам: новая парадигма ИИ",
                    "desc": "Этот обзор рассматривает архитектуры и методы для самоэволюционирующих агентов в средах непрерывного обучения. Авторы анализируют различные компоненты, этапы адаптации и аспекты проектирования таких агентов. Особое внимание уделяется трем ключевым измерениям: что эволюционирует, когда происходит эволюция и как она осуществляется. Обзор также затрагивает вопросы оценки, применения и будущих направлений исследований в области самоэволюционирующих агентов."
                },
                "en": {
                    "title": "Empowering Agents: The Future of Self-Evolving Intelligence",
                    "desc": "This paper surveys the development of self-evolving agents that can learn continuously in dynamic environments. It highlights the limitations of current large language models (LLMs) which are static and cannot adapt to new tasks or contexts. The authors categorize the evolution of agents based on what, when, and how they adapt, examining various components and methods for continual learning. The survey also discusses evaluation metrics and applications in fields like coding and healthcare, aiming to guide future research towards creating more adaptive and intelligent systems."
                },
                "zh": {
                    "title": "自我进化代理：推动智能系统的未来",
                    "desc": "这篇综述文章探讨了自我进化代理在持续学习环境中的架构和方法。文章分析了不同的组成部分、适应阶段和设计考虑因素，强调了从静态模型向自我进化代理的转变。自我进化代理能够实时适应新任务和动态环境，解决了大型语言模型在开放式交互环境中的局限性。通过提供一个结构化的框架，文章为理解和设计自我进化代理奠定了基础，推动了自适应系统的研究和实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20673",
            "title": "Geometric-Mean Policy Optimization",
            "url": "https://huggingface.co/papers/2507.20673",
            "abstract": "Geometric-Mean Policy Optimization (GMPO) stabilizes policy updates in large language models by maximizing the geometric mean of token-level rewards, improving performance on mathematical and multimodal reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO.",
            "score": 15,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "44c96360ea6066e6",
            "authors": [
                "Yuzhong Zhao",
                "Yue Liu",
                "Junpeng Liu",
                "Jingye Chen",
                "Xun Wu",
                "Yaru Hao",
                "Tengchao Lv",
                "Shaohan Huang",
                "Lei Cui",
                "Qixiang Ye",
                "Fang Wan",
                "Furu Wei"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "Microsoft Research",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20673.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Стабильное обучение языковых моделей через геометрическое усреднение",
                    "desc": "Геометрическая оптимизация политики (GMPO) стабилизирует обновления политики в больших языковых моделях путем максимизации геометрического среднего токен-уровневых наград. Этот метод улучшает производительность на математических и мультимодальных тестах по сравнению с предыдущим подходом GRPO, который страдал от нестабильных обновлений при обработке токенов с выбросами в наградах. GMPO менее чувствителен к выбросам и поддерживает более стабильный диапазон коэффициентов важности выборки. Экспериментальные результаты показывают улучшение на 4.1% на математических тестах и на 1.4% на мультимодальных тестах рассуждений."
                },
                "en": {
                    "title": "Stabilizing Policy Updates with Geometric-Mean Optimization",
                    "desc": "Geometric-Mean Policy Optimization (GMPO) is a new method that enhances the stability of policy updates in large language models by focusing on the geometric mean of token-level rewards. This approach addresses the issues faced by previous methods like Group Relative Policy Optimization (GRPO), which can become unstable due to outlier rewards. By using the geometric mean, GMPO reduces sensitivity to extreme values, leading to more consistent training outcomes. Experimental results show that GMPO outperforms GRPO on various mathematical and multimodal reasoning tasks, demonstrating its effectiveness in improving model performance."
                },
                "zh": {
                    "title": "几何均值优化，稳定策略更新！",
                    "desc": "几何均值策略优化（GMPO）通过最大化令牌级奖励的几何均值来稳定大型语言模型的策略更新，从而提高在数学和多模态推理基准上的表现。与算术均值优化的组相对策略优化（GRPO）相比，GMPO对异常值的敏感性更低，能够保持更稳定的重要性采样比。本文提供了全面的理论和实验分析，以证明GMPO的设计和稳定性优势。GMPO-7B在多个数学基准上平均超越GRPO 4.1%，在多模态推理基准上超越1.4%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21033",
            "title": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
            "url": "https://huggingface.co/papers/2507.21033",
            "abstract": "Recent advancements in large multimodal models like GPT-4o have set a new standard for high-fidelity, instruction-guided image editing. However, the proprietary nature of these models and their training data creates a significant barrier for open-source research. To bridge this gap, we introduce GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus containing more than 1.5 million high-quality triplets (instruction, source image, edited image). We systematically construct this dataset by leveraging the versatile capabilities of GPT-4o to unify and refine three popular image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our methodology involves 1) regenerating output images to enhance visual quality and instruction alignment, and 2) selectively rewriting prompts to improve semantic clarity. To validate the efficacy of our dataset, we fine-tune advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are exciting, e.g., the fine-tuned FluxKontext achieves highly competitive performance across a comprehensive suite of benchmarks, including 7.24 on GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger instruction following and higher perceptual quality while maintaining identity. These scores markedly exceed all previously published open-source methods and substantially narrow the gap to leading proprietary models. We hope the full release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in instruction-guided image editing.",
            "score": 13,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "7ce17dfe6eea7e71",
            "authors": [
                "Yuhan Wang",
                "Siwei Yang",
                "Bingchen Zhao",
                "Letian Zhang",
                "Qing Liu",
                "Yuyin Zhou",
                "Cihang Xie"
            ],
            "affiliations": [
                "Adobe",
                "The University of Edinburgh",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21033.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Открытый датасет для прорыва в редактировании изображений искусственным интеллектом",
                    "desc": "Статья представляет GPT-IMAGE-EDIT-1.5M - публично доступный набор данных для редактирования изображений, содержащий более 1,5 миллиона высококачественных триплетов (инструкция, исходное изображение, отредактированное изображение). Набор данных создан путем объединения и улучшения трех популярных датасетов с использованием возможностей GPT-4o. Авторы провели эксперименты по дообучению открытых моделей на GPT-IMAGE-EDIT-1.5M, показав значительное улучшение производительности, например, модель FluxKontext достигла конкурентоспособных результатов на нескольких бенчмарках. Исследователи надеются, что публичный выпуск GPT-IMAGE-EDIT-1.5M будет способствовать дальнейшим открытым исследованиям в области редактирования изображений под управлением инструкций."
                },
                "en": {
                    "title": "Empowering Open-Source Image Editing with GPT-IMAGE-EDIT-1.5M",
                    "desc": "This paper presents GPT-IMAGE-EDIT-1.5M, a large-scale, open-source dataset designed for instruction-guided image editing. It consists of over 1.5 million triplets of instructions, source images, and edited images, created by enhancing existing datasets using the capabilities of GPT-4o. The authors demonstrate that fine-tuning open-source models on this dataset leads to significant improvements in performance, surpassing previous methods and approaching the quality of proprietary models. The goal is to promote further research in the field by providing accessible resources for the community."
                },
                "zh": {
                    "title": "推动开源图像编辑的新里程碑",
                    "desc": "最近大型多模态模型如GPT-4o在高保真、指令引导的图像编辑方面取得了显著进展。然而，这些模型的专有性质和训练数据对开源研究构成了重大障碍。为了解决这个问题，我们推出了GPT-IMAGE-EDIT-1.5M，这是一个公开可用的大规模图像编辑数据集，包含超过150万个高质量的三元组（指令、源图像、编辑图像）。我们通过利用GPT-4o的多功能能力，系统性地构建了这个数据集，以统一和优化三个流行的图像编辑数据集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20025",
            "title": "Region-based Cluster Discrimination for Visual Representation Learning",
            "url": "https://huggingface.co/papers/2507.20025",
            "abstract": "RICE enhances region-level visual and OCR capabilities through a novel Region Transformer and cluster discrimination loss, achieving superior performance across dense prediction and perception tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning visual representations is foundational for a broad spectrum of downstream tasks. Although recent vision-language contrastive models, such as CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale vision-language alignment, their reliance on global representations constrains their effectiveness for dense prediction tasks, such as grounding, OCR, and segmentation. To address this gap, we introduce Region-Aware Cluster Discrimination (RICE), a novel method that enhances region-level visual and OCR capabilities. We first construct a billion-scale candidate region dataset and propose a Region Transformer layer to extract rich regional semantics. We further design a unified region cluster discrimination loss that jointly supports object and OCR learning within a single classification framework, enabling efficient and scalable distributed training on large-scale data. Extensive experiments show that RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs). The pre-trained models have been released at https://github.com/deepglint/MVT.",
            "score": 12,
            "issue_id": 5058,
            "pub_date": "2025-07-26",
            "pub_date_card": {
                "ru": "26 июля",
                "en": "July 26",
                "zh": "7月26日"
            },
            "hash": "f356ee0be3cc35de",
            "authors": [
                "Yin Xie",
                "Kaicheng Yang",
                "Xiang An",
                "Kun Wu",
                "Yongle Zhao",
                "Weimo Deng",
                "Zimin Ran",
                "Yumeng Wang",
                "Ziyong Feng",
                "Roy Miles",
                "Ismail Elezi",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint",
                "Huawei London Research Center",
                "Imperial College London",
                "University of Technology Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20025.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#games",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "RICE: улучшение региональных возможностей для задач плотного предсказания",
                    "desc": "RICE - это новый метод, улучшающий возможности визуального восприятия и распознавания текста на уровне регионов изображения. Он использует новаторский Region Transformer и функцию потерь для кластерной дискриминации. RICE обучается на огромном наборе данных кандидатов-регионов и применяет единую функцию потерь для обучения распознаванию объектов и текста. Метод превосходит существующие подходы в задачах сегментации, плотного обнаружения и визуального восприятия для мультимодальных больших языковых моделей."
                },
                "en": {
                    "title": "RICE: Revolutionizing Region-Level Visual and OCR Learning",
                    "desc": "The paper introduces RICE, a method that improves visual and OCR tasks by focusing on specific regions in images rather than using global representations. It utilizes a Region Transformer to capture detailed regional information and a cluster discrimination loss to enhance learning for both object recognition and OCR in a unified framework. This approach allows for efficient training on large datasets, leading to better performance in tasks like segmentation and dense detection. RICE demonstrates superior results compared to existing models, making it a significant advancement in the field of visual representation learning."
                },
                "zh": {
                    "title": "RICE：提升区域视觉与OCR能力的创新方法",
                    "desc": "RICE是一种新颖的方法，通过区域变换器和聚类区分损失，增强了区域级的视觉和光学字符识别（OCR）能力。该方法构建了一个十亿规模的候选区域数据集，并提出了区域变换器层，以提取丰富的区域语义。RICE设计了一个统一的区域聚类区分损失，支持在单一分类框架内同时进行物体和OCR学习，从而实现高效的分布式训练。实验结果表明，RICE在分割、密集检测和多模态大语言模型的视觉感知等任务上，始终优于之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.19766",
            "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing\n  Large Language Models' Reasoning Abilities",
            "url": "https://huggingface.co/papers/2507.19766",
            "abstract": "A novel reinforcement learning approach for large language models addresses inefficiencies in handling ultra-long outputs, enhancing performance and training speed through segmentation and dynamic masking techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to 85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.",
            "score": 8,
            "issue_id": 5062,
            "pub_date": "2025-07-26",
            "pub_date_card": {
                "ru": "26 июля",
                "en": "July 26",
                "zh": "7月26日"
            },
            "hash": "08ac12a595460a4b",
            "authors": [
                "Dong Du",
                "Shulin Liu",
                "Tao Yang",
                "Shaohua Chen",
                "Yang Li"
            ],
            "affiliations": [
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.19766.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#long_context",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в обучении языковых моделей: эффективная работа со сверхдлинными текстами",
                    "desc": "Новый подход к обучению с подкреплением для больших языковых моделей решает проблемы неэффективности при работе с сверхдлинными выходными данными. Метод UloRL разделяет декодирование на короткие сегменты и использует динамическое маскирование токенов. Это позволяет ускорить обучение в 2,06 раза и значительно улучшить производительность модели на сложных задачах рассуждения. Эксперименты показывают, что подход эффективно повышает способности больших языковых моделей к генерации длинных последовательностей и решению сложных задач."
                },
                "en": {
                    "title": "Enhancing LLMs with Efficient Ultra-Long Output Learning",
                    "desc": "This paper introduces a new reinforcement learning method called Ultra-Long Output Reinforcement Learning (UloRL) designed to improve large language models (LLMs) when generating very long outputs. The approach tackles inefficiencies in traditional reinforcement learning by breaking down long output sequences into shorter segments, which speeds up training and enhances performance. Additionally, it employs dynamic masking of well-mastered positive tokens to prevent issues like entropy collapse during training. Experimental results show significant improvements in training speed and reasoning capabilities of LLMs, demonstrating the effectiveness of the proposed techniques."
                },
                "zh": {
                    "title": "提升大型语言模型推理能力的新方法",
                    "desc": "本文提出了一种新颖的强化学习方法，旨在提高大型语言模型在处理超长输出时的效率。通过将超长输出解码分割为短段，减少了长尾样本带来的延迟，从而加快了训练速度。我们还引入了动态掩蔽技术，以防止熵崩溃，进一步提升模型的推理能力。实验结果表明，该方法在多个任务上显著提高了模型的性能和训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17189",
            "title": "Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for\n  Complex Meteorological Systems",
            "url": "https://huggingface.co/papers/2507.17189",
            "abstract": "A proposed implicit two-stage training approach with separate encoders and decoders and self-attention for multivariable fusion improves weather prediction performance in end-to-end deep learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing frequency of extreme weather events due to global climate change urges accurate weather prediction. Recently, great advances have been made by the end-to-end methods, thanks to deep learning techniques, but they face limitations of representation inconsistency in multivariable integration and struggle to effectively capture the dependency between variables, which is required in complex weather systems. Treating different variables as distinct modalities and applying a two-stage training approach from multimodal models can partially alleviate this issue, but due to the inconformity in training tasks between the two stages, the results are often suboptimal. To address these challenges, we propose an implicit two-stage training method, configuring separate encoders and decoders for each variable. In detailed, in the first stage, the Translator is frozen while the Encoders and Decoders learn a shared latent space, in the second stage, the Encoders and Decoders are frozen, and the Translator captures inter-variable interactions for prediction. Besides, by introducing a self-attention mechanism for multivariable fusion in the latent space, the performance achieves further improvements. Empirically, extensive experiments show the state-of-the-art performance of our method. Specifically, it reduces the MSE for near-surface air temperature and relative humidity predictions by 28.82\\% and 23.39\\%, respectively. The source code is available at https://github.com/ShremG/Met2Net.",
            "score": 8,
            "issue_id": 5063,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "38ffbf9cc27bae27",
            "authors": [
                "Shaohan Li",
                "Hao Yang",
                "Min Chen",
                "Xiaolin Qin"
            ],
            "affiliations": [
                "Chengdu Institute of Computer Applications, Chinese Academy of Sciences",
                "Chengdu University of Information Technology",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17189.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#science",
                    "#optimization"
                ],
                "emoji": "🌦️",
                "ru": {
                    "title": "Улучшение прогнозов погоды с помощью двухэтапного обучения нейросетей",
                    "desc": "Предложен новый подход к обучению моделей глубокого обучения для прогнозирования погоды. Метод использует отдельные энкодеры и декодеры для разных переменных, а также механизм самовнимания для объединения мультивариативных данных. Двухэтапный процесс обучения позволяет эффективно захватывать зависимости между переменными. Экспериментальные результаты показывают значительное улучшение точности прогнозов температуры и влажности воздуха."
                },
                "en": {
                    "title": "Enhancing Weather Prediction with Two-Stage Deep Learning",
                    "desc": "This paper presents a novel implicit two-stage training approach for improving weather prediction using deep learning. It utilizes separate encoders and decoders for different weather variables, allowing for better representation and integration of multivariable data. The method incorporates a self-attention mechanism to enhance the fusion of these variables in a shared latent space. Experimental results demonstrate significant improvements in prediction accuracy, achieving state-of-the-art performance in forecasting near-surface air temperature and humidity."
                },
                "zh": {
                    "title": "隐式两阶段训练提升天气预测性能",
                    "desc": "本文提出了一种隐式的两阶段训练方法，采用独立的编码器和解码器，并引入自注意力机制来进行多变量融合，从而提高天气预测的性能。随着全球气候变化，极端天气事件频发，准确的天气预测变得尤为重要。传统的端到端深度学习方法在多变量集成中存在表示不一致的问题，难以有效捕捉变量之间的依赖关系。通过将不同变量视为独立模态，并采用两阶段训练方法，我们的研究在实验中显示出显著的性能提升，尤其在近地面气温和相对湿度的预测中，均方误差分别降低了28.82%和23.39%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.19804",
            "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
            "url": "https://huggingface.co/papers/2507.19804",
            "abstract": "A Foreground-Centric Network for document image rectification improves state-of-the-art by effectively handling foreground elements and layout distortions.  \t\t\t\t\tAI-generated summary \t\t\t\t Document image rectification aims to eliminate geometric deformation in photographed documents to facilitate text recognition. However, existing methods often neglect the significance of foreground elements, which provide essential geometric references and layout information for document image correction. In this paper, we introduce Foreground-Centric Network (ForCenNet) to eliminate geometric distortions in document images. Specifically, we initially propose a foreground-centric label generation method, which extracts detailed foreground elements from an undistorted image. Then we introduce a foreground-centric mask mechanism to enhance the distinction between readable and background regions. Furthermore, we design a curvature consistency loss to leverage the detailed foreground labels to help the model understand the distorted geometric distribution. Extensive experiments demonstrate that ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the proposed method effectively undistorts layout elements, such as text lines and table borders. The resources for further comparison are provided at https://github.com/caipeng328/ForCenNet.",
            "score": 7,
            "issue_id": 5058,
            "pub_date": "2025-07-26",
            "pub_date_card": {
                "ru": "26 июля",
                "en": "July 26",
                "zh": "7月26日"
            },
            "hash": "171763e95f15cfb1",
            "authors": [
                "Peng Cai",
                "Qiang Li",
                "Kaicheng Yang",
                "Dong Guo",
                "Jia Li",
                "Nan Zhou",
                "Xiang An",
                "Ninghua Yang",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint",
                "Imperial College London",
                "Qihoo Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.19804.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Исправление искажений в документах с фокусом на важных элементах",
                    "desc": "ForCenNet - это новая нейронная сеть для устранения геометрических искажений в отсканированных документах. Она фокусируется на элементах переднего плана, таких как текст и границы таблиц, чтобы лучше понять и исправить искажения. Сеть использует специальный метод генерации меток и механизм маскирования для различения читаемых областей и фона. ForCenNet достигает наилучших результатов на нескольких наборах данных реальных документов."
                },
                "en": {
                    "title": "Enhancing Document Rectification with Foreground Focus",
                    "desc": "This paper presents the Foreground-Centric Network (ForCenNet), which focuses on improving document image rectification by addressing the importance of foreground elements. Traditional methods often overlook these elements, which are crucial for understanding the layout and geometry of documents. ForCenNet introduces a novel label generation method to extract detailed foreground features and a mask mechanism to differentiate between text and background. The model also employs a curvature consistency loss to better capture geometric distortions, achieving state-of-the-art results on multiple benchmarks."
                },
                "zh": {
                    "title": "以前景为中心的文档图像矫正新方法",
                    "desc": "本文提出了一种以前景为中心的网络（ForCenNet），用于文档图像的矫正，旨在消除拍摄文档中的几何变形，以提高文本识别的准确性。现有方法往往忽视前景元素的重要性，而这些元素为文档图像的校正提供了必要的几何参考和布局信息。我们提出了一种前景中心标签生成方法，提取未失真图像中的详细前景元素，并引入前景中心掩膜机制，以增强可读区域与背景区域之间的区分。通过大量实验，ForCenNet在多个真实世界基准测试中实现了新的最先进水平，有效地纠正了文本行和表格边框等布局元素的失真。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.19058",
            "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with\n  Concept Relation Alignment",
            "url": "https://huggingface.co/papers/2507.19058",
            "abstract": "ScenePainter framework uses a hierarchical graph structure to ensure semantically consistent 3D scene generation by addressing the semantic drift problem in successive view expansion.  \t\t\t\t\tAI-generated summary \t\t\t\t Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.",
            "score": 6,
            "issue_id": 5062,
            "pub_date": "2025-07-25",
            "pub_date_card": {
                "ru": "25 июля",
                "en": "July 25",
                "zh": "7月25日"
            },
            "hash": "81138769af5fca87",
            "authors": [
                "Chong Xia",
                "Shengjun Zhang",
                "Fangfu Liu",
                "Chang Liu",
                "Khodchaphun Hirunyaratsameewong",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.19058.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#graphs"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Семантически согласованная генерация 3D-сцен с помощью концептуальных графов",
                    "desc": "ScenePainter - это новая система для семантически согласованной генерации 3D-сцен. Она использует иерархическую графовую структуру SceneConceptGraph для построения связей между концепциями сцены на разных уровнях. Это позволяет направлять модуль расширения изображения для создания согласованных новых ракурсов и динамически уточнять граф для повышения разнообразия. ScenePainter решает проблему семантического дрейфа, возникающую при последовательном расширении ракурсов в существующих методах."
                },
                "en": {
                    "title": "Consistent 3D Scene Generation with ScenePainter",
                    "desc": "The ScenePainter framework addresses the challenge of generating coherent 3D scenes by using a hierarchical graph structure to maintain semantic consistency. It specifically targets the semantic drift problem that occurs during successive view expansion in 3D scene generation. By aligning the outpainter's scene-specific prior with the current scene understanding, ScenePainter ensures that new views are generated in a consistent manner. The introduction of the SceneConceptGraph allows for the construction of relationships among various scene concepts, enhancing the diversity and quality of the generated 3D view sequences."
                },
                "zh": {
                    "title": "ScenePainter：解决3D场景生成中的语义漂移问题",
                    "desc": "ScenePainter框架使用层次图结构来确保3D场景生成的语义一致性，解决了连续视图扩展中的语义漂移问题。该框架旨在生成长时间且连贯的3D视图序列，适用于长期视频合成和3D场景重建。通过引入名为SceneConceptGraph的层次图结构，构建多层次场景概念之间的关系，从而指导生成器生成一致的新视图。实验表明，ScenePainter有效克服了语义漂移问题，生成了更一致和沉浸的3D视图序列。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20900",
            "title": "Music Arena: Live Evaluation for Text-to-Music",
            "url": "https://huggingface.co/papers/2507.20900",
            "abstract": "Music Arena provides a scalable, interactive platform for evaluating text-to-music models through user-generated preferences and detailed feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.   Music Arena is available at: https://music-arena.org",
            "score": 5,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "53def6d52ac1ffb0",
            "authors": [
                "Yonghyun Kim",
                "Wayne Chi",
                "Anastasios N. Angelopoulos",
                "Wei-Lin Chiang",
                "Koichi Saito",
                "Shinji Watanabe",
                "Yuki Mitsufuji",
                "Chris Donahue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Georgia Tech",
                "LMArena",
                "Sony AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20900.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Интерактивная арена для оценки музыкального ИИ",
                    "desc": "Music Arena - это открытая платформа для масштабируемой оценки моделей преобразования текста в музыку на основе предпочтений пользователей. Она позволяет пользователям вводить текстовые запросы и сравнивать результаты работы двух систем text-to-music, собирая детальные данные о предпочтениях. Платформа использует маршрутизацию на основе языковой модели для работы с разнородными форматами вывода различных систем. Music Arena предоставляет регулярно обновляемые наборы данных о предпочтениях, соблюдая при этом конфиденциальность пользователей."
                },
                "en": {
                    "title": "Empowering Music Creation through User-Driven Evaluation",
                    "desc": "Music Arena is an interactive platform designed to evaluate text-to-music (TTM) models by gathering user preferences and feedback. It addresses the challenges of traditional human preference studies, which are often costly and inconsistent. By allowing users to compare outputs from different TTM systems based on their own text prompts, it creates a leaderboard that reflects real-world preferences. Additionally, Music Arena incorporates features like an LLM-based routing system and detailed feedback collection to enhance the evaluation process while ensuring user privacy."
                },
                "zh": {
                    "title": "音乐评估新平台：Music Arena",
                    "desc": "Music Arena 是一个可扩展的互动平台，用于评估文本到音乐（TTM）模型的用户偏好和详细反馈。该平台允许用户输入文本提示，并比较两个 TTM 系统的输出，从而收集用户的偏好数据。通过标准化的评估协议和透明的数据访问政策，Music Arena 解决了 TTM 生态系统中的关键挑战，并提供了一个可再生的偏好数据源。该平台还设计了专门针对音乐的功能，以提高评估的有效性和用户体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20880",
            "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability\n  and Aesthetic Alignment",
            "url": "https://huggingface.co/papers/2507.20880",
            "abstract": "A flow-matching-based model enhances lyrics-to-song generation by providing word-level control over vocal timing and duration, improving quality through aesthetic alignment and surpassing current models in music-specific attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes.",
            "score": 3,
            "issue_id": 5058,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "850946325eebad11",
            "authors": [
                "Renhang Liu",
                "Chia-Yu Hung",
                "Navonil Majumder",
                "Taylor Gautreaux",
                "Amir Ali Bagherzadeh",
                "Chuan Li",
                "Dorien Herremans",
                "Soujanya Poria"
            ],
            "affiliations": [
                "Lambda Labs",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20880.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#open_source",
                    "#diffusion",
                    "#dataset",
                    "#training",
                    "#data",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Точный контроль вокала в ИИ-генерации песен",
                    "desc": "Статья представляет новую модель генерации песен на основе текста лирики, использующую метод согласования потоков (flow matching). Модель обеспечивает контроль над временем и длительностью произношения слов на уровне отдельных слов, что улучшает качество генерации. Для повышения качества генерируемых песен применяется эстетическое выравнивание с помощью прямой оптимизации предпочтений (Direct Preference Optimization). Авторы утверждают, что их модель превосходит существующие аналоги по музыкальным характеристикам."
                },
                "en": {
                    "title": "JAM: Fine-Grained Control for Better Song Generation",
                    "desc": "This paper presents a new model called JAM that improves the generation of songs from lyrics by allowing precise control over the timing and duration of words. By using flow-matching techniques, JAM enhances the quality of the generated music, making it more aesthetically pleasing and aligned with human preferences. The model employs Direct Preference Optimization to refine its outputs without needing manual data annotations, streamlining the creative process for musicians. Additionally, the authors introduce a public evaluation dataset, JAME, to standardize assessments of lyrics-to-song models, demonstrating that JAM surpasses existing models in music-specific qualities."
                },
                "zh": {
                    "title": "流匹配模型提升歌词生成质量",
                    "desc": "这篇论文介绍了一种基于流匹配的模型，旨在改善歌词到歌曲的生成过程。该模型提供了对词汇时机和持续时间的控制，使得生成的歌曲在质量上更符合人类的审美偏好。通过直接偏好优化，模型能够在没有人工标注的情况下，迭代地提升生成歌曲的质量。此外，研究还推出了公共评估数据集JAME，以标准化歌词到歌曲模型的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16806",
            "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
            "url": "https://huggingface.co/papers/2507.16806",
            "abstract": "Reward augmented reinforcement learning improves both accuracy and confidence calibration of language models across in-domain and out-of-domain evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models.",
            "score": 3,
            "issue_id": 5063,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "a7c255689a6eaeb1",
            "authors": [
                "Mehul Damani",
                "Isha Puri",
                "Stewart Slocum",
                "Idan Shenfeld",
                "Leshem Choshen",
                "Yoon Kim",
                "Jacob Andreas"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16806.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#hallucinations",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Калибровка уверенности языковых моделей через обучение с подкреплением",
                    "desc": "Статья представляет метод RLCR (Reinforcement Learning with Calibration Rewards) для обучения языковых моделей. RLCR использует функцию вознаграждения, которая сочетает оценку правильности ответа с оценкой Бриера для калибровки уверенности модели. Эксперименты показывают, что RLCR значительно улучшает калибровку без потери точности как на внутридоменных, так и на внедоменных оценках. Метод превосходит обычное обучение с подкреплением и классификаторы, обученные присваивать оценки уверенности постфактум."
                },
                "en": {
                    "title": "Boosting Accuracy and Confidence in Language Models with RLCR",
                    "desc": "This paper introduces RLCR (Reinforcement Learning with Calibration Rewards), a novel approach that enhances the performance of language models (LMs) in reasoning tasks. By incorporating a Brier score into the reward function, RLCR not only focuses on the accuracy of predictions but also ensures that the confidence estimates of these predictions are well-calibrated. The study demonstrates that this method significantly improves the calibration of confidence without sacrificing accuracy, outperforming traditional reinforcement learning methods. Ultimately, the findings suggest that optimizing for calibration leads to more reliable and effective reasoning models in various evaluation scenarios."
                },
                "zh": {
                    "title": "优化校准，提升语言模型的准确性与信心",
                    "desc": "本论文提出了一种新的强化学习方法，称为RLCR（带有校准奖励的强化学习），旨在提高语言模型的准确性和置信度校准。通过在生成推理链时同时输出预测和置信度估计，RLCR优化了一种结合了二元正确性评分和Brier评分的奖励函数。研究表明，RLCR在多个数据集上显著改善了模型的校准性，同时保持了准确性，超越了传统的强化学习训练方法。最终，论文还展示了如何在测试时利用口头化的置信度来进一步提升模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21035",
            "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
            "url": "https://huggingface.co/papers/2507.21035",
            "abstract": "A system using LLM-based agents enhances gene expression analysis by integrating workflow reliability and autonomous adaptability to improve preprocessing and identification accuracy while uncovering biologically meaningful associations.  \t\t\t\t\tAI-generated summary \t\t\t\t Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.   On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F_1 of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
            "score": 1,
            "issue_id": 5062,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "d694c1f330b0cab3",
            "authors": [
                "Haoyang Liu",
                "Yijiang Li",
                "Haohan Wang"
            ],
            "affiliations": [
                "University of California, San Diego",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21035.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#science",
                    "#healthcare",
                    "#benchmark"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "GenoMAS: Интеллектуальный анализ генов с помощью команды ИИ-ученых",
                    "desc": "GenoMAS представляет собой систему на основе LLM-агентов для улучшения анализа экспрессии генов. Она сочетает надежность структурированных рабочих процессов с адаптивностью автономных агентов. Система использует шесть специализированных LLM-агентов, работающих через протоколы обмена типизированными сообщениями. GenoMAS превосходит существующие методы в предобработке данных и идентификации генов, а также выявляет биологически значимые ассоциации."
                },
                "en": {
                    "title": "GenoMAS: Merging Reliability and Adaptability in Gene Expression Analysis",
                    "desc": "This paper presents GenoMAS, a system that utilizes large language model (LLM)-based agents to enhance gene expression analysis. It combines the reliability of structured workflows with the flexibility of autonomous agents, allowing for improved preprocessing and identification accuracy of genomic data. The system employs a guided-planning framework where agents collaborate through message-passing protocols, adapting to the complexities of the data while maintaining logical coherence. GenoMAS achieves significant performance improvements on the GenoTEX benchmark, demonstrating its ability to uncover biologically meaningful associations in gene-phenotype relationships."
                },
                "zh": {
                    "title": "GenoMAS：基于LLM的基因表达分析新方法",
                    "desc": "本论文介绍了一种名为GenoMAS的系统，该系统利用基于大语言模型（LLM）的代理来增强基因表达分析。GenoMAS结合了结构化工作流程的可靠性和自主代理的适应性，从而提高了数据预处理和基因识别的准确性。通过六个专业的LLM代理，GenoMAS能够在分析过程中灵活应对基因组数据的复杂性。实验结果显示，GenoMAS在数据预处理和基因识别方面的性能均超过了之前的最佳成果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20187",
            "title": "Diversity-Enhanced Reasoning for Subjective Questions",
            "url": "https://huggingface.co/papers/2507.20187",
            "abstract": "A diversity-enhanced framework with multiple role perspectives improves accuracy and diversity in subjective reasoning tasks through unsupervised data construction and reinforcement learning with reward shaping.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs.",
            "score": 1,
            "issue_id": 5068,
            "pub_date": "2025-07-27",
            "pub_date_card": {
                "ru": "27 июля",
                "en": "July 27",
                "zh": "7月27日"
            },
            "hash": "29b178594cfc4a66",
            "authors": [
                "Yumeng Wang",
                "Zhiyuan Fan",
                "Jiayu Liu",
                "Yi R. Fung"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.20187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#data",
                    "#rlhf",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Разнообразие перспектив - ключ к улучшению рассуждений ИИ",
                    "desc": "В статье представлен фреймворк MultiRole-R1, улучшающий точность и разнообразие в задачах субъективных рассуждений с помощью множественных ролевых перспектив. Метод включает конвейер неконтролируемого построения данных, генерирующий цепочки рассуждений с разнообразными точками зрения. Применяется обучение с подкреплением с использованием Group Relative Policy Optimization и формированием вознаграждения, учитывающего разнообразие. Эксперименты на шести бенчмарках показали эффективность MultiRole-R1 в улучшении как субъективных, так и объективных рассуждений."
                },
                "en": {
                    "title": "Enhancing Reasoning with Diverse Perspectives",
                    "desc": "This paper introduces MultiRole-R1, a framework designed to enhance the performance of large reasoning models (LRMs) on subjective reasoning tasks. It addresses the limitations of traditional supervised fine-tuning by incorporating multiple role perspectives, which helps to generate diverse reasoning chains. The framework utilizes unsupervised data construction and reinforcement learning with a focus on diversity as a reward signal, promoting both perspective and lexical diversity. Experimental results show that MultiRole-R1 significantly improves accuracy and diversity in reasoning tasks, highlighting the benefits of diversity-enhanced training in machine learning models."
                },
                "zh": {
                    "title": "多样性增强，提升推理准确性与多样性",
                    "desc": "本文提出了一种名为MultiRole-R1的框架，旨在通过多角色视角来提高主观推理任务的准确性和多样性。该框架利用无监督数据构建生成包含多样化角色视角的推理链，并通过强化学习中的奖励塑造来优化模型。研究发现，增加角色视角能够有效提升推理的多样性和准确性。实验结果表明，MultiRole-R1在六个基准测试中表现出色，证明了多样性增强训练在大型推理模型中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20527",
            "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful\n  Mathematics Questions and Answers",
            "url": "https://huggingface.co/papers/2507.20527",
            "abstract": "A pipeline called SAND-Math generates and complexifies synthetic mathematics problems, enhancing the performance of large language models in mathematical reasoning tasks beyond existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce SAND-Math (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new Difficulty Hiking step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by uparrow 17.85 absolute points on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: https://huggingface.co/datasets/amd/SAND-MATH{https://huggingface.co/datasets/amd/SAND-MATH}",
            "score": 0,
            "issue_id": 5071,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "7191a2ca864f2e6c",
            "authors": [
                "Chaitanya Manem",
                "Pratik Prabhanjan Brahma",
                "Prakamya Mishra",
                "Zicheng Liu",
                "Emad Barsoum"
            ],
            "affiliations": [
                "Advanced Micro Devices, Inc. (AMD)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20527.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#reasoning",
                    "#dataset",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "SAND-Math: Синтетические задачи для улучшения математических способностей ИИ",
                    "desc": "SAND-Math - это конвейер для генерации и усложнения синтетических математических задач. Он направлен на улучшение производительности больших языковых моделей (LLM) в задачах математического рассуждения. SAND-Math включает в себя создание качественных задач с нуля и их систематическое усложнение с помощью нового шага 'Difficulty Hiking'. Исследование показало, что дополнение базовой модели данными SAND-Math значительно повышает ее эффективность, превосходя следующий лучший синтетический набор данных на 17.85 абсолютных пунктов в тесте AIME25."
                },
                "en": {
                    "title": "Boosting Mathematical Reasoning with SAND-Math",
                    "desc": "The paper presents SAND-Math, a novel pipeline designed to generate and increase the complexity of synthetic mathematics problems. This approach addresses the challenge of limited training data for Large Language Models (LLMs) in mathematical reasoning tasks. By creating high-quality problems and applying a Difficulty Hiking method, SAND-Math significantly enhances the performance of LLMs on benchmarks like AIME25. The results show that using SAND-Math data can improve model accuracy by over 17 points, demonstrating its effectiveness in training more capable mathematical reasoning models."
                },
                "zh": {
                    "title": "SAND-Math：提升数学推理能力的创新工具",
                    "desc": "SAND-Math是一个生成和复杂化合成数学问题的管道，旨在提升大型语言模型在数学推理任务中的表现。该方法通过从零开始生成高质量的问题，并通过新的难度提升步骤系统地增加其复杂性。研究表明，使用SAND-Math数据增强的基线模型在AIME25基准测试中表现显著提升，超出下一个最佳合成数据集17.85个百分点。此外，难度提升过程有效地将平均问题难度从5.02提高到5.98，进一步提升了模型的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-28.html",
    "link_next": "2025-07-30.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "28.07",
        "en": "07/28",
        "zh": "7月28日"
    },
    "short_date_next": {
        "ru": "30.07",
        "en": "07/30",
        "zh": "7月30日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 4,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 2,
        "#training": 13,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}