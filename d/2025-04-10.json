{
    "date": {
        "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 10",
        "zh": "4æœˆ10æ—¥"
    },
    "time_utc": "2025-04-10 08:14",
    "weekday": 3,
    "issue_id": 3165,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05741",
            "title": "DDT: Decoupled Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.05741",
            "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
            "score": 29,
            "issue_id": 3159,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "2f4cd9583b2418f3",
            "authors": [
                "Shuai Wang",
                "Zhi Tian",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed Vision",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DDT: Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Decoupled Diffusion Transformer (DDT). DDT Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DDT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "Decoupling for Faster and Better Image Generation",
                    "desc": "This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance."
                },
                "zh": {
                    "title": "è§£è€¦æ‰©æ•£å˜æ¢å™¨ï¼šæå‡ç”Ÿæˆè´¨é‡ä¸æ¨ç†é€Ÿåº¦çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒè¿­ä»£æ—¶é—´è¾ƒé•¿ä¸”æ¨ç†æ­¥éª¤è¾ƒå¤šã€‚æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæ‰©æ•£å˜æ¢å™¨å¯¹å™ªå£°è¾“å…¥è¿›è¡Œç¼–ç ï¼Œä»¥æå–ä½é¢‘è¯­ä¹‰æˆåˆ†ï¼Œç„¶åç”¨ç›¸åŒçš„æ¨¡å—è§£ç é«˜é¢‘æˆåˆ†ã€‚è¿™ç§æ–¹æ¡ˆå¯¼è‡´äº†ä¸€ä¸ªå›ºæœ‰çš„ä¼˜åŒ–å›°å¢ƒï¼šç¼–ç ä½é¢‘è¯­ä¹‰éœ€è¦å‡å°‘é«˜é¢‘æˆåˆ†ï¼Œä»è€Œåœ¨è¯­ä¹‰ç¼–ç å’Œé«˜é¢‘è§£ç ä¹‹é—´äº§ç”Ÿç´§å¼ å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£è€¦æ‰©æ•£å˜æ¢å™¨ï¼ˆDDTï¼‰ï¼Œå®ƒé‡‡ç”¨ä¸“é—¨çš„æ¡ä»¶ç¼–ç å™¨è¿›è¡Œè¯­ä¹‰æå–ï¼Œå¹¶é…å¤‡ä¸“é—¨çš„é€Ÿåº¦è§£ç å™¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07083",
            "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
            "url": "https://huggingface.co/papers/2504.07083",
            "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
            "score": 17,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "67e58f651d865bad",
            "authors": [
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Ziwei Liu",
                "Gordon Wetzstein",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07083.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ˜Ğ˜-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ĞºĞ¸Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DataDoP, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 29 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GenDoP Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GenDoP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Camera Movement with GenDoP",
                    "desc": "This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography."
                },
                "zh": {
                    "title": "åˆ›æ–°ç›¸æœºè½¨è¿¹ç”Ÿæˆï¼Œæå‡è§†è§‰å™äº‹æ•ˆæœ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç›¸æœºè½¨è¿¹ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†é¢‘åˆ¶ä½œä¸­çš„è§†è§‰å™äº‹æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGenDoPçš„è‡ªå›å½’æ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ•°æ®é›†DataDoPï¼ŒåŒ…å«29000ä¸ªçœŸå®é•œå¤´åŠå…¶ç›¸æœºè½¨è¿¹ã€æ·±åº¦å›¾å’Œè¯¦ç»†æè¿°ã€‚é€šè¿‡ç»“åˆæ–‡æœ¬æŒ‡å¯¼å’ŒRGBDè¾“å…¥ï¼ŒGenDoPèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç›¸æœºè¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenDoPåœ¨å¯æ§æ€§ã€è½¨è¿¹è°ƒæ•´ç²¾ç»†åº¦å’Œè¿åŠ¨ç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07096",
            "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
            "url": "https://huggingface.co/papers/2504.07096",
            "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
            "score": 9,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "68eafe783b5816f6",
            "authors": [
                "Jiacheng Liu",
                "Taylor Blanton",
                "Yanai Elazar",
                "Sewon Min",
                "YenSung Chen",
                "Arnavi Chheda-Kothary",
                "Huy Tran",
                "Byron Bischoff",
                "Eric Marsh",
                "Michael Schmitz",
                "Cassidy Trier",
                "Aaron Sarnat",
                "Jenna James",
                "Jon Borchardt",
                "Bailey Kuehl",
                "Evie Cheng",
                "Karen Farley",
                "Sruthi Sreeram",
                "Taira Anderson",
                "David Albright",
                "Carissa Schoenick",
                "Luca Soldaini",
                "Dirk Groeneveld",
                "Rock Yuren Pang",
                "Pang Wei Koh",
                "Noah A. Smith",
                "Sophie Lebrecht",
                "Yejin Choi",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Jesse Dodge"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Stanford University",
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07096.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#inference",
                    "#open_source",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "OLMoTrace - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². OLMoTrace Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° infini-gram Ğ¸ Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞºÑƒĞ½Ğ´. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Trace the Truth: Unveiling Language Model Outputs with OLMoTrace",
                    "desc": "OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output."
                },
                "zh": {
                    "title": "å®æ—¶è¿½è¸ªè¯­è¨€æ¨¡å‹è¾“å‡ºçš„é©å‘½æ€§å·¥å…·",
                    "desc": "OLMoTraceæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå®æ—¶è¿½è¸ªè¯­è¨€æ¨¡å‹è¾“å‡ºä¸å…¶è®­ç»ƒæ•°æ®ä¹‹é—´å…³ç³»çš„ç³»ç»Ÿã€‚å®ƒå¯ä»¥æ‰¾åˆ°è¯­è¨€æ¨¡å‹è¾“å‡ºç‰‡æ®µä¸è®­ç»ƒæ–‡æœ¬åº“ä¸­æ–‡æ¡£çš„é€å­—åŒ¹é…ã€‚è¯¥ç³»ç»ŸåŸºäºæ‰©å±•ç‰ˆçš„infini-gramæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…è¿”å›è¿½è¸ªç»“æœã€‚OLMoTraceå¸®åŠ©ç”¨æˆ·é€šè¿‡è®­ç»ƒæ•°æ®ç†è§£è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œé€‚ç”¨äºäº‹å®æ£€æŸ¥ã€å¹»è§‰å’Œè¯­è¨€æ¨¡å‹çš„åˆ›é€ åŠ›æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06514",
            "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
            "url": "https://huggingface.co/papers/2504.06514",
            "abstract": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.",
            "score": 8,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "6ca6e88a5650dba9",
            "authors": [
                "Chenrui Fan",
                "Ming Li",
                "Lichao Sun",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06514.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (overthinking) Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Tackling MiP-Overthinking in Reasoning LLMs",
                    "desc": "This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue."
                },
                "zh": {
                    "title": "æ­ç¤ºæ¨ç†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒé—®é¢˜",
                    "desc": "æˆ‘ä»¬å‘ç°ï¼Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹ç¼ºä¹å‰æçš„æ¨¡ç³Šé—®é¢˜æ—¶ï¼Œå“åº”é•¿åº¦æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´å†—ä½™å’Œæ— æ•ˆçš„æ€è€ƒã€‚è¿™ç§æ–°å¼•å…¥çš„åœºæ™¯åŠ å‰§äº†æ™®éçš„è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºMiP-è¿‡åº¦æ€è€ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„æ¨ç†LLMsè®­ç»ƒæ–¹æ³•æœªèƒ½æœ‰æ•ˆé¼“åŠ±é«˜æ•ˆæ€è€ƒï¼Œå¯¼è‡´æ€ç»´æ¨¡å¼çš„æ»¥ç”¨ã€‚ç›¸åï¼Œæœªä¸“é—¨è®­ç»ƒç”¨äºæ¨ç†çš„LLMsåœ¨å¤„ç†æ¨¡ç³Šé—®é¢˜æ—¶è¡¨ç°æ›´å¥½ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«é—®é¢˜å¹¶ç»™å‡ºæ›´ç®€çŸ­çš„å›ç­”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04842",
            "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
            "url": "https://huggingface.co/papers/2504.04842",
            "abstract": "Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.",
            "score": 5,
            "issue_id": 3160,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "5b592626aeda4ec8",
            "authors": [
                "Mengchao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Yaqi Fan",
                "Yunpeng Zhang",
                "Yonggang Qi",
                "Kun Zhao",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04842.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Realistic Talking Avatars: Synchronizing Motion and Expression",
                    "desc": "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."
                },
                "zh": {
                    "title": "ç”Ÿæˆå¯æ§åŠ¨ç”»å¤´åƒçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸€é™æ€è‚–åƒç”Ÿæˆé€¼çœŸçš„å¯åŠ¨ç”»å¤´åƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€è¿è´¯çš„å¯¹è¯è‚–åƒï¼Œå¹¶å…·å¤‡å¯æ§çš„è¿åŠ¨åŠ¨æ€ã€‚æ ¸å¿ƒæ˜¯åŒé˜¶æ®µçš„éŸ³è§†é¢‘å¯¹é½ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡éŸ³é¢‘é©±åŠ¨çš„åŠ¨æ€å¯¹é½å…¨åœºæ™¯ï¼Œç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨å”‡éƒ¨è¿½è¸ªæ©æ¨¡ç²¾ç»†è°ƒæ•´å”‡éƒ¨åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®æ„Ÿã€ä¸€è‡´æ€§ã€è¿åŠ¨å¼ºåº¦å’Œèº«ä»½ä¿ç•™æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07086",
            "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
            "url": "https://huggingface.co/papers/2504.07086",
            "abstract": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.",
            "score": 4,
            "issue_id": 3163,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "b06c700fb29c005d",
            "authors": [
                "Andreas Hochlehnert",
                "Hardik Bhatnagar",
                "Vishaal Udandarao",
                "Samuel Albanie",
                "Ameya Prabhu",
                "Matthias Bethge"
            ],
            "affiliations": [
                "TÃ¼bingen AI Center, University of TÃ¼bingen",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07086.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞŸĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ° Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº supervised fine-tuning Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Standardizing Reasoning Evaluations for Language Models",
                    "desc": "This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks."
                },
                "zh": {
                    "title": "æ¨ç†èƒ½åŠ›è¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•ï¼ŒæŒ‡å‡ºå½“å‰çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹å®ç°ç»†èŠ‚éå¸¸æ•æ„Ÿã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šè¯„ä¼°æ–¹æ³•ç¼ºä¹é€æ˜æ€§å’Œç»Ÿè®¡åŸºç¡€ï¼Œå¯¼è‡´æ€§èƒ½æå‡çš„æ¯”è¾ƒä¸å¤Ÿæ¸…æ™°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ˜ç¡®äº†æœ€ä½³å®è·µå’ŒæŠ¥å‘Šæ ‡å‡†ï¼Œå¹¶é‡æ–°è¯„ä¼°äº†ç°æœ‰æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ”¹è¿›æœ‰é™ï¼Œè€Œç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æ›´å¼ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07089",
            "title": "OmniCaptioner: One Captioner to Rule Them All",
            "url": "https://huggingface.co/papers/2504.07089",
            "abstract": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.",
            "score": 3,
            "issue_id": 3164,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "9b09a9ebdec71131",
            "authors": [
                "Yiting Lu",
                "Jiakang Yuan",
                "Zhen Li",
                "Shitian Zhao",
                "Qi Qin",
                "Xinyue Li",
                "Le Zhuo",
                "Licheng Wen",
                "Dongyang Liu",
                "Yuewen Cao",
                "Xiangchao Yan",
                "Xin Li",
                "Botian Shi",
                "Tao Chen",
                "Zhibo Chen",
                "Lei Bai",
                "Bo Zhang",
                "Peng Gao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07089.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#long_context",
                    "#cv",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "OmniCaptioner - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. OmniCaptioner Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging Visuals and Text with OmniCaptioner",
                    "desc": "OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data."
                },
                "zh": {
                    "title": "OmniCaptionerï¼šè§†è§‰ä¸æ–‡æœ¬çš„æ¡¥æ¢",
                    "desc": "æˆ‘ä»¬æå‡ºäº†OmniCaptionerï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†è§‰æè¿°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§è§†è§‰é¢†åŸŸç”Ÿæˆç»†è‡´çš„æ–‡æœ¬æè¿°ã€‚ä¸ä¹‹å‰ä»…é™äºç‰¹å®šå›¾åƒç±»å‹çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¯¹è‡ªç„¶å›¾åƒã€è§†è§‰æ–‡æœ¬ï¼ˆå¦‚æµ·æŠ¥ã€ç”¨æˆ·ç•Œé¢ã€æ•™ç§‘ä¹¦ï¼‰å’Œç»“æ„åŒ–è§†è§‰ï¼ˆå¦‚æ–‡æ¡£ã€è¡¨æ ¼ã€å›¾è¡¨ï¼‰è¿›è¡Œæè¿°ã€‚é€šè¿‡å°†ä½çº§åƒç´ ä¿¡æ¯è½¬æ¢ä¸ºè¯­ä¹‰ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¼¥åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒOmniCaptioneråœ¨è§†è§‰æ¨ç†ã€å›¾åƒç”Ÿæˆå’Œé«˜æ•ˆçš„ç›‘ç£å¾®è°ƒæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04010",
            "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
            "url": "https://huggingface.co/papers/2504.04010",
            "abstract": "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.",
            "score": 3,
            "issue_id": 3164,
            "pub_date": "2025-04-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 5",
                "zh": "4æœˆ5æ—¥"
            },
            "hash": "d006058dfff067dc",
            "authors": [
                "Maksim Siniukov",
                "Di Chang",
                "Minh Tran",
                "Hongkun Gong",
                "Ashutosh Chaubey",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "University of Southern California Los Angeles, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04010.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "DiTaiListener: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiTaiListener - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ² Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. DiTaiListener ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: DiTaiListener-Gen Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ¸ DiTaiListener-Edit Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ CTM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾- Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiTaiListener Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DiTaiListener: Realistic Listener Motions for Engaging Interactions",
                    "desc": "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."
                },
                "zh": {
                    "title": "è‡ªç„¶äº’åŠ¨ä¸­çš„å¬ä¼—åŠ¨ä½œç”Ÿæˆæ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDiTaiListenerçš„æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆè‡ªç„¶ä¸”ç»†è…»çš„å¬ä¼—åŠ¨ä½œï¼Œä»¥æ”¹å–„é•¿æ—¶é—´äº’åŠ¨ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€æ¡ä»¶ï¼Œé¦–å…ˆç”ŸæˆåŸºäºè¯´è¯è€…è¯­éŸ³å’Œé¢éƒ¨åŠ¨ä½œçš„çŸ­æ®µå¬ä¼—ååº”ã€‚æ¥ç€ï¼Œé€šè¿‡DiTaiListener-Editå¯¹è¿‡æ¸¡å¸§è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œä»¥ç¡®ä¿è§†é¢‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiTaiListeneråœ¨è§†è§‰çœŸå®æ„Ÿå’ŒåŠ¨ä½œè¡¨ç°ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿæ˜¾ç¤ºå…¶åœ¨åé¦ˆã€å¤šæ ·æ€§å’Œæµç•…æ€§æ–¹é¢æ˜æ˜¾ä¼˜äºå…¶ä»–ç«äº‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05287",
            "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
            "url": "https://huggingface.co/papers/2504.05287",
            "abstract": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/",
            "score": 0,
            "issue_id": 3165,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "fab0fa176a952987",
            "authors": [
                "Hui Zhang",
                "Zijian Wu",
                "Linyi Huang",
                "Sammy Christen",
                "Jie Song"
            ],
            "affiliations": [
                "ETH Zurich, Switzerland",
                "HKUST (Guangzhou), China",
                "HKUST, Hong Kong (China)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05287.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#games",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ÑƒĞºĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Dynamic Grasping: Robots That Adapt and Overcome!",
                    "desc": "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."
                },
                "zh": {
                    "title": "å®ç°çµå·§æŠ“å–çš„é›¶-shotå­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»å•è§†è§’æ„ŸçŸ¥ä¸­å¯¹å„ç§æœªçŸ¥ç‰©ä½“çš„é›¶-shotåŠ¨æ€çµå·§æŠ“å–ã€‚ä¸ä»¥å¾€ä¾èµ–å®Œå…¨å¯è§‚å¯Ÿç‰©ä½“æˆ–ä¸“å®¶æ¼”ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å¤–éƒ¨å¹²æ‰°å¹¶è¿›è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚æˆ‘ä»¬é‡‡ç”¨ä»¥æ‰‹ä¸ºä¸­å¿ƒçš„ç‰©ä½“è¡¨ç¤ºæ³•ï¼Œæå–ä¸äº¤äº’ç›¸å…³çš„å±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œä»è€Œå¢å¼ºå¯¹å½¢çŠ¶å˜åŒ–å’Œä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠ“å–æœªçŸ¥ç‰©ä½“æ—¶å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸç‡é«˜è¾¾97.0%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-09.html",
    "link_next": "2025-04-11.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Skywork R1Vï¼Œä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•ï¼Œå°† R1-series å¤§è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è§†è§‰æ¨¡æ€ã€‚Skywork R1V ä½¿ç”¨è½»é‡çº§çš„è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒè¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„å¤šæ¨¡æ€é€‚åº”ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥å’Œè‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾æç‚¼æ–¹æ³•ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkywork R1V åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²å…¬å¼€ã€‚",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Skywork R1Vï¼Œä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•ï¼Œå°† R1-series å¤§è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è§†è§‰æ¨¡æ€ã€‚Skywork R1V ä½¿ç”¨è½»é‡çº§çš„è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒè¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„å¤šæ¨¡æ€é€‚åº”ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥å’Œè‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾æç‚¼æ–¹æ³•ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkywork R1V åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²å…¬å¼€ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le Skywork R1V, yÄ«zhÇ’ng duÅ mÃ³shuÃ i tuÄ«lÇ mÃ³xÃ­ng. TÄ tÅngguÃ² gÄoxiÃ o de duÅ mÃ³shuÃ i zhuÇnwÃ©i fÄngfÇ, jiÄng R1-series dÃ  yÇ”yÃ¡n mÃ³xÃ­ng kuÃ²zhÇn dÃ o shÃ¬juÃ© mÃ³shuÃ i. Skywork R1V shÇyÃ²ng qÄ«ngliÃ ngjÃ­ de shÃ¬juÃ© tÃ³ujÄ«ngqÃ¬, shÃ­xiÃ n wÃºxÅ« chÃ³ngxÄ«n xÃ¹nliÃ n yÇ”yÃ¡n mÃ³xÃ­ng huÃ² shÃ¬juÃ© biÄnmÇqÃ¬ de duÅ mÃ³shuÃ i shÃ¬yÃ¬ng. WÃ©nzhÄng hÃ¡i tÃ­chÅ« le yÄ«zhÇ’ng hÃ¹nhÃ© yÅuhuÃ  cÃ¨lÃ¼Ã¨ hÃ© zÃ¬ shÃ¬yÃ¬ng chÃ¡ngdÃ¹ de sÄ«wÃ©i liÃ n tÃ­xiÃ ng fÄngfÇ, tÃ­gÄo tuÄ«lÇ xiÃ oyÃ¬ng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, Skywork R1V zÃ i duÅ gÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, bÃ¬ngqiÄ› mÃ³xÃ­ng quÃ¡nzhÃ²ng yÇ gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"è½¬ç§»\", \"pinyin\": \"zhuÇn yÃ­\", \"trans\": \"transfer\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ² zhÇn\", \"trans\": \"extend\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"æŠ•å½±å™¨\", \"pinyin\": \"tÃ³u yÇng qÃ¬\", \"trans\": \"projector\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é‡æ–°\", \"pinyin\": \"chÃ³ng xÄ«n\", \"trans\": \"re-\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"ç¼–ç å™¨\", \"pinyin\": \"biÄn mÇ qÃ¬\", \"trans\": \"encoder\"},\n    {\"word\": \"é€‚åº”\", \"pinyin\": \"shÃ¬ yÃ¬ng\", \"trans\": \"adapt\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"è‡ªé€‚åº”\", \"pinyin\": \"zÃ¬ shÃ¬ yÃ¬ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"é•¿åº¦\", \"pinyin\": \"chÃ¡ng dÃ¹\", \"trans\": \"length\"},\n    {\"word\": \"æ€ç»´é“¾\", \"pinyin\": \"sÄ« wÃ©i liÃ¡n\", \"trans\": \"chain of thought\"},\n    {\"word\": \"æç‚¼\", \"pinyin\": \"tÃ­ liÃ n\", \"trans\": \"extract\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ o lÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨ shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"æƒé‡\", \"pinyin\": \"quÃ¡n zhÃ²ng\", \"trans\": \"weights\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}