{
    "date": {
        "ru": "10 апреля",
        "en": "April 10",
        "zh": "4月10日"
    },
    "time_utc": "2025-04-10 09:12",
    "weekday": 3,
    "issue_id": 3166,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05741",
            "title": "DDT: Decoupled Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.05741",
            "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
            "score": 32,
            "issue_id": 3159,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "2f4cd9583b2418f3",
            "authors": [
                "Shuai Wang",
                "Zhi Tian",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed Vision",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DDT: Разделяй и властвуй в мире диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга."
                },
                "en": {
                    "title": "Decoupling for Faster and Better Image Generation",
                    "desc": "This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance."
                },
                "zh": {
                    "title": "解耦扩散变换器：提升生成质量与推理速度的创新方案",
                    "desc": "扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07083",
            "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
            "url": "https://huggingface.co/papers/2504.07083",
            "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
            "score": 17,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "67e58f651d865bad",
            "authors": [
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Ziwei Liu",
                "Gordon Wetzstein",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07083.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "ИИ-оператор: новый стандарт компьютерной кинематографии",
                    "desc": "Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы создали большой мультимодальный датасет DataDoP, содержащий 29 тысяч реальных кадров с траекториями камеры, картами глубины и подробными описаниями. На основе этих данных была обучена авторегрессионная модель GenDoP на базе архитектуры Transformer для генерации художественных и выразительных движений камеры. Эксперименты показали, что GenDoP превосходит существующие методы по управляемости, точности настройки траекторий и стабильности движения."
                },
                "en": {
                    "title": "Revolutionizing Camera Movement with GenDoP",
                    "desc": "This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography."
                },
                "zh": {
                    "title": "创新相机轨迹生成，提升视觉叙事效果",
                    "desc": "本论文介绍了一种新的相机轨迹生成方法，旨在提升视频制作中的视觉叙事效果。我们提出了一个名为GenDoP的自回归模型，利用大型多模态数据集DataDoP，包含29000个真实镜头及其相机轨迹、深度图和详细描述。通过结合文本指导和RGBD输入，GenDoP能够生成高质量、上下文感知的相机运动。实验结果表明，GenDoP在可控性、轨迹调整精细度和运动稳定性方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07096",
            "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
            "url": "https://huggingface.co/papers/2504.07096",
            "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
            "score": 12,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "68eafe783b5816f6",
            "authors": [
                "Jiacheng Liu",
                "Taylor Blanton",
                "Yanai Elazar",
                "Sewon Min",
                "YenSung Chen",
                "Arnavi Chheda-Kothary",
                "Huy Tran",
                "Byron Bischoff",
                "Eric Marsh",
                "Michael Schmitz",
                "Cassidy Trier",
                "Aaron Sarnat",
                "Jenna James",
                "Jon Borchardt",
                "Bailey Kuehl",
                "Evie Cheng",
                "Karen Farley",
                "Sruthi Sreeram",
                "Taira Anderson",
                "David Albright",
                "Carissa Schoenick",
                "Luca Soldaini",
                "Dirk Groeneveld",
                "Rock Yuren Pang",
                "Pang Wei Koh",
                "Noah A. Smith",
                "Sophie Lebrecht",
                "Yejin Choi",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Jesse Dodge"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Stanford University",
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07096.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#inference",
                    "#open_source",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Заглянуть в память языковой модели",
                    "desc": "OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделей и их многотриллионным обучающим датасетом. Система находит дословные совпадения между сегментами вывода модели и документами в обучающем корпусе текстов. OLMoTrace использует расширенную версию алгоритма infini-gram и выдает результаты трассировки за несколько секунд. Эта система помогает пользователям понять поведение языковых моделей через призму их обучающих данных, что полезно для проверки фактов, выявления галлюцинаций и исследования креативности моделей."
                },
                "en": {
                    "title": "Trace the Truth: Unveiling Language Model Outputs with OLMoTrace",
                    "desc": "OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output."
                },
                "zh": {
                    "title": "实时追踪语言模型输出的革命性工具",
                    "desc": "OLMoTrace是第一个能够实时追踪语言模型输出与其训练数据之间关系的系统。它可以找到语言模型输出片段与训练文本库中文档的逐字匹配。该系统基于扩展版的infini-gram技术，能够在几秒钟内返回追踪结果。OLMoTrace帮助用户通过训练数据理解语言模型的行为，适用于事实检查、幻觉和语言模型的创造力探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06514",
            "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
            "url": "https://huggingface.co/papers/2504.06514",
            "abstract": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.",
            "score": 10,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "6ca6e88a5650dba9",
            "authors": [
                "Chenrui Fan",
                "Ming Li",
                "Lichao Sun",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06514.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Осторожно: языковые модели склонны к избыточным рассуждениям",
                    "desc": "В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные вопросы с отсутствующими предпосылками. Авторы обнаружили, что модели, обученные рассуждать, генерируют избыточно длинные и неэффективные ответы в таких ситуациях. Интересно, что модели без специального обучения рассуждениям показывают лучшие результаты, быстро идентифицируя некорректность вопроса. Исследование выявляет недостатки в текущих методах обучения моделей рассуждениям и предлагает новые подходы к решению проблемы чрезмерного мышления."
                },
                "en": {
                    "title": "Tackling MiP-Overthinking in Reasoning LLMs",
                    "desc": "This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue."
                },
                "zh": {
                    "title": "揭示推理模型的过度思考问题",
                    "desc": "我们发现，推理大型语言模型（LLMs）在面对缺乏前提的模糊问题时，响应长度显著增加，导致冗余和无效的思考。这种新引入的场景加剧了普遍的过度思考问题，我们称之为MiP-过度思考。研究表明，当前的推理LLMs训练方法未能有效鼓励高效思考，导致思维模式的滥用。相反，未专门训练用于推理的LLMs在处理模糊问题时表现更好，能够快速识别问题并给出更简短的回答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04842",
            "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
            "url": "https://huggingface.co/papers/2504.04842",
            "abstract": "Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.",
            "score": 6,
            "issue_id": 3160,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "5b592626aeda4ec8",
            "authors": [
                "Mengchao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Yaqi Fan",
                "Yunpeng Zhang",
                "Yonggang Qi",
                "Kun Zhao",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04842.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Оживление статичных портретов: новый уровень реализма и контроля",
                    "desc": "Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-диффузионной трансформерной модели. Авторы предлагают двухэтапную стратегию аудиовизуального выравнивания для генерации реалистичных говорящих портретов с контролируемой динамикой движений. Метод включает модуль кросс-внимания для сохранения идентичности лица и модуль модуляции интенсивности движения для управления выразительностью. Экспериментальные результаты показывают улучшение качества, реалистичности и сохранения идентичности по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Realistic Talking Avatars: Synchronizing Motion and Expression",
                    "desc": "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."
                },
                "zh": {
                    "title": "生成可控动画头像的新方法",
                    "desc": "本论文提出了一种新颖的框架，用于从单一静态肖像生成逼真的可动画头像。我们采用了预训练的视频扩散变换器模型，能够生成高保真、连贯的对话肖像，并具备可控的运动动态。核心是双阶段的音视频对齐策略，第一阶段通过音频驱动的动态对齐全场景，第二阶段则使用唇部追踪掩模精细调整唇部动作。实验结果表明，我们的方法在真实感、一致性、运动强度和身份保留方面均优于现有技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07089",
            "title": "OmniCaptioner: One Captioner to Rule Them All",
            "url": "https://huggingface.co/papers/2504.07089",
            "abstract": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.",
            "score": 5,
            "issue_id": 3164,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "9b09a9ebdec71131",
            "authors": [
                "Yiting Lu",
                "Jiakang Yuan",
                "Zhen Li",
                "Shitian Zhao",
                "Qi Qin",
                "Xinyue Li",
                "Le Zhuo",
                "Licheng Wen",
                "Dongyang Liu",
                "Yuewen Cao",
                "Xiangchao Yan",
                "Xin Li",
                "Botian Shi",
                "Tao Chen",
                "Zhibo Chen",
                "Lei Bai",
                "Bo Zhang",
                "Peng Gao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07089.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#long_context",
                    "#cv",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальное описание изображений для улучшения мультимодального ИИ",
                    "desc": "OmniCaptioner - это универсальная система для создания подробных текстовых описаний различных визуальных данных. Она может работать с естественными изображениями, визуальным текстом и структурированными визуальными данными, преобразуя пиксельную информацию в семантически богатые текстовые представления. Система улучшает визуальное рассуждение с помощью языковых моделей, повышает качество генерации изображений и обеспечивает эффективное обучение с учителем. OmniCaptioner предлагает новый подход к преодолению разрыва между языковыми и визуальными модальностями."
                },
                "en": {
                    "title": "Bridging Visuals and Text with OmniCaptioner",
                    "desc": "OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data."
                },
                "zh": {
                    "title": "OmniCaptioner：视觉与文本的桥梁",
                    "desc": "我们提出了OmniCaptioner，这是一个多功能的视觉描述框架，能够在多种视觉领域生成细致的文本描述。与之前仅限于特定图像类型的方法不同，我们的框架提供了一个统一的解决方案，可以对自然图像、视觉文本（如海报、用户界面、教科书）和结构化视觉（如文档、表格、图表）进行描述。通过将低级像素信息转换为语义丰富的文本表示，我们的框架弥合了视觉和文本模态之间的差距。我们的研究结果显示，OmniCaptioner在视觉推理、图像生成和高效的监督微调方面具有显著优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07086",
            "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
            "url": "https://huggingface.co/papers/2504.07086",
            "abstract": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.",
            "score": 5,
            "issue_id": 3163,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "b06c700fb29c005d",
            "authors": [
                "Andreas Hochlehnert",
                "Hardik Bhatnagar",
                "Vishaal Udandarao",
                "Samuel Albanie",
                "Ameya Prabhu",
                "Matthias Bethge"
            ],
            "affiliations": [
                "Tübingen AI Center, University of Tübingen",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07086.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Стандартизация оценки математических рассуждений языковых моделей",
                    "desc": "Статья посвящена проблемам оценки способностей языковых моделей к математическим рассуждениям. Авторы обнаружили, что существующие бенчмарки очень чувствительны к тонким деталям реализации, таким как параметры декодирования и форматирование промптов. Предложена стандартизированная система оценки с четко определенными лучшими практиками. Переоценка недавних методов показала, что обучение с подкреплением дает лишь скромные улучшения, в то время как supervised fine-tuning демонстрирует более стабильную генерализацию."
                },
                "en": {
                    "title": "Standardizing Reasoning Evaluations for Language Models",
                    "desc": "This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks."
                },
                "zh": {
                    "title": "推理能力评估的新标准",
                    "desc": "本文探讨了语言模型在推理能力方面的进展，指出当前的数学推理基准测试对实现细节非常敏感。研究发现，许多评估方法缺乏透明性和统计基础，导致性能提升的比较不够清晰。我们提出了一个标准化的评估框架，明确了最佳实践和报告标准，并重新评估了现有方法。结果显示，强化学习方法的改进有限，而监督微调方法在泛化能力上表现更强。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06947",
            "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
            "url": "https://huggingface.co/papers/2504.06947",
            "abstract": "In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.",
            "score": 3,
            "issue_id": 3166,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "00cf6cc0a0c3d1c7",
            "authors": [
                "Natalia Loukachevitch",
                "Natalia Tkachenko",
                "Anna Lapanitsyna",
                "Mikhail Tikhomirov",
                "Nicolay Rusnachenko"
            ],
            "affiliations": [
                "Bauman Moscow State Technical University",
                "Lomonosov Moscow State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06947.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#translation",
                    "#training",
                    "#multilingual",
                    "#open_source"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Извлечение мнений из текста: соревнование по оценке диалогов",
                    "desc": "Эта статья представляет задачу извлечения структурированных мнений из русскоязычных новостных текстов. Участники соревнования должны были извлекать кортежи мнений, состоящие из источника мнения, его цели, выражения и сентимента. Большинство участников экспериментировали с большими языковыми моделями в форматах zero-shot, few-shot и fine-tuning. Лучший результат на тестовом наборе был получен с помощью дообучения большой языковой модели."
                },
                "en": {
                    "title": "Extracting Structured Opinions from Russian News Using Language Models",
                    "desc": "This paper presents a shared task focused on extracting structured opinions from Russian news articles. The goal is to identify opinion tuples that include a sentiment holder, target, expression, and sentiment direction. Over 100 submissions were received, with participants primarily utilizing large language models in various training formats such as zero-shot, few-shot, and fine-tuning. The best performance on the test set was achieved through fine-tuning a large language model, and the study also evaluated different prompts and open-source models to determine the most effective combinations."
                },
                "zh": {
                    "title": "从新闻文本中提取结构化意见的挑战",
                    "desc": "本文介绍了一个关于从俄语新闻文本中提取结构化意见的对话评估共享任务。该任务要求参赛者为给定句子提取意见元组，这些元组由情感持有者、目标、表达和情感组成。总共收到了超过100个提交，参与者主要使用大型语言模型进行零样本、少样本和微调实验。测试集上最佳结果是通过对大型语言模型进行微调获得的，同时我们还比较了30个提示和11个开源语言模型，参数范围从3亿到32亿，找出了最佳模型和提示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04010",
            "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
            "url": "https://huggingface.co/papers/2504.04010",
            "abstract": "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.",
            "score": 3,
            "issue_id": 3164,
            "pub_date": "2025-04-05",
            "pub_date_card": {
                "ru": "5 апреля",
                "en": "April 5",
                "zh": "4月5日"
            },
            "hash": "d006058dfff067dc",
            "authors": [
                "Maksim Siniukov",
                "Di Chang",
                "Minh Tran",
                "Hongkun Gong",
                "Ashutosh Chaubey",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "University of Southern California Los Angeles, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04010.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DiTaiListener: революция в генерации реалистичных реакций слушателя",
                    "desc": "Статья представляет DiTaiListener - новый метод генерации естественных движений слушателя в длительных диалогах с использованием видео-диффузионной модели. DiTaiListener состоит из двух компонентов: DiTaiListener-Gen для создания коротких сегментов реакций слушателя и DiTaiListener-Edit для плавного объединения этих сегментов. Модель использует адаптер CTM для обработки аудио- и визуальных сигналов говорящего, обеспечивая согласованность во времени. Количественные и качественные оценки показывают превосходство DiTaiListener над существующими методами в фотореалистичности и выразительности движений."
                },
                "en": {
                    "title": "DiTaiListener: Realistic Listener Motions for Engaging Interactions",
                    "desc": "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."
                },
                "zh": {
                    "title": "自然互动中的听众动作生成新突破",
                    "desc": "本文介绍了一种名为DiTaiListener的模型，旨在生成自然且细腻的听众动作，以改善长时间互动中的表现。该模型利用视频扩散模型和多模态条件，首先生成基于说话者语音和面部动作的短段听众反应。接着，通过DiTaiListener-Edit对过渡帧进行精细化处理，以确保视频的平滑过渡。实验结果表明，DiTaiListener在视觉真实感和动作表现上均达到了最先进的性能，用户研究也显示其在反馈、多样性和流畅性方面明显优于其他竞争模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07092",
            "title": "Are We Done with Object-Centric Learning?",
            "url": "https://huggingface.co/papers/2504.07092",
            "abstract": "Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.",
            "score": 2,
            "issue_id": 3166,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "8e182461a15aee14",
            "authors": [
                "Alexander Rubinstein",
                "Ameya Prabhu",
                "Matthias Bethge",
                "Seong Joon Oh"
            ],
            "affiliations": [
                "Tubingen AI Center, University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07092.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#transfer_learning",
                    "#cv",
                    "#training",
                    "#science"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Революция в объектно-центрическом обучении: от слотов к сегментации",
                    "desc": "Статья посвящена объектно-центрическому обучению (OCL) в машинном обучении, которое стремится создавать представления, кодирующие только отдельные объекты. Авторы предлагают новый метод OCCAM, основанный на сегментации и кодировании отдельных объектов, который превосходит существующие подходы OCL. Исследование демонстрирует, что такой подход значительно улучшает обобщение вне распределения (OOD) и справляется с проблемой ложных фоновых признаков. Статья открывает новые перспективы для практического применения OCL и изучения фундаментальных вопросов восприятия объектов."
                },
                "en": {
                    "title": "Unlocking Object-Centric Learning for Better Generalization",
                    "desc": "This paper discusses Object-Centric Learning (OCL), which aims to create representations that focus solely on individual objects, ignoring other elements in a scene. The authors highlight advancements in segmentation models that allow for effective separation of objects at the pixel level, leading to improved performance in out-of-distribution (OOD) object discovery tasks. They introduce a new method called Object-Centric Classification with Applied Masks (OCCAM), which shows that this segmentation approach outperforms traditional slot-based methods. The paper also addresses ongoing challenges in applying these techniques to real-world scenarios and emphasizes the importance of understanding object perception in human cognition."
                },
                "zh": {
                    "title": "实现对象中心表示的突破",
                    "desc": "对象中心学习（OCL）旨在学习仅编码对象的表示，独立于场景中的其他对象或背景线索。这种方法支持多种目标，包括分布外（OOD）泛化、样本高效组合和结构化环境建模。我们提出了一种新的无训练探测器，称为应用掩码的对象中心分类（OCCAM），显示出基于分割的个体对象编码显著优于基于槽的OCL方法。尽管在实际应用中仍面临挑战，但我们为OCL社区提供了可扩展的对象中心表示工具箱。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07081",
            "title": "Self-Steering Language Models",
            "url": "https://huggingface.co/papers/2504.07081",
            "abstract": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.",
            "score": 0,
            "issue_id": 3166,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "b0b90ba5f1b881da",
            "authors": [
                "Gabriel Grand",
                "Joshua B. Tenenbaum",
                "Vikash K. Mansinghka",
                "Alexander K. Lew",
                "Jacob Andreas"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07081.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Языковые модели учатся писать программы для самоуправления",
                    "desc": "Статья представляет метод DisCIPL для 'самоуправления' языковых моделей. В этом подходе модель Planner генерирует специфичную для задачи программу вывода, которую выполняет группа моделей Follower. DisCIPL позволяет языковым моделям создавать рекурсивные процедуры поиска для управления выводом. Метод показывает результаты на уровне гораздо более крупных моделей на сложных задачах генерации с ограничениями. DisCIPL открывает возможности для высокопараллельных стратегий вывода по методу Монте-Карло."
                },
                "en": {
                    "title": "Empowering Language Models with Self-Steering Inference Programs",
                    "desc": "This paper presents DisCIPL, a novel method that enhances language models (LMs) by allowing them to generate task-specific inference programs through a Planner model. These programs are then executed by multiple Follower models, enabling efficient and verifiable reasoning processes. The approach allows LMs to create recursive search procedures that improve their problem-solving capabilities, even when they struggle with direct reasoning. Remarkably, DisCIPL demonstrates that smaller models can achieve performance comparable to larger models on complex tasks, while also introducing new strategies for parallelized inference without the need for fine-tuning."
                },
                "zh": {
                    "title": "自我引导的语言模型推理新方法",
                    "desc": "本文介绍了一种名为DisCIPL的方法，旨在提高语言模型在复杂任务中的推理能力。该方法通过一个规划模型生成特定任务的推理程序，并由多个跟随模型执行，从而实现自我引导。DisCIPL使语言模型能够编写递归搜索程序，提升推理的可验证性和效率。实验表明，使用小型跟随模型时，DisCIPL在一些受限生成任务上表现出色，甚至超过了更大的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06958",
            "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2504.06958",
            "abstract": "Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.",
            "score": 0,
            "issue_id": 3166,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "b88359823eee79f5",
            "authors": [
                "Xinhao Li",
                "Ziang Yan",
                "Desen Meng",
                "Lu Dong",
                "Xiangyu Zeng",
                "Yinan He",
                "Yali Wang",
                "Yu Qiao",
                "Yi Wang",
                "Limin Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06958.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#video",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "RFT: Прорыв в пространственно-временном восприятии видео для MLLM",
                    "desc": "Статья представляет систематическое исследование применения метода Reinforcement Fine-Tuning (RFT) с использованием Group Relative Policy Optimization (GRPO) для улучшения пространственно-временного восприятия в мультимодальных больших языковых моделях (MLLM) для видео. Авторы разработали VideoChat-R1 - мощную видео MLLM, достигающую передовых результатов в задачах пространственно-временного восприятия без ущерба для способности к диалогу. По сравнению с Qwen2.5-VL-7B, VideoChat-R1 значительно улучшает производительность в таких задачах, как временная локализация и отслеживание объектов. Исследование подчеркивает потенциал RFT для специализированного улучшения видео MLLM в конкретных задачах."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Reinforcement Fine-Tuning",
                    "desc": "This paper explores the use of Reinforcement Fine-Tuning (RFT) combined with Group Relative Policy Optimization (GRPO) to improve video understanding in multimodal large language models (MLLMs). The authors demonstrate that RFT is effective in enhancing spatio-temporal perception while retaining the model's general capabilities. Their experiments show that the newly developed VideoChat-R1 model significantly outperforms existing models in tasks like temporal grounding and object tracking, achieving state-of-the-art results. The findings highlight the efficiency of RFT for specialized improvements in video MLLMs, paving the way for future research in this area."
                },
                "zh": {
                    "title": "强化学习助力视频理解的突破",
                    "desc": "本论文探讨了强化学习在多模态大语言模型（MLLMs）中的应用，特别是视频理解方面。我们提出了一种系统的强化微调（RFT）方法，结合了群体相对策略优化（GRPO），以增强视频模型的时空感知能力。实验结果表明，RFT在特定任务的改进上具有很高的数据效率，开发了名为VideoChat-R1的强大视频MLLM。该模型在时空感知任务上表现出色，同时保持了良好的对话能力，显著提升了多个基准测试的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05287",
            "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
            "url": "https://huggingface.co/papers/2504.05287",
            "abstract": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/",
            "score": 0,
            "issue_id": 3165,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "fab0fa176a952987",
            "authors": [
                "Hui Zhang",
                "Zijian Wu",
                "Linyi Huang",
                "Sammy Christen",
                "Jie Song"
            ],
            "affiliations": [
                "ETH Zurich, Switzerland",
                "HKUST (Guangzhou), China",
                "HKUST, Hong Kong (China)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05287.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#games",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Адаптивный захват объектов роботом по одному изображению",
                    "desc": "Статья представляет метод обучения с подкреплением для захвата разнообразных объектов роботизированной рукой на основе одного изображения. Авторы используют особое представление объекта, ориентированное на взаимодействие с рукой, что повышает устойчивость к вариациям формы. Предложена стратегия смешанного обучения, сочетающая имитационное обучение и обучение с подкреплением для адаптации к внешним помехам. Эксперименты показывают высокую обобщающую способность метода при захвате новых объектов в симуляции и реальности."
                },
                "en": {
                    "title": "Dynamic Grasping: Robots That Adapt and Overcome!",
                    "desc": "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."
                },
                "zh": {
                    "title": "实现灵巧抓取的零-shot学习新方法",
                    "desc": "本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。"
                }
            }
        }
    ],
    "link_prev": "2025-04-09.html",
    "link_next": "2025-04-11.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "short_date_next": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4月11日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#translation": 1
    },
    "zh": {
        "text": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。",
        "title": "DDT: Decoupled Diffusion Transformer",
        "pinyin": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。\n\nkuò sàn biàn yā qì zài shēng chéng zhì liàng shàng biǎo xiàn chū sè, dàn xū yào gèng cháng de xùn liàn dié dǎi hé duō cì tuī lǐ bù zhòu. měi gè qù zào bù zhòu zhōng, kuò sàn biàn yā qì biān mǎ shēng yīn yùn tǐ yǐ tí qǔ dī pín yǔ yì chéng fēn, rán hòu jiě mǎ gāo pín chéng fēn. zhè zhǒng fāng àn dǎo zhì yòu huà kùn jìng: biān mǎ dī pín yǔ yì xū yào jiǎn shǎo gāo pín chéng fēn, zào chéng yǔ yì biān mǎ hé gāo pín jiě mǎ zhī jiān de máo dùn. wǒ men tí chū yī zhǒng xīn de jiě kǒu kuò sàn biàn yā qì (DDT), jù yǒu zhuān mén de tiáo jiàn biān mǎ qì hé sù dù jiě mǎ qì. shí yàn shì zhù, gèng dà de biān mǎ qì suí zhě mó xíng guī mó zēng jiā tí gāo xíng néng. zài ImageNet 256x256 shàng, DDT-XL/2 dá dào 1.31 FID (xùn liàn shōu liǎn sù dù jìn hū kuài 4 bèi); zài ImageNet 512x512 shàng, dá dào 1.28 FID. cǐ wài, jiě kǒu jià gòu tí gāo tuī lǐ sù dù, yǔn xǔ xiāng lín qù zào bù zhòu jiān gòng xiǎng zì wǒ tiáo jiàn. wǒ men tí chū yī zhǒng xīn de tǒng jì dòng tài guī huà fāng fǎ lái zuì shǎo huà xíng néng xià jiàng.",
        "vocab": "[{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'},\n{'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iteration'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '去噪', 'pinyin': 'qù zào', 'trans': 'denoising'},\n{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encode'},\n{'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'},\n{'word': '成分', 'pinyin': 'chéng fèn', 'trans': 'component'},\n{'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '困境', 'pinyin': 'kùn jìng', 'trans': 'dilemma'},\n{'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'},\n{'word': '矛盾', 'pinyin': 'máo dùn', 'trans': 'contradiction'},\n{'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'},\n{'word': '条件', 'pinyin': 'tiāo jiàn', 'trans': 'condition'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '收敛', 'pinyin': 'shōu liǎn', 'trans': 'convergence'},\n{'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'},\n{'word': '共享', 'pinyin': 'gòng xiǎng', 'trans': 'share'},\n{'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '规划', 'pinyin': 'guī huà', 'trans': 'planning'},\n{'word': '最小化', 'pinyin': 'zuì xiǎo huà', 'trans': 'minimize'},\n{'word': '下降', 'pinyin': 'xià jiàng', 'trans': 'decrease'}]",
        "trans": "The diffusion transformer performs excellently in terms of generation quality but requires longer training iterations and multiple inference steps. In each denoising step, the diffusion transformer encodes noisy input to extract low-frequency semantic components and then decodes high-frequency components. This scheme leads to an optimization dilemma: encoding low-frequency semantics requires reducing high-frequency components, creating a conflict between semantic encoding and high-frequency decoding. We propose a new decoupled diffusion transformer (DDT) with dedicated conditional encoders and velocity decoders. Experiments show that larger encoders improve performance as the model scale increases. On ImageNet 256x256, DDT-XL/2 achieves 1.31 FID (with training convergence speed nearly 4 times faster); on ImageNet 512x512, it achieves 1.28 FID. Additionally, the decoupled architecture increases inference speed, allowing adjacent denoising steps to share self-conditioning. We propose a new statistical dynamic programming method to minimize performance degradation.",
        "update_ts": "2025-04-10 09:12"
    }
}