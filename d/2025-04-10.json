{
    "date": {
        "ru": "10 апреля",
        "en": "April 10",
        "zh": "4月10日"
    },
    "time_utc": "2025-04-10 04:13",
    "weekday": 3,
    "issue_id": 3161,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05741",
            "title": "DDT: Decoupled Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.05741",
            "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
            "score": 18,
            "issue_id": 3159,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "2f4cd9583b2418f3",
            "authors": [
                "Shuai Wang",
                "Zhi Tian",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed Vision",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DDT: Разделяй и властвуй в мире диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга."
                },
                "en": {
                    "title": "Decoupling for Faster and Better Image Generation",
                    "desc": "This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance."
                },
                "zh": {
                    "title": "解耦扩散变换器：提升生成质量与推理速度的创新方案",
                    "desc": "扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07083",
            "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
            "url": "https://huggingface.co/papers/2504.07083",
            "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
            "score": 13,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "67e58f651d865bad",
            "authors": [
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Ziwei Liu",
                "Gordon Wetzstein",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07083.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "ИИ-оператор: новый стандарт компьютерной кинематографии",
                    "desc": "Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы создали большой мультимодальный датасет DataDoP, содержащий 29 тысяч реальных кадров с траекториями камеры, картами глубины и подробными описаниями. На основе этих данных была обучена авторегрессионная модель GenDoP на базе архитектуры Transformer для генерации художественных и выразительных движений камеры. Эксперименты показали, что GenDoP превосходит существующие методы по управляемости, точности настройки траекторий и стабильности движения."
                },
                "en": {
                    "title": "Revolutionizing Camera Movement with GenDoP",
                    "desc": "This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography."
                },
                "zh": {
                    "title": "创新相机轨迹生成，提升视觉叙事效果",
                    "desc": "本论文介绍了一种新的相机轨迹生成方法，旨在提升视频制作中的视觉叙事效果。我们提出了一个名为GenDoP的自回归模型，利用大型多模态数据集DataDoP，包含29000个真实镜头及其相机轨迹、深度图和详细描述。通过结合文本指导和RGBD输入，GenDoP能够生成高质量、上下文感知的相机运动。实验结果表明，GenDoP在可控性、轨迹调整精细度和运动稳定性方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06514",
            "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
            "url": "https://huggingface.co/papers/2504.06514",
            "abstract": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.",
            "score": 8,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "6ca6e88a5650dba9",
            "authors": [
                "Chenrui Fan",
                "Ming Li",
                "Lichao Sun",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06514.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Осторожно: языковые модели склонны к избыточным рассуждениям",
                    "desc": "В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные вопросы с отсутствующими предпосылками. Авторы обнаружили, что модели, обученные рассуждать, генерируют избыточно длинные и неэффективные ответы в таких ситуациях. Интересно, что модели без специального обучения рассуждениям показывают лучшие результаты, быстро идентифицируя некорректность вопроса. Исследование выявляет недостатки в текущих методах обучения моделей рассуждениям и предлагает новые подходы к решению проблемы чрезмерного мышления."
                },
                "en": {
                    "title": "Tackling MiP-Overthinking in Reasoning LLMs",
                    "desc": "This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue."
                },
                "zh": {
                    "title": "揭示推理模型的过度思考问题",
                    "desc": "我们发现，推理大型语言模型（LLMs）在面对缺乏前提的模糊问题时，响应长度显著增加，导致冗余和无效的思考。这种新引入的场景加剧了普遍的过度思考问题，我们称之为MiP-过度思考。研究表明，当前的推理LLMs训练方法未能有效鼓励高效思考，导致思维模式的滥用。相反，未专门训练用于推理的LLMs在处理模糊问题时表现更好，能够快速识别问题并给出更简短的回答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07096",
            "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
            "url": "https://huggingface.co/papers/2504.07096",
            "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
            "score": 5,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "68eafe783b5816f6",
            "authors": [
                "Jiacheng Liu",
                "Taylor Blanton",
                "Yanai Elazar",
                "Sewon Min",
                "YenSung Chen",
                "Arnavi Chheda-Kothary",
                "Huy Tran",
                "Byron Bischoff",
                "Eric Marsh",
                "Michael Schmitz",
                "Cassidy Trier",
                "Aaron Sarnat",
                "Jenna James",
                "Jon Borchardt",
                "Bailey Kuehl",
                "Evie Cheng",
                "Karen Farley",
                "Sruthi Sreeram",
                "Taira Anderson",
                "David Albright",
                "Carissa Schoenick",
                "Luca Soldaini",
                "Dirk Groeneveld",
                "Rock Yuren Pang",
                "Pang Wei Koh",
                "Noah A. Smith",
                "Sophie Lebrecht",
                "Yejin Choi",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Jesse Dodge"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Stanford University",
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07096.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#inference",
                    "#open_source",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Заглянуть в память языковой модели",
                    "desc": "OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделей и их многотриллионным обучающим датасетом. Система находит дословные совпадения между сегментами вывода модели и документами в обучающем корпусе текстов. OLMoTrace использует расширенную версию алгоритма infini-gram и выдает результаты трассировки за несколько секунд. Эта система помогает пользователям понять поведение языковых моделей через призму их обучающих данных, что полезно для проверки фактов, выявления галлюцинаций и исследования креативности моделей."
                },
                "en": {
                    "title": "Trace the Truth: Unveiling Language Model Outputs with OLMoTrace",
                    "desc": "OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output."
                },
                "zh": {
                    "title": "实时追踪语言模型输出的革命性工具",
                    "desc": "OLMoTrace是第一个能够实时追踪语言模型输出与其训练数据之间关系的系统。它可以找到语言模型输出片段与训练文本库中文档的逐字匹配。该系统基于扩展版的infini-gram技术，能够在几秒钟内返回追踪结果。OLMoTrace帮助用户通过训练数据理解语言模型的行为，适用于事实检查、幻觉和语言模型的创造力探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04842",
            "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
            "url": "https://huggingface.co/papers/2504.04842",
            "abstract": "Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.",
            "score": 3,
            "issue_id": 3160,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "5b592626aeda4ec8",
            "authors": [
                "Mengchao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Yaqi Fan",
                "Yunpeng Zhang",
                "Yonggang Qi",
                "Kun Zhao",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04842.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Оживление статичных портретов: новый уровень реализма и контроля",
                    "desc": "Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-диффузионной трансформерной модели. Авторы предлагают двухэтапную стратегию аудиовизуального выравнивания для генерации реалистичных говорящих портретов с контролируемой динамикой движений. Метод включает модуль кросс-внимания для сохранения идентичности лица и модуль модуляции интенсивности движения для управления выразительностью. Экспериментальные результаты показывают улучшение качества, реалистичности и сохранения идентичности по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Realistic Talking Avatars: Synchronizing Motion and Expression",
                    "desc": "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."
                },
                "zh": {
                    "title": "生成可控动画头像的新方法",
                    "desc": "本论文提出了一种新颖的框架，用于从单一静态肖像生成逼真的可动画头像。我们采用了预训练的视频扩散变换器模型，能够生成高保真、连贯的对话肖像，并具备可控的运动动态。核心是双阶段的音视频对齐策略，第一阶段通过音频驱动的动态对齐全场景，第二阶段则使用唇部追踪掩模精细调整唇部动作。实验结果表明，我们的方法在真实感、一致性、运动强度和身份保留方面均优于现有技术。"
                }
            }
        }
    ],
    "link_prev": "2025-04-09.html",
    "link_next": "2025-04-11.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "short_date_next": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4月11日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。\n\nZhè piān wénzhāng jièshào le Skywork R1V, yīzhǒng duō móshuài tuīlǐ móxíng. Tā tōngguò gāoxiào de duō móshuài zhuǎnwéi fāngfǎ, jiāng R1-series dà yǔyán móxíng kuòzhǎn dào shìjué móshuài. Skywork R1V shǐyòng qīngliàngjí de shìjué tóujīngqì, shíxiàn wúxū chóngxīn xùnliàn yǔyán móxíng huò shìjué biānmǎqì de duō móshuài shìyìng. Wénzhāng hái tíchū le yīzhǒng hùnhé yōuhuà cèlüè hé zì shìyìng chángdù de sīwéi liàn tíxiàng fāngfǎ, tígāo tuīlǐ xiàoyìng. Shíyàn jiéguǒ xiǎnshì, Skywork R1V zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìngqiě móxíng quánzhòng yǐ gōngkāi.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"转移\", \"pinyin\": \"zhuǎn yí\", \"trans\": \"transfer\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"extend\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"投影器\", \"pinyin\": \"tóu yǐng qì\", \"trans\": \"projector\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóng xīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"自适应\", \"pinyin\": \"zì shì yìng\", \"trans\": \"adaptive\"},\n    {\"word\": \"长度\", \"pinyin\": \"cháng dù\", \"trans\": \"length\"},\n    {\"word\": \"思维链\", \"pinyin\": \"sī wéi lián\", \"trans\": \"chain of thought\"},\n    {\"word\": \"提炼\", \"pinyin\": \"tí liàn\", \"trans\": \"extract\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"权重\", \"pinyin\": \"quán zhòng\", \"trans\": \"weights\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}