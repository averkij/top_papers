{
    "date": {
        "ru": "3 Ğ¸ÑĞ½Ñ",
        "en": "June 3",
        "zh": "6æœˆ3æ—¥"
    },
    "time_utc": "2025-06-03 21:10",
    "weekday": 1,
    "issue_id": 4106,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.01939",
            "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
            "url": "https://huggingface.co/papers/2506.01939",
            "abstract": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
            "score": 92,
            "issue_id": 4090,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "caf288ab8a8d11b2",
            "authors": [
                "Shenzhi Wang",
                "Le Yu",
                "Chang Gao",
                "Chujie Zheng",
                "Shixuan Liu",
                "Rui Lu",
                "Kai Dang",
                "Xionghui Chen",
                "Jianxin Yang",
                "Zhenru Zhang",
                "Yuqiong Liu",
                "An Yang",
                "Andrew Zhao",
                "Yang Yue",
                "Shiji Song",
                "Bowen Yu",
                "Gao Huang",
                "Junyang Lin"
            ],
            "affiliations": [
                "LeapLab, Tsinghua University",
                "Qwen Team, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01939.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RLVR Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 20% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with High-Entropy Tokens in RLVR",
                    "desc": "This paper investigates the role of token entropy patterns in Reinforcement Learning with Verifiable Rewards (RLVR) to enhance the reasoning abilities of Large Language Models (LLMs). It finds that high-entropy tokens, which are rare, significantly influence the model's reasoning pathways and performance. The study shows that by focusing on these high-entropy tokens during training, the model can achieve better results than traditional methods, even when using a smaller subset of tokens. Overall, the research emphasizes the importance of understanding and optimizing high-entropy tokens to improve RLVR outcomes."
                },
                "zh": {
                    "title": "é«˜ç†µä»¤ç‰Œï¼šæå‡RLVRæ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æå‡ä¸­çš„ä½œç”¨ï¼Œé‡ç‚¹åˆ†æäº†ä»¤ç‰Œç†µæ¨¡å¼å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåªæœ‰å°‘é‡é«˜ç†µä»¤ç‰Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹èµ°å‘å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚é€šè¿‡å¯¹RLVRè®­ç»ƒä¸­ç†µæ¨¡å¼çš„æ¼”å˜è¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°RLVRä¸»è¦éµå¾ªåŸºç¡€æ¨¡å‹çš„ç†µæ¨¡å¼ï¼Œä¸»è¦è°ƒæ•´é«˜ç†µä»¤ç‰Œçš„ç†µå€¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–é«˜ç†µä»¤ç‰Œæ˜¯æå‡RLVRæ•ˆæœçš„å…³é”®ï¼Œåˆ©ç”¨è¿™äº›ä»¤ç‰Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24760",
            "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
            "url": "https://huggingface.co/papers/2505.24760",
            "abstract": "We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.",
            "score": 43,
            "issue_id": 4088,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "c7587646c2140ddd",
            "authors": [
                "Zafir Stojanovski",
                "Oliver Stanley",
                "Joe Sharratt",
                "Richard Jones",
                "Abdulhakeem Adefioye",
                "Jean Kaddour",
                "Andreas KÃ¶pf"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.24760.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#games",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ˜Ğ˜ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Reasoning Gym (RG) - ÑÑ‚Ğ¾ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ°, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ¸Ğ³Ñ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ RG - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RG ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlock Infinite Reasoning with Reasoning Gym!",
                    "desc": "The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings."
                },
                "zh": {
                    "title": "æ¨ç†è®­ç»ƒåœºï¼šæ— é™ç”Ÿæˆï¼ŒæŒç»­è¯„ä¼°",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†æ¨ç†è®­ç»ƒåœºï¼ˆReasoning Gymï¼ŒRGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†ç¯å¢ƒåº“ï¼Œå…·æœ‰å¯éªŒè¯çš„å¥–åŠ±ã€‚å®ƒæä¾›äº†è¶…è¿‡100ä¸ªæ•°æ®ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œæ¶µç›–ä»£æ•°ã€ç®—æœ¯ã€è®¡ç®—ã€è®¤çŸ¥ã€å‡ ä½•ã€å›¾è®ºã€é€»è¾‘å’Œå„ç§å¸¸è§æ¸¸æˆç­‰å¤šä¸ªé¢†åŸŸã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºèƒ½å¤Ÿç”Ÿæˆå‡ ä¹æ— é™çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥è°ƒæ•´å¤æ‚æ€§ï¼Œè¿™ä¸å¤§å¤šæ•°å›ºå®šçš„æ¨ç†æ•°æ®é›†ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRGåœ¨è¯„ä¼°å’Œå¼ºåŒ–å­¦ä¹ æ¨ç†æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01844",
            "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
            "url": "https://huggingface.co/papers/2506.01844",
            "abstract": "SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.",
            "score": 42,
            "issue_id": 4094,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "64cdbe1cd5ffbfc4",
            "authors": [
                "Mustafa Shukor",
                "Dana Aubakirova",
                "Francesco Capuano",
                "Pepijn Kooijmans",
                "Steven Palma",
                "Adil Zouitine",
                "Michel Aractingi",
                "Caroline Pascal",
                "Martino Russi",
                "Andres Marafioti",
                "Simon Alibert",
                "Matthieu Cord",
                "Thomas Wolf",
                "Remi Cadene"
            ],
            "affiliations": [
                "Hugging Face",
                "Sorbonne University",
                "valeo.ai",
                "Ã‰cole Normale SupÃ©rieure Paris-Saclay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01844.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#small_models",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "SmolVLA - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. SmolVLA Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° CPU, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, SmolVLA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ."
                },
                "en": {
                    "title": "SmolVLA: Efficient Vision-Language-Action for Everyone",
                    "desc": "SmolVLA is a compact vision-language-action model that efficiently integrates visual and linguistic understanding for robotics. It significantly reduces the computational costs associated with training and inference, making it suitable for consumer-grade hardware. By leveraging community-collected data, SmolVLA maintains competitive performance while being much smaller than traditional models. The introduction of an asynchronous inference stack enhances responsiveness, allowing for faster action execution without sacrificing accuracy."
                },
                "zh": {
                    "title": "SmolVLAï¼šå°å·§é«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "SmolVLAæ˜¯ä¸€ç§ç´§å‡‘é«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å¯åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç¤¾åŒºæ”¶é›†çš„æ•°æ®ï¼Œé¿å…äº†ä¼ ç»Ÿæ¨¡å‹å¯¹å¤§å‹æ•°æ®é›†çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†çš„æˆæœ¬ã€‚SmolVLAè®¾è®¡ä¸ºå¯ä»¥åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒï¼Œå¹¶åœ¨æ¶ˆè´¹çº§GPUæˆ–CPUä¸Šè¿è¡Œï¼Œæå‡äº†å“åº”é€Ÿåº¦ã€‚å°½ç®¡ä½“ç§¯å°ï¼ŒSmolVLAçš„æ€§èƒ½ä¸ä½“ç§¯åå€çš„æ¨¡å‹ç›¸å½“ï¼Œé€‚ç”¨äºå¤šç§æœºå™¨äººåŸºå‡†æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01049",
            "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
            "url": "https://huggingface.co/papers/2506.01049",
            "abstract": "SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.",
            "score": 31,
            "issue_id": 4087,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "350401d748400bad",
            "authors": [
                "Siyuan Li",
                "Juanxi Tian",
                "Zedong Wang",
                "Xin Jin",
                "Zicheng Liu",
                "Wentao Zhang",
                "Dan Xu"
            ],
            "affiliations": [
                "Peking University",
                "The Hong Kong University of Science and Technology",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01049.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "SGG: Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SGG (Scaling with Gradient Grouping). SGG Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SGG Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "SGG: Optimizing Learning Rates for Better LLM Training",
                    "desc": "This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions."
                },
                "zh": {
                    "title": "SGGï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨",
                    "desc": "SGGæ˜¯ä¸€ç§ä¼˜åŒ–å™¨åŒ…è£…å™¨ï¼Œé€šè¿‡å¯¹æ¢¯åº¦è¿›è¡Œåˆ†ç»„å’Œåº”ç”¨ç‰¹å®šäºé›†ç¾¤çš„ç¼©æ”¾ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚å®ƒè§£å†³äº†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚SGGé€šè¿‡åŠ¨æ€åˆ†ç»„å’Œé›†ç¾¤ç‰¹å®šçš„ç¼©æ”¾æ¥æ”¹å–„å­¦ä¹ ç‡ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„å‚æ•°è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGGä¸ç°æœ‰ä¼˜åŒ–å™¨æ— ç¼é›†æˆï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šæä¾›äº†ä¸€è‡´çš„æ€§èƒ½æå‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00539",
            "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
            "url": "https://huggingface.co/papers/2506.00539",
            "abstract": "ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.",
            "score": 25,
            "issue_id": 4090,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "49b915ea5a1db300",
            "authors": [
                "Ruihan Yang",
                "Yikai Zhang",
                "Aili Chen",
                "Xintao Wang",
                "Siyu Yuan",
                "Jiangjie Chen",
                "Deqing Yang",
                "Yanghua Xiao"
            ],
            "affiliations": [
                "Bytedance Seed",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00539.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#reasoning",
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ARIA: ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ARIA - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARIA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 9,95% Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Language Agents with Intention-Based Reward Aggregation",
                    "desc": "ARIA is a method designed to improve reinforcement learning in language-based tasks by addressing the issue of reward sparsity. It does this by aggregating rewards in an intention space, which clusters semantically similar actions and assigns them shared rewards. This approach reduces the variance of rewards, making it easier for agents to learn effective policies. Experiments show that ARIA leads to significant performance improvements across various tasks compared to traditional reinforcement learning methods."
                },
                "zh": {
                    "title": "æ„å›¾ç©ºé—´ä¸­çš„å¥–åŠ±èšåˆï¼Œæå‡è¯­è¨€ä»£ç†çš„å­¦ä¹ æ•ˆç‡",
                    "desc": "ARIAæ˜¯ä¸€ç§åœ¨æ„å›¾ç©ºé—´ä¸­èšåˆå¥–åŠ±çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç¼“è§£å¥–åŠ±ç¨€ç–æ€§å¹¶æ”¹å–„åŸºäºè¯­è¨€çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„ç­–ç•¥ä¼˜åŒ–ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€åŠ¨ä½œä»é«˜ç»´çš„è”åˆæ ‡è®°åˆ†å¸ƒç©ºé—´æŠ•å½±åˆ°ä½ç»´çš„æ„å›¾ç©ºé—´ï¼ŒARIAèƒ½å¤Ÿå°†è¯­ä¹‰ç›¸ä¼¼çš„åŠ¨ä½œèšé›†åœ¨ä¸€èµ·å¹¶åˆ†é…å…±äº«å¥–åŠ±ã€‚è¿™ç§åŸºäºæ„å›¾çš„å¥–åŠ±èšåˆå‡å°‘äº†å¥–åŠ±æ–¹å·®ï¼Œå¢å¼ºäº†å¥–åŠ±ä¿¡å·çš„å¯†åº¦ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å¥½çš„ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARIAæ˜¾è‘—é™ä½äº†ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ï¼Œå¹¶åœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†9.95%çš„æ€§èƒ½ï¼Œå§‹ç»ˆä¼˜äºç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23590",
            "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles",
            "url": "https://huggingface.co/papers/2505.23590",
            "abstract": "The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
            "score": 24,
            "issue_id": 4088,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "583be7c36c626f1e",
            "authors": [
                "Zifu Wang",
                "Junyi Zhu",
                "Bo Tang",
                "Zhiyu Li",
                "Feiyu Xiong",
                "Jiaqian Yu",
                "Matthew B. Blaschko"
            ],
            "affiliations": [
                "ESAT-PSI, KU Leuven",
                "Institute for Advanced Algorithms Research, Shanghai",
                "Memory Tensor, Shanghai",
                "Samsung R&D Institute China, Beijing",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23590.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#transfer_learning",
                    "#rl",
                    "#cv",
                    "#open_source",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ğ·Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº-Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ğ·Ğ»Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multimodal Learning with Jigsaw Puzzles and RL",
                    "desc": "This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks."
                },
                "zh": {
                    "title": "åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ–°å‘ç°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ‹¼å›¾ä½œä¸ºå®éªŒæ¡†æ¶ï¼Œå‘ç°ç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨ç®€å•æ‹¼å›¾ä¸Šçš„è¡¨ç°ä»éšæœºçŒœæµ‹æå‡è‡³æ¥è¿‘å®Œç¾çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°å¤æ‚çš„æœªè§é…ç½®ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ¨¡å‹å¯ä»¥åœ¨æœ‰æ— æ˜¾å¼æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ³›åŒ–ï¼Œå°½ç®¡å¼€æºæ¨¡å‹æ›´å€¾å‘äºç›´æ¥å›ç­”ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”åˆå§‹çš„ç›‘ç£å¾®è°ƒå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šå¦¨ç¢åç»­çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01853",
            "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
            "url": "https://huggingface.co/papers/2506.01853",
            "abstract": "A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni",
            "score": 22,
            "issue_id": 4091,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "241ff6937e6642f5",
            "authors": [
                "Junliang Ye",
                "Zhengyi Wang",
                "Ruowen Zhao",
                "Shenghao Xie",
                "Jun Zhu"
            ],
            "affiliations": [
                "Peking University",
                "ShengShu",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01853.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#games",
                    "#dataset",
                    "#agi",
                    "#multimodal"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ°Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ShapeLLM-Omni - Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ 3D Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VQVAE) Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Alpaca. ShapeLLM-Omni Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ 3D-Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Bridging Text and 3D: ShapeLLM-Omni Unleashes Multimodal Potential",
                    "desc": "ShapeLLM-Omni is a novel large language model designed to understand and generate 3D assets alongside text. It utilizes a 3D vector-quantized variational autoencoder (VQVAE) to efficiently represent and reconstruct 3D shapes in a discrete latent space. The model is trained on a new dataset called 3D-Alpaca, which includes diverse tasks such as generation, comprehension, and editing of 3D content. This research aims to enhance multimodal AI capabilities by integrating 3D understanding, paving the way for future advancements in 3D-native artificial intelligence."
                },
                "zh": {
                    "title": "ShapeLLM-Omniï¼šå¼€å¯3Då†…å®¹ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºShapeLLM-Omniçš„åŸç”Ÿ3Då¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ç†è§£å’Œç”Ÿæˆ3Dèµ„äº§åŠæ–‡æœ¬ã€‚è¯¥æ¨¡å‹ä½¿ç”¨3Då‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVQVAEï¼‰è¿›è¡Œè®­ç»ƒï¼Œå°†3Då¯¹è±¡æ˜ å°„åˆ°ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œä»¥å®ç°é«˜æ•ˆå‡†ç¡®çš„å½¢çŠ¶è¡¨ç¤ºå’Œé‡å»ºã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåä¸º3D-Alpacaçš„å¤§è§„æ¨¡è¿ç»­è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–ç”Ÿæˆã€ç†è§£å’Œç¼–è¾‘ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œè®­ç»ƒæä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚é€šè¿‡å¯¹Qwen-2.5-vl-7B-Instructæ¨¡å‹è¿›è¡ŒåŸºäºæŒ‡ä»¤çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºæ‰©å±•å¤šæ¨¡æ€æ¨¡å‹çš„åŸºæœ¬3Dèƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„å°è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00996",
            "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2506.00996",
            "abstract": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/",
            "score": 22,
            "issue_id": 4090,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "25ea795d193b8719",
            "authors": [
                "Kinam Kim",
                "Junha Hyung",
                "Jaegul Choo"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00996.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#video",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Temporal In-Context Fine-Tuning (TIC-FT) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. TIC-FT Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ¸, Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ±ÑƒÑ„ĞµÑ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ÑÑ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 10-30 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TIC-FT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Efficient Video Generation with Minimal Data Using TIC-FT",
                    "desc": "Temporal In-Context Fine-Tuning (TIC-FT) is a novel method that improves pretrained video diffusion models for various conditional generation tasks without needing extensive data or changing the model architecture. It works by combining condition and target frames along the time axis and adding intermediate buffer frames with increasing noise, which helps maintain the model's temporal coherence. This approach allows for effective fine-tuning with as few as 10-30 training samples, making it efficient and scalable. TIC-FT has been shown to outperform existing methods in generating high-quality videos while ensuring fidelity to the given conditions."
                },
                "zh": {
                    "title": "æ—¶é—´ä¸Šä¸‹æ–‡å¾®è°ƒï¼šé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´ä¸Šä¸‹æ–‡å¾®è°ƒï¼ˆTIC-FTï¼‰ï¼Œç”¨äºå¢å¼ºé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ ·åŒ–çš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚TIC-FTé€šè¿‡åœ¨æ—¶é—´è½´ä¸Šè¿æ¥æ¡ä»¶å¸§å’Œç›®æ ‡å¸§ï¼Œå¹¶æ’å…¥é€æ¸å¢åŠ å™ªå£°æ°´å¹³çš„ä¸­é—´ç¼“å†²å¸§ï¼Œæ¥å®ç°å¹³æ»‘è¿‡æ¸¡ï¼Œä»è€Œä¸é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶é—´åŠ¨æ€å¯¹é½ã€‚è¯¥æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œä¸”åªéœ€10åˆ°30ä¸ªè®­ç»ƒæ ·æœ¬å³å¯å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTIC-FTåœ¨æ¡ä»¶ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒé«˜æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00411",
            "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
            "url": "https://huggingface.co/papers/2506.00411",
            "abstract": "A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.",
            "score": 22,
            "issue_id": 4091,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "16fa44208bf3618c",
            "authors": [
                "Yi Yang",
                "Jiaxuan Sun",
                "Siqi Kou",
                "Yihan Wang",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Jiao Tong University",
                "ShanghaiTech University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00411.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#robotics",
                    "#agents",
                    "#dataset",
                    "#agi",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "LoHoVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. LoHoVLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ LoHoVLA Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ Ravens."
                },
                "en": {
                    "title": "Empowering Robots with Unified Vision and Language for Complex Tasks",
                    "desc": "The paper presents LoHoVLA, a new framework that integrates a large pretrained vision language model with hierarchical closed-loop control to enhance performance in long-horizon embodied tasks. It addresses the challenges of high-level task planning and low-level motion control by generating language and action tokens for effective sub-task generation and robot action prediction. The framework is trained on a unique dataset, LoHoSet, which includes a variety of long-horizon tasks with expert demonstrations. Experimental results demonstrate that LoHoVLA outperforms existing models, highlighting the potential of unified architectures in improving embodied intelligence."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶æå‡é•¿æ—¶é—´ä»»åŠ¡è¡¨ç°",
                    "desc": "LoHoVLAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„è¡¨ç°ã€‚å®ƒç»“åˆäº†å¤§å‹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å±‚é—­ç¯æ§åˆ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œé«˜å±‚æ¬¡ä»»åŠ¡è§„åˆ’å’Œä½å±‚æ¬¡è¿åŠ¨æ§åˆ¶ã€‚é€šè¿‡ç”Ÿæˆè¯­è¨€å’ŒåŠ¨ä½œæ ‡è®°ï¼ŒLoHoVLAä¿ƒè¿›äº†ä»»åŠ¡é—´çš„æ›´å¥½æ³›åŒ–ã€‚æ­¤å¤–ï¼ŒLoHoVLAåœ¨Ravensæ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01943",
            "title": "Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control",
            "url": "https://huggingface.co/papers/2506.01943",
            "abstract": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.",
            "score": 20,
            "issue_id": 4088,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "4acb1e4fc9635b8a",
            "authors": [
                "Xiao Fu",
                "Xintao Wang",
                "Xian Liu",
                "Jianhong Bai",
                "Runsen Xu",
                "Pengfei Wan",
                "Di Zhang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01943.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RoboMaster: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RoboMaster Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´Ğ¾, Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling",
                    "desc": "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."
                },
                "zh": {
                    "title": "RoboMasterï¼šæå‡æœºå™¨äººæ“ä½œçš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRoboMasterçš„æ–°æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡å¤šç‰©ä½“ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œä»¥æ”¹å–„æœºå™¨äººå†³ç­–æ•°æ®çš„ç”Ÿæˆã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒRoboMasterå°†äº¤äº’è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå­é˜¶æ®µï¼šé¢„äº¤äº’ã€äº¤äº’å’Œåäº¤äº’ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸»å¯¼ç‰©ä½“çš„ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRoboMasteræœ‰æ•ˆåœ°è§£å†³äº†å¤šç‰©ä½“ç‰¹å¾èåˆå¸¦æ¥çš„é—®é¢˜ï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01713",
            "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.01713",
            "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",
            "score": 20,
            "issue_id": 4095,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "fc955e3f149c1b08",
            "authors": [
                "Zhongwei Wan",
                "Zhihao Dou",
                "Che Liu",
                "Yu Zhang",
                "Dongfei Cui",
                "Qinjian Zhao",
                "Hui Shen",
                "Jing Xiong",
                "Yi Xin",
                "Yifan Jiang",
                "Yangfan He",
                "Mi Zhang",
                "Shen Yan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Case Western Reserve University",
                "Duke University",
                "Imperial College London",
                "Kean University Minnesota",
                "Nanjing University",
                "The Ohio State University",
                "The University of Hong Kong",
                "Tongji University",
                "University of Michigan",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01713.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SRPO: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). SRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸ĞµĞ¹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning through Self-Reflection",
                    "desc": "This paper introduces a new approach called Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO) to improve the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing reflection methods are inadequate for generating useful feedback, which limits the models' performance on complex reasoning tasks. SRPO consists of two stages: first, it creates a high-quality dataset for training the model to reflect on its responses, and second, it implements a reward mechanism that promotes meaningful reflections. Experimental results show that SRPO significantly enhances both the accuracy of reasoning and the quality of reflections compared to current leading models."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è‡ªæˆ‘åæ€æ¡†æ¶",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æ˜ç¡®è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘çº æ­£çš„å¤æ‚é—®é¢˜ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åæ€æ–¹æ³•è¿‡äºç®€å•ï¼Œéš¾ä»¥ç”Ÿæˆæœ‰æ„ä¹‰å’ŒæŒ‡å¯¼æ€§çš„åé¦ˆï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†åœ¨åˆå§‹è®­ç»ƒæœŸé—´åŸºæœ¬å›ºå®šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè‡ªæˆ‘åæ€å¢å¼ºæ¨ç†çš„å¤šæ¨¡æ€è‡ªæˆ‘åæ€æ¡†æ¶ï¼ˆSRPOï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µçš„åæ€æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡å¤šæ¨¡æ€LLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRPOåœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ¨ç†å‡†ç¡®æ€§å’Œåæ€è´¨é‡éƒ½æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01667",
            "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
            "url": "https://huggingface.co/papers/2506.01667",
            "abstract": "EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework.",
            "score": 17,
            "issue_id": 4093,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "90528034771977ef",
            "authors": [
                "Yan Shu",
                "Bin Ren",
                "Zhitong Xiong",
                "Danda Pani Paudel",
                "Luc Van Gool",
                "Begum Demir",
                "Nicu Sebe",
                "Paolo Rota"
            ],
            "affiliations": [
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Technical University of Munich",
                "Technische UniversitÃ¤t Berlin",
                "University of Pisa",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01667.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#survey",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "EarthMind: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸",
                    "desc": "EarthMind - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. EarthMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸."
                },
                "en": {
                    "title": "EarthMind: Revolutionizing Earth Observation with Vision-Language Fusion",
                    "desc": "EarthMind is a new framework designed to improve the understanding of Earth Observation (EO) data by combining vision and language processing. It uses Spatial Attention Prompting (SAP) to focus on important details at the pixel level, enhancing the model's ability to interpret images. Additionally, Cross-modal Fusion aligns different types of data, allowing the model to weigh information based on its relevance. This approach not only outperforms larger models like GPT-4o on specialized benchmarks but also effectively handles complex multi-sensor data tasks."
                },
                "zh": {
                    "title": "EarthMindï¼šé«˜æ•ˆç†è§£åœ°çƒè§‚æµ‹æ•°æ®çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "EarthMindæ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆç†è§£å¤šç²’åº¦å’Œå¤šä¼ æ„Ÿå™¨çš„åœ°çƒè§‚æµ‹æ•°æ®ã€‚å®ƒé‡‡ç”¨ç©ºé—´æ³¨æ„åŠ›æç¤ºå’Œè·¨æ¨¡æ€èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸“é—¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚EarthMindçš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶åˆ†åˆ«æ˜¯ç©ºé—´æ³¨æ„åŠ›æç¤ºï¼ˆSAPï¼‰ï¼Œç”¨äºå¢å¼ºåƒç´ çº§ç†è§£ï¼Œä»¥åŠè·¨æ¨¡æ€èåˆï¼Œèƒ½å¤Ÿå°†ä¸åŒæ¨¡æ€å¯¹é½åˆ°å…±äº«ç©ºé—´å¹¶æ ¹æ®ä¿¡æ¯å¯†åº¦è‡ªé€‚åº”è°ƒæ•´æƒé‡ã€‚é€šè¿‡EarthMind-BenchåŸºå‡†æµ‹è¯•ï¼ŒEarthMindåœ¨å¤šä¸ªå…¬å…±åœ°çƒè§‚æµ‹åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å¤„ç†å¤šç²’åº¦å’Œå¤šä¼ æ„Ÿå™¨æŒ‘æˆ˜çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24298",
            "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
            "url": "https://huggingface.co/papers/2505.24298",
            "abstract": "AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57times training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
            "score": 16,
            "issue_id": 4092,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "fad566ec1d2ba264",
            "authors": [
                "Wei Fu",
                "Jiaxuan Gao",
                "Xujie Shen",
                "Chen Zhu",
                "Zhiyu Mei",
                "Chuyi He",
                "Shusheng Xu",
                "Guo Wei",
                "Jun Mei",
                "Jiashu Wang",
                "Tongkai Yang",
                "Binhang Yuan",
                "Yi Wu"
            ],
            "affiliations": [
                "Ant Research",
                "HKUST",
                "IIIS, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24298.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "AReaL: ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "AReaL - ÑÑ‚Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.57 Ñ€Ğ°Ğ· Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ÑĞ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AReaL Ğ½Ğ°Ğ´ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "AReaL: Revolutionizing Reinforcement Learning with Asynchronous Training",
                    "desc": "AReaL is an innovative reinforcement learning system designed to enhance the training of large language models by decoupling the generation and training processes. This fully asynchronous approach allows for continuous output generation without waiting for the longest tasks to finish, leading to improved GPU utilization. By balancing the workload between rollout and training workers, AReaL effectively manages data staleness and employs a modified Proximal Policy Optimization (PPO) to optimize training with outdated samples. Experimental results demonstrate that AReaL can achieve up to 2.57 times faster training speeds while maintaining or improving performance on reasoning tasks."
                },
                "zh": {
                    "title": "AReaLï¼šå¼‚æ­¥å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆè®­ç»ƒæ–°æ¨¡å¼",
                    "desc": "AReaLæ˜¯ä¸€ç§å®Œå…¨å¼‚æ­¥çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒå°†ç”Ÿæˆå’Œè®­ç»ƒè§£è€¦ï¼Œä»è€Œæé«˜GPUçš„åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾2.57å€çš„è®­ç»ƒåŠ é€Ÿã€‚ä¼ ç»Ÿçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿé€šå¸¸æ˜¯åŒæ­¥çš„ï¼Œç”Ÿæˆå’Œè®­ç»ƒäº¤æ›¿è¿›è¡Œï¼Œè¿™å¯¼è‡´äº†ç³»ç»Ÿæ•ˆç‡ä½ä¸‹ã€‚AReaLé€šè¿‡è®©ç”Ÿæˆå·¥ä½œè€…æŒç»­ç”Ÿæˆæ–°è¾“å‡ºï¼Œè€Œè®­ç»ƒå·¥ä½œè€…åœ¨æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®åç«‹å³æ›´æ–°æ¨¡å‹ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å¹³è¡¡ç”Ÿæˆå’Œè®­ç»ƒå·¥ä½œè€…çš„å·¥ä½œè´Ÿè½½ï¼ŒAReaLæœ‰æ•ˆæ§åˆ¶äº†æ•°æ®çš„è¿‡æ—¶æ€§ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºè¿‡æ—¶æ€§çš„PPOå˜ä½“æ¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01863",
            "title": "Unified Scaling Laws for Compressed Representations",
            "url": "https://huggingface.co/papers/2506.01863",
            "abstract": "A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple \"capacity\" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.",
            "score": 14,
            "issue_id": 4099,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "0f97dac5f5c8309f",
            "authors": [
                "Andrei Panferov",
                "Alexandra Volkova",
                "Ionut-Vlad Modoranu",
                "Vage Egiazarian",
                "Mher Safaryan",
                "Dan Alistarh"
            ],
            "affiliations": [
                "ISTA",
                "Red Hat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01863.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾-ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unified Capacity Metric: Predicting Performance in Compressed Models",
                    "desc": "This paper explores how scaling laws in machine learning can predict the performance of models when they are compressed using different techniques. It focuses on various compression formats like scalar-quantized, sparse-quantized, and vector-quantized representations. The authors propose a unified capacity metric that can effectively gauge the efficiency of these compressed models based on their ability to handle random Gaussian data. Their findings suggest that this metric not only applies to individual compression types but can also be used to compare and improve training algorithms across different formats."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å®¹é‡åº¦é‡ï¼Œé¢„æµ‹æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç¼©æ”¾æ³•åˆ™ä¸å‹ç¼©æŠ€æœ¯ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®¹é‡åº¦é‡ï¼Œå¯ä»¥é¢„æµ‹ä¸åŒå‹ç¼©æ ¼å¼ä¸‹æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯ä»¥æ ¹æ®æ¨¡å‹å¤§å°ã€è®¡ç®—é‡å’Œæ•°æ®é‡è¿›è¡Œå¯é¢„æµ‹çš„ç¼©æ”¾ã€‚é€šè¿‡éªŒè¯é€šç”¨çš„ç¼©æ”¾æ³•åˆ™å…¬å¼ï¼Œç ”ç©¶å‘ç°è¯¥å…¬å¼é€‚ç”¨äºä¸åŒçš„å‹ç¼©ç±»å‹ã€‚æœ€ç»ˆï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„â€œå®¹é‡â€åº¦é‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å¤šç§å‹ç¼©è¡¨ç¤ºä¸‹çš„å‚æ•°æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24846",
            "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
            "url": "https://huggingface.co/papers/2505.24846",
            "abstract": "MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.",
            "score": 14,
            "issue_id": 4091,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "ed5c25a307e9093d",
            "authors": [
                "Jingyan Shen",
                "Jiarui Yao",
                "Rui Yang",
                "Yifan Sun",
                "Feng Luo",
                "Rui Pan",
                "Tong Zhang",
                "Han Zhao"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24846.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MiCRo: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "MiCRo - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ° ÑĞ¼ĞµÑĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. MiCRo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MiCRo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "MiCRo: Dynamic Personalization for Diverse Human Preferences",
                    "desc": "MiCRo is a two-stage framework designed to enhance personalized preference learning for large language models (LLMs). It utilizes binary preference datasets and employs a context-aware mixture modeling approach to better capture the diverse preferences of humans. The framework dynamically adjusts mixture weights based on the context, allowing for more accurate and scalable preference adaptation. Experimental results show that MiCRo significantly improves the ability of LLMs to personalize responses according to varied human preferences."
                },
                "zh": {
                    "title": "MiCRoï¼šæ•æ‰å¤šæ ·åŒ–äººç±»åå¥½çš„æ–°æ¡†æ¶",
                    "desc": "MiCRoæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–åå¥½å­¦ä¹ ã€‚å®ƒåˆ©ç”¨äºŒå…ƒåå¥½æ•°æ®é›†ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ··åˆæƒé‡ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰å¤šæ ·åŒ–çš„äººç±»åå¥½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ··åˆå»ºæ¨¡ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹æ— æ³•å……åˆ†åæ˜ äººç±»å¤šæ ·æ€§çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMiCRoåœ¨å¤šä¸ªåå¥½æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä¸ªæ€§åŒ–æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01413",
            "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
            "url": "https://huggingface.co/papers/2506.01413",
            "abstract": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.",
            "score": 11,
            "issue_id": 4088,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "3f0db6c1e3cc1878",
            "authors": [
                "Yulei Qin",
                "Gang Li",
                "Zongyi Li",
                "Zihan Xu",
                "Yuchen Shi",
                "Zhekai Lin",
                "Xiao Cui",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Tencent YouTu Lab",
                "The Chinese University of Hong Kong",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01413.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ ÑƒĞ¼Ğ½ĞµĞµ, Ğ° Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Enhancing LLMs: From Shallow Reasoning to Deep Understanding",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›",
                    "desc": "ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“æŒ‡ä»¤åŒ…å«å¤šä¸ªå¹¶è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„çº¦æŸæ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œé€šè¿‡æ¿€åŠ±æ¨ç†æ¥æå‡LLMså¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯çš„è§„åˆ™ä¸­å¿ƒå¥–åŠ±ä¿¡å·ï¼ŒåŸ¹å…»æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”æ ·æœ¬ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨å¤æ‚æŒ‡ä»¤ä¸‹æ¨ç†çš„æµ…å±‚å’Œéæœ¬è´¨ç‰¹æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00577",
            "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
            "url": "https://huggingface.co/papers/2506.00577",
            "abstract": "Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .",
            "score": 10,
            "issue_id": 4088,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "0ed5d0b7064f9962",
            "authors": [
                "Yufa Zhou",
                "Shaobo Wang",
                "Xingyu Dong",
                "Xiangqi Jin",
                "Yifang Chen",
                "Yue Min",
                "Kexin Yang",
                "Xingzhang Ren",
                "Dayiheng Liu",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "Duke University",
                "EPIC Lab, Shanghai Jiao Tong University",
                "Qwen Team, Alibaba Group",
                "The University of Chicago",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00577.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#agents",
                    "#open_source",
                    "#reasoning",
                    "#games",
                    "#dataset"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Recon - 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Economic Reasoning in LLMs through Post-Training Techniques",
                    "desc": "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."
                },
                "zh": {
                    "title": "åè®­ç»ƒæŠ€æœ¯æå‡æ™ºèƒ½ä½“æ¨ç†ä¸ç»æµç†æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åè®­ç»ƒæŠ€æœ¯å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›å’Œç»æµç†æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé’ˆå¯¹ç»æµæ¨ç†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¼•å…¥Reconæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨é«˜è´¨é‡ç»æµæ¨ç†é—®é¢˜çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†åè®­ç»ƒï¼Œå¹¶åœ¨ç»æµæ¨ç†åŸºå‡†å’Œå¤šæ™ºèƒ½ä½“æ¸¸æˆä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡åè®­ç»ƒçš„æ¨¡å‹åœ¨ç»“æ„åŒ–æ¨ç†å’Œç»æµç†æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†é¢†åŸŸå¯¹é½çš„åè®­ç»ƒåœ¨å¢å¼ºæ¨ç†å’Œæ™ºèƒ½ä½“å¯¹é½æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23907",
            "title": "Cora: Correspondence-aware image editing using few step diffusion",
            "url": "https://huggingface.co/papers/2505.23907",
            "abstract": "Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.",
            "score": 10,
            "issue_id": 4088,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "7de1457440a0b449",
            "authors": [
                "Amirhossein Almohammadi",
                "Aryan Mikaeili",
                "Sauradip Nag",
                "Negar Hassanpour",
                "Andrea Tagliasacchi",
                "Ali Mahdavi-Amiri"
            ],
            "affiliations": [
                "Google Deepmind, Canada",
                "Huawei, Canada",
                "Simon Fraser University, Canada",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23907.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€",
                    "desc": "Cora - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°. Cora Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾."
                },
                "en": {
                    "title": "Cora: Revolutionizing Image Editing with Precision and Control",
                    "desc": "The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods."
                },
                "zh": {
                    "title": "Coraï¼šå›¾åƒç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "Coraæ¡†æ¶é€šè¿‡å¼•å…¥å¯¹åº”æ„ŸçŸ¥å™ªå£°æ ¡æ­£å’Œæ’å€¼æ³¨æ„åŠ›å›¾ï¼Œå¢å¼ºäº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚å®ƒèƒ½å¤Ÿåœ¨æºå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å¯¹é½çº¹ç†å’Œç»“æ„ï¼Œä»è€Œå®ç°å‡†ç¡®çš„çº¹ç†è½¬ç§»å’Œå¿…è¦çš„æ–°å†…å®¹ç”Ÿæˆã€‚Coraåœ¨å†…å®¹ç”Ÿæˆå’Œä¿ç•™ä¹‹é—´æä¾›äº†è‰¯å¥½çš„æ§åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å§¿æ€å˜åŒ–ã€ç‰©ä½“æ·»åŠ å’Œçº¹ç†ç»†åŒ–ç­‰å¤šç§ç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoraåœ¨ç»“æ„ã€çº¹ç†å’Œèº«ä»½çš„ä¿æŒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿè¯å®äº†å…¶ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01952",
            "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
            "url": "https://huggingface.co/papers/2506.01952",
            "abstract": "WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.",
            "score": 9,
            "issue_id": 4095,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "aa260fbf373a4f2c",
            "authors": [
                "Atsuyuki Miyai",
                "Zaiying Zhao",
                "Kazuki Egashira",
                "Atsuki Sato",
                "Tatsumi Sunada",
                "Shota Onohara",
                "Hiromasa Yamanishi",
                "Mashiro Toyooka",
                "Kunato Nishina",
                "Ryoma Maeda",
                "Kiyoharu Aizawa",
                "Toshihiko Yamasaki"
            ],
            "affiliations": [
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01952.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#agi",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "WebChoreArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "WebChoreArena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 532 Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ WebArena Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‚Ğ¾Ğ¼Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4, Claude 3.7 Sonnet Ğ¸ Gemini 2.5 Pro, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° WebChoreArena. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ Gemini 2.5 Pro Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ WebArena."
                },
                "en": {
                    "title": "WebChoreArena: Elevating LLMs to Tackle Tedious Web Tasks",
                    "desc": "WebChoreArena is a new benchmark that includes 532 tasks designed to evaluate the capabilities of large language models (LLMs) in handling complex web browsing chores. It extends the previous WebArena benchmark by focusing on more tedious tasks that require advanced skills such as massive memory retrieval, precise calculations, and long-term memory management across multiple web pages. The benchmark allows for reproducible experiments and fair comparisons with existing models, showcasing the progress of LLMs like GPT-4o and Gemini 2.5 Pro. Despite improvements in performance, the results indicate that there is still significant room for enhancement in tackling the challenges presented by WebChoreArena compared to general browsing tasks."
                },
                "zh": {
                    "title": "WebChoreArenaï¼šè¯„ä¼°LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„èƒ½åŠ›",
                    "desc": "WebChoreArenaæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«532ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å’Œç¹ççš„ç½‘é¡µæµè§ˆä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ‰©å±•äº†WebArenaçš„èŒƒå›´ï¼Œä¸“æ³¨äºäººç±»é€šå¸¸é¿å…çš„ç¹é‡ä»»åŠ¡ã€‚WebChoreArenaæ•´åˆäº†ä¸‰å¤§æŒ‘æˆ˜ï¼šå¤§è§„æ¨¡è®°å¿†ä»»åŠ¡ã€è®¡ç®—ä»»åŠ¡å’Œé•¿æœŸè®°å¿†ä»»åŠ¡ï¼Œç¡®ä¿äº†ä¸¥æ ¼çš„å¯é‡å¤æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€LLMçš„è¿›æ­¥ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œæ˜¾ç¤ºå‡ºWebChoreArenaçš„æŒ‘æˆ˜æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23977",
            "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
            "url": "https://huggingface.co/papers/2505.23977",
            "abstract": "VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
            "score": 8,
            "issue_id": 4087,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "fef2cab0e56bc9bd",
            "authors": [
                "Yichen Feng",
                "Zhangchen Xu",
                "Fengqing Jiang",
                "Yuetai Li",
                "Bhaskar Ramasubramanian",
                "Luyao Niu",
                "Bill Yuchen Lin",
                "Radha Poovendran"
            ],
            "affiliations": [
                "University of Washington",
                "Western Washington University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23977.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "VisualSphinx - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° VisualSphinx ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° VisualSphinx, Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with VisualSphinx",
                    "desc": "VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry."
                },
                "zh": {
                    "title": "VisualSphinxï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›",
                    "desc": "VisualSphinxæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºé€»è¾‘æ¨ç†ä»»åŠ¡ï¼Œè§£å†³äº†å½“å‰æ¨¡å‹ç¼ºä¹ç»“æ„åŒ–è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é€šè¿‡è§„åˆ™åˆ°å›¾åƒçš„åˆæˆæµç¨‹ï¼ŒVisualSphinxèƒ½å¤Ÿç”Ÿæˆä¸é—®é¢˜ç›¸å…³çš„å›¾åƒï¼Œå¢å¼ºæ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ã€ä»£æ•°æ¨ç†ã€ç®—æœ¯æ¨ç†å’Œå‡ ä½•æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23059",
            "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval",
            "url": "https://huggingface.co/papers/2505.23059",
            "abstract": "State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.",
            "score": 8,
            "issue_id": 4088,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "83af42c01de2e64c",
            "authors": [
                "Dohyeon Lee",
                "Yeonseok Jeong",
                "Seung-won Hwang"
            ],
            "affiliations": [
                "Computer Science and Engineering, Seoul National University",
                "Interdisciplinary Program in Artificial Intelligence, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23059.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SMR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State Machine Reasoning (SMR). SMR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SMR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° 3.4% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 74.4%."
                },
                "en": {
                    "title": "Streamlining Retrieval with State Machine Reasoning",
                    "desc": "State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments."
                },
                "zh": {
                    "title": "çŠ¶æ€æœºæ¨ç†ï¼šæå‡æ£€ç´¢æ•ˆç‡ï¼Œå‡å°‘èµ„æºæ¶ˆè€—",
                    "desc": "çŠ¶æ€æœºæ¨ç†ï¼ˆSMRï¼‰é€šè¿‡ç¦»æ•£åŠ¨ä½œæ¡†æ¶æ¥æ”¹å–„ä¿¡æ¯æ£€ç´¢æ€§èƒ½ï¼Œå¹¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œä½¿ç”¨ï¼Œè§£å†³äº†è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¯†åˆ«äº†ä¿¡æ¯æ£€ç´¢ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå†—ä½™è½¨è¿¹å’Œè¯¯å¯¼æ€§æ¨ç†ã€‚SMRé‡‡ç”¨åŸºäºè½¬ç§»çš„æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«ç²¾ç»†æ§åˆ¶çš„ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚ç²¾ç‚¼ã€é‡æ–°æ’åºå’Œåœæ­¢ï¼‰ï¼Œæ”¯æŒæ—©æœŸåœæ­¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMRåœ¨BEIRå’ŒBRIGHTåŸºå‡†ä¸Šæé«˜äº†3.4%çš„æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†74.4%çš„ä»¤ç‰Œä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23001",
            "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
            "url": "https://huggingface.co/papers/2505.23001",
            "abstract": "DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.",
            "score": 8,
            "issue_id": 4087,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "cd584a75fce48ae2",
            "authors": [
                "Yize Cheng",
                "Wenxiao Wang",
                "Mazda Moayeri",
                "Soheil Feizi"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23001.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#security",
                    "#leakage"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "DyePack: Ğ›Ğ¾Ğ²ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ğ½ĞµÑ‡ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "DyePack - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ‚Ğ¸Ğ¿Ğ° backdoor Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ backdoor Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ½Ğ° Ğ½Ğ¸Ñ…. DyePack Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ²Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "DyePack: Safeguarding Model Integrity with Backdoor Detection",
                    "desc": "DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates."
                },
                "zh": {
                    "title": "DyePackï¼šç²¾å‡†è¯†åˆ«è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹",
                    "desc": "DyePackæ˜¯ä¸€ä¸ªåˆ©ç”¨åé—¨æ”»å‡»çš„æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«åœ¨è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥è‰¯æ€§åé—¨æ ·æœ¬ï¼Œç¡®ä¿å‡†ç¡®çš„å‡é˜³æ€§ç‡ï¼ŒåŒæ—¶é˜²æ­¢é”™è¯¯æŒ‡æ§ã€‚DyePackçš„è®¾è®¡ç»“åˆäº†å¤šä¸ªå…·æœ‰éšæœºç›®æ ‡çš„åé—¨ï¼Œä½¿å¾—åœ¨æ ‡è®°æ¯ä¸ªæ¨¡å‹æ—¶èƒ½å¤Ÿç²¾ç¡®è®¡ç®—å‡é˜³æ€§ç‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒDyePackæˆåŠŸæ£€æµ‹åˆ°æ‰€æœ‰å—æ±¡æŸ“çš„æ¨¡å‹ï¼Œä¸”å‡é˜³æ€§ç‡æä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24523",
            "title": "Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors",
            "url": "https://huggingface.co/papers/2505.24523",
            "abstract": "Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.",
            "score": 7,
            "issue_id": 4096,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "4c6278bf22171f39",
            "authors": [
                "Andrea Pedrotti",
                "Michele Papucci",
                "Cristiano Ciaccio",
                "Alessio Miaschi",
                "Giovanni Puccetti",
                "Felice Dell'Orletta",
                "Andrea Esuli"
            ],
            "affiliations": [
                "Department of Computer Science, University of Pisa",
                "Istituto di Scienza Tecnologie dellInformazione A. Faedo (CNR-ISTI)",
                "ItaliaNLP Lab, Istituto di Linguistica Computazionale Antonio Zampolli (CNR-ILC)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24523.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#benchmark",
                    "#security",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "ĞĞ±Ğ¼Ğ°Ğ½ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: ĞºĞ°Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾-ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (MGT) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Direct Preference Optimization. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² MGT, Ğ´ĞµĞ»Ğ°Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ MGT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Fooling the Detectives: Enhancing MGT Stealth with DPO",
                    "desc": "This paper discusses how adversarial attacks can be used to improve the stealth of machine-generated text (MGT) by fine-tuning language models through Direct Preference Optimization (DPO). The authors demonstrate that these attacks can significantly reduce the effectiveness of current MGT detectors by altering the style of generated text to resemble human-written content. They also analyze the linguistic features that detectors rely on, revealing vulnerabilities in their detection capabilities. The findings emphasize the need for more robust detection methods to handle the evolving challenges posed by advanced generative AI."
                },
                "zh": {
                    "title": "æå‡æ£€æµ‹å™¨é²æ£’æ€§ï¼ŒæŠµå¾¡å¯¹æŠ—æ€§æ”»å‡»",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¯¹æŠ—æ€§æ”»å‡»å¦‚ä½•åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»è€Œä½¿å…¶ç”Ÿæˆçš„æ–‡æœ¬æ›´éš¾è¢«æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰æ£€æµ‹å™¨è¯†åˆ«ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„MGTæ£€æµ‹å™¨åœ¨é¢å¯¹ç»è¿‡ä¼˜åŒ–çš„æ–‡æœ¬æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå®¹æ˜“è¢«æ¬ºéª—ã€‚é€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„é£æ ¼è½¬å˜ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ£€æµ‹å™¨ä¾èµ–çš„è¯­è¨€ç‰¹å¾ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ”¹è¿›æ£€æµ‹æ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥å¢å¼ºå…¶å¯¹æœªçŸ¥æ–‡æœ¬çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01881",
            "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue",
            "url": "https://huggingface.co/papers/2506.01881",
            "abstract": "STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.",
            "score": 6,
            "issue_id": 4088,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "e82ff37de8341d1a",
            "authors": [
                "Yaoyao Qian",
                "Jindan Huang",
                "Yuanli Wang",
                "Simon Yu",
                "Kyrie Zhixuan Zhou",
                "Jiayuan Mao",
                "Mingfu Liang",
                "Hanhan Zhou"
            ],
            "affiliations": [
                "Boston University, Boston, MA",
                "George Washington University, Washington, DC",
                "Massachusetts Institute of Technology, Cambridge, MA",
                "Northeastern University, Boston, MA",
                "Northwestern University, Evanston, IL",
                "Tufts University, Medford, MA",
                "University of Texas at San Antonio, San Antonio, TX"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01881.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#alignment",
                    "#agents"
                ],
                "emoji": "ğŸŒªï¸",
                "ru": {
                    "title": "STORM: Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº STORM Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. STORM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - UserLLM Ğ¸ AgentLLM - Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Enhancing Dialogue Systems through Collaborative Intent Formation",
                    "desc": "The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions."
                },
                "zh": {
                    "title": "STORMï¼šä¿ƒè¿›äººæœºåä½œçš„æ„å›¾å½¢æˆ",
                    "desc": "STORMæ¡†æ¶é€šè¿‡å»ºæ¨¡ç”¨æˆ·å’Œä»£ç†ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°åŠ¨æ€ï¼Œä¿ƒè¿›äº†ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„åä½œæ„å›¾å½¢æˆã€‚ç”¨æˆ·çš„è¡¨è¾¾è™½ç„¶åœ¨è¯­è¨€ä¸Šå®Œæ•´ï¼Œä½†å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ‰€éœ€çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´ç³»ç»Ÿæ— æ³•æ­£ç¡®å“åº”ã€‚STORMæ¡†æ¶èƒ½å¤Ÿæ•æ‰è¡¨è¾¾è½¨è¿¹å’Œæ½œåœ¨çš„è®¤çŸ¥è½¬å˜ï¼Œä»è€Œç³»ç»ŸåŒ–åˆ†æåä½œç†è§£çš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé€‚åº¦çš„ä¸ç¡®å®šæ€§ï¼ˆ40-60%ï¼‰å¯ä»¥ä¼˜äºå®Œå…¨é€æ˜çš„ä¿¡æ¯ï¼Œè¿™ä¸ºäººæœºåä½œä¸­çš„ä¿¡æ¯å®Œæ•´æ€§æä¾›äº†æ–°çš„æ€è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00338",
            "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning",
            "url": "https://huggingface.co/papers/2506.00338",
            "abstract": "The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.",
            "score": 6,
            "issue_id": 4088,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "2f4783eb2db68192",
            "authors": [
                "Yifan Peng",
                "Shakeel Muhammad",
                "Yui Sudo",
                "William Chen",
                "Jinchuan Tian",
                "Chyi-Jiunn Lin",
                "Shinji Watanabe"
            ],
            "affiliations": [
                "Carnegie Mellon University, United States",
                "Honda Research Institute Japan, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00338.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#low_resource",
                    "#open_source",
                    "#multilingual",
                    "#data",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€Ğ¾ĞµĞºÑ‚ OWSM ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 166 000 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 75 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ OWSM v4, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Whisper Ğ¸ MMS, Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multilingual Speech Models with Cleaned Web Data",
                    "desc": "The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition."
                },
                "zh": {
                    "title": "æå‡å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„å¼€åˆ›æ€§è¿›å±•",
                    "desc": "OWSMé¡¹ç›®é€šè¿‡æ•´åˆä¸€ä¸ªå¤§å‹æ¸…æ´—è¿‡çš„ç½‘ç»œæ•°æ®é›†YODASï¼Œæå‡äº†å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„æ€§èƒ½ã€‚YODASæ•°æ®é›†åŒ…å«äº†å¤§é‡çš„è¯­éŸ³æ•°æ®ï¼Œä½†ç”±äºå…¶åŸå§‹ç‰¹æ€§ï¼Œå­˜åœ¨è¯­è¨€æ ‡ç­¾é”™è¯¯å’ŒéŸ³é¢‘æ–‡æœ¬ä¸å¯¹é½ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«75ç§è¯­è¨€ã€166,000å°æ—¶è¯­éŸ³çš„æ•°æ®é›†ã€‚æ–°çš„OWSM v4æ¨¡å‹åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨å¤šä¸ªåœºæ™¯ä¸­ä¸é¢†å…ˆçš„å·¥ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24842",
            "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
            "url": "https://huggingface.co/papers/2505.24842",
            "abstract": "Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.",
            "score": 6,
            "issue_id": 4092,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "15a12805380711b7",
            "authors": [
                "Harsh Chaudhari",
                "Jamie Hayes",
                "Matthew Jagielski",
                "Ilia Shumailov",
                "Milad Nasr",
                "Alina Oprea"
            ],
            "affiliations": [
                "Google DeepMind",
                "Northeastern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24842.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#ethics",
                    "#training",
                    "#inference",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°: ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸: Ğ½ĞµÑ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Strengthening Distilled Models Against Adversarial Bias Injection",
                    "desc": "This paper explores the vulnerabilities of distilled language models to adversarial attacks, specifically through the injection of biased content during their training phase. It shows that adversaries can subtly poison teacher models with minimal data, which then amplifies biases in the student models that are derived from them. The study identifies two modes of bias propagation: Untargeted, affecting multiple tasks, and Targeted, which focuses on specific tasks while keeping normal behavior intact. The findings reveal that current defenses are inadequate, emphasizing the need for improved strategies to safeguard against these security threats in distilled models."
                },
                "zh": {
                    "title": "ä¿æŠ¤è’¸é¦æ¨¡å‹ï¼ŒæŠµå¾¡å¯¹æŠ—æ€§åè§æ”»å‡»ï¼",
                    "desc": "æ¨¡å‹è’¸é¦åœ¨åˆ›å»ºå°å‹å¯éƒ¨ç½²è¯­è¨€æ¨¡å‹ä¸­å˜å¾—è‡³å…³é‡è¦ï¼Œè¿™äº›æ¨¡å‹ä¿ç•™äº†æ›´å¤§ç³»ç»Ÿçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¹¿æ³›éƒ¨ç½²å¼•å‘äº†å¯¹æŠ—æ€§æ“æ§çš„è„†å¼±æ€§é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†è’¸é¦æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹åè§å†…å®¹çš„å¯¹æŠ—æ€§æ³¨å…¥çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¼ æ’­æ¨¡å¼ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æœ€å°çš„æ•°æ®æ±¡æŸ“ä½¿æ•™å¸ˆæ¨¡å‹æ³¨å…¥å¾®å¦™çš„åè§ï¼Œè¿™äº›åè§åœ¨å­¦ç”Ÿæ¨¡å‹ä¸­è¢«æ˜¾è‘—æ”¾å¤§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24625",
            "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
            "url": "https://huggingface.co/papers/2505.24625",
            "abstract": "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.",
            "score": 6,
            "issue_id": 4087,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "8bfa132788ee6990",
            "authors": [
                "Duo Zheng",
                "Shijia Huang",
                "Yanyang Li",
                "Liwei Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#video",
                    "#games",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D Geometry Large Language Model (VG LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², VG LLM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ¼ ÑĞ²ĞµÑ€Ñ…Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ 3D-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VG LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Understanding from Video Alone!",
                    "desc": "The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data."
                },
                "zh": {
                    "title": "è§†é¢‘é©±åŠ¨çš„3Dç†è§£æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘-3Då‡ ä½•å¤§è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ä»è§†é¢‘åºåˆ—ä¸­æå–3Dä¿¡æ¯ï¼Œä»è€Œå¢å¼º3Dåœºæ™¯ç†è§£ï¼Œè€Œæ— éœ€é¢å¤–çš„3Dæ•°æ®ã€‚è¯¥æ¨¡å‹åˆ©ç”¨3Dè§†è§‰å‡ ä½•ç¼–ç å™¨ï¼Œä»è§†é¢‘ä¸­æå–3Då…ˆéªŒä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æ ‡è®°ç»“åˆï¼Œè¾“å…¥åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„4Bæ¨¡å‹åœ¨ä¸ä¾èµ–æ˜¾å¼3Dæ•°æ®è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾çš„ç»“æœï¼Œç”šè‡³åœ¨VSI-Benchè¯„ä¼°ä¸­è¶…è¶Šäº†Gemini-1.5-Proã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24183",
            "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
            "url": "https://huggingface.co/papers/2505.24183",
            "abstract": "CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.",
            "score": 6,
            "issue_id": 4093,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "b542a58b96860ad6",
            "authors": [
                "Yaoyu Zhu",
                "Di Huang",
                "Hanqi Lyu",
                "Xiaoyun Zhang",
                "Chongxiao Li",
                "Wenxuan Shi",
                "Yutong Wu",
                "Jianan Mu",
                "Jinghua Wang",
                "Yang Zhao",
                "Pengwei Jin",
                "Shuyao Cheng",
                "Shengwen Liang",
                "Xishan Zhang",
                "Rui Zhang",
                "Zidong Du",
                "Qi Guo",
                "Xing Hu",
                "Yunji Chen"
            ],
            "affiliations": [
                "Cambricon Technologies",
                "SKL of Processors, Institute of Computing Technology, CAS",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24183.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "CodeV-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CodeV-R1 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Verilog Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° RLVR. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ \"ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº - ĞºĞ¾Ğ´\" Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. CodeV-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ CodeV-R1-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing Verilog Generation with CodeV-R1",
                    "desc": "The paper presents CodeV-R1, a reinforcement learning with verifiable reward (RLVR) framework designed for generating Verilog code from natural language specifications. It addresses challenges in electronic design automation (EDA) by introducing a rule-based testbench for equivalence checking and a round-trip data synthesis method to create a high-quality dataset of NL-code pairs. The training process utilizes a two-stage approach, combining knowledge distillation with an adaptive RLVR algorithm to optimize training efficiency. CodeV-R1 demonstrates significant improvements in performance metrics, surpassing previous state-of-the-art models in Verilog generation tasks."
                },
                "zh": {
                    "title": "CodeV-R1ï¼šç”µå­è®¾è®¡è‡ªåŠ¨åŒ–çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†CodeV-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºVerilogç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼€å‘åŸºäºè§„åˆ™çš„æµ‹è¯•å¹³å°ç”Ÿæˆå™¨å’Œå›åˆæ•°æ®åˆæˆæ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç ä¸è‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆè¿›è¡ŒçŸ¥è¯†è’¸é¦ä»¥æå‡æ¨ç†èƒ½åŠ›ï¼Œç„¶åä½¿ç”¨è‡ªé€‚åº”çš„RLVRç®—æ³•é™ä½è®­ç»ƒæˆæœ¬ã€‚æœ€ç»ˆï¼ŒCodeV-R1-7Bæ¨¡å‹åœ¨VerilogEval v2å’ŒRTLLM v1.1ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21179",
            "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
            "url": "https://huggingface.co/papers/2505.21179",
            "abstract": "Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a universal plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!",
            "score": 6,
            "issue_id": 4095,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "3e7694e3e9f014f5",
            "authors": [
                "Dar-Yen Chen",
                "Hmrishav Bandyopadhyay",
                "Kai Zou",
                "Yi-Zhe Song"
            ],
            "affiliations": [
                "NetMind.AI",
                "SketchX, CVSSP, University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21179.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#cv",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "NAG: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Normalized Attention Guidance (NAG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. NAG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², NAG Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Effortless Negative Guidance for Diffusion Models with NAG",
                    "desc": "Normalized Attention Guidance (NAG) is a novel method that improves diffusion models by providing effective negative guidance without the need for retraining. It addresses the challenge of suppressing unwanted attributes, especially in scenarios with few sampling steps where traditional methods like Classifier-Free Guidance (CFG) struggle. NAG utilizes an efficient mechanism that normalizes attention using L1-based techniques, allowing it to maintain high fidelity while enhancing negative guidance. This approach is versatile, working across different architectures, sampling regimes, and modalities, making it a universal solution for modern diffusion frameworks."
                },
                "zh": {
                    "title": "å½’ä¸€åŒ–æ³¨æ„åŠ›å¼•å¯¼ï¼šæ— ç¼è´Ÿå¼•å¯¼çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "å½’ä¸€åŒ–æ³¨æ„åŠ›å¼•å¯¼ï¼ˆNAGï¼‰é€šè¿‡åœ¨ä¸åŒçš„é‡‡æ ·é˜¶æ®µå’Œæ¨¡æ€ä¸­æä¾›æœ‰æ•ˆçš„è´Ÿå¼•å¯¼ï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è´Ÿå¼•å¯¼çš„æŒ‘æˆ˜åœ¨äºåœ¨å°‘æ­¥é‡‡æ ·ä¸­æ˜¾å¾—å°¤ä¸ºçªå‡ºï¼Œä¼ ç»Ÿçš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰åœ¨æ¿€è¿›çš„é‡‡æ ·æ­¥éª¤å‹ç¼©ä¸‹è¡¨ç°ä¸ä½³ã€‚NAGé‡‡ç”¨åŸºäºL1çš„å½’ä¸€åŒ–å’Œç²¾ç‚¼æ–¹æ³•ï¼Œåœ¨æ³¨æ„åŠ›ç©ºé—´ä¸­è¿›è¡Œå¤–æ¨ï¼Œæ¢å¤äº†æœ‰æ•ˆçš„è´Ÿå¼•å¯¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†NAGåœ¨æ–‡æœ¬å¯¹é½ã€ä¿çœŸåº¦å’Œäººç±»æ„ŸçŸ¥è´¨é‡æ–¹é¢çš„ä¸€è‡´æ€§æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01084",
            "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
            "url": "https://huggingface.co/papers/2506.01084",
            "abstract": "A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable \"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency.",
            "score": 5,
            "issue_id": 4093,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "f9927f51990f811a",
            "authors": [
                "Saibo Geng",
                "Nathan Ranchin",
                "Yunzhen yao",
                "Maxime Peyrard",
                "Chris Wendler",
                "Michael Gastpar",
                "Robert West"
            ],
            "affiliations": [
                "EPFL",
                "Microsoft",
                "Northeastern University",
                "UniversitÃ© Grenoble Alpes, CNRS, Grenoble INP, LIG"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01084.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° zip2zip, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ LZW, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° LZW, ÑĞ»Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ zip2zip Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° 20-60% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ."
                },
                "en": {
                    "title": "Dynamic Tokenization for Faster Inference in LLMs",
                    "desc": "The paper presents zip2zip, a novel framework that enhances the efficiency of large language models (LLMs) by dynamically adjusting their token vocabulary during inference. By utilizing Lempel-Ziv-Welch (LZW) compression, zip2zip reduces the length of token sequences, which leads to faster inference speeds. The framework includes a tokenizer that creates reusable 'hypertokens', an embedding layer for these hypertokens, and a causal language model that operates on compressed sequences. The results demonstrate that zip2zip can significantly decrease input and output lengths by 20-60%, improving overall model performance and reducing computational costs."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ä»¤ç‰Œï¼Œæå‡æ¨ç†é€Ÿåº¦",
                    "desc": "zip2zipæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒåœ¨æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»¤ç‰Œè¯æ±‡ï¼Œä½¿ç”¨LZWå‹ç¼©æŠ€æœ¯æ¥å‡å°‘ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€çš„ä»¤ç‰Œå™¨ï¼Œè¿™äº›ä»¤ç‰Œå™¨çš„å›ºå®šè¯æ±‡æ— æ³•é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–è¯­è¨€çš„è¾“å…¥ï¼Œå¯¼è‡´ç”Ÿæˆæ›´é•¿çš„ä»¤ç‰Œåºåˆ—å’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚zip2zipé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°å…¶åŠŸèƒ½ï¼šåŸºäºLZWå‹ç¼©çš„ä»¤ç‰Œå™¨ã€å®æ—¶è®¡ç®—æ–°å½¢æˆçš„è¶…ä»¤ç‰Œçš„åµŒå…¥å±‚ï¼Œä»¥åŠè®­ç»ƒæ¨¡å‹å¤„ç†å‹ç¼©åºåˆ—çš„å› æœè¯­è¨€å»ºæ¨¡å˜ä½“ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡zip2zipå¤„ç†çš„LLMåœ¨æ¨ç†æ—¶èƒ½å¤Ÿæœ‰æ•ˆä½¿ç”¨è¶…ä»¤ç‰Œï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—é•¿åº¦å‡å°‘20-60%ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—é™ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00512",
            "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
            "url": "https://huggingface.co/papers/2506.00512",
            "abstract": "A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.",
            "score": 5,
            "issue_id": 4094,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "0a9ce5d9ebc76a52",
            "authors": [
                "Yang Zheng",
                "Mengqi Huang",
                "Nan Chen",
                "Zhendong Mao"
            ],
            "affiliations": [
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00512.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğº Ğ¼ĞµĞ½ĞµĞµ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Pro3D-Editor Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Achieving Consistent 3D Editing with Pro3D-Editor",
                    "desc": "This paper introduces a new approach for 3D editing called Pro3D-Editor, which focuses on maintaining consistency across different views of a 3D object. Unlike traditional methods that treat all views equally, this framework uses a progressive-views paradigm to propagate editing information from the most important view to others. It consists of three main components: a Primary-view Sampler that identifies and edits the most relevant view, a Key-view Render that transfers the editing semantics to other views, and a Full-view Refiner that finalizes the 3D object based on the edited views. The results show that Pro3D-Editor achieves better accuracy and consistency compared to existing 3D editing techniques."
                },
                "zh": {
                    "title": "æ¸è¿›è§†å›¾èŒƒå¼å®ç°ä¸€è‡´çš„3Dç¼–è¾‘",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›è§†å›¾èŒƒå¼ï¼Œé€šè¿‡Pro3D-Editorå®ç°ä¸€è‡´çš„3Dç¼–è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å…³é”®è§†å›¾å‘è¾ƒå°‘ç¼–è¾‘çš„è§†å›¾ä¼ æ’­è¯­ä¹‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šè§†å›¾ç¼–è¾‘ä¸­å­˜åœ¨çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚Pro3D-Editoræ¡†æ¶åŒ…æ‹¬ä¸»è¦è§†å›¾é‡‡æ ·å™¨ã€å…³é”®è§†å›¾æ¸²æŸ“å’Œå…¨è§†å›¾ç²¾ç‚¼å™¨ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€é‡è¦çš„è§†å›¾è¿›è¡Œç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘ç²¾åº¦å’Œç©ºé—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24452",
            "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
            "url": "https://huggingface.co/papers/2505.24452",
            "abstract": "A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of varphi.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.",
            "score": 5,
            "issue_id": 4091,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "82972c2646341cc9",
            "authors": [
                "Anda Tang",
                "Yiming Dong",
                "Yutao Zeng",
                "zhou Xun",
                "Zhouchen Lin"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute for Artificial Intelligence, Peking University",
                "Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China",
                "State Key Lab of General AI, School of Intelligence Science and Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24452.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Unified Budget-Aware (UBA), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. UBA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ï†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Optimizing Training with Unified Budget-Aware Learning Rates",
                    "desc": "This paper introduces a new learning rate schedule called the Unified Budget-Aware (UBA) schedule, designed to optimize training when there are limits on the number of iterations. Traditional learning rate schedules often rely on trial-and-error and lack a solid theoretical basis, making them inefficient. The UBA schedule is grounded in a novel optimization framework that considers the curvature of the loss landscape, allowing it to adapt better to various tasks and network architectures. Experimental results demonstrate that UBA outperforms standard schedules across different vision and language tasks, providing a more effective training strategy within constrained budgets."
                },
                "zh": {
                    "title": "ç»Ÿä¸€é¢„ç®—æ„ŸçŸ¥å­¦ä¹ ç‡è°ƒåº¦ï¼Œä¼˜åŒ–æœ‰é™è®­ç»ƒé¢„ç®—",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„é¢„ç®—æ„ŸçŸ¥å­¦ä¹ ç‡è°ƒåº¦ï¼ˆUBAï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–åœ¨æœ‰é™è¿­ä»£é¢„ç®—ä¸‹çš„è®­ç»ƒæ•ˆæœã€‚ä¼ ç»Ÿçš„å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•å¾€å¾€ä¾èµ–ç»éªŒï¼Œç¼ºä¹ç†è®ºåŸºç¡€ï¼Œè€ŒUBAåˆ™é€šè¿‡æ„å»ºä¸€ä¸ªæ–°çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè€ƒè™‘äº†å¯¹æŸå¤±å‡½æ•°æ›²ç‡å˜åŒ–çš„é²æ£’æ€§ã€‚è¯¥è°ƒåº¦ç”±ä¸€ä¸ªè¶…å‚æ•°æ§åˆ¶ï¼Œèƒ½å¤Ÿåœ¨çµæ´»æ€§å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé¿å…äº†å¯¹æ¯ä¸ªç½‘ç»œè¿›è¡Œæ•°å€¼ä¼˜åŒ–çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUBAåœ¨å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºå¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23504",
            "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.23504",
            "abstract": "VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.",
            "score": 5,
            "issue_id": 4087,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "c243189c9ec32d1f",
            "authors": [
                "Liyun Zhu",
                "Qixiang Chen",
                "Xi Shen",
                "Xiaodong Cun"
            ],
            "affiliations": [
                "Australian National University",
                "GVC Lab, Great Bay University",
                "Intellindust AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23504.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸",
                    "desc": "VAU-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VAU-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° VAU-R1 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench",
                    "desc": "The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘å¼‚å¸¸æ¨ç†çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "VAU-R1 æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆReinforcement Fine-Tuningï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº† VAU-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè§†é¢‘å¼‚å¸¸æ¨ç†çš„é“¾å¼æ€ç»´åŸºå‡†ï¼ŒåŒ…å«å¤šé¡¹é€‰æ‹©é—®ç­”ã€è¯¦ç»†æ¨ç†ã€æ—¶é—´æ ‡æ³¨å’Œæè¿°æ€§æ ‡é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAU-R1 åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01484",
            "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
            "url": "https://huggingface.co/papers/2506.01484",
            "abstract": "A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  \t\t\t\t\tAI-generated summary \t\t\t\t Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.",
            "score": 4,
            "issue_id": 4095,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "422c267bbe9577db",
            "authors": [
                "Shuzhou Yuan",
                "Ercong Nie",
                "Lukas Kouba",
                "Ashish Yashwanth Kangen",
                "Helmut Schmid",
                "Hinrich Schutze",
                "Michael Farber"
            ],
            "affiliations": [
                "LMU Munich",
                "Munich Center for Machine Learning (MCML)",
                "ScaDS.AI and TU Dresden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01484.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "ğŸ§¼",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ° Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GPT-4o-mini. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PARADEHATE, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 8000 Ğ¿Ğ°Ñ€ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Automating Hate Speech Detoxification with GPT-4o-mini",
                    "desc": "This paper introduces a new method for creating a large dataset aimed at detoxifying hate speech using the GPT-4o-mini model. Detoxification involves rewriting harmful language into non-toxic text, which is crucial due to the rise of toxic content online. The authors developed a pipeline that automates this process, replacing human annotators with a language model, and found that the model's performance is comparable to that of humans. They also created a dataset called PARADEHATE, consisting of over 8,000 pairs of hate and non-hate text, which significantly improves the performance of various models in terms of style accuracy, content preservation, and fluency."
                },
                "zh": {
                    "title": "åˆ©ç”¨GPT-4o-miniç”Ÿæˆä»‡æ¨è¨€è®ºå»æ¯’åŒ–æ•°æ®é›†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œåˆ©ç”¨GPT-4o-miniç”Ÿæˆå¤§è§„æ¨¡çš„ä»‡æ¨è¨€è®ºå»æ¯’åŒ–æ•°æ®é›†ï¼Œä»è€Œæé«˜åŸºçº¿æ¨¡å‹åœ¨é£æ ¼å‡†ç¡®æ€§ã€å†…å®¹ä¿ç•™å’Œæµç•…æ€§æ–¹é¢çš„è¡¨ç°ã€‚å»æ¯’åŒ–æ˜¯å°†æœ‰å®³è¯­è¨€é‡å†™ä¸ºéæœ‰å®³æ–‡æœ¬çš„ä»»åŠ¡ï¼Œéšç€ç½‘ç»œä¸Šæœ‰æ¯’å†…å®¹çš„å¢åŠ ï¼Œè¿™ä¸€ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç”±äºäººå·¥æ ‡æ³¨çš„æˆæœ¬å’Œæ•æ„Ÿæ€§ï¼Œé«˜è´¨é‡çš„å»æ¯’åŒ–å¹³è¡Œæ•°æ®é›†ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æ„å»ºäº†PARADEHATEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä»‡æ¨è¨€è®ºå»æ¯’åŒ–çš„å¤§è§„æ¨¡å¹³è¡Œæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åŸºäºè¯¥æ•°æ®é›†çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00643",
            "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions",
            "url": "https://huggingface.co/papers/2506.00643",
            "abstract": "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.",
            "score": 4,
            "issue_id": 4088,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "f95c367c9eaf00a9",
            "authors": [
                "Weijie Xu",
                "Shixian Cui",
                "Xi Fang",
                "Chi Xue",
                "Stephanie Eckman",
                "Chandan Reddy"
            ],
            "affiliations": [
                "Amazon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00643.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#interpretability",
                    "#data",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SATA-BENCH - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ»Ğ¸ÑˆÑŒ 41.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Choice Funnel, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel",
                    "desc": "The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks."
                },
                "zh": {
                    "title": "æå‡å¤šç­”æ¡ˆæ¨ç†çš„å‡†ç¡®æ€§ä¸æ•ˆç‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SATA-BENCHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç­”æ¡ˆé—®é¢˜ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨é€‰æ‹©æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶å­˜åœ¨æ˜¾è‘—çš„é€‰æ‹©åå·®å’Œè®¡æ•°åå·®ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Choice Funnelè§£ç ç­–ç•¥ï¼Œé€šè¿‡å»åå’Œè‡ªé€‚åº”é˜ˆå€¼å¼•å¯¼æ¨¡å‹åšå‡ºæ›´å®Œæ•´å’Œå‡†ç¡®çš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChoice Funnelåœ¨å‡†ç¡®åŒ¹é…ç‡ä¸Šæ¯”ç«äº‰åŸºçº¿æé«˜äº†29%ï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬è¶…è¿‡64%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24086",
            "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2505.24086",
            "abstract": "ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.",
            "score": 4,
            "issue_id": 4095,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "2bd92a7129e6945b",
            "authors": [
                "Zeeshan Khan",
                "Shizhe Chen",
                "Cordelia Schmid"
            ],
            "affiliations": [
                "Inria, Ã‰cole normale supÃ©rieure, CNRS, PSL Research University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24086.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#diffusion",
                    "#interpretability",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2.5D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ²",
                    "desc": "ComposeAnything - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 2.5D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… 2D Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼Ğ°ĞºĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ComposeAnything Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-CompBench Ğ¸ NSR-1K Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 2D/3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑÑ€Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ComposeAnything: Elevating Text-to-Image Generation with 2.5D Layouts",
                    "desc": "ComposeAnything is a framework that enhances text-to-image generation by utilizing large language models (LLMs) to create 2.5D semantic layouts. This method improves the arrangement of objects in images by incorporating depth information, which helps maintain coherence and quality in the generated images. Unlike previous models that rely solely on 2D layouts, ComposeAnything provides a more accurate representation of spatial relationships, allowing for better object placement. The framework has shown superior performance on benchmark tests, producing high-quality images that align closely with the provided text descriptions."
                },
                "zh": {
                    "title": "ComposeAnythingï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ComposeAnything æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç”ŸæˆåŒ…å«æ·±åº¦ä¿¡æ¯çš„2.5Dè¯­ä¹‰å¸ƒå±€ï¼Œä»è€Œå¢å¼ºå¯¹è±¡çš„æ”¾ç½®å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆç©ºé—´å’Œæ·±åº¦æ„ŸçŸ¥çš„ç²—ç•¥åˆæˆå›¾åƒï¼Œæ¥æŒ‡å¯¼å»å™ªè¿‡ç¨‹ã€‚ComposeAnything åœ¨å¤„ç†å¤æ‚çš„2D/3Dç©ºé—´å¸ƒå±€å’Œè¶…ç°å®ç»„åˆæ—¶ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22954",
            "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
            "url": "https://huggingface.co/papers/2505.22954",
            "abstract": "The Darwin G\\\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\\\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.",
            "score": 4,
            "issue_id": 4104,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "014c0d439b8212f8",
            "authors": [
                "Jenny Zhang",
                "Shengran Hu",
                "Cong Lu",
                "Robert Lange",
                "Jeff Clune"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Sakana AI",
                "University of British Columbia",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22954.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#agi",
                    "#agents",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ˜Ğ˜: ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Darwin GÃ¶del Machine (DGM) - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. DGM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ñ€Ğ²Ğ¸Ğ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ñ… Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑĞ²Ğ¾Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SWE-bench Ğ¸ Polyglot. DGM Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Evolving AI: The Future of Self-Improvement",
                    "desc": "The Darwin G\"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ”¹è¿›çš„AIï¼šè¾¾å°”æ–‡å“¥å¾·å°”æœºå™¨çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "è¾¾å°”æ–‡å“¥å¾·å°”æœºå™¨ï¼ˆDGMï¼‰é€šè¿‡è¿­ä»£è‡ªæˆ‘ä¿®æ”¹å’Œå¼€æ”¾å¼æ¢ç´¢æ¥æé«˜å…¶ç¼–ç èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šæ¶æ„AIç³»ç»Ÿä¸åŒï¼ŒDGMèƒ½å¤Ÿè‡ªä¸»ä¸”æŒç»­åœ°æ”¹è¿›è‡ªèº«ã€‚å®ƒå€Ÿé‰´äº†è¾¾å°”æ–‡è¿›åŒ–çš„ç†å¿µï¼Œç»´æŠ¤ä¸€ä¸ªç”Ÿæˆç¼–ç ä»£ç†çš„æ¡£æ¡ˆåº“ï¼Œå¹¶é€šè¿‡é‡‡æ ·å’ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆæ–°ç‰ˆæœ¬ï¼Œå½¢æˆå¤šæ ·åŒ–çš„é«˜è´¨é‡ä»£ç†æ ‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGMåœ¨ç¼–ç èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„ä»£ç ç¼–è¾‘å·¥å…·å’ŒåŒè¡Œè¯„å®¡æœºåˆ¶ï¼Œæ ‡å¿—ç€è‡ªæˆ‘æ”¹è¿›AIçš„é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01928",
            "title": "Esoteric Language Models",
            "url": "https://huggingface.co/papers/2506.01928",
            "abstract": "Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
            "score": 3,
            "issue_id": 4100,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "6b37258ad3883db7",
            "authors": [
                "Subham Sekhar Sahoo",
                "Zhihan Yang",
                "Yash Akhauri",
                "Johnna Liu",
                "Deepansha Singh",
                "Zhoujun Cheng",
                "Zhengzhong Liu",
                "Eric Xing",
                "John Thickstun",
                "Arash Vahdat"
            ],
            "affiliations": [
                "Cornell Tech",
                "Cornell University",
                "MBZUAI",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01928.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Eso-LMs: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Eso-LMs Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Eso-LMs Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ 65 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Eso-LMs: Fast and Efficient Language Modeling Revolution",
                    "desc": "Eso-LMs are a new type of language model that combines features from both autoregressive and masked diffusion models. This fusion allows for better performance in language tasks by improving perplexity and inference speed. A key innovation is the introduction of KV caching in masked diffusion models, which enhances efficiency during inference while still allowing for parallel generation. As a result, Eso-LMs achieve significantly faster inference times compared to traditional models, setting new benchmarks in language modeling."
                },
                "zh": {
                    "title": "Eso-LMsï¼šè‡ªå›å½’ä¸æ©è”½æ‰©æ•£æ¨¡å‹çš„å®Œç¾èåˆ",
                    "desc": "Eso-LMsæ˜¯ä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ã€‚å®ƒå¼•å…¥äº†KVç¼“å­˜æŠ€æœ¯ï¼Œä½¿å¾—åœ¨æ¨ç†æ—¶çš„æ•ˆç‡å¤§å¹…æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¹¶è¡Œç”Ÿæˆçš„èƒ½åŠ›ã€‚é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥ï¼ŒEso-LMsåœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œæ¨ç†é€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ©è”½æ‰©æ•£æ¨¡å‹å¿«65å€ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆåœ°è§£å†³äº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œæä¾›äº†æ›´å¥½çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01920",
            "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
            "url": "https://huggingface.co/papers/2506.01920",
            "abstract": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.",
            "score": 3,
            "issue_id": 4095,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "42a070cbc2d3afee",
            "authors": [
                "Serry Sibaee",
                "Omer Nacar",
                "Adel Ammar",
                "Yasser Al-Habashi",
                "Abdulrahman Al-Batati",
                "Wadii Boulila"
            ],
            "affiliations": [
                "Prince Sultan University, Riyadh, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01920.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#dataset",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡¦ğŸ‡ª",
                "ru": {
                    "title": "ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADMD Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ADMD ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 490 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 10 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Enhancing Arabic Language Models with Cultural Competence",
                    "desc": "This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation."
                },
                "zh": {
                    "title": "æå‡é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹è¯„ä¼°çš„æ–‡åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ADMDï¼Œç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹ï¼Œå¼ºè°ƒäº†æ€§èƒ½å·®å¼‚å’Œæ–‡åŒ–èƒ½åŠ›çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­è¯„ä¼°æ•°æ®é›†ï¼Œå‘ç°äº†è¯­è¨€å‡†ç¡®æ€§ã€æ–‡åŒ–å¯¹é½å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§æ–¹é¢çš„é‡å¤§é—®é¢˜ã€‚ADMDåŒ…å«490ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ¶µç›–åä¸ªä¸»è¦é¢†åŸŸï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ä½¿ç”¨ADMDè¯„ä¼°äº”ä¸ªé¢†å…ˆçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åšæ–‡åŒ–ç†è§£å’Œä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01920",
            "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
            "url": "https://huggingface.co/papers/2506.01920",
            "abstract": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.",
            "score": 3,
            "issue_id": 4095,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "42a070cbc2d3afee",
            "authors": [
                "Serry Sibaee",
                "Omer Nacar",
                "Adel Ammar",
                "Yasser Al-Habashi",
                "Abdulrahman Al-Batati",
                "Wadii Boulila"
            ],
            "affiliations": [
                "Prince Sultan University, Riyadh, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01920.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#dataset",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡¦ğŸ‡ª",
                "ru": {
                    "title": "ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADMD Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ADMD ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 490 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 10 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Enhancing Arabic Language Models with Cultural Competence",
                    "desc": "This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation."
                },
                "zh": {
                    "title": "æå‡é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹è¯„ä¼°çš„æ–‡åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ADMDï¼Œç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹ï¼Œå¼ºè°ƒäº†æ€§èƒ½å·®å¼‚å’Œæ–‡åŒ–èƒ½åŠ›çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­è¯„ä¼°æ•°æ®é›†ï¼Œå‘ç°äº†è¯­è¨€å‡†ç¡®æ€§ã€æ–‡åŒ–å¯¹é½å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§æ–¹é¢çš„é‡å¤§é—®é¢˜ã€‚ADMDåŒ…å«490ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ¶µç›–åä¸ªä¸»è¦é¢†åŸŸï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ä½¿ç”¨ADMDè¯„ä¼°äº”ä¸ªé¢†å…ˆçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åšæ–‡åŒ–ç†è§£å’Œä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00723",
            "title": "Pitfalls in Evaluating Language Model Forecasters",
            "url": "https://huggingface.co/papers/2506.00723",
            "abstract": "Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.",
            "score": 3,
            "issue_id": 4097,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "4260f72a1f88e8cd",
            "authors": [
                "Daniel Paleka",
                "Shashwat Goel",
                "Jonas Geiping",
                "Florian TramÃ¨r"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "ETH Zurich",
                "MPI TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00723.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#leakage",
                    "#benchmark",
                    "#evaluation"
                ],
                "emoji": "âš ï¸",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑĞ¾Ğ¼Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼ Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Rethinking Evaluation: Ensuring Trust in LLM Forecasting",
                    "desc": "This paper discusses the challenges of evaluating large language models (LLMs) in forecasting tasks, highlighting that claims of LLMs matching or exceeding human performance may be misleading. The authors identify two main issues: the risk of temporal leakage, which can distort evaluation results, and the difficulty in translating evaluation performance to real-world scenarios. They provide a systematic analysis and examples from previous studies to illustrate how these evaluation flaws can undermine confidence in LLM performance claims. The paper calls for the development of more rigorous evaluation methodologies to accurately assess the forecasting capabilities of LLMs."
                },
                "zh": {
                    "title": "è°¨æ…è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘è¢«åº”ç”¨äºé¢„æµ‹ä»»åŠ¡ï¼Œæœ‰äº›ç ”ç©¶å£°ç§°è¿™äº›ç³»ç»Ÿçš„è¡¨ç°ä¸äººç±»ç›¸å½“æˆ–æ›´å¥½ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¯„ä¼°LLMé¢„æµ‹è€…å­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬åº”å¯¹è¿™äº›ç»“è®ºä¿æŒè°¨æ…ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç”±äºå¤šç§æ—¶é—´æ³„æ¼å½¢å¼ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœéš¾ä»¥ä¿¡ä»»ï¼›äºŒæ˜¯ä»è¯„ä¼°è¡¨ç°æ¨æ–­åˆ°ç°å®ä¸–ç•Œé¢„æµ‹çš„éš¾åº¦ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æå’Œå…·ä½“å®ä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯„ä¼°ç¼ºé™·å¦‚ä½•å¼•å‘å¯¹å½“å‰å’Œæœªæ¥æ€§èƒ½å£°æ˜çš„æ‹…å¿§ï¼Œå¹¶ä¸»å¼ éœ€è¦æ›´ä¸¥æ ¼çš„è¯„ä¼°æ–¹æ³•æ¥è‡ªä¿¡åœ°è¯„ä¼°LLMçš„é¢„æµ‹èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21724",
            "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
            "url": "https://huggingface.co/papers/2505.21724",
            "abstract": "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.",
            "score": 3,
            "issue_id": 4095,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "309d90ff41ad30b0",
            "authors": [
                "Cheng Luo",
                "Jianghui Wang",
                "Bing Li",
                "Siyang Song",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "King Abdullah University of Science and Technology",
                "University of Exeter"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21724.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#audio",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniResponse - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹. OmniResponse Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Chrono-Text Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ TempoVoice Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ResponseNet Ñ 696 Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Synchronized Responses for Natural Conversations",
                    "desc": "This paper presents OmniResponse, a Multimodal Large Language Model designed to generate synchronized verbal and non-verbal responses in conversations. It introduces a new task called Online Multimodal Conversational Response Generation (OMCRG), which focuses on creating real-time feedback based on multimodal inputs from speakers. The model uses text as an intermediate step to ensure that audio and facial responses are well-coordinated. Additionally, it introduces two innovative components, Chrono-Text and TempoVoice, to enhance the quality and synchronization of the generated responses."
                },
                "zh": {
                    "title": "OmniResponseï¼šåŒæ­¥ç”Ÿæˆå¤šæ¨¡æ€å“åº”çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºåœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ï¼Œæ—¨åœ¨æ ¹æ®è¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥åœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¯­è¨€å’Œéè¯­è¨€åé¦ˆã€‚ä¸ºäº†è§£å†³ç”Ÿæˆçš„éŸ³é¢‘å’Œé¢éƒ¨ååº”ä¹‹é—´çš„åŒæ­¥é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬åˆ›æ–°æ€§åœ°å¼•å…¥äº†æ–‡æœ¬ä½œä¸ºä¸­ä»‹æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†OmniResponseï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œèƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚é€šè¿‡ä½¿ç”¨Chrono-Textå’ŒTempoVoiceç­‰æ–°ç»„ä»¶ï¼ŒOmniResponseåœ¨è¯­ä¹‰å†…å®¹ã€éŸ³è§†é¢‘åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19621",
            "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
            "url": "https://huggingface.co/papers/2505.19621",
            "abstract": "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS",
            "score": 3,
            "issue_id": 4093,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "4358fd586e320601",
            "authors": [
                "George Kour",
                "Itay Nakash",
                "Ateret Anaby-Tavor",
                "Michal Shmueli-Scheuer"
            ],
            "affiliations": [
                "IBM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19621.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#hallucinations",
                    "#multimodal",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Preference, Opinion, and Belief survey (POBs) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğº Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ LLM, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¼ĞµĞ½ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Assessing Bias in Language Models: A Call for Neutrality",
                    "desc": "This paper introduces the Preference, Opinion, and Belief survey (POBs), which evaluates the subjective biases of Large Language Models (LLMs) in various domains. It highlights that as LLMs are increasingly used in decision-making, their inherent biases can shape the advice they provide, potentially reinforcing certain viewpoints. The study assesses leading LLMs for properties like reliability and neutrality, revealing that newer models tend to exhibit greater bias and inconsistency. Additionally, it examines the impact of advanced reasoning techniques on these biases, finding only marginal improvements in performance."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»è§‚åè§",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºåå¥½ã€è§‚ç‚¹å’Œä¿¡å¿µè°ƒæŸ¥ï¼ˆPOBsï¼‰çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šã€æ–‡åŒ–ã€ä¼¦ç†å’Œä¸ªäººé¢†åŸŸçš„ä¸»è§‚å€¾å‘ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ¨¡å‹ç‰ˆæœ¬çš„æ›´æ–°ï¼Œå®ƒä»¬çš„åè§å’Œä¸ä¸€è‡´æ€§æœ‰æ‰€å¢åŠ ï¼Œè¿™å¯èƒ½å½±å“å®ƒä»¬å¯¹ç”¨æˆ·çš„å»ºè®®å’Œæ¨èã€‚é€šè¿‡å¯¹é¢†å…ˆçš„å¼€æºå’Œé—­æºLLMsè¿›è¡Œè¯„ä¼°ï¼Œè®ºæ–‡æµ‹é‡äº†æ¨¡å‹çš„å¯é æ€§ã€ä¸­ç«‹æ€§å’Œä¸€è‡´æ€§ç­‰å±æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨ç†å’Œè‡ªæˆ‘åæ€æœºåˆ¶åœ¨å…¶ä»–ä»»åŠ¡ä¸­æœ‰æ•ˆï¼Œä½†åœ¨æœ¬ç ”ç©¶é¢†åŸŸçš„æå‡æœ‰é™ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹åœ¨æŸäº›è§‚ç‚¹ä¸Šçš„åè§åŠ å‰§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01074",
            "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
            "url": "https://huggingface.co/papers/2506.01074",
            "abstract": "LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.",
            "score": 2,
            "issue_id": 4101,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "4fca9ba1a062ca80",
            "authors": [
                "Amir Hossein Kargaran",
                "Yihong Liu",
                "FranÃ§ois Yvon",
                "Hinrich SchÃ¼tze"
            ],
            "affiliations": [
                "LMU Munich & Munich Center for Machine Learning",
                "Sorbonne UniversitÃ© & CNRS, ISIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01074.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#plp",
                    "#machine_translation",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸: Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼Ñƒ, Ñ‡ĞµĞ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼Ñƒ ÑĞ·Ñ‹ĞºÑƒ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ° Ğ² Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¯Ğ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº LLM Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unraveling Language Representations in Large Language Models",
                    "desc": "This paper explores how large language models (LLMs) represent multiple programming languages (PLs) alongside English in their internal concept space. It reveals that LLMs cluster PLs closer to English, particularly in the upper layers, where distinct neuron activations occur for specific languages. The study employs a few-shot translation task across 21 PL pairs, analyzing embeddings and neuron activations to uncover structural patterns in the model's representation of PLs. The findings suggest that highly aligned PLs share similar representations and exhibit unique neuron activations, enhancing our understanding of LLMs in coding tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç¼–ç¨‹è¯­è¨€è¡¨ç¤ºç»“æ„",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆPLsï¼‰ä¸è‹±è¯­ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMsçš„æ¦‚å¿µç©ºé—´ä¸­ï¼Œç¼–ç¨‹è¯­è¨€çš„è¡¨ç¤ºæ›´æ¥è¿‘è‹±è¯­ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­é—´å±‚çš„ååŠéƒ¨åˆ†ï¼Œè‹±è¯­çš„æ ‡è®°æ¦‚ç‡è¾ƒé«˜ã€‚é€šè¿‡åˆ†æç¥ç»å…ƒæ¿€æ´»ï¼Œæˆ‘ä»¬å‘ç°ç‰¹å®šè¯­è¨€çš„ç¥ç»å…ƒä¸»è¦é›†ä¸­åœ¨åº•å±‚ï¼Œè€Œæ¯ç§ç¼–ç¨‹è¯­è¨€ç‹¬æœ‰çš„ç¥ç»å…ƒåˆ™å‡ºç°åœ¨ä¸Šå±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†LLMså¦‚ä½•åœ¨å†…éƒ¨è¡¨ç¤ºç¼–ç¨‹è¯­è¨€ï¼Œå¹¶å±•ç¤ºäº†æ¨¡å‹æ¦‚å¿µç©ºé—´ä¸­çš„ç»“æ„æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00930",
            "title": "Aligning VLM Assistants with Personalized Situated Cognition",
            "url": "https://huggingface.co/papers/2506.00930",
            "abstract": "A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",
            "score": 2,
            "issue_id": 4099,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "113b1c51e51c3619",
            "authors": [
                "Yongqi Li",
                "Shen Zhou",
                "Xiaohu Li",
                "Xin Miao",
                "Jintao Wen",
                "Mayi Xu",
                "Jianhao Chen",
                "Birong Pan",
                "Hankun Kang",
                "Yuanyuan Zhu",
                "Ming Zhong",
                "Tieyun Qian"
            ],
            "affiliations": [
                "School of Computer Science, Wuhan University, China",
                "Zhongguancun Academy, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00930.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ",
                    "desc": "PCogAlign - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PCogAlignBench, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 18 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 20 Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒÑƒĞ¼Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…, Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Aligning AI with Personalized Human Cognition",
                    "desc": "The paper introduces PCogAlign, a framework designed to create a reward model that aligns vision-language models (VLMs) with personalized situated cognition. It recognizes that individuals from diverse backgrounds have unique cognitive expectations, which can affect their interactions with VLMs. To address this, the authors utilize the sociological concept of Role-Set to categorize individuals and evaluate their actions for personalized alignment. The study includes a benchmark called PCogAlignBench, featuring 18,000 instances across 20 different Role-Sets, demonstrating the framework's effectiveness through experimental results and human evaluations."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è®¤çŸ¥å¯¹é½çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPCogAlignçš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºä¸ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥å¯¹é½çš„è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒèƒŒæ™¯çš„äººåœ¨ç›¸åŒæƒ…å¢ƒä¸‹å¯èƒ½æœ‰ä¸åŒçš„è®¤çŸ¥å’ŒæœŸæœ›ï¼Œå› æ­¤éœ€è¦é’ˆå¯¹ä¸ªä½“çš„ä¸ªæ€§åŒ–éœ€æ±‚è¿›è¡Œå¯¹é½ã€‚ä¸ºæ­¤ï¼Œä½œè€…åŸºäºç¤¾ä¼šå­¦çš„è§’è‰²é›†æ¦‚å¿µï¼Œç®€åŒ–äº†ä¸ªä½“ç‰¹å¾çš„æè¿°ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«18000ä¸ªå®ä¾‹å’Œ20ä¸ªä¸åŒè§’è‰²é›†ä¸ªä½“çš„åŸºå‡†æ•°æ®é›†PCogAlignBenchã€‚å®éªŒç»“æœå’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼ŒPCogAlignBenchçš„å¯é æ€§å’ŒPCogAlignçš„æœ‰æ•ˆæ€§ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†å¼€æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00789",
            "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems",
            "url": "https://huggingface.co/papers/2506.00789",
            "abstract": "RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.",
            "score": 2,
            "issue_id": 4102,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "5c7a672484e9a0fa",
            "authors": [
                "Yixiao Zeng",
                "Tianyu Cao",
                "Danqing Wang",
                "Xinran Zhao",
                "Zimeng Qiu",
                "Morteza Ziyadi",
                "Tongshuang Wu",
                "Lei Li"
            ],
            "affiliations": [
                "Amazon",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00789.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#graphs",
                    "#rag",
                    "#security"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RARE: Ğ¡Ñ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "RARE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG) Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑˆÑƒĞ¼Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ², ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. RARE Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing RAG Systems: Evaluating Robustness in Real-World Scenarios",
                    "desc": "The paper introduces RARE, a framework designed to evaluate the robustness of Retrieval-Augmented Generation (RAG) systems in real-world scenarios. It addresses how these systems handle noise, conflicting contexts, and rapidly changing information by using a knowledge-graph-driven benchmark. RARE includes a synthesis pipeline that generates complex question sets from a dynamic dataset of time-sensitive documents. The findings reveal that RAG systems are particularly vulnerable to perturbations, especially in multi-hop queries, highlighting the need for improved robustness in document retrieval."
                },
                "zh": {
                    "title": "è¯„ä¼°RAGç³»ç»Ÿçš„é²æ£’æ€§",
                    "desc": "RAREæ˜¯ä¸€ä¸ªè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œå™ªå£°ã€ä¸Šä¸‹æ–‡å†²çªå’Œæ—¶é—´æ•æ„Ÿæ•°æ®ä¸‹é²æ£’æ€§çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡ä¸€ä¸ªçŸ¥è¯†å›¾è°±é©±åŠ¨çš„åŸºå‡†æµ‹è¯•ï¼Œè”åˆæµ‹è¯•æŸ¥è¯¢å’Œæ–‡æ¡£çš„æ‰°åŠ¨ã€‚RAREæ„å»ºäº†ä¸€ä¸ªåŒ…å«400ä¸ªä¸“å®¶çº§æ—¶é—´æ•æ„Ÿæ–‡æ¡£å’Œ48,322ä¸ªé—®é¢˜çš„æ•°æ®é›†ï¼Œä»¥é‡åŒ–æ¨¡å‹åœ¨é¢å¯¹å˜åŒ–æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRAGç³»ç»Ÿåœ¨å¤šè·³æŸ¥è¯¢ä¸Šçš„é²æ£’æ€§æ™®éä½äºå•è·³æŸ¥è¯¢ï¼Œä¸”æ–‡æ¡£çš„é²æ£’æ€§æ˜¯æœ€è–„å¼±çš„ç¯èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00772",
            "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
            "url": "https://huggingface.co/papers/2506.00772",
            "abstract": "Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.",
            "score": 2,
            "issue_id": 4095,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "d66b2e2afe71cfd9",
            "authors": [
                "Zihang Liu",
                "Tianyu Pang",
                "Oleg Balabanov",
                "Chaoqun Yang",
                "Tianjin Huang",
                "Lu Yin",
                "Yaoqing Yang",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Dartmouth College, NH, USA",
                "Eindhoven University of Technology, the Netherlands",
                "International Computer Science Institute, CA, USA",
                "Lawrence Berkeley National Laboratory, CA, USA",
                "Tsinghua University, China",
                "University of California, Berkeley, CA, USA",
                "University of Exeter, Exeter, UK",
                "University of Oxford, Oxford, UK",
                "University of Surrey, Guildford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00772.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#low_resource",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LIFT (Low-rank Informed Sparse Fine-Tuning). LIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 5% Ğ¾Ñ‚ Ğ¸Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. LIFT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Efficient Fine-Tuning with Critical Weights",
                    "desc": "This paper introduces a method called Low-rank Informed Sparse Fine-Tuning (LIFT) that improves the efficiency and performance of large language models (LLMs) by focusing on critical weights identified through low-rank approximation. Instead of updating all parameters during fine-tuning, LIFT selectively updates only the top 5% of Principal Weights, which are determined to be the most important for reasoning tasks. This approach not only enhances reasoning capabilities but also reduces the risk of overfitting and catastrophic forgetting, common issues in full fine-tuning. The results show that LIFT outperforms traditional full fine-tuning while preserving more knowledge from the original model, making it a promising strategy for efficient model adaptation."
                },
                "zh": {
                    "title": "ä½ç§©å¾®è°ƒï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºä½ç§©çŸ¥æƒ…ç¨€ç–å¾®è°ƒï¼ˆLIFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡ä½ç§©è¿‘ä¼¼ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºå¯¹æ¨ç†è‡³å…³é‡è¦çš„æƒé‡ï¼Œç§°ä¸ºä¸»æƒé‡ï¼Œå¹¶ä»…æ›´æ–°è¿™äº›æƒé‡çš„å‰5%ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒLIFTåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶åœ¨å†…å­˜ä½¿ç”¨ä¸Šä¿æŒé«˜æ•ˆã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæºé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…è¿‡æ‹Ÿåˆå’Œç¾éš¾æ€§é—å¿˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00530",
            "title": "CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing",
            "url": "https://huggingface.co/papers/2506.00530",
            "abstract": "Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",
            "score": 2,
            "issue_id": 4097,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "4c9476ad7ef056e6",
            "authors": [
                "Tianhui Liu",
                "Jie Feng",
                "Hetian Pang",
                "Xin Zhang",
                "Tianjian Ouyang",
                "Zhiyuan Zhang",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China",
                "School of Electronic and Information Engineering, Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00530.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#survey"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "CityLens: Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ³Ğ»Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "CityLens - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLVM) Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 17 Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ¼Ğ¸Ñ€Ñƒ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 6 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹: ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°, Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ°Ñ ÑÑ€ĞµĞ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ 11 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ LLVM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "CityLens: Bridging Visual Data and Urban Socioeconomic Insights",
                    "desc": "This paper presents CityLens, a benchmark for assessing large language-vision models (LLVMs) in predicting urban socioeconomic indicators using visual data from satellite and street view images. The study creates a multi-modal dataset that includes 17 cities and covers six important domains such as economy and health, reflecting the complexity of urban environments. It defines 11 prediction tasks and employs three evaluation methods to analyze the performance of 17 advanced LLVMs. The findings indicate that while these models show potential in understanding urban data, they still face challenges in accurately predicting socioeconomic conditions, highlighting the need for further research in this area."
                },
                "zh": {
                    "title": "é€šè¿‡è§†è§‰æ•°æ®ç†è§£åŸå¸‚ç¤¾ä¼šç»æµæ¡ä»¶",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†CityLensï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€-è§†è§‰æ¨¡å‹ï¼ˆLLVMsï¼‰åœ¨ä»å«æ˜Ÿå’Œè¡—æ™¯å›¾åƒä¸­é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–å…¨çƒ17ä¸ªåŸå¸‚ï¼Œæ¶‰åŠç»æµã€æ•™è‚²ã€çŠ¯ç½ªã€äº¤é€šã€å¥åº·å’Œç¯å¢ƒç­‰6ä¸ªå…³é”®é¢†åŸŸï¼Œåæ˜ äº†åŸå¸‚ç”Ÿæ´»çš„å¤šé¢æ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å®šä¹‰äº†11ä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼šç›´æ¥åº¦é‡é¢„æµ‹ã€æ ‡å‡†åŒ–åº¦é‡ä¼°è®¡å’ŒåŸºäºç‰¹å¾çš„å›å½’ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLVMsåœ¨æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00385",
            "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
            "url": "https://huggingface.co/papers/2506.00385",
            "abstract": "MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.",
            "score": 2,
            "issue_id": 4091,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "421c283aaadfdd94",
            "authors": [
                "Yakun Song",
                "Jiawei Chen",
                "Xiaobin Zhuang",
                "Chenpeng Du",
                "Ziyang Ma",
                "Jian Wu",
                "Jian Cong",
                "Dongya Jia",
                "Zhuo Chen",
                "Yuping Wang",
                "Yuxuan Wang",
                "Xie Chen"
            ],
            "affiliations": [
                "Bytedance Inc.",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00385.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#optimization",
                    "#diffusion",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "MagiCodec: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "MagiCodec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ ĞºĞ¾Ğ´ĞµĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MagiCodec Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´ĞµĞºĞ¸ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¢Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ MagiCodec, Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞµ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¦Ğ¸Ğ¿Ñ„Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "MagiCodec: Transforming Audio for Better AI Compatibility",
                    "desc": "MagiCodec is a new audio codec that uses a Transformer model to improve how audio is represented as tokens. It focuses on enhancing the semantic meaning of these tokens while still ensuring that the audio can be reconstructed accurately. The codec employs a special training method that includes adding noise and regularization to make the tokens more expressive and useful for generative models. Tests show that MagiCodec outperforms existing codecs in both audio quality and its ability to work with other AI models."
                },
                "zh": {
                    "title": "MagiCodecï¼šæå‡éŸ³é¢‘è¯­ä¹‰è¡¨è¾¾çš„ç¼–è§£ç å™¨",
                    "desc": "MagiCodecæ˜¯ä¸€ç§åŸºäºTransformerçš„éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰æ ‡è®°çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„é‡å»ºæ•ˆæœã€‚ä¸ä¼ ç»Ÿç¼–è§£ç å™¨ä¸åŒï¼ŒMagiCodecåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†é«˜æ–¯å™ªå£°å’Œæ½œåœ¨æ­£åˆ™åŒ–ï¼Œä»¥å¢å¼ºç”Ÿæˆä»£ç çš„è¯­ä¹‰è¡¨ç°åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagiCodecåœ¨é‡å»ºè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ç¼–è§£ç å™¨ã€‚å…¶ç”Ÿæˆçš„æ ‡è®°å‘ˆç°å‡ºç±»ä¼¼Zipfåˆ†å¸ƒçš„ç‰¹å¾ï¼Œå¢å¼ºäº†ä¸åŸºäºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæ¶æ„çš„å…¼å®¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00381",
            "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG",
            "url": "https://huggingface.co/papers/2506.00381",
            "abstract": "Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.",
            "score": 2,
            "issue_id": 4101,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "78b01a3a61f68dc7",
            "authors": [
                "Siavash Shams",
                "Richard Antonello",
                "Gavin Mischler",
                "Stephan Bickel",
                "Ashesh Mehta",
                "Nima Mesgarani"
            ],
            "affiliations": [
                "Department of Electrical Engineering, Columbia University, USA",
                "The Feinstein Institutes for Medical Research, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00381.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal",
                    "#healthcare",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ»Ğ½ Ğº ÑĞ»Ğ¾Ğ²Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Neuro2Semantic - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LSTM-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ-ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (30 Ğ¼Ğ¸Ğ½ÑƒÑ‚). Neuro2Semantic Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…."
                },
                "en": {
                    "title": "Transforming Brain Signals into Natural Language",
                    "desc": "Neuro2Semantic is a new framework that translates brain signals into meaningful text. It uses a Long Short-Term Memory (LSTM) model to align neural signals from intracranial EEG with existing text representations. After alignment, a correction module generates coherent and natural language from these signals. This method is effective even with limited data, making it a significant advancement in decoding language from neural activity."
                },
                "zh": {
                    "title": "ç¥ç»ä¿¡å·é‡å»ºè¯­ä¹‰å†…å®¹çš„æ–°æ–¹æ³•",
                    "desc": "Neuro2Semantic æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»è„‘å†…ç”µæè®°å½•çš„ç¥ç»ä¿¡å·ä¸­é‡å»ºè¯­ä¹‰å†…å®¹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†åŸºäº LSTM çš„å¯¹é½æŠ€æœ¯ï¼Œå°†ç¥ç»ä¿¡å·ä¸é¢„è®­ç»ƒçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ ¡æ­£æ¨¡å—ç›´æ¥ç”Ÿæˆè‡ªç„¶çš„è¿ç»­æ–‡æœ¬ã€‚ä¸ä»¥å¾€çš„è§£ç æ–¹æ³•ç›¸æ¯”ï¼ŒNeuro2Semantic åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰ 30 åˆ†é’Ÿçš„ç¥ç»æ•°æ®ä¸‹å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºè„‘æœºæ¥å£å’Œç¥ç»è§£ç æŠ€æœ¯çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17127",
            "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts",
            "url": "https://huggingface.co/papers/2505.17127",
            "abstract": "Visual CounterFact and PvP steering vectors help interpret and control the competition between visual input and memorized world knowledge in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.",
            "score": 2,
            "issue_id": 4105,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "1c1f950796c1f608",
            "authors": [
                "Michal Golovanevsky",
                "William Rudman",
                "Michael Lepori",
                "Amir Bar",
                "Ritambhara Singh",
                "Carsten Eickhoff"
            ],
            "affiliations": [
                "Brown University",
                "Tel Aviv University",
                "University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17127.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Visual CounterFact, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¼ĞµÑ‰Ğ°ÑÑ‚ÑÑ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ PvP (Pixels Versus Priors), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Balancing Visual Input and Knowledge in MLLMs",
                    "desc": "This paper explores how Multimodal Large Language Models (MLLMs) balance visual information and memorized knowledge when making predictions. It introduces a dataset called Visual CounterFact, which presents scenarios where visual cues conflict with known facts, allowing researchers to observe how models prioritize information. The study finds that while models initially rely on memorized knowledge, they increasingly favor visual evidence as processing continues. To manage this competition, the authors propose Pixels Versus Priors (PvP) steering vectors, which can adjust model outputs to emphasize either visual input or prior knowledge effectively."
                },
                "zh": {
                    "title": "æ§åˆ¶è§†è§‰ä¸è®°å¿†çŸ¥è¯†çš„ç«äº‰",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¾“å…¥å’Œè®°å¿†çŸ¥è¯†ä¹‹é—´çš„ç«äº‰ã€‚æˆ‘ä»¬å¼•å…¥äº†Visual CounterFactæ•°æ®é›†ï¼Œé€šè¿‡è§†è§‰åäº‹å®æ¥ç›´æ¥å¯¹æŠ—ä¸–ç•ŒçŸ¥è¯†å’Œè§†è§‰ä¿¡æ¯ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„é¢„æµ‹æœ€åˆä¾èµ–äºè®°å¿†çŸ¥è¯†ï¼Œä½†åœ¨åæœŸå±‚æ¬¡ä¸­é€æ¸è½¬å‘è§†è§‰è¯æ®ã€‚ä¸ºæ§åˆ¶è¿™ç§è¡Œä¸ºï¼Œæˆ‘ä»¬æå‡ºäº†Pixels Versus Priorsï¼ˆPvPï¼‰å¼•å¯¼å‘é‡ï¼Œå¯ä»¥é€šè¿‡æ¿€æ´»çº§åˆ«å¹²é¢„æ¥è°ƒæ•´æ¨¡å‹è¾“å‡ºï¼ŒæˆåŠŸå°†å¤§éƒ¨åˆ†é¢„æµ‹ä»è®°å¿†çŸ¥è¯†è½¬å‘è§†è§‰è¾“å…¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01062",
            "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models",
            "url": "https://huggingface.co/papers/2506.01062",
            "abstract": "SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the \"lost-in-the-middle\" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.",
            "score": 1,
            "issue_id": 4104,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "32b5670dde584ad1",
            "authors": [
                "Thinh Pham",
                "Nguyen Nguyen",
                "Pratibha Zunjare",
                "Weiyuan Chen",
                "Yu-Min Tseng",
                "Tu Vu"
            ],
            "affiliations": [
                "Virginia Tech, Blacksburg, VA 24061"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01062.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SealQA: Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "SealQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Seal-0, Seal-Hard Ğ¸ LongSeal, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… SealQA. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "SealQA: Unveiling the Limits of Search-Augmented Language Models",
                    "desc": "SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks."
                },
                "zh": {
                    "title": "SealQAï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æœç´¢ä¸­çš„è¡¨ç°",
                    "desc": "SealQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¢å¼ºæœç´¢çš„è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å†²çªæˆ–å™ªå£°æœç´¢ç»“æœæ—¶çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹å®å¯»æ±‚é—®é¢˜ä¸Šã€‚è¯¥åŸºå‡†åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼šSeal-0å’ŒSeal-Hardä¸»è¦è¯„ä¼°æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ï¼Œè€ŒLongSealåˆ™æµ‹è¯•é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ–‡æ¡£æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨æ‰€æœ‰SealQAç±»å‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨Seal-0ä¸­ï¼Œå‡†ç¡®ç‡æä½ã€‚å°½ç®¡ä¸€äº›å…ˆè¿›çš„æ¨ç†æ¨¡å‹å­˜åœ¨ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹å™ªå£°æœç´¢ç»“æœæ—¶ä»ç„¶éå¸¸è„†å¼±ï¼Œä¸”å¢åŠ è®¡ç®—èµ„æºå¹¶æœªæ˜¾è‘—æé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00979",
            "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection",
            "url": "https://huggingface.co/papers/2506.00979",
            "abstract": "IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.",
            "score": 1,
            "issue_id": 4101,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "13910274f7d956aa",
            "authors": [
                "Wayne Zhang",
                "Changjiang Jiang",
                "Zhonghao Zhang",
                "Chenyang Si",
                "Fengchang Yu",
                "Wei Peng"
            ],
            "affiliations": [
                "Wuhan University",
                "Ï€3 AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00979.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#interpretability",
                    "#multimodal",
                    "#synthetic",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IVY-FAKE Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ IVY-XDETECTOR Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (AIGC). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 150 000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ AIGC Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Unifying AIGC Detection with Explainability",
                    "desc": "The paper introduces the IVY-FAKE dataset and the Ivy Explainable Detector (IVY-XDETECTOR) to improve the detection of AI-generated content (AIGC) in images and videos. Current detection methods often lack transparency and only classify content without providing explanations, which can undermine trust. IVY-FAKE offers a large-scale dataset with over 150,000 annotated samples, enhancing the interpretability of detection results through detailed reasoning. The IVY-XDETECTOR utilizes a unified vision-language model to achieve state-of-the-art performance in detecting both images and videos, addressing the limitations of existing approaches."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¯è§£é‡Šçš„ AIGC æ£€æµ‹æ–°æ¡†æ¶",
                    "desc": "IVY-FAKE æ•°æ®é›†å’Œ Ivy Explainable Detector (IVY-XDETECTOR) æ¶æ„æ—¨åœ¨è§£å†³å½“å‰ AIGC æ£€æµ‹çš„å±€é™æ€§ï¼Œæä¾›ä¸€ä¸ªç»Ÿä¸€ä¸”å¯è§£é‡Šçš„å›¾åƒå’Œè§†é¢‘æ£€æµ‹æ¡†æ¶ã€‚éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨è§†è§‰é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆçš„å›¾åƒå’Œè§†é¢‘å˜å¾—è¶Šæ¥è¶ŠçœŸå®ï¼Œè¿™å¼•å‘äº†å¯¹å†…å®¹çœŸå®æ€§çš„æ‹…å¿§ã€‚ç°æœ‰çš„ AIGC æ£€æµ‹æ–¹æ³•é€šå¸¸ä½œä¸ºé»‘ç®±äºŒå…ƒåˆ†ç±»å™¨ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”æ— æ³•åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹åŒæ—¶æ£€æµ‹å›¾åƒå’Œè§†é¢‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡å¼•å…¥ IVY-FAKE æ•°æ®é›†å’Œ IVY-XDETECTORï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ AIGC æ£€æµ‹çš„æ€§èƒ½å’Œé€æ˜åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00469",
            "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
            "url": "https://huggingface.co/papers/2506.00469",
            "abstract": "Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.",
            "score": 1,
            "issue_id": 4093,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "2ebeb941a6f4f7cc",
            "authors": [
                "Shaoxiong Ji",
                "Zihao Li",
                "Jaakko Paavola",
                "Indraneil Paul",
                "Hengyu Luo",
                "JÃ¶rg Tiedemann"
            ],
            "affiliations": [
                "Technical University of Darmstadt",
                "University of Helsinki"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00469.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#transfer_learning",
                    "#open_source",
                    "#machine_translation",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Llama3 Ğº 500 ÑĞ·Ñ‹ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ MaLA, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2500 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ EMMA-500 Llama 3 Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 7 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ° Ğ²ÑĞµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°."
                },
                "en": {
                    "title": "Boosting Multilingual Models with Bilingual Data",
                    "desc": "This paper explores how using bilingual translation data can improve the performance of the Llama3 family of models in multilingual settings. It introduces the MaLA bilingual translation corpus, which includes data from over 2,500 language pairs, to facilitate better language adaptation. The authors develop the EMMA-500 suite of models, which are continually pre-trained on a vast amount of diverse data. Their findings show that incorporating bilingual data significantly enhances language transfer, especially for languages with fewer resources."
                },
                "zh": {
                    "title": "åŒè¯­æ•°æ®åŠ©åŠ›å¤šè¯­è¨€æ¨¡å‹æå‡æ€§èƒ½",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æŒç»­é¢„è®­ç»ƒä¸­ï¼ŒåŒè¯­ç¿»è¯‘æ•°æ®çš„å…³é”®è®¾è®¡å†³ç­–ã€‚æˆ‘ä»¬æ„å»ºäº†MaLAåŒè¯­ç¿»è¯‘è¯­æ–™åº“ï¼ŒåŒ…å«2500å¤šä¸ªè¯­è¨€å¯¹çš„æ•°æ®ï¼Œä»¥æ”¯æŒLlama3æ¨¡å‹åœ¨500ç§è¯­è¨€ä¸Šçš„é€‚åº”ã€‚é€šè¿‡å¼€å‘EMMA-500 Llama 3æ¨¡å‹å¥—ä»¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨æˆ–ä¸ä½¿ç”¨åŒè¯­ç¿»è¯‘æ•°æ®çš„æŒç»­é¢„è®­ç»ƒæ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼ŒåŒè¯­æ•°æ®èƒ½å¤Ÿå¢å¼ºè¯­è¨€è¿ç§»å’Œæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºç¨€ç¼ºçš„è¯­è¨€ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24216",
            "title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation",
            "url": "https://huggingface.co/papers/2505.24216",
            "abstract": "A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM",
            "score": 1,
            "issue_id": 4102,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "4e2243b50d13c1bf",
            "authors": [
                "Prasanna Reddy Pulakurthi",
                "Majid Rabbani",
                "Jamison Heard",
                "Sohail Dianat",
                "Celso M. de Melo",
                "Raghuveer Rao"
            ],
            "affiliations": [
                "DEVCOM Army Research Laboratory, Adelphi, MD, USA",
                "Rochester Institute of Technology, Rochester, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24216.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#benchmark",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Shuffle PatchMix (SPM) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Source-Free Domain Adaptation, SFDA). SPM Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: PACS, VisDA-C Ğ¸ DomainNet-126."
                },
                "en": {
                    "title": "Enhancing Source-Free Domain Adaptation with Shuffle PatchMix",
                    "desc": "This paper presents a novel approach to Source-Free Domain Adaptation (SFDA) using a technique called Shuffle PatchMix (SPM) and a reweighting strategy. SPM enhances data augmentation by shuffling and blending image patches, creating diverse training samples that help the model generalize better. The reweighting strategy focuses on selecting reliable pseudo-labels, reducing the impact of label noise during training. The proposed methods achieve state-of-the-art performance on several benchmarks, demonstrating significant improvements in accuracy, especially on smaller datasets like PACS."
                },
                "zh": {
                    "title": "æ— æºé¢†åŸŸé€‚åº”çš„æ–°çªç ´ï¼šShuffle PatchMix",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæŠ€æœ¯Shuffle PatchMixï¼ˆSPMï¼‰å’Œé‡åŠ æƒç­–ç•¥ï¼Œä»¥æé«˜æ— æºé¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰çš„æ€§èƒ½ã€‚SPMé€šè¿‡æ‰“ä¹±å’Œæ··åˆå›¾åƒå—ç”Ÿæˆå¤šæ ·åŒ–çš„å¢å¼ºæ ·æœ¬ï¼Œè€Œé‡åŠ æƒç­–ç•¥åˆ™ä¼˜å…ˆè€ƒè™‘å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»¥å‡å°‘æ ‡ç­¾å™ªå£°çš„å½±å“ã€‚è¿™äº›æŠ€æœ¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¦‚PACSï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹è¿‡æ‹Ÿåˆå’Œä¼ªæ ‡ç­¾å™ªå£°çš„é—®é¢˜ã€‚æœ€ç»ˆï¼Œåœ¨PACSã€VisDA-Cå’ŒDomainNet-126ç­‰ä¸‰ä¸ªä¸»è¦åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22865",
            "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models",
            "url": "https://huggingface.co/papers/2505.22865",
            "abstract": "A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a 42% confusion rate.",
            "score": 1,
            "issue_id": 4101,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "0d57f1fd2f698379",
            "authors": [
                "Susan Liang",
                "Dejan Markovic",
                "Israel D. Gebru",
                "Steven Krenn",
                "Todd Keebler",
                "Jacob Sandakly",
                "Frank Yu",
                "Samuel Hassel",
                "Chenliang Xu",
                "Alexander Richard"
            ],
            "affiliations": [
                "1",
                "2"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22865.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#diffusion"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "BinauralFlow - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ U-Net Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ½ĞµĞ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ STFT/ISTFT, Ğ±Ğ°Ğ½Ğº Ğ±ÑƒÑ„ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "BinauralFlow: Real-Time, High-Quality Binaural Audio Synthesis",
                    "desc": "The paper presents BinauralFlow, a novel framework for generating high-quality binaural audio that closely resembles natural hearing. It utilizes a causal U-Net architecture and a flow matching approach to enhance the synthesis of binaural cues, room reverb, and ambient sounds. The framework is designed for streaming inference, allowing for real-time audio generation by processing only past audio frames. Evaluations show that BinauralFlow outperforms state-of-the-art methods, achieving audio quality that is nearly indistinguishable from real-world recordings."
                },
                "zh": {
                    "title": "é«˜è´¨é‡åŒè€³éŸ³é¢‘åˆæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„æµå¼åŒè€³è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œç§°ä¸ºBinauralFlowã€‚è¯¥æ¡†æ¶ä½¿ç”¨å› æœU-Netæ¶æ„å’Œè¿ç»­æ¨ç†ç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸çœŸå®å½•éŸ³å‡ ä¹æ— æ³•åŒºåˆ†çš„åŒè€³éŸ³é¢‘ã€‚æˆ‘ä»¬å°†åŒè€³æ¸²æŸ“è§†ä¸ºç”Ÿæˆé—®é¢˜ï¼Œè®¾è®¡äº†æ¡ä»¶æµåŒ¹é…æ¨¡å‹æ¥æé«˜éŸ³é¢‘æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†è¿ç»­æ¨ç†ç®¡é“ï¼Œä»¥æ”¹å–„æ¸²æŸ“çš„è¿ç»­æ€§å’Œé€Ÿåº¦ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21668",
            "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.21668",
            "abstract": "R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.",
            "score": 1,
            "issue_id": 4101,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "a1b1a393cb209a0c",
            "authors": [
                "Yongchao Chen",
                "Yueying Liu",
                "Junwei Zhou",
                "Yilun Hao",
                "Jingquan Wang",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard",
                "MIT",
                "MIT-IBM Watson AI Lab",
                "University of Illinois Urbana-Champaign",
                "University of Michigan",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21668.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#data",
                    "#dataset",
                    "#training",
                    "#rl",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°",
                    "desc": "R1-Code-Interpreter - ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° 144 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R1-CI-14B ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 37 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 44.0% Ğ´Ğ¾ 64.1%, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ GPT-4 (58.6%) Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ğ²ÑˆĞ¸ÑÑŒ Ğº GPT-4 Ñ Code Interpreter (70.9%). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Code Interpreter Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Code: R1-Code-Interpreter",
                    "desc": "R1-Code-Interpreter enhances text-only Large Language Models (LLMs) by integrating improved code generation capabilities through supervised fine-tuning and reinforcement learning. This model addresses the limitations of LLMs in tasks that require precise computation and algorithmic reasoning, allowing it to autonomously generate code queries during multi-step reasoning. The research involves fine-tuning Qwen-2.5 models on a diverse set of reasoning and planning tasks, demonstrating significant improvements in accuracy compared to previous models. The findings highlight the importance of the supervised fine-tuning stage in effectively training LLMs to leverage code for better performance across various tasks."
                },
                "zh": {
                    "title": "æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›çš„R1-Code-Interpreter",
                    "desc": "R1-Code-Interpreter æ˜¯ä¸€ç§æ‰©å±•æ–‡æœ¬æ¨¡å‹çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—å’Œç¬¦å·æ“ä½œçš„ä»»åŠ¡ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚R1-Code-Interpreter é€šè¿‡å¤šè½®çš„è®­ç»ƒï¼Œèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆä»£ç æŸ¥è¯¢ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè‡ªæˆ‘æ£€æŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20285",
            "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability",
            "url": "https://huggingface.co/papers/2505.20285",
            "abstract": "A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MaskSearch. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MaskSearch significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.",
            "score": 1,
            "issue_id": 4098,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "fb55a3a486e6e5a9",
            "authors": [
                "Weiqi Wu",
                "Xin Guan",
                "Shen Huang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jiuxin Cao",
                "Hai Zhao",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20285.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#optimization",
                    "#rag",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "MaskSearch: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MaskSearch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (Retrieval Augmented Mask Prediction). MaskSearch Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Universal Retrieval and Reasoning through MaskSearch",
                    "desc": "The paper introduces MaskSearch, a new pre-training framework designed to improve Large Language Models (LLMs) by enhancing their retrieval and reasoning abilities. It employs a Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to use search tools to predict masked spans in data, thereby gaining universal capabilities. The training process includes Supervised Fine-tuning (SFT) and Reinforcement Learning (RL), utilizing a multi-agent system and a hybrid reward system to optimize performance. The results show that MaskSearch significantly boosts the effectiveness of LLMs in open-domain multi-hop question answering tasks."
                },
                "zh": {
                    "title": "MaskSearchï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºMaskSearchï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºçš„æ©ç é¢„æµ‹ä»»åŠ¡ï¼ˆRAMPï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æœç´¢å·¥å…·å¡«è¡¥å¤§é‡é¢„è®­ç»ƒæ•°æ®ä¸­çš„æ©ç éƒ¨åˆ†ï¼Œä»è€Œè·å¾—é€šç”¨çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥ä»ç®€å•åˆ°å¤æ‚çš„å®ä¾‹ä¸­å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskSearchæ˜¾è‘—æå‡äº†åŸºäºLLMçš„æœç´¢ä»£ç†åœ¨å¼€æ”¾é¢†åŸŸå¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18128",
            "title": "Frankentext: Stitching random text fragments into long-form narratives",
            "url": "https://huggingface.co/papers/2505.18128",
            "abstract": "We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.",
            "score": 1,
            "issue_id": 4100,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "2cd0e9db501521da",
            "authors": [
                "Chau Minh Pham",
                "Jenna Russell",
                "Dzung Pham",
                "Mohit Iyyer"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18128.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#story_generation",
                    "#training"
                ],
                "emoji": "ğŸ§Ÿâ€â™‚ï¸",
                "ru": {
                    "title": "Frankentexts: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Frankentexts, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ¾Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Gemini-2.5-Pro Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑƒÑÑĞ¸Ñ Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Frankentexts: Merging Human Creativity with AI Precision",
                    "desc": "Frankentexts are a novel form of long narratives created by large language models (LLMs) that must predominantly use human-written text. This approach tests the model's ability to generate coherent stories while adhering to strict guidelines on text copying. The process involves drafting a narrative by merging human passages and revising it to meet a specified ratio of copied content. The results show that while the generated texts are often coherent and relevant, they can still be distinguished from human writing due to inconsistencies in tone and grammar, highlighting challenges in AI text detection."
                },
                "zh": {
                    "title": "Frankentextsï¼šäººæœºå…±åˆ›çš„æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é•¿ç¯‡å™äº‹æ–‡æœ¬ï¼Œç§°ä¸ºFrankentextsï¼Œè¿™äº›æ–‡æœ¬ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æç«¯çº¦æŸä¸‹ç”Ÿæˆï¼Œè¦æ±‚å¤§éƒ¨åˆ†ï¼ˆä¾‹å¦‚90%ï¼‰çš„è¯æ±‡å¿…é¡»é€å­—å¤åˆ¶è‡ªäººç±»å†™ä½œã€‚è¿™é¡¹ä»»åŠ¡å¯¹å¯æ§ç”Ÿæˆæå‡ºäº†æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹æ»¡è¶³å†™ä½œæç¤ºï¼Œæ•´åˆä¸åŒçš„æ–‡æœ¬ç‰‡æ®µï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„å™äº‹ã€‚æˆ‘ä»¬é€šè¿‡æŒ‡å¯¼æ¨¡å‹é€‰æ‹©å’Œç»„åˆäººç±»å†™ä½œçš„æ®µè½æ¥ç”Ÿæˆåˆç¨¿ï¼Œç„¶ååœ¨ä¿æŒç”¨æˆ·æŒ‡å®šçš„å¤åˆ¶æ¯”ä¾‹çš„åŒæ—¶ï¼Œè¿­ä»£ä¿®è®¢åˆç¨¿ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGemini-2.5-Proåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œ81%çš„Frankentextsè¿è´¯ä¸”100%ä¸æç¤ºç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16122",
            "title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning",
            "url": "https://huggingface.co/papers/2505.16122",
            "abstract": "Plan-and-Budget framework enhances reasoning efficiency in LLMs by allocating token budgets based on estimated sub-question complexity, improving accuracy, reducing token usage, and boosting $E^3$ metric.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the E^3 metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in E^3. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.",
            "score": 1,
            "issue_id": 4105,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "fbd65f85b45afd41",
            "authors": [
                "Junhong Lin",
                "Xinyue Zeng",
                "Jie Zhu",
                "Song Wang",
                "Julian Shun",
                "Jun Wu",
                "Dawei Zhou"
            ],
            "affiliations": [
                "MIT CSAIL",
                "Michigan State University",
                "University of Virginia",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16122.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#small_models",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Plan-and-Budget Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ $E^3$. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ»Ğ°Ğ½-Ğ¸-Ğ‘ÑĞ´Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Smart Token Allocation for Efficient Reasoning in LLMs",
                    "desc": "The paper introduces the Plan-and-Budget framework, which enhances the reasoning efficiency of Large Language Models (LLMs) by intelligently allocating token budgets based on the complexity of sub-questions. It addresses the common issue of overthinking in LLMs, where they generate excessive and irrelevant reasoning for simple queries. By employing the Bayesian Budget Allocation Model (BBAM), the framework decomposes complex queries into manageable parts and adapts token usage to improve both accuracy and computational efficiency. The results show significant improvements, including up to 70% accuracy gains and a 39% reduction in token usage, demonstrating the framework's effectiveness across various tasks and models."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œæå‡æ¨¡å‹è¡¨ç°çš„Plan-and-Budgetæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPlan-and-Budgetçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡æ ¹æ®å­é—®é¢˜çš„å¤æ‚æ€§åˆ†é…ä»¤ç‰Œé¢„ç®—ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šLLMsåœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶ä¼šå‡ºç°è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œè€ŒPlan-and-Budgetèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä»è€Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å®è¯åˆ†æï¼ŒPlan-and-Budgetåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ç¼©å°æ¨¡å‹æ€§èƒ½å·®è·çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15772",
            "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
            "url": "https://huggingface.co/papers/2505.15772",
            "abstract": "Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning.",
            "score": 1,
            "issue_id": 4095,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "6b39ad10d21b05d8",
            "authors": [
                "Yifan Cheng",
                "Ruoyi Zhang",
                "Jiatong Shi"
            ],
            "affiliations": [
                "Carnegie Mellon University, Pittsburgh, PA, USA",
                "Fish Audio, Santa Clara, CA, USA",
                "Huazhong University of Science and Technology, Wuhan, Hubei, China",
                "Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15772.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#audio",
                    "#data"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "MIKU-PAL - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ†, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. MIKU-PAL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (68,5% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MELD) Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (0,93 Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ¤Ğ»ĞµĞ¹ÑĞ° ĞºĞ°Ğ¿Ğ¿Ğ°), Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MIKU-EmoBench Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 131,2 Ñ‡Ğ°ÑĞ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 26 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Automating Emotional Speech Extraction with MIKU-PAL",
                    "desc": "This paper introduces MIKU-PAL, an automated system designed to extract emotional speech data from unlabeled video sources. It utilizes face detection and tracking, combined with a multimodal large language model, to analyze emotions effectively. The system achieves high accuracy and consistency in emotional speech annotation, outperforming traditional human methods in both cost and speed. Additionally, it provides a new dataset, MIKU-EmoBench, which includes a diverse range of emotional speech categories for further research in speech synthesis."
                },
                "zh": {
                    "title": "MIKU-PALï¼šé«˜æ•ˆä¸€è‡´çš„æƒ…æ„Ÿè¯­éŸ³æå–æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMIKU-PALçš„å…¨è‡ªåŠ¨å¤šæ¨¡æ€ç®¡é“ï¼Œç”¨äºä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­æå–é«˜ä¸€è‡´æ€§çš„æƒ…æ„Ÿè¯­éŸ³ã€‚æˆ‘ä»¬åˆ©ç”¨äººè„¸æ£€æµ‹å’Œè·Ÿè¸ªç®—æ³•ï¼Œå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIKU-PALåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„å‡†ç¡®ç‡ï¼ˆ68.5%ï¼‰ï¼Œå¹¶ä¸”ä¸€è‡´æ€§æ˜¾è‘—ä¼˜äºäººå·¥æ ‡æ³¨ï¼ˆ0.93 Fleiss kappaåˆ†æ•°ï¼‰ã€‚åŸºäºè¯¥ç³»ç»Ÿï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªç»†ç²’åº¦æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†MIKU-EmoBenchï¼ˆ131.2å°æ—¶ï¼‰ï¼Œä¸ºæƒ…æ„Ÿæ–‡æœ¬åˆ°è¯­éŸ³å’Œè§†è§‰è¯­éŸ³å…‹éš†æä¾›äº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01666",
            "title": "Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models",
            "url": "https://huggingface.co/papers/2506.01666",
            "abstract": "A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.",
            "score": 0,
            "issue_id": 4096,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "cf55396f01f6e92c",
            "authors": [
                "Florian FÃ¼rrutter",
                "Zohim Chandani",
                "Ikko Hamamura",
                "Hans J. Briegel",
                "Gorka MuÃ±oz-Gil"
            ],
            "affiliations": [
                "Institute for Theoretical Physics University of Innsbruck",
                "NVIDIA Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01666.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#diffusion",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ…ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ĞµĞ¹, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ ÑÑ…ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Quantum Circuit Compilation with Diffusion Models",
                    "desc": "This paper presents a multimodal denoising diffusion model designed to efficiently generate both the structure and continuous parameters of quantum circuits. Unlike traditional methods that rely on lengthy search algorithms and gradient-based optimizations, this model utilizes two independent diffusion processes to handle discrete gate selection and parameter prediction simultaneously. The authors benchmark the model's performance across various qubit counts and circuit complexities, demonstrating its accuracy and efficiency. Additionally, the rapid circuit generation capability allows for the creation of large datasets, which can be used to uncover new insights into quantum circuit synthesis."
                },
                "zh": {
                    "title": "é«˜æ•ˆç¼–è¯‘é‡å­ç”µè·¯çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé‡å­ç”µè·¯çš„ç»“æ„å’Œè¿ç»­å‚æ•°ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ›¿ä»£ä¼ ç»Ÿé‡å­æ“ä½œç¼–è¯‘æ–¹æ³•ã€‚å½“å‰çš„ç¼–è¯‘æ–¹æ³•è™½ç„¶èƒ½é™ä½ç¼–è¯‘è¯¯å·®ï¼Œä½†è¿è¡Œæ—¶é—´é•¿ä¸”éœ€è¦å¤šæ¬¡è°ƒç”¨é‡å­ç¡¬ä»¶æˆ–æ˜‚è´µçš„ç»å…¸æ¨¡æ‹Ÿï¼Œé™åˆ¶äº†å…¶æ‰©å±•æ€§ã€‚æ–°æå‡ºçš„æ¨¡å‹åŒæ—¶ç”Ÿæˆç”µè·¯çš„ç¦»æ•£é—¨é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„æ‰©æ•£è¿‡ç¨‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡å¯¹ä¸åŒå®éªŒçš„åŸºå‡†æµ‹è¯•ï¼Œåˆ†æäº†è¯¥æ–¹æ³•åœ¨ä¸åŒé‡å­æ¯”ç‰¹æ•°é‡ã€ç”µè·¯æ·±åº¦å’Œå‚æ•°åŒ–é—¨æ¯”ä¾‹ä¸‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00523",
            "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation",
            "url": "https://huggingface.co/papers/2506.00523",
            "abstract": "Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.",
            "score": 0,
            "issue_id": 4099,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "6f9b962d86942eda",
            "authors": [
                "Xingtong Ge",
                "Xin Zhang",
                "Tongda Xu",
                "Yi Zhang",
                "Xinjie Zhang",
                "Yan Wang",
                "Jun Zhang"
            ],
            "affiliations": [
                "Institute for AI Industry Research, Tsinghua University",
                "SenseTime Research",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00523.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ (IDA) Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (ISG) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğº Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ĞºĞ°Ğº Stable Diffusion 3.5 Ğ¸ FLUX, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing DMD for Better Convergence in Text-to-Image Models",
                    "desc": "This paper addresses the challenges of applying Distribution Matching Distillation (DMD) to large-scale text-to-image models, particularly focusing on convergence issues. The authors introduce implicit distribution alignment (IDA) to help align the generator's output with the target distribution, improving the training process. Additionally, they propose intra-segment guidance (ISG) to enhance the importance of timesteps derived from the teacher model, further aiding convergence. The resulting model, SenseFlow, demonstrates improved performance in distillation tasks for both diffusion and flow-based models."
                },
                "zh": {
                    "title": "æå‡å¤§è§„æ¨¡æ¨¡å‹æ€§èƒ½çš„è’¸é¦æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åˆ†å¸ƒåŒ¹é…è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ”¶æ•›é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†éšå¼åˆ†å¸ƒå¯¹é½ï¼ˆIDAï¼‰å’Œæ®µå†…å¼•å¯¼ï¼ˆISGï¼‰æ¥ä¼˜åŒ–ç”Ÿæˆå™¨ä¸å‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è¿™ä¸¤ç§æ–¹æ³•ï¼ŒDMDåœ¨å¤§å‹æ¨¡å‹å¦‚SD 3.5å’ŒFLUXä¸Šå®ç°äº†æ›´å¥½çš„æ”¶æ•›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹SenseFlowåœ¨è’¸é¦è¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºæ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-02.html",
    "link_next": "2025-06-04.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "02.06",
        "en": "06/02",
        "zh": "6æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.06",
        "en": "06/04",
        "zh": "6æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 28,
        "#data": 11,
        "#benchmark": 29,
        "#agents": 7,
        "#cv": 10,
        "#rl": 12,
        "#rlhf": 3,
        "#rag": 2,
        "#plp": 1,
        "#inference": 7,
        "#3d": 3,
        "#audio": 5,
        "#video": 6,
        "#multimodal": 22,
        "#math": 0,
        "#multilingual": 5,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 26,
        "#robotics": 3,
        "#agi": 4,
        "#games": 9,
        "#interpretability": 6,
        "#reasoning": 24,
        "#transfer_learning": 5,
        "#graphs": 1,
        "#ethics": 3,
        "#security": 4,
        "#optimization": 26,
        "#survey": 2,
        "#diffusion": 11,
        "#alignment": 5,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 4,
        "#leakage": 2,
        "#open_source": 13,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 5,
        "#evaluation": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æ¢è®¨äº†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜ç†µå€¼çš„è¯å¯¹æ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œä¼˜åŒ–æœ‰æ˜¾è‘—å½±å“ã€‚é€šè¿‡åˆ†æè¯çš„ç†µå€¼æ¨¡å¼ï¼Œç ”ç©¶äººå‘˜è§‚å¯Ÿåˆ°åªæœ‰å°‘é‡è¯å…·æœ‰é«˜ç†µå€¼ï¼Œè¿™äº›è¯å†³å®šäº†æ¨¡å‹çš„æ¨ç†è·¯å¾„ã€‚è¿›ä¸€æ­¥çš„è®­ç»ƒè¡¨æ˜ï¼ŒRLVRä¸»è¦è°ƒæ•´é«˜ç†µå€¼è¯çš„ç†µå€¼ã€‚é€šè¿‡ä»…æ›´æ–°é«˜ç†µå€¼è¯çš„ç­–ç•¥æ¢¯åº¦ï¼Œç ”ç©¶äººå‘˜åœ¨ä¸åŒæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÃ ntÃ o le zÄ“ngqiÃ¡ng dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLM) tuÄ«lÇ nÃ©nglÃ¬ de xÄ«n fÄngfÇ, chÄ“ngwÃ©i kÄ› yÃ nzhÃ¨ng jiÇnglÃ¬ de qiÃ¡ngzhÃ¹ xuÃ©xÃ­ (RLVR). YÃ¡njiÅ« fÄxiÃ n, gÄo shÄngzhÃ­ de cÃ­ duÃ¬ mÃ³xÃ­ng de tuÄ«lÇ xÃ¬ngnÃ©ng hÃ© yÅuhuÃ  yÇ’u xiÇnzhÃ¹ yÇngxiÇng. TÅngguÃ² fÄ“nxÄ« cÃ­ de shÄngzhÃ­ mÃ³shÃ¬, yÃ¡njiÅ« rÃ©nyuÃ¡n guÄnchÃ¡ dÃ o zhÇyÇ’u shÇoliÃ ng cÃ­ jÃ¹yÇ’u gÄo shÄngzhÃ­, zhÃ¨xiÄ“ cÃ­ juÃ©dÃ¬ngle mÃ³xÃ­ng de tuÄ«lÇ lÃ¹jÃ¬ng. JÃ¬n yÄ«bÃ¹ de xÃ¹nliÃ n biÇomÃ­ng, RLVR zhÇ”yÃ o tiÃ¡ozhÄ›ng gÄo shÄngzhÃ­ cÃ­ de shÄngzhÃ­. TÅngguÃ² jÇn gÄ“ngxÄ«n gÄo shÄngzhÃ­ cÃ­ de cÃ¨lÃ¼Ã¨ tiÄndÃ¹, yÃ¡njiÅ« rÃ©nyuÃ¡n zÃ i bÃ¹tÃ³ng mÃ³xÃ­ng shÃ ng qÇ”dÃ©le xiÇnzhÃ¹ de xÃ¬ngnÃ©ng tÃ­shÄ“ng.",
        "vocab": "[\n    {\"word\": \"æ¢è®¨\", \"pinyin\": \"tÃ n tÇo\", \"trans\": \"discuss\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"å¯éªŒè¯\", \"pinyin\": \"kÄ› yÃ n zhÃ¨ng\", \"trans\": \"verifiable\"},\n    {\"word\": \"å¥–åŠ±\", \"pinyin\": \"jiÇng lÃ¬\", \"trans\": \"reward\"},\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡ng huÃ \", \"trans\": \"reinforce\"},\n    {\"word\": \"å­¦ä¹ \", \"pinyin\": \"xuÃ© xÃ­\", \"trans\": \"learning\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discover\"},\n    {\"word\": \"ç†µå€¼\", \"pinyin\": \"shÄng zhÃ­\", \"trans\": \"entropy value\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"å½±å“\", \"pinyin\": \"yÇng xiÇng\", \"trans\": \"impact\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"æ¨¡å¼\", \"pinyin\": \"mÃ³ shÃ¬\", \"trans\": \"pattern\"},\n    {\"word\": \"è§‚å¯Ÿ\", \"pinyin\": \"guÄn chÃ¡\", \"trans\": \"observe\"},\n    {\"word\": \"å°‘é‡\", \"pinyin\": \"shÇo liÃ ng\", \"trans\": \"small amount\"},\n    {\"word\": \"å†³å®š\", \"pinyin\": \"juÃ© dÃ¬ng\", \"trans\": \"determine\"},\n    {\"word\": \"è·¯å¾„\", \"pinyin\": \"lÃ¹ jÃ¬ng\", \"trans\": \"path\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡o zhÄ›ng\", \"trans\": \"adjust\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"æ¢¯åº¦\", \"pinyin\": \"tÄ« dÃ¹\", \"trans\": \"gradient\"},\n    {\"word\": \"æ›´æ–°\", \"pinyin\": \"gÃ¨ng xÄ«n\", \"trans\": \"update\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ” dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"}\n]",
        "trans": "This article discusses a new method for enhancing the reasoning capabilities of large language models (LLMs), known as Reinforcement Learning with Verifiable Rewards (RLVR). The study found that words with high entropy values have a significant impact on the model's reasoning performance and optimization. By analyzing the entropy patterns of words, researchers observed that only a small number of words have high entropy values, and these words determine the model's reasoning path. Further training indicated that RLVR primarily adjusts the entropy values of high-entropy words. By updating the policy gradients of only high-entropy words, researchers achieved significant performance improvements across different models.",
        "update_ts": "2025-06-03 09:13"
    }
}