{
    "date": {
        "ru": "17 марта",
        "en": "March 17",
        "zh": "3月17日"
    },
    "time_utc": "2025-03-17 04:13",
    "weekday": 0,
    "issue_id": 2732,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07677",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "url": "https://huggingface.co/papers/2503.07677",
            "abstract": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.",
            "score": 28,
            "issue_id": 2730,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "913b88ac595cc8b6",
            "authors": [
                "Kwanyoung Kim",
                "Byeongsu Sim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания",
                    "desc": "Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче генерации изображений по текстовому описанию. PLADIS использует разреженное внимание для экстраполяции корреляций запрос-ключ в слое кросс-внимания во время вывода, не требуя дополнительного обучения или оценок нейронных функций. Метод хорошо сочетается с существующими техниками направленной генерации, включая модели с дистиллированным направлением. Эксперименты показывают значительное улучшение соответствия текста и изображения, а также предпочтений пользователей."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion Models with Sparse Attention",
                    "desc": "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."
                },
                "zh": {
                    "title": "PLADIS：高效提升扩散模型的稀疏注意力方法",
                    "desc": "扩散模型在生成高质量条件样本方面表现出色，尤其是使用无分类器引导（CFG）等技术。然而，现有方法通常需要额外的训练或神经功能评估（NFE），这使得它们与引导蒸馏模型不兼容。本文提出了一种新颖高效的方法，称为PLADIS，通过利用稀疏注意力来增强预训练模型（如U-Net/Transformer）。PLADIS在推理过程中利用交叉注意力层中的softmax和稀疏对应物，提升文本到图像的扩散模型的潜力，显著改善文本对齐和人类偏好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11647",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "url": "https://huggingface.co/papers/2503.11647",
            "abstract": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/",
            "score": 27,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "7e72838ea84ed904",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xiao Fu",
                "Xintao Wang",
                "Lianrui Mu",
                "Jinwen Cao",
                "Zuozhu Liu",
                "Haoji Hu",
                "Xiang Bai",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "HUST",
                "Kuaishou Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11647.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#video",
                    "#games"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Управление камерой в видео с помощью генеративных моделей",
                    "desc": "ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-to-video и механизм видео-кондиционирования для воспроизведения динамической сцены с новых ракурсов. Для обучения был создан специальный датасет синхронизированных мультикамерных видео в Unreal Engine 5. Система превосходит существующие подходы и находит применение в стабилизации, суперразрешении и аутпейнтинге видео."
                },
                "en": {
                    "title": "ReCamMaster: Mastering Camera Control in Video Generation",
                    "desc": "This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution."
                },
                "zh": {
                    "title": "重塑视频动态，掌控相机轨迹",
                    "desc": "本论文研究了在文本或图像条件下生成视频时的相机控制问题。尽管改变视频的相机轨迹很重要，但这一领域的研究仍然较少。我们提出了ReCamMaster，一个基于生成模型的视频重渲染框架，能够在新的相机轨迹下重现输入视频的动态场景。通过构建一个多相机同步视频数据集，并采用精心设计的训练策略，我们的方法在多种输入下表现出色，超越了现有的最先进技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11646",
            "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
            "url": "https://huggingface.co/papers/2503.11646",
            "abstract": "The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.",
            "score": 16,
            "issue_id": 2731,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "efdf1296bc567414",
            "authors": [
                "Siyuan Huang",
                "Yue Liao",
                "Siyuan Feng",
                "Shu Jiang",
                "Si Liu",
                "Hongsheng Li",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "Agibot",
                "Beihang University",
                "MMLab, CUHK",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11646.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#open_source",
                    "#agents",
                    "#training",
                    "#data"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Меньше данных, больше эффективности: революция в обучении роботов",
                    "desc": "Статья представляет новый подход к сбору данных для обучения роботов манипуляции - Adversarial Data Collection (ADC). ADC использует взаимодействие человека-оператора и среды в реальном времени для создания информационно насыщенных демонстраций. Эксперименты показывают, что модели, обученные на ADC-данных, достигают лучшей композиционной генерализации и устойчивости к возмущениям, чем традиционные подходы. Авторы также создают открытый набор данных ADC-Robotics для продвижения исследований в области имитационного обучения роботов."
                },
                "en": {
                    "title": "Maximizing Data Efficiency in Robotic Learning with Adversarial Collection",
                    "desc": "This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods."
                },
                "zh": {
                    "title": "对抗性数据收集：提升机器人学习效率的关键",
                    "desc": "本论文提出了一种新的数据收集方法，称为对抗性数据收集（ADC），旨在提高机器人操作的效率。通过实时的人机交互，ADC能够在动态环境中收集高信息密度的演示数据，从而减少对大规模数据集的依赖。实验表明，使用ADC训练的模型在面对未见任务指令时表现出更好的组合泛化能力和对环境干扰的鲁棒性。最终，ADC方法显著提高了机器人学习的实用性，展示了战略性数据获取的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11224",
            "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
            "url": "https://huggingface.co/papers/2503.11224",
            "abstract": "State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.",
            "score": 10,
            "issue_id": 2731,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "fb4219d497e59f64",
            "authors": [
                "Xingtai Lv",
                "Youbang Sun",
                "Kaiyan Zhang",
                "Shang Qu",
                "Xuekai Zhu",
                "Yuchen Fan",
                "Yi Wu",
                "Ermo Hua",
                "Xinwei Long",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University",
                "Robotics Institute, Carnegie Mellon University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11224.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#survey",
                    "#math",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SSM: Эффективная альтернатива трансформерам для обработки последовательностей",
                    "desc": "Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучении. SSM показывают сравнимую производительность с трансформерами, но более эффективны для задач с последовательными данными и длинным контекстом. В статье рассматриваются теоретические основы, математические формулировки и применения SSM. Авторы выделяют три основных типа SSM: оригинальные, структурированные (например, S4) и селективные (например, Mamba)."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of State Space Models",
                    "desc": "State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications."
                },
                "zh": {
                    "title": "状态空间模型：高效处理序列数据的新选择",
                    "desc": "状态空间模型（SSMs）作为一种有前景的替代方案，逐渐受到关注，尤其是在处理序列数据或长上下文任务时表现优异。与流行的变换器模型相比，SSMs在效率上有显著提升，同时在性能上也能与之媲美。本文对SSMs进行了系统的概述，包括其理论动机、数学公式、与现有模型的比较以及各种应用。我们将SSM系列分为三个主要部分，详细介绍了原始SSM、结构化SSM（如S4）和选择性SSM（如Mamba），并强调了提高SSM有效性和效率的关键技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11069",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "url": "https://huggingface.co/papers/2503.11069",
            "abstract": "Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.",
            "score": 8,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "29e714954ed20978",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям",
                    "desc": "Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Авторы анализируют различия в архитектуре, разработке и взаимодействии с пользователем для обоих подходов. Они предлагают критерии выбора и описывают сценарии, где гибридные решения могут быть эффективны. Исследование показывает, что инновации в автоматизации на основе LLM стирают границы между этими парадигмами."
                },
                "en": {
                    "title": "Bridging the Gap: API and GUI LLM Agents Unite",
                    "desc": "This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents."
                },
                "zh": {
                    "title": "API与GUI代理的比较与融合之路",
                    "desc": "大型语言模型（LLMs）已经从简单的文本生成发展到能够将自然语言命令直接转化为实际操作的软件代理。本文首次全面比较了基于API的LLM代理和基于GUI的LLM代理，分析了它们在架构复杂性、开发工作流程和用户交互模型上的显著差异。我们探讨了关键维度，并强调了混合方法在利用两者互补优势方面的场景。最终，我们指出LLM驱动的自动化创新将模糊API和GUI代理之间的界限，为各种实际应用提供更灵活、适应性强的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11514",
            "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2503.11514",
            "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.",
            "score": 8,
            "issue_id": 2730,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "d31bf6f9bd4bc86b",
            "authors": [
                "Pengxin Guo",
                "Runxi Wang",
                "Shuang Zeng",
                "Jinjing Zhu",
                "Haoning Jiang",
                "Yanran Wang",
                "Yuyin Zhou",
                "Feifei Wang",
                "Hui Xiong",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA",
                "Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA",
                "Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China",
                "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
                "Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China",
                "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China",
                "School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China",
                "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11514.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#benchmark",
                    "#security",
                    "#survey",
                    "#healthcare",
                    "#data"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита приватности в федеративном обучении: анализ атак с инверсией градиента",
                    "desc": "Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обучения (FL). Авторы классифицируют существующие методы GIA на три типа: оптимизационные, генеративные и аналитические. Проводится комплексная оценка эффективности и практичности каждого типа атак в FL. Исследование показывает, что оптимизационные GIA являются наиболее практичными, несмотря на их неудовлетворительную производительность."
                },
                "en": {
                    "title": "Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks",
                    "desc": "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."
                },
                "zh": {
                    "title": "提升联邦学习隐私保护的防御策略",
                    "desc": "联邦学习（FL）是一种保护隐私的协作模型训练方法，不需要共享原始数据。然而，最近的研究表明，通过共享梯度信息，私密信息仍然可能被泄露，并受到梯度反演攻击（GIA）的威胁。本文对现有的GIA方法进行了系统的回顾和分类，并分析了三种类型的GIA在FL中的表现和局限性。最后，我们提出了一个三阶段的防御方案，以帮助用户在设计FL框架时更好地保护隐私。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10772",
            "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
            "url": "https://huggingface.co/papers/2503.10772",
            "abstract": "Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.",
            "score": 5,
            "issue_id": 2731,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "548255900cd1ec21",
            "authors": [
                "Ju He",
                "Qihang Yu",
                "Qihao Liu",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10772.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "FlowTok: эффективный переход между текстом и изображением через токены",
                    "desc": "FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования потоков для прямого перехода между модальностями текста и изображения в общем латентном пространстве. Ключевая идея заключается в кодировании изображений в компактное одномерное токенное представление, что значительно уменьшает размер латентного пространства. Это позволяет упростить архитектуру, сократить потребление памяти и ускорить обучение и генерацию по сравнению с существующими методами."
                },
                "en": {
                    "title": "FlowTok: Simplifying Cross-Modality Generation with 1D Tokens",
                    "desc": "This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa."
                },
                "zh": {
                    "title": "FlowTok：简化跨模态生成的高效框架",
                    "desc": "这篇论文探讨了跨模态生成中的不同模态之间的桥接问题。传统方法将文本模态视为引导信号，逐步引导去噪过程，而我们提出了一种更简单的方法，通过流匹配直接在文本和图像模态之间演变。我们引入了FlowTok框架，将图像编码为紧凑的1D标记表示，从而在共享潜在空间中流动，显著减少了潜在空间的大小。FlowTok不仅提高了内存效率和采样速度，还在图像到文本生成方面表现出色，性能与最先进的模型相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09279",
            "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
            "url": "https://huggingface.co/papers/2503.09279",
            "abstract": "Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.",
            "score": 4,
            "issue_id": 2730,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "edf6712b564fd37a",
            "authors": [
                "Luozheng Qin",
                "Zhiyu Tan",
                "Mengping Yang",
                "Xiaomeng Yang",
                "Hao Li"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#alignment"
                ],
                "emoji": "🦜",
                "ru": {
                    "title": "Cockatiel: Новый стандарт в детальном описании видео",
                    "desc": "Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэтапный процесс обучения, который объединяет синтетические и человеко-ориентированные данные для улучшения производительности. Метод включает отбор высококачественных синтетических подписей, обучение большой модели Cockatiel-13B и ее дистилляцию в меньшую Cockatiel-8B. Эксперименты показали, что Cockatiel превосходит существующие методы по метрике VDCSCORE и человеческим предпочтениям."
                },
                "en": {
                    "title": "Bridging Vision and Language with Cockatiel for Enhanced Video Captioning",
                    "desc": "This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations."
                },
                "zh": {
                    "title": "提升视频描述的智能化与人性化",
                    "desc": "视频详细描述（VDC）是连接视觉和语言的重要任务，能够对复杂视频内容进行细致的描述。本文首先对当前最先进的方法进行了全面评估，并系统地识别出两个关键限制：对特定描述方面的偏见能力和与人类偏好的不一致。为了解决这些问题，我们提出了Cockatiel，这是一种新颖的三阶段训练流程，结合了合成和人类对齐的训练，以提高VDC性能。通过大量的定量和定性实验，我们的方法在VDCSCORE上设定了新的最先进性能，并在与人类偏好的比较中大幅超越了领先的替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10632",
            "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
            "url": "https://huggingface.co/papers/2503.10632",
            "abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt",
            "score": 3,
            "issue_id": 2731,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "46504216bbce5b86",
            "authors": [
                "Subhajit Maity",
                "Killian Hitsman",
                "Xin Li",
                "Aritra Dutta"
            ],
            "affiliations": [
                "Department of Computer Science, University of Central Florida, Orlando, FL, USA",
                "Department of Mathematics, University of Central Florida, Orlando, FL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10632.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers",
                    "desc": "Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt) для моделей Vision Transformer. Авторы разработали модульную версию KArAt на основе преобразования Фурье, которая показывает сопоставимую или превосходящую производительность по сравнению с обычными ViT на нескольких наборах данных. В работе проводится глубокий анализ свойств новой архитектуры, включая ландшафт функции потерь, распределение весов и визуализацию внимания. Исследование призывает сообщество изучать сети Колмогорова-Арнольда в сочетании с передовыми архитектурами глубокого обучения."
                },
                "en": {
                    "title": "Unlocking Complex Relationships with Learnable Activations in Vision Transformers",
                    "desc": "Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency."
                },
                "zh": {
                    "title": "探索可学习激活函数的潜力",
                    "desc": "Kolmogorov-Arnold网络（KANs）是一种创新的可学习激活函数，能够捕捉数据中的复杂关系。尽管KANs在一维函数的符号表示和持续学习中表现出色，但在视觉等多种机器学习任务中的有效性仍然存在疑问。本文首次为普通的视觉变换器（ViTs）设计了一种通用的可学习Kolmogorov-Arnold注意力（KArAt），并提出了更模块化的Fourier-KArAt版本。实验结果表明，Fourier-KArAt及其变体在CIFAR-10、CIFAR-100和ImageNet-1K数据集上表现优于或与ViT相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05689",
            "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2503.05689",
            "abstract": "We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.",
            "score": 2,
            "issue_id": 2731,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 марта",
                "en": "March 7",
                "zh": "3月7日"
            },
            "hash": "eef61e2f2b4c0760",
            "authors": [
                "Zebin Xing",
                "Xingyu Zhang",
                "Yang Hu",
                "Bo Jiang",
                "Tong He",
                "Qian Zhang",
                "Xiaoxiao Long",
                "Wei Yin"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Huazhong University of Science & Technology",
                "Nanjing University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05689.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "GoalFlow: Точное планирование траекторий для автономного вождения",
                    "desc": "GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает проблему расхождения траекторий, вводя целевую точку и механизм оценки для выбора наиболее подходящей. GoalFlow использует эффективный генеративный метод Flow Matching для создания мультимодальных траекторий. Экспериментальные результаты показывают, что GoalFlow достигает передового уровня производительности, обеспечивая надежные мультимодальные траектории для автономного вождения."
                },
                "en": {
                    "title": "GoalFlow: Driving the Future with High-Quality Multimodal Trajectories",
                    "desc": "GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps."
                },
                "zh": {
                    "title": "GoalFlow：高质量多模态轨迹生成的创新方法",
                    "desc": "我们提出了GoalFlow，这是一种端到端的自动驾驶方法，用于生成高质量的多模态轨迹。在自动驾驶场景中，通常没有单一合适的轨迹，最近的方法越来越关注多模态轨迹分布的建模。为了克服轨迹选择的复杂性和轨迹质量下降的问题，GoalFlow通过引入目标点来有效约束生成过程，从而生成高质量的多模态轨迹。我们的实验结果表明，GoalFlow在NavsimDauner2024_navsim上实现了最先进的性能，提供了稳健的多模态轨迹。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10970",
            "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
            "url": "https://huggingface.co/papers/2503.10970",
            "abstract": "Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.",
            "score": 1,
            "issue_id": 2732,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "b7a03e6b34c3c0de",
            "authors": [
                "Shanghua Gao",
                "Richard Zhu",
                "Zhenglun Kong",
                "Ayush Noori",
                "Xiaorui Su",
                "Curtis Ginder",
                "Theodoros Tsiligkaridis",
                "Marinka Zitnik"
            ],
            "affiliations": [
                "Broad Institute of MIT and Harvard, Cambridge, MA",
                "Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA",
                "Department of Biomedical Informatics, Harvard Medical School, Boston, MA",
                "Harvard Data Science Initiative, Cambridge, MA",
                "Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA",
                "MIT Lincoln Laboratory, Lexington, MA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10970.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#healthcare",
                    "#science",
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "💊",
                "ru": {
                    "title": "TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии",
                    "desc": "TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэтапное рассуждение и извлечение биомедицинских знаний в реальном времени. Он анализирует взаимодействия лекарств, противопоказания и индивидуальные стратегии лечения, используя набор из 211 инструментов. TxAgent превосходит ведущие языковые модели и другие агенты в пяти новых бенчмарках, охватывающих 3168 задач по рассуждению о лекарствах и 456 персонализированных сценариев лечения. Интегрируя многоэтапный вывод, актуальные знания и инструментальное принятие решений, TxAgent обеспечивает соответствие рекомендаций клиническим руководствам и реальным данным."
                },
                "en": {
                    "title": "TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning",
                    "desc": "The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence."
                },
                "zh": {
                    "title": "个性化治疗的智能助手TxAgent",
                    "desc": "精准治疗需要多模态自适应模型来生成个性化的治疗建议。我们介绍了TxAgent，这是一种利用多步推理和实时生物医学知识检索的人工智能代理，能够分析药物相互作用、禁忌症和患者特定的治疗策略。TxAgent在分子、药代动力学和临床层面评估药物相互作用，并根据患者的合并症和同时用药识别禁忌症，量身定制治疗策略。通过整合多步推理、实时知识基础和工具辅助决策，TxAgent确保治疗建议符合既定的临床指南和现实世界证据，从而降低不良事件的风险，改善治疗决策。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06674",
            "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
            "url": "https://huggingface.co/papers/2503.06674",
            "abstract": "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
            "score": 1,
            "issue_id": 2732,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 марта",
                "en": "March 9",
                "zh": "3月9日"
            },
            "hash": "1ce5d8eb2086abfc",
            "authors": [
                "Yihong Luo",
                "Tianyang Hu",
                "Jiacheng Sun",
                "Yujun Cai",
                "Jing Tang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06674.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#cv",
                    "#video"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "TDM: Революция в ускорении диффузионных моделей",
                    "desc": "Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TDM объединяет преимущества методов сопоставления распределений и траекторий, позволяя создавать высококачественные изображения за меньшее количество шагов. Метод вводит безданную цель дистилляции оценки и цель, учитывающую количество шагов сэмплирования, что обеспечивает гибкость и эффективность. TDM превосходит существующие методы на различных архитектурах, таких как SDXL и PixArt-alpha, значительно сокращая время обучения и улучшая качество генерации."
                },
                "en": {
                    "title": "Efficient Few-Step Diffusion with TDM",
                    "desc": "This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation."
                },
                "zh": {
                    "title": "提升扩散模型采样效率的创新方法",
                    "desc": "加速扩散模型采样对于高效的AIGC部署至关重要。本文提出了一种新的学习少步扩散模型的方法，称为轨迹分布匹配（TDM），它结合了分布匹配和轨迹匹配的优点。通过引入无数据的分数蒸馏目标，我们的模型能够在不同采样步骤之间解耦学习目标，从而实现更灵活的采样。实验结果表明，TDM在多个基准上超越了现有方法，显著提高了图像质量并降低了训练成本。"
                }
            }
        }
    ],
    "link_prev": "2025-03-14.html",
    "link_next": "2025-03-18.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3月14日"
    },
    "short_date_next": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3月18日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 4,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 8,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 3,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了正则化层在现代神经网络中的作用。研究发现，即使不使用正则化层，Transformers也能通过一种简单的技术达到相同或更好的性能。这种技术叫做Dynamic Tanh (DyT)，是一种元素级操作。DyT的灵感来自于层正则化在Transformers中常常产生类似tanh的S形输入-输出映射。通过使用DyT，Transformers可以在没有正则化的情况下匹敌或超越有正则化的性能，通常不需要超参数调整。",
        "title": "Transformers without Normalization",
        "pinyin": "这篇文章讨论了正则化层在现代神经网络中的作用。研究发现，即使不使用正则化层，Transformers也能通过一种简单的技术达到相同或更好的性能。这种技术叫做Dynamic Tanh (DyT)，是一种元素级操作。DyT的灵感来自于层正则化在Transformers中常常产生类似tanh的S形输入-输出映射。通过使用DyT，Transformers可以在没有正则化的情况下匹敌或超越有正则化的性能，通常不需要超参数调整。\n\nZhè piān wénzhāng tǎolùn le zhèngzéhuà céng zài xiàndài shénjīng wǎngluò zhōng de zuòyòng. Yánjiū fāxiàn, jíshǐ bù shǐyòng zhèngzéhuà céng, Transformers yě néng tōngguò yīzhǒng jiǎndān de jìshù dá dào xiāngtóng huò gèng hǎo de xíngnéng. Zhè zhǒng jìshù jiàozuò Dynamic Tanh (DyT), shì yīzhǒng yuánsù jí cǎozuò. DyT de línggǎn láizìyú céng zhèngzéhuà zài Transformers zhōng chángcháng chǎnshēng lèisì tanh de S xíng shūrù-shūchū yíngshè. Tōngguò shǐyòng DyT, Transformers kěyǐ zài méiyǒu zhèngzéhuà de qíngkuàng xià pǐdí huò chāoyué yǒu zhèngzéhuà de xíngnéng, tōngcháng bù xūyào chāocānshù tiáozhěng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"正则化\", \"pinyin\": \"zhèng zé huà\", \"trans\": \"regularization\"},\n    {\"word\": \"现代\", \"pinyin\": \"xiàn dài\", \"trans\": \"modern\"},\n    {\"word\": \"神经网络\", \"pinyin\": \"shén jīng wǎng luò\", \"trans\": \"neural network\"},\n    {\"word\": \"作用\", \"pinyin\": \"zuò yòng\", \"trans\": \"role\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"即使\", \"pinyin\": \"jí shǐ\", \"trans\": \"even if\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"Transformers\", \"pinyin\": \"Tèi huàn fā mǔ\", \"trans\": \"Transformers\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"技术\", \"pinyin\": \"jì shù\", \"trans\": \"technique\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"相同\", \"pinyin\": \"xiāng tóng\", \"trans\": \"same\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"元素级\", \"pinyin\": \"yuán sù jí\", \"trans\": \"element-wise\"},\n    {\"word\": \"操作\", \"pinyin\": \"cāo zuò\", \"trans\": \"operation\"},\n    {\"word\": \"灵感\", \"pinyin\": \"líng gǎn\", \"trans\": \"inspiration\"},\n    {\"word\": \"层\", \"pinyin\": \"céng\", \"trans\": \"layer\"},\n    {\"word\": \"常常\", \"pinyin\": \"cháng cháng\", \"trans\": \"often\"},\n    {\"word\": \"产生\", \"pinyin\": \"chǎn shēng\", \"trans\": \"generate\"},\n    {\"word\": \"类似\", \"pinyin\": \"lèi sì\", \"trans\": \"similar\"},\n    {\"word\": \"tanh\", \"pinyin\": \"tǎn h\", \"trans\": \"tanh\"},\n    {\"word\": \"S形\", \"pinyin\": \"S xíng\", \"trans\": \"S-shaped\"},\n    {\"word\": \"输入-输出\", \"pinyin\": \"shū rù - shū chū\", \"trans\": \"input-output\"},\n    {\"word\": \"映射\", \"pinyin\": \"yìng shè\", \"trans\": \"mapping\"},\n    {\"word\": \"匹敌\", \"pinyin\": \"pǐ dí\", \"trans\": \"match\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"超参数\", \"pinyin\": \"chāo cān shù\", \"trans\": \"hyperparameter\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"}\n]",
        "trans": "This article discusses the role of regularization layers in modern neural networks. Research has found that even without using regularization layers, Transformers can achieve the same or better performance through a simple technique called Dynamic Tanh (DyT), which is an element-wise operation. The inspiration for DyT comes from the observation that layer regularization in Transformers often produces tanh-like S-shaped input-output mappings. By using DyT, Transformers can match or exceed the performance of regularized models without the need for hyperparameter tuning.",
        "update_ts": "2025-03-16 12:40"
    }
}