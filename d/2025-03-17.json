{
    "date": {
        "ru": "17 марта",
        "en": "March 17",
        "zh": "3月17日"
    },
    "time_utc": "2025-03-17 02:21",
    "weekday": 0,
    "issue_id": 2730,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07677",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "url": "https://huggingface.co/papers/2503.07677",
            "abstract": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.",
            "score": 6,
            "issue_id": 2730,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "913b88ac595cc8b6",
            "authors": [
                "Kwanyoung Kim",
                "Byeongsu Sim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания",
                    "desc": "Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче генерации изображений по текстовому описанию. PLADIS использует разреженное внимание для экстраполяции корреляций запрос-ключ в слое кросс-внимания во время вывода, не требуя дополнительного обучения или оценок нейронных функций. Метод хорошо сочетается с существующими техниками направленной генерации, включая модели с дистиллированным направлением. Эксперименты показывают значительное улучшение соответствия текста и изображения, а также предпочтений пользователей."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion Models with Sparse Attention",
                    "desc": "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."
                },
                "zh": {
                    "title": "PLADIS：高效提升扩散模型的稀疏注意力方法",
                    "desc": "扩散模型在生成高质量条件样本方面表现出色，尤其是使用无分类器引导（CFG）等技术。然而，现有方法通常需要额外的训练或神经功能评估（NFE），这使得它们与引导蒸馏模型不兼容。本文提出了一种新颖高效的方法，称为PLADIS，通过利用稀疏注意力来增强预训练模型（如U-Net/Transformer）。PLADIS在推理过程中利用交叉注意力层中的softmax和稀疏对应物，提升文本到图像的扩散模型的潜力，显著改善文本对齐和人类偏好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11514",
            "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2503.11514",
            "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.",
            "score": 4,
            "issue_id": 2730,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "d31bf6f9bd4bc86b",
            "authors": [
                "Pengxin Guo",
                "Runxi Wang",
                "Shuang Zeng",
                "Jinjing Zhu",
                "Haoning Jiang",
                "Yanran Wang",
                "Yuyin Zhou",
                "Feifei Wang",
                "Hui Xiong",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA",
                "Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA",
                "Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China",
                "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
                "Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China",
                "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China",
                "School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China",
                "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11514.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#benchmark",
                    "#security",
                    "#survey",
                    "#healthcare",
                    "#data"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита приватности в федеративном обучении: анализ атак с инверсией градиента",
                    "desc": "Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обучения (FL). Авторы классифицируют существующие методы GIA на три типа: оптимизационные, генеративные и аналитические. Проводится комплексная оценка эффективности и практичности каждого типа атак в FL. Исследование показывает, что оптимизационные GIA являются наиболее практичными, несмотря на их неудовлетворительную производительность."
                },
                "en": {
                    "title": "Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks",
                    "desc": "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."
                },
                "zh": {
                    "title": "提升联邦学习隐私保护的防御策略",
                    "desc": "联邦学习（FL）是一种保护隐私的协作模型训练方法，不需要共享原始数据。然而，最近的研究表明，通过共享梯度信息，私密信息仍然可能被泄露，并受到梯度反演攻击（GIA）的威胁。本文对现有的GIA方法进行了系统的回顾和分类，并分析了三种类型的GIA在FL中的表现和局限性。最后，我们提出了一个三阶段的防御方案，以帮助用户在设计FL框架时更好地保护隐私。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11647",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "url": "https://huggingface.co/papers/2503.11647",
            "abstract": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "7e72838ea84ed904",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xiao Fu",
                "Xintao Wang",
                "Lianrui Mu",
                "Jinwen Cao",
                "Zuozhu Liu",
                "Haoji Hu",
                "Xiang Bai",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "HUST",
                "Kuaishou Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11647.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#video",
                    "#games"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Управление камерой в видео с помощью генеративных моделей",
                    "desc": "ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-to-video и механизм видео-кондиционирования для воспроизведения динамической сцены с новых ракурсов. Для обучения был создан специальный датасет синхронизированных мультикамерных видео в Unreal Engine 5. Система превосходит существующие подходы и находит применение в стабилизации, суперразрешении и аутпейнтинге видео."
                },
                "en": {
                    "title": "ReCamMaster: Mastering Camera Control in Video Generation",
                    "desc": "This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution."
                },
                "zh": {
                    "title": "重塑视频动态，掌控相机轨迹",
                    "desc": "本论文研究了在文本或图像条件下生成视频时的相机控制问题。尽管改变视频的相机轨迹很重要，但这一领域的研究仍然较少。我们提出了ReCamMaster，一个基于生成模型的视频重渲染框架，能够在新的相机轨迹下重现输入视频的动态场景。通过构建一个多相机同步视频数据集，并采用精心设计的训练策略，我们的方法在多种输入下表现出色，超越了现有的最先进技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11069",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "url": "https://huggingface.co/papers/2503.11069",
            "abstract": "Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "29e714954ed20978",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям",
                    "desc": "Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Авторы анализируют различия в архитектуре, разработке и взаимодействии с пользователем для обоих подходов. Они предлагают критерии выбора и описывают сценарии, где гибридные решения могут быть эффективны. Исследование показывает, что инновации в автоматизации на основе LLM стирают границы между этими парадигмами."
                },
                "en": {
                    "title": "Bridging the Gap: API and GUI LLM Agents Unite",
                    "desc": "This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents."
                },
                "zh": {
                    "title": "API与GUI代理的比较与融合之路",
                    "desc": "大型语言模型（LLMs）已经从简单的文本生成发展到能够将自然语言命令直接转化为实际操作的软件代理。本文首次全面比较了基于API的LLM代理和基于GUI的LLM代理，分析了它们在架构复杂性、开发工作流程和用户交互模型上的显著差异。我们探讨了关键维度，并强调了混合方法在利用两者互补优势方面的场景。最终，我们指出LLM驱动的自动化创新将模糊API和GUI代理之间的界限，为各种实际应用提供更灵活、适应性强的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09279",
            "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
            "url": "https://huggingface.co/papers/2503.09279",
            "abstract": "Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "edf6712b564fd37a",
            "authors": [
                "Luozheng Qin",
                "Zhiyu Tan",
                "Mengping Yang",
                "Xiaomeng Yang",
                "Hao Li"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#alignment"
                ],
                "emoji": "🦜",
                "ru": {
                    "title": "Cockatiel: Новый стандарт в детальном описании видео",
                    "desc": "Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэтапный процесс обучения, который объединяет синтетические и человеко-ориентированные данные для улучшения производительности. Метод включает отбор высококачественных синтетических подписей, обучение большой модели Cockatiel-13B и ее дистилляцию в меньшую Cockatiel-8B. Эксперименты показали, что Cockatiel превосходит существующие методы по метрике VDCSCORE и человеческим предпочтениям."
                },
                "en": {
                    "title": "Bridging Vision and Language with Cockatiel for Enhanced Video Captioning",
                    "desc": "This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations."
                },
                "zh": {
                    "title": "提升视频描述的智能化与人性化",
                    "desc": "视频详细描述（VDC）是连接视觉和语言的重要任务，能够对复杂视频内容进行细致的描述。本文首先对当前最先进的方法进行了全面评估，并系统地识别出两个关键限制：对特定描述方面的偏见能力和与人类偏好的不一致。为了解决这些问题，我们提出了Cockatiel，这是一种新颖的三阶段训练流程，结合了合成和人类对齐的训练，以提高VDC性能。通过大量的定量和定性实验，我们的方法在VDCSCORE上设定了新的最先进性能，并在与人类偏好的比较中大幅超越了领先的替代方案。"
                }
            }
        }
    ],
    "link_prev": "2025-03-14.html",
    "link_next": "2025-03-18.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3月14日"
    },
    "short_date_next": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3月18日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了正则化层在现代神经网络中的作用。研究发现，即使不使用正则化层，Transformers也能通过一种简单的技术达到相同或更好的性能。这种技术叫做Dynamic Tanh (DyT)，是一种元素级操作。DyT的灵感来自于层正则化在Transformers中常常产生类似tanh的S形输入-输出映射。通过使用DyT，Transformers可以在没有正则化的情况下匹敌或超越有正则化的性能，通常不需要超参数调整。",
        "title": "Transformers without Normalization",
        "pinyin": "这篇文章讨论了正则化层在现代神经网络中的作用。研究发现，即使不使用正则化层，Transformers也能通过一种简单的技术达到相同或更好的性能。这种技术叫做Dynamic Tanh (DyT)，是一种元素级操作。DyT的灵感来自于层正则化在Transformers中常常产生类似tanh的S形输入-输出映射。通过使用DyT，Transformers可以在没有正则化的情况下匹敌或超越有正则化的性能，通常不需要超参数调整。\n\nZhè piān wénzhāng tǎolùn le zhèngzéhuà céng zài xiàndài shénjīng wǎngluò zhōng de zuòyòng. Yánjiū fāxiàn, jíshǐ bù shǐyòng zhèngzéhuà céng, Transformers yě néng tōngguò yīzhǒng jiǎndān de jìshù dá dào xiāngtóng huò gèng hǎo de xíngnéng. Zhè zhǒng jìshù jiàozuò Dynamic Tanh (DyT), shì yīzhǒng yuánsù jí cǎozuò. DyT de línggǎn láizìyú céng zhèngzéhuà zài Transformers zhōng chángcháng chǎnshēng lèisì tanh de S xíng shūrù-shūchū yíngshè. Tōngguò shǐyòng DyT, Transformers kěyǐ zài méiyǒu zhèngzéhuà de qíngkuàng xià pǐdí huò chāoyué yǒu zhèngzéhuà de xíngnéng, tōngcháng bù xūyào chāocānshù tiáozhěng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"正则化\", \"pinyin\": \"zhèng zé huà\", \"trans\": \"regularization\"},\n    {\"word\": \"现代\", \"pinyin\": \"xiàn dài\", \"trans\": \"modern\"},\n    {\"word\": \"神经网络\", \"pinyin\": \"shén jīng wǎng luò\", \"trans\": \"neural network\"},\n    {\"word\": \"作用\", \"pinyin\": \"zuò yòng\", \"trans\": \"role\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"即使\", \"pinyin\": \"jí shǐ\", \"trans\": \"even if\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"Transformers\", \"pinyin\": \"Tèi huàn fā mǔ\", \"trans\": \"Transformers\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"技术\", \"pinyin\": \"jì shù\", \"trans\": \"technique\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"相同\", \"pinyin\": \"xiāng tóng\", \"trans\": \"same\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"元素级\", \"pinyin\": \"yuán sù jí\", \"trans\": \"element-wise\"},\n    {\"word\": \"操作\", \"pinyin\": \"cāo zuò\", \"trans\": \"operation\"},\n    {\"word\": \"灵感\", \"pinyin\": \"líng gǎn\", \"trans\": \"inspiration\"},\n    {\"word\": \"层\", \"pinyin\": \"céng\", \"trans\": \"layer\"},\n    {\"word\": \"常常\", \"pinyin\": \"cháng cháng\", \"trans\": \"often\"},\n    {\"word\": \"产生\", \"pinyin\": \"chǎn shēng\", \"trans\": \"generate\"},\n    {\"word\": \"类似\", \"pinyin\": \"lèi sì\", \"trans\": \"similar\"},\n    {\"word\": \"tanh\", \"pinyin\": \"tǎn h\", \"trans\": \"tanh\"},\n    {\"word\": \"S形\", \"pinyin\": \"S xíng\", \"trans\": \"S-shaped\"},\n    {\"word\": \"输入-输出\", \"pinyin\": \"shū rù - shū chū\", \"trans\": \"input-output\"},\n    {\"word\": \"映射\", \"pinyin\": \"yìng shè\", \"trans\": \"mapping\"},\n    {\"word\": \"匹敌\", \"pinyin\": \"pǐ dí\", \"trans\": \"match\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"超参数\", \"pinyin\": \"chāo cān shù\", \"trans\": \"hyperparameter\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"}\n]",
        "trans": "This article discusses the role of regularization layers in modern neural networks. Research has found that even without using regularization layers, Transformers can achieve the same or better performance through a simple technique called Dynamic Tanh (DyT), which is an element-wise operation. The inspiration for DyT comes from the observation that layer regularization in Transformers often produces tanh-like S-shaped input-output mappings. By using DyT, Transformers can match or exceed the performance of regularized models without the need for hyperparameter tuning.",
        "update_ts": "2025-03-16 12:40"
    }
}