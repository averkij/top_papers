{
    "date": {
        "ru": "31 июля",
        "en": "July 31",
        "zh": "7月31日"
    },
    "time_utc": "2025-07-31 14:15",
    "weekday": 3,
    "issue_id": 5113,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.22827",
            "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
            "url": "https://huggingface.co/papers/2507.22827",
            "abstract": "A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder.",
            "score": 54,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "a09c860f7fa98aea",
            "authors": [
                "Yilei Jiang",
                "Yaozhi Zheng",
                "Yuxuan Wan",
                "Jiaming Han",
                "Qunzhong Wang",
                "Michael R. Lyu",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "2ARISE Lab",
                "CUHK 1MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22827.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#interpretability",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Умное превращение дизайна в код с помощью ИИ",
                    "desc": "Статья представляет модульную мультиагентную систему для генерации кода пользовательского интерфейса на основе визуального дизайна. Система использует три этапа: распознавание компонентов с помощью мультимодальной модели, планирование иерархической структуры и генерация HTML/CSS кода. Авторы также создали масштабируемый механизм для автоматического создания обучающих данных. Эксперименты показывают, что подход достигает наилучших результатов по точности макета, структурной согласованности и корректности кода."
                },
                "en": {
                    "title": "Transforming UI Designs into Code with Modular Intelligence",
                    "desc": "This paper presents a modular multi-agent framework designed to enhance the process of converting user interface (UI) designs into front-end code. It integrates vision-language models to accurately identify UI components, employs hierarchical layout planning to organize these components, and utilizes adaptive prompt-based synthesis for code generation. By breaking down the UI-to-code generation into three distinct stages—grounding, planning, and generation—the framework improves the robustness and interpretability of the output compared to traditional end-to-end methods. The authors also introduce a scalable data engine that generates large datasets of image-code pairs, which are used to fine-tune a vision-language model, resulting in improved performance in layout accuracy and code quality."
                },
                "zh": {
                    "title": "模块化多智能体框架提升UI到代码生成",
                    "desc": "这篇论文提出了一种模块化的多智能体框架，用于将用户界面（UI）设计转化为前端代码。该框架通过整合视觉-语言模型、层次布局规划和自适应提示合成，分为三个可解释的阶段：基础、规划和生成。基础代理使用视觉-语言模型检测和标记UI组件，规划代理构建层次布局，生成代理则通过自适应提示合成生成HTML/CSS代码。实验结果表明，该方法在布局准确性、结构一致性和代码正确性方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21493",
            "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
            "url": "https://huggingface.co/papers/2507.21493",
            "abstract": "BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.",
            "score": 38,
            "issue_id": 5102,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "28c2cc57bc1db4c8",
            "authors": [
                "Longwen Zhang",
                "Qixuan Zhang",
                "Haoran Jiang",
                "Yinuo Bai",
                "Wei Yang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21493.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "BANG: Интуитивная декомпозиция 3D-объектов для творческого моделирования",
                    "desc": "BANG - это генеративный подход, использующий латентные диффузионные модели и временное внимание для интуитивной декомпозиции и манипуляции 3D-объектами на уровне частей. Он основан на концепции 'Generative Exploded Dynamics', создающей плавную последовательность разобранных состояний входной геометрии. BANG использует предобученную крупномасштабную латентную диффузионную модель, дообученную для разобранной динамики с помощью легковесного адаптера. Подход позволяет улучшить рабочие процессы 3D-создания, предлагая интуитивно понятный способ трансформации концепций в детализированные 3D-активы."
                },
                "en": {
                    "title": "BANG: Intuitive 3D Creation through Generative Decomposition",
                    "desc": "BANG is a generative approach that enhances 3D creation by allowing users to intuitively decompose and manipulate 3D objects at a part level. It employs latent diffusion models and a temporal attention mechanism to ensure smooth transitions and maintain the coherence of geometric and semantic properties during the decomposition process. The system allows for precise control through spatial prompts, enabling users to specify which parts to manipulate, and can integrate with multimodal models for enhanced creativity. BANG's capabilities support detailed geometry generation and are particularly useful in applications like 3D printing, where it facilitates the creation of separable parts for easy assembly."
                },
                "zh": {
                    "title": "BANG：直观的3D对象分解与创作新方法",
                    "desc": "BANG是一种生成性方法，利用潜在扩散模型和时间注意力机制，实现3D对象的直观部分级分解和操作，提升3D创作流程。它通过“生成性爆炸动态”技术，创建输入几何体的平滑爆炸状态序列，逐步分离部件，同时保持几何和语义的一致性。BANG使用经过预训练的大规模潜在扩散模型，并通过轻量级的爆炸视图适配器进行微调，确保分解过程的精确控制。该方法还结合了时间注意力模块，确保时间上的平滑过渡和一致性，极大地增强了3D创作的灵活性和直观性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22448",
            "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
            "url": "https://huggingface.co/papers/2507.22448",
            "abstract": "Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.",
            "score": 36,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "1023abbaabd95fa0",
            "authors": [
                "Jingwei Zuo",
                "Maksim Velikanov",
                "Ilyas Chahed",
                "Younes Belkada",
                "Dhia Eddine Rhayem",
                "Guillaume Kunsch",
                "Hakim Hacid",
                "Hamza Yous",
                "Brahim Farhat",
                "Ibrahim Khadraoui",
                "Mugariya Farooq",
                "Giulia Campesan",
                "Ruxandra Cojocaru",
                "Yasser Djilali",
                "Shi Hu",
                "Iheb Chaabane",
                "Puneesh Khanna",
                "Mohamed El Amine Seddik",
                "Ngoc Dung Huynh",
                "Phuc Le Khac",
                "Leen AlQadi",
                "Billel Mokeddem",
                "Mohamed Chami",
                "Abdalgader Abubaker",
                "Mikhail Lubinets",
                "Kacper Piskorski",
                "Slim Frikha"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22448.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#agi",
                    "#architecture",
                    "#science",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "Falcon-H1: Гибридная мощь в мире языковых моделей",
                    "desc": "Falcon-H1 - это новая серия больших языковых моделей с гибридной архитектурой, сочетающей внимание на основе трансформеров и модели пространства состояний. Модели демонстрируют передовую производительность и эффективность в различных задачах и размерах. Falcon-H1 доступен в нескольких конфигурациях, включая базовые и инструктированные варианты с количеством параметров от 0,5B до 34B. Модели превосходят аналоги с большим количеством параметров и показывают отличные результаты в рассуждениях, математике, многоязычных задачах и научных знаниях."
                },
                "en": {
                    "title": "Falcon-H1: Redefining Efficiency in Language Models",
                    "desc": "Falcon-H1 introduces a new series of large language models that utilize a hybrid architecture, merging Transformer-based attention with State Space Models for enhanced performance and efficiency. This innovative design allows the models to handle long-context memory better while maintaining computational efficiency. The models are available in various sizes and configurations, demonstrating state-of-the-art capabilities across multiple tasks, including reasoning and multilingual processing. By outperforming larger models with fewer parameters, Falcon-H1 sets a new standard in the field of AI language models."
                },
                "zh": {
                    "title": "Falcon-H1：高效与性能的完美结合",
                    "desc": "Falcon-H1是一系列新的大型语言模型，采用混合架构，结合了基于Transformer的注意力机制和状态空间模型（SSM）。这种设计使得Falcon-H1在多种任务中表现出色，具有高效的计算能力和优越的长时记忆能力。与之前的Falcon模型不同，Falcon-H1提供了多种配置，能够在参数较少的情况下与更大规模的模型竞争。所有模型都以开放源代码的方式发布，体现了我们对可及性和影响力的承诺。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22607",
            "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2507.22607",
            "abstract": "VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.",
            "score": 24,
            "issue_id": 5106,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "ce7a62814ba40250",
            "authors": [
                "Ruifeng Yuan",
                "Chenghao Xiao",
                "Sicong Leng",
                "Jianyu Wang",
                "Long Li",
                "Weiwen Xu",
                "Hou Pong Chan",
                "Deli Zhao",
                "Tingyang Xu",
                "Zhongyu Wei",
                "Hao Zhang",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Fudan University",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22607.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное обучение для улучшения мультимодальных рассуждений ИИ",
                    "desc": "VL-Cogito - это мультимодальная модель рассуждений, использующая прогрессивное обучение с подкреплением. Модель динамически адаптирует сложность задач и длину пути рассуждений, что позволяет улучшить производительность на разнообразных задачах. VL-Cogito использует механизм мягкой настройки сложности и динамическое вознаграждение за длину рассуждений. Эксперименты показывают, что модель превосходит существующие аналоги на различных мультимодальных тестах."
                },
                "en": {
                    "title": "Empowering Multimodal Reasoning with Progressive Learning",
                    "desc": "VL-Cogito is a multimodal reasoning model that enhances performance on various tasks using a Progressive Curriculum Reinforcement Learning (PCuRL) framework. This framework allows the model to learn by gradually increasing the difficulty of tasks, which helps improve its reasoning skills in complex multimodal scenarios. It introduces two main innovations: an online difficulty soft weighting mechanism that adjusts the training difficulty dynamically, and a dynamic length reward mechanism that helps the model manage its reasoning path length based on task complexity. Experimental results show that VL-Cogito outperforms existing models in multiple reasoning tasks, demonstrating its effectiveness in multimodal reasoning."
                },
                "zh": {
                    "title": "渐进式课程强化学习提升多模态推理能力",
                    "desc": "VL-Cogito是一种多模态推理模型，采用渐进式课程强化学习框架来提升在不同任务上的表现。该模型通过逐步增加任务难度和推理路径长度，系统性地引导模型进行训练。它引入了在线难度软加权机制和动态长度奖励机制，以适应任务复杂性，从而平衡推理效率和正确性。实验结果表明，VL-Cogito在多个主流多模态基准测试中表现优异，超越了现有的推理模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20976",
            "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision",
            "url": "https://huggingface.co/papers/2507.20976",
            "abstract": "A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA",
            "score": 7,
            "issue_id": 5103,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "c48709f829f62b89",
            "authors": [
                "Xiao Fang",
                "Minhyek Jeon",
                "Zheyang Qin",
                "Stanislav Panev",
                "Celso de Melo",
                "Shuowen Hu",
                "Shayok Chakraborty",
                "Fernando De la Torre"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Florida State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20976.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Генеративный ИИ повышает точность обнаружения автомобилей на аэрофотоснимках",
                    "desc": "Статья представляет новый метод обнаружения транспортных средств на аэрофотоснимках с использованием генеративного ИИ. Авторы разработали многоступенчатую мультимодальную систему передачи знаний на основе тонко настроенных латентных диффузионных моделей (LDM) для преодоления разрыва между исходной и целевой средой. Эксперименты показали значительное улучшение производительности по сравнению с существующими методами обучения и адаптации доменов. Также были представлены два новых аннотированных набора данных аэрофотоснимков для дальнейших исследований в этой области."
                },
                "en": {
                    "title": "Bridging Domain Gaps for Better Vehicle Detection in Aerial Imagery",
                    "desc": "This paper presents a new framework for improving vehicle detection in aerial images by using fine-tuned latent diffusion models (LDMs). The challenge addressed is the difficulty of models trained in one area to perform well in different geographic regions due to varying conditions. The proposed method enhances training by generating high-quality synthetic aerial images and their labels, effectively bridging the gap between different domains. Experimental results demonstrate significant performance gains in vehicle detection accuracy compared to existing methods, showcasing the effectiveness of this multi-stage, multi-modal approach."
                },
                "zh": {
                    "title": "多模态知识转移提升航空图像车辆检测",
                    "desc": "本论文提出了一种多阶段、多模态的知识转移框架，利用微调的潜在扩散模型来改善不同领域的航空图像中的车辆检测。通过生成高质量的航空图像及其标签，该方法增强了检测器的训练，解决了模型在不同地理区域间的泛化问题。实验结果表明，与传统的监督学习和其他适应方法相比，该方法在AP50指标上提高了4-23%、6-10%、7-40%及超过50%的性能。我们还引入了来自新西兰和犹他州的两个新标注的航空数据集，以支持该领域的进一步研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22886",
            "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
            "url": "https://huggingface.co/papers/2507.22886",
            "abstract": "Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.",
            "score": 6,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "99373a83d84e0212",
            "authors": [
                "Kaining Ying",
                "Henghui Ding",
                "Guanquan Jie",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22886.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Мультимодальная сегментация: новый уровень понимания аудио-визуального контента",
                    "desc": "OmniAVS - это новый набор данных для сегментации аудио-визуального контента, включающий 2,098 видео и 59,458 мультимодальных выражений. Датасет отличается гибким сочетанием текста, речи, звука и визуальных подсказок, а также акцентом на понимание аудиоконтента и сложные рассуждения. OISA - это модель, использующая мультимодальную языковую модель (MLLM) для понимания сложных сигналов и выполнения сегментации на основе рассуждений. Эксперименты показывают, что OISA превосходит существующие методы на OmniAVS и достигает конкурентоспособных результатов в других связанных задачах."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Segmentation with OmniAVS and OISA",
                    "desc": "This paper introduces Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset designed to enhance audio-visual segmentation by incorporating diverse multimodal expressions. It features 2,098 videos and 59,458 multimodal referring expressions, which include text, speech, sound, and visual cues. The study also presents the Omnimodal Instructed Segmentation Assistant (OISA), which utilizes a Multimodal Large Language Model (MLLM) to improve reasoning and understanding of complex audiovisual content. Experimental results demonstrate that OISA significantly outperforms existing segmentation methods on the OmniAVS dataset and shows competitive performance on related tasks."
                },
                "zh": {
                    "title": "全模态音视频分割的创新与突破",
                    "desc": "本文提出了全模态引用音视频分割（OmniAVS）和全模态指令分割助手（OISA），旨在提升音视频分割的能力。OmniAVS是一个新数据集，包含2098个视频和59458个多模态引用表达，具有8种灵活结合文本、语音、声音和视觉线索的多模态表达类型。OISA利用多语言大模型（MLLM）来理解复杂线索并进行推理分割，从而解决多模态推理和音视频内容的细致理解问题。实验结果表明，OISA在OmniAVS上优于现有方法，并在其他相关任务中也取得了竞争性结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22565",
            "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2507.22565",
            "abstract": "RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.  \t\t\t\t\tAI-generated summary \t\t\t\t The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same (epsilon, delta)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.",
            "score": 6,
            "issue_id": 5106,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "0bf987cb582a1073",
            "authors": [
                "Afshin Khadangi",
                "Amir Sartipi",
                "Igor Tchappi",
                "Ramin Bahmani",
                "Gilbert Fridgen"
            ],
            "affiliations": [
                "SnT, University of Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22565.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#training",
                    "#security",
                    "#rlhf",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Динамическая оптимизация приватности в обучении языковых моделей",
                    "desc": "RLDP - это фреймворк глубокого обучения с подкреплением, который оптимизирует дифференциально приватное обучение путем динамической настройки ограничения градиентов и шума. Он использует агент мягкого актор-критика (SAC) для выбора параметров ограничения градиентов и уровня шума во время обучения языковой модели. RLDP позволяет достичь лучшего баланса между полезностью модели и конфиденциальностью данных по сравнению с базовыми методами. Эксперименты показали значительное улучшение перплексии и downstream-метрик при сохранении тех же гарантий дифференциальной приватности."
                },
                "en": {
                    "title": "Optimizing Privacy and Performance with RLDP",
                    "desc": "RLDP is a novel deep reinforcement learning framework designed to optimize differentially private training for machine learning models. It addresses the challenge of balancing data privacy with model performance by dynamically adjusting gradient clipping and noise levels based on real-time learning statistics. This approach allows for more efficient use of the privacy budget, leading to improved model accuracy and faster convergence during training. Through extensive experiments, RLDP demonstrates significant reductions in perplexity and enhanced utility across various language models while maintaining strong privacy guarantees."
                },
                "zh": {
                    "title": "动态优化隐私与模型效用的平衡",
                    "desc": "RLDP是一个深度强化学习框架，旨在优化差分隐私训练。它通过动态调整梯度裁剪和噪声的大小，提升模型的实用性和训练速度，同时保持数据隐私。传统的差分隐私随机梯度下降方法在保证隐私的同时，往往会降低样本效率和最终准确性。RLDP通过实时感知学习动态，智能选择每个参数的梯度裁剪阈值和噪声大小，从而有效分配隐私预算。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22853",
            "title": "Repair-R1: Better Test Before Repair",
            "url": "https://huggingface.co/papers/2507.22853",
            "abstract": "Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.  \t\t\t\t\tAI-generated summary \t\t\t\t APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
            "score": 4,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "3f84dd190fcc7d27",
            "authors": [
                "Haichuan Hu",
                "Xiaochen Xie",
                "Quanjun Zhang"
            ],
            "affiliations": [
                "Alibaba Cloud",
                "Nanjing University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22853.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Repair-R1: Революция в автоматическом исправлении программ через интеграцию тестирования",
                    "desc": "Repair-R1 - это новый подход к автоматическому исправлению программ (APR), который интегрирует тестовые случаи в фазу обучения и приоритезирует генерацию тестов перед исправлением. Метод использует обучение с подкреплением для совместной оптимизации генерации тестов и исправления ошибок. Эксперименты показали значительное улучшение успешности исправлений, генерации тестов и тестового покрытия по сравнению с традиционными моделями. Repair-R1 может быть реализован с различными базовыми моделями, что демонстрирует его гибкость и эффективность."
                },
                "en": {
                    "title": "Revolutionizing Automated Program Repair with Test-Driven Training",
                    "desc": "Repair-R1 is a novel approach to Automated Program Repair (APR) that enhances the repair process by incorporating test cases during the training phase. Unlike traditional methods that only use tests after generating patches, Repair-R1 prioritizes test generation before the repair, allowing the model to create targeted tests that identify defects more effectively. This method leverages reinforcement learning to optimize both test generation and bug repair simultaneously, leading to improved performance metrics. Experimental results show significant increases in repair success rates, test generation success, and overall test coverage compared to standard models."
                },
                "zh": {
                    "title": "Repair-R1：优先生成测试，提升自动修复效果",
                    "desc": "Repair-R1 是一种自动程序修复方法，它通过将测试用例整合到训练阶段来增强修复效果。该方法优先生成测试用例，然后再进行修复，从而提高了修复成功率和测试生成成功率。与传统方法不同，Repair-R1 在修复之前生成能够区分缺陷行为的测试用例，使模型更好地定位缺陷并理解其根本原因。实验结果表明，Repair-R1 在多个基准测试中表现优越，显著提高了修复和测试的成功率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21802",
            "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
            "url": "https://huggingface.co/papers/2507.21802",
            "abstract": "MixGRPO, a novel framework integrating SDE and ODE, enhances flow matching models for image generation by optimizing only within a sliding window, improving efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}.",
            "score": 1,
            "issue_id": 5112,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "69f28d4329fb0385",
            "authors": [
                "Junzhe Li",
                "Yutao Cui",
                "Tao Huang",
                "Yinping Ma",
                "Chun Fan",
                "Miles Yang",
                "Zhao Zhong"
            ],
            "affiliations": [
                "Computer Center, Peking University",
                "Hunyuan, Tencent",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21802.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективная оптимизация генерации изображений с помощью гибридного подхода SDE-ODE",
                    "desc": "MixGRPO - это новая фреймворк, объединяющий стохастические дифференциальные уравнения (SDE) и обычные дифференциальные уравнения (ODE) для улучшения моделей согласования потоков в генерации изображений. Он оптимизирует только в пределах скользящего окна, что повышает эффективность и производительность. MixGRPO превосходит DanceGRPO по эффективности и результативности, сокращая время обучения почти на 50%. Вариант MixGRPO-Flash дополнительно уменьшает время обучения на 71%."
                },
                "en": {
                    "title": "Streamlining Image Generation with MixGRPO",
                    "desc": "MixGRPO is a new framework that combines stochastic differential equations (SDE) and ordinary differential equations (ODE) to enhance flow matching models for generating images. It optimizes the process by using a sliding window approach, which allows for focused gradient updates and reduces the computational burden during training. This method confines randomness to a specific time-frame, improving efficiency and performance in human preference alignment tasks. Additionally, MixGRPO-Flash, a faster version of the framework, significantly cuts down training time while maintaining high performance levels."
                },
                "zh": {
                    "title": "MixGRPO：高效图像生成的新框架",
                    "desc": "MixGRPO是一种新颖的框架，它结合了随机微分方程（SDE）和常微分方程（ODE），旨在提高图像生成中的流匹配模型的效率和性能。通过在滑动窗口内进行优化，MixGRPO减少了优化开销，使得梯度更新更加集中，从而加速收敛。该方法在多个维度上显著提升了人类偏好对齐的效果，相比于DanceGRPO，训练时间减少了近50%。此外，MixGRPO-Flash进一步将训练时间缩短了71%，展现了更高的训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22062",
            "title": "MetaCLIP 2: A Worldwide Scaling Recipe",
            "url": "https://huggingface.co/papers/2507.22062",
            "abstract": "MetaCLIP 2, trained on worldwide web-scale image-text pairs, improves zero-shot classification and multilingual benchmarks without system-level confounding factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.",
            "score": 0,
            "issue_id": 5112,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "989c6f6b088a6366",
            "authors": [
                "Yung-Sung Chuang",
                "Yang Li",
                "Dong Wang",
                "Ching-Feng Yeh",
                "Kehan Lyu",
                "Ramya Raghavendra",
                "James Glass",
                "Lifei Huang",
                "Jason Weston",
                "Luke Zettlemoyer",
                "Xinlei Chen",
                "Zhuang Liu",
                "Saining Xie",
                "Wen-tau Yih",
                "Shang-Wen Li",
                "Hu Xu"
            ],
            "affiliations": [
                "FAIR, Meta",
                "MIT",
                "New York University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22062.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#low_resource",
                    "#cv",
                    "#machine_translation",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "MetaCLIP 2: Прорыв в мультиязычной классификации",
                    "desc": "В статье представлена новая модель MetaCLIP 2, которая обучена на данных из интернета, включающих изображения и текст на разных языках. Эта модель улучшает результаты классификации без необходимости в дополнительных системных изменениях. MetaCLIP 2 преодолевает \"проклятие мультиязычности\", улучшая производительность как на английском, так и на других языках. В результате, модель достигает новых высот в задачах классификации и извлечения информации из изображений."
                },
                "en": {
                    "title": "MetaCLIP 2: Bridging Multilingual Gaps in Image-Text Understanding",
                    "desc": "MetaCLIP 2 is a new model that enhances the performance of zero-shot classification and multilingual tasks by training on a vast amount of image-text pairs from the entire web. It addresses the challenges of previous models, particularly the 'curse of multilinguality', which caused poorer performance in non-English contexts. By using a carefully designed training approach, MetaCLIP 2 achieves better results than its English-only predecessors without introducing confounding factors like translation. This model sets new records in various benchmarks, demonstrating its effectiveness in both English and non-English data scenarios."
                },
                "zh": {
                    "title": "MetaCLIP 2：全球数据驱动的多语言图像-文本模型",
                    "desc": "MetaCLIP 2 是一种新型的对比语言-图像预训练模型，旨在通过全球范围内的图像-文本对进行训练，以提高零-shot 分类和多语言基准的性能。该模型解决了现有多语言 CLIP 在非英语数据处理中的挑战，并克服了多语言模型常见的“多语言诅咒”问题。通过严格的消融实验，MetaCLIP 2 在零-shot ImageNet 分类中超越了仅使用英语的模型，并在多个多语言基准上设定了新的最先进记录。该研究表明，利用英语和非英语数据的互惠互利可以显著提升模型的整体性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-30.html",
    "link_next": "2025-08-01.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.07",
        "en": "07/30",
        "zh": "7月30日"
    },
    "short_date_next": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8月1日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}