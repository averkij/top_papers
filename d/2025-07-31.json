{
    "date": {
        "ru": "31 Ğ¸ÑĞ»Ñ",
        "en": "July 31",
        "zh": "7æœˆ31æ—¥"
    },
    "time_utc": "2025-07-31 14:15",
    "weekday": 3,
    "issue_id": 5113,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.22827",
            "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
            "url": "https://huggingface.co/papers/2507.22827",
            "abstract": "A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder.",
            "score": 54,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "a09c860f7fa98aea",
            "authors": [
                "Yilei Jiang",
                "Yaozhi Zheng",
                "Yuxuan Wan",
                "Jiaming Han",
                "Qunzhong Wang",
                "Michael R. Lyu",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "2ARISE Lab",
                "CUHK 1MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22827.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#interpretability",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ² ĞºĞ¾Ğ´ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ HTML/CSS ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Transforming UI Designs into Code with Modular Intelligence",
                    "desc": "This paper presents a modular multi-agent framework designed to enhance the process of converting user interface (UI) designs into front-end code. It integrates vision-language models to accurately identify UI components, employs hierarchical layout planning to organize these components, and utilizes adaptive prompt-based synthesis for code generation. By breaking down the UI-to-code generation into three distinct stagesâ€”grounding, planning, and generationâ€”the framework improves the robustness and interpretability of the output compared to traditional end-to-end methods. The authors also introduce a scalable data engine that generates large datasets of image-code pairs, which are used to fine-tune a vision-language model, resulting in improved performance in layout accuracy and code quality."
                },
                "zh": {
                    "title": "æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡UIåˆ°ä»£ç ç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå°†ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡è½¬åŒ–ä¸ºå‰ç«¯ä»£ç ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€å±‚æ¬¡å¸ƒå±€è§„åˆ’å’Œè‡ªé€‚åº”æç¤ºåˆæˆï¼Œåˆ†ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„é˜¶æ®µï¼šåŸºç¡€ã€è§„åˆ’å’Œç”Ÿæˆã€‚åŸºç¡€ä»£ç†ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ£€æµ‹å’Œæ ‡è®°UIç»„ä»¶ï¼Œè§„åˆ’ä»£ç†æ„å»ºå±‚æ¬¡å¸ƒå±€ï¼Œç”Ÿæˆä»£ç†åˆ™é€šè¿‡è‡ªé€‚åº”æç¤ºåˆæˆç”ŸæˆHTML/CSSä»£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸ƒå±€å‡†ç¡®æ€§ã€ç»“æ„ä¸€è‡´æ€§å’Œä»£ç æ­£ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21493",
            "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
            "url": "https://huggingface.co/papers/2507.21493",
            "abstract": "BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.",
            "score": 38,
            "issue_id": 5102,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "28c2cc57bc1db4c8",
            "authors": [
                "Longwen Zhang",
                "Qixuan Zhang",
                "Haoran Jiang",
                "Yinuo Bai",
                "Wei Yang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21493.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "BANG: Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "BANG - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ 'Generative Exploded Dynamics', ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. BANG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ 3D-ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹."
                },
                "en": {
                    "title": "BANG: Intuitive 3D Creation through Generative Decomposition",
                    "desc": "BANG is a generative approach that enhances 3D creation by allowing users to intuitively decompose and manipulate 3D objects at a part level. It employs latent diffusion models and a temporal attention mechanism to ensure smooth transitions and maintain the coherence of geometric and semantic properties during the decomposition process. The system allows for precise control through spatial prompts, enabling users to specify which parts to manipulate, and can integrate with multimodal models for enhanced creativity. BANG's capabilities support detailed geometry generation and are particularly useful in applications like 3D printing, where it facilitates the creation of separable parts for easy assembly."
                },
                "zh": {
                    "title": "BANGï¼šç›´è§‚çš„3Då¯¹è±¡åˆ†è§£ä¸åˆ›ä½œæ–°æ–¹æ³•",
                    "desc": "BANGæ˜¯ä¸€ç§ç”Ÿæˆæ€§æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°3Då¯¹è±¡çš„ç›´è§‚éƒ¨åˆ†çº§åˆ†è§£å’Œæ“ä½œï¼Œæå‡3Dåˆ›ä½œæµç¨‹ã€‚å®ƒé€šè¿‡â€œç”Ÿæˆæ€§çˆ†ç‚¸åŠ¨æ€â€æŠ€æœ¯ï¼Œåˆ›å»ºè¾“å…¥å‡ ä½•ä½“çš„å¹³æ»‘çˆ†ç‚¸çŠ¶æ€åºåˆ—ï¼Œé€æ­¥åˆ†ç¦»éƒ¨ä»¶ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚BANGä½¿ç”¨ç»è¿‡é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„çˆ†ç‚¸è§†å›¾é€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿åˆ†è§£è¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ï¼Œç¡®ä¿æ—¶é—´ä¸Šçš„å¹³æ»‘è¿‡æ¸¡å’Œä¸€è‡´æ€§ï¼Œæå¤§åœ°å¢å¼ºäº†3Dåˆ›ä½œçš„çµæ´»æ€§å’Œç›´è§‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22448",
            "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
            "url": "https://huggingface.co/papers/2507.22448",
            "abstract": "Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.",
            "score": 36,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "1023abbaabd95fa0",
            "authors": [
                "Jingwei Zuo",
                "Maksim Velikanov",
                "Ilyas Chahed",
                "Younes Belkada",
                "Dhia Eddine Rhayem",
                "Guillaume Kunsch",
                "Hakim Hacid",
                "Hamza Yous",
                "Brahim Farhat",
                "Ibrahim Khadraoui",
                "Mugariya Farooq",
                "Giulia Campesan",
                "Ruxandra Cojocaru",
                "Yasser Djilali",
                "Shi Hu",
                "Iheb Chaabane",
                "Puneesh Khanna",
                "Mohamed El Amine Seddik",
                "Ngoc Dung Huynh",
                "Phuc Le Khac",
                "Leen AlQadi",
                "Billel Mokeddem",
                "Mohamed Chami",
                "Abdalgader Abubaker",
                "Mikhail Lubinets",
                "Kacper Piskorski",
                "Slim Frikha"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22448.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#agi",
                    "#architecture",
                    "#science",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Falcon-H1: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰ÑŒ Ğ² Ğ¼Ğ¸Ñ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Falcon-H1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ…. Falcon-H1 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ñ‚ 0,5B Ğ´Ğ¾ 34B. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Falcon-H1: Redefining Efficiency in Language Models",
                    "desc": "Falcon-H1 introduces a new series of large language models that utilize a hybrid architecture, merging Transformer-based attention with State Space Models for enhanced performance and efficiency. This innovative design allows the models to handle long-context memory better while maintaining computational efficiency. The models are available in various sizes and configurations, demonstrating state-of-the-art capabilities across multiple tasks, including reasoning and multilingual processing. By outperforming larger models with fewer parameters, Falcon-H1 sets a new standard in the field of AI language models."
                },
                "zh": {
                    "title": "Falcon-H1ï¼šé«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆ",
                    "desc": "Falcon-H1æ˜¯ä¸€ç³»åˆ—æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¶æ„ï¼Œç»“åˆäº†åŸºäºTransformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚è¿™ç§è®¾è®¡ä½¿å¾—Falcon-H1åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å’Œä¼˜è¶Šçš„é•¿æ—¶è®°å¿†èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„Falconæ¨¡å‹ä¸åŒï¼ŒFalcon-H1æä¾›äº†å¤šç§é…ç½®ï¼Œèƒ½å¤Ÿåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç«äº‰ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä»¥å¼€æ”¾æºä»£ç çš„æ–¹å¼å‘å¸ƒï¼Œä½“ç°äº†æˆ‘ä»¬å¯¹å¯åŠæ€§å’Œå½±å“åŠ›çš„æ‰¿è¯ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22607",
            "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2507.22607",
            "abstract": "VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.",
            "score": 24,
            "issue_id": 5106,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "ce7a62814ba40250",
            "authors": [
                "Ruifeng Yuan",
                "Chenghao Xiao",
                "Sicong Leng",
                "Jianyu Wang",
                "Long Li",
                "Weiwen Xu",
                "Hou Pong Chan",
                "Deli Zhao",
                "Tingyang Xu",
                "Zhongyu Wei",
                "Hao Zhang",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Fudan University",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22607.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "VL-Cogito - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. VL-Cogito Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Multimodal Reasoning with Progressive Learning",
                    "desc": "VL-Cogito is a multimodal reasoning model that enhances performance on various tasks using a Progressive Curriculum Reinforcement Learning (PCuRL) framework. This framework allows the model to learn by gradually increasing the difficulty of tasks, which helps improve its reasoning skills in complex multimodal scenarios. It introduces two main innovations: an online difficulty soft weighting mechanism that adjusts the training difficulty dynamically, and a dynamic length reward mechanism that helps the model manage its reasoning path length based on task complexity. Experimental results show that VL-Cogito outperforms existing models in multiple reasoning tasks, demonstrating its effectiveness in multimodal reasoning."
                },
                "zh": {
                    "title": "æ¸è¿›å¼è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›",
                    "desc": "VL-Cogitoæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥æå‡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦å’Œæ¨ç†è·¯å¾„é•¿åº¦ï¼Œç³»ç»Ÿæ€§åœ°å¼•å¯¼æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚å®ƒå¼•å…¥äº†åœ¨çº¿éš¾åº¦è½¯åŠ æƒæœºåˆ¶å’ŒåŠ¨æ€é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œä»¥é€‚åº”ä»»åŠ¡å¤æ‚æ€§ï¼Œä»è€Œå¹³è¡¡æ¨ç†æ•ˆç‡å’Œæ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVL-Cogitoåœ¨å¤šä¸ªä¸»æµå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ¨ç†æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20976",
            "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision",
            "url": "https://huggingface.co/papers/2507.20976",
            "abstract": "A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA",
            "score": 7,
            "issue_id": 5103,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ»Ñ",
                "en": "July 28",
                "zh": "7æœˆ28æ—¥"
            },
            "hash": "c48709f829f62b89",
            "authors": [
                "Xiao Fang",
                "Minhyek Jeon",
                "Zheyang Qin",
                "Stanislav Panev",
                "Celso de Melo",
                "Shuowen Hu",
                "Shayok Chakraborty",
                "Fernando De la Torre"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Florida State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20976.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ² Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LDM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Bridging Domain Gaps for Better Vehicle Detection in Aerial Imagery",
                    "desc": "This paper presents a new framework for improving vehicle detection in aerial images by using fine-tuned latent diffusion models (LDMs). The challenge addressed is the difficulty of models trained in one area to perform well in different geographic regions due to varying conditions. The proposed method enhances training by generating high-quality synthetic aerial images and their labels, effectively bridging the gap between different domains. Experimental results demonstrate significant performance gains in vehicle detection accuracy compared to existing methods, showcasing the effectiveness of this multi-stage, multi-modal approach."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€çŸ¥è¯†è½¬ç§»æå‡èˆªç©ºå›¾åƒè½¦è¾†æ£€æµ‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µã€å¤šæ¨¡æ€çš„çŸ¥è¯†è½¬ç§»æ¡†æ¶ï¼Œåˆ©ç”¨å¾®è°ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥æ”¹å–„ä¸åŒé¢†åŸŸçš„èˆªç©ºå›¾åƒä¸­çš„è½¦è¾†æ£€æµ‹ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„èˆªç©ºå›¾åƒåŠå…¶æ ‡ç­¾ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†æ£€æµ‹å™¨çš„è®­ç»ƒï¼Œè§£å†³äº†æ¨¡å‹åœ¨ä¸åŒåœ°ç†åŒºåŸŸé—´çš„æ³›åŒ–é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’Œå…¶ä»–é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨AP50æŒ‡æ ‡ä¸Šæé«˜äº†4-23%ã€6-10%ã€7-40%åŠè¶…è¿‡50%çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ¥è‡ªæ–°è¥¿å…°å’ŒçŠ¹ä»–å·çš„ä¸¤ä¸ªæ–°æ ‡æ³¨çš„èˆªç©ºæ•°æ®é›†ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22886",
            "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
            "url": "https://huggingface.co/papers/2507.22886",
            "abstract": "Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.",
            "score": 6,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "99373a83d84e0212",
            "authors": [
                "Kaining Ying",
                "Henghui Ding",
                "Guanquan Jie",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22886.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "OmniAVS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 2,098 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 59,458 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. OISA - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OISA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° OmniAVS Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Segmentation with OmniAVS and OISA",
                    "desc": "This paper introduces Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset designed to enhance audio-visual segmentation by incorporating diverse multimodal expressions. It features 2,098 videos and 59,458 multimodal referring expressions, which include text, speech, sound, and visual cues. The study also presents the Omnimodal Instructed Segmentation Assistant (OISA), which utilizes a Multimodal Large Language Model (MLLM) to improve reasoning and understanding of complex audiovisual content. Experimental results demonstrate that OISA significantly outperforms existing segmentation methods on the OmniAVS dataset and shows competitive performance on related tasks."
                },
                "zh": {
                    "title": "å…¨æ¨¡æ€éŸ³è§†é¢‘åˆ†å‰²çš„åˆ›æ–°ä¸çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†å…¨æ¨¡æ€å¼•ç”¨éŸ³è§†é¢‘åˆ†å‰²ï¼ˆOmniAVSï¼‰å’Œå…¨æ¨¡æ€æŒ‡ä»¤åˆ†å‰²åŠ©æ‰‹ï¼ˆOISAï¼‰ï¼Œæ—¨åœ¨æå‡éŸ³è§†é¢‘åˆ†å‰²çš„èƒ½åŠ›ã€‚OmniAVSæ˜¯ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«2098ä¸ªè§†é¢‘å’Œ59458ä¸ªå¤šæ¨¡æ€å¼•ç”¨è¡¨è¾¾ï¼Œå…·æœ‰8ç§çµæ´»ç»“åˆæ–‡æœ¬ã€è¯­éŸ³ã€å£°éŸ³å’Œè§†è§‰çº¿ç´¢çš„å¤šæ¨¡æ€è¡¨è¾¾ç±»å‹ã€‚OISAåˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç†è§£å¤æ‚çº¿ç´¢å¹¶è¿›è¡Œæ¨ç†åˆ†å‰²ï¼Œä»è€Œè§£å†³å¤šæ¨¡æ€æ¨ç†å’ŒéŸ³è§†é¢‘å†…å®¹çš„ç»†è‡´ç†è§£é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOISAåœ¨OmniAVSä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…¶ä»–ç›¸å…³ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†ç«äº‰æ€§ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22565",
            "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2507.22565",
            "abstract": "RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.  \t\t\t\t\tAI-generated summary \t\t\t\t The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same (epsilon, delta)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.",
            "score": 6,
            "issue_id": 5106,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "0bf987cb582a1073",
            "authors": [
                "Afshin Khadangi",
                "Amir Sartipi",
                "Igor Tchappi",
                "Ramin Bahmani",
                "Gilbert Fridgen"
            ],
            "affiliations": [
                "SnT, University of Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22565.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#training",
                    "#security",
                    "#rlhf",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RLDP - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑˆÑƒĞ¼Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ°ĞºÑ‚Ğ¾Ñ€-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° (SAC) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RLDP Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ downstream-Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimizing Privacy and Performance with RLDP",
                    "desc": "RLDP is a novel deep reinforcement learning framework designed to optimize differentially private training for machine learning models. It addresses the challenge of balancing data privacy with model performance by dynamically adjusting gradient clipping and noise levels based on real-time learning statistics. This approach allows for more efficient use of the privacy budget, leading to improved model accuracy and faster convergence during training. Through extensive experiments, RLDP demonstrates significant reductions in perplexity and enhanced utility across various language models while maintaining strong privacy guarantees."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¼˜åŒ–éšç§ä¸æ¨¡å‹æ•ˆç”¨çš„å¹³è¡¡",
                    "desc": "RLDPæ˜¯ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å·®åˆ†éšç§è®­ç»ƒã€‚å®ƒé€šè¿‡åŠ¨æ€è°ƒæ•´æ¢¯åº¦è£å‰ªå’Œå™ªå£°çš„å¤§å°ï¼Œæå‡æ¨¡å‹çš„å®ç”¨æ€§å’Œè®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒæ•°æ®éšç§ã€‚ä¼ ç»Ÿçš„å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨ä¿è¯éšç§çš„åŒæ—¶ï¼Œå¾€å¾€ä¼šé™ä½æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚RLDPé€šè¿‡å®æ—¶æ„ŸçŸ¥å­¦ä¹ åŠ¨æ€ï¼Œæ™ºèƒ½é€‰æ‹©æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼å’Œå™ªå£°å¤§å°ï¼Œä»è€Œæœ‰æ•ˆåˆ†é…éšç§é¢„ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22853",
            "title": "Repair-R1: Better Test Before Repair",
            "url": "https://huggingface.co/papers/2507.22853",
            "abstract": "Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.  \t\t\t\t\tAI-generated summary \t\t\t\t APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
            "score": 4,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "3f84dd190fcc7d27",
            "authors": [
                "Haichuan Hu",
                "Xiaochen Xie",
                "Quanjun Zhang"
            ],
            "affiliations": [
                "Alibaba Cloud",
                "Nanjing University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22853.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Repair-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Repair-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ (APR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ² Ñ„Ğ°Ğ·Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Repair-R1 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Automated Program Repair with Test-Driven Training",
                    "desc": "Repair-R1 is a novel approach to Automated Program Repair (APR) that enhances the repair process by incorporating test cases during the training phase. Unlike traditional methods that only use tests after generating patches, Repair-R1 prioritizes test generation before the repair, allowing the model to create targeted tests that identify defects more effectively. This method leverages reinforcement learning to optimize both test generation and bug repair simultaneously, leading to improved performance metrics. Experimental results show significant increases in repair success rates, test generation success, and overall test coverage compared to standard models."
                },
                "zh": {
                    "title": "Repair-R1ï¼šä¼˜å…ˆç”Ÿæˆæµ‹è¯•ï¼Œæå‡è‡ªåŠ¨ä¿®å¤æ•ˆæœ",
                    "desc": "Repair-R1 æ˜¯ä¸€ç§è‡ªåŠ¨ç¨‹åºä¿®å¤æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†æµ‹è¯•ç”¨ä¾‹æ•´åˆåˆ°è®­ç»ƒé˜¶æ®µæ¥å¢å¼ºä¿®å¤æ•ˆæœã€‚è¯¥æ–¹æ³•ä¼˜å…ˆç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç„¶åå†è¿›è¡Œä¿®å¤ï¼Œä»è€Œæé«˜äº†ä¿®å¤æˆåŠŸç‡å’Œæµ‹è¯•ç”ŸæˆæˆåŠŸç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRepair-R1 åœ¨ä¿®å¤ä¹‹å‰ç”Ÿæˆèƒ½å¤ŸåŒºåˆ†ç¼ºé™·è¡Œä¸ºçš„æµ‹è¯•ç”¨ä¾‹ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°å®šä½ç¼ºé™·å¹¶ç†è§£å…¶æ ¹æœ¬åŸå› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepair-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤å’Œæµ‹è¯•çš„æˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21802",
            "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
            "url": "https://huggingface.co/papers/2507.21802",
            "abstract": "MixGRPO, a novel framework integrating SDE and ODE, enhances flow matching models for image generation by optimizing only within a sliding window, improving efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}.",
            "score": 1,
            "issue_id": 5112,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "69f28d4329fb0385",
            "authors": [
                "Junzhe Li",
                "Yutao Cui",
                "Tao Huang",
                "Yinping Ma",
                "Chun Fan",
                "Miles Yang",
                "Zhao Zhong"
            ],
            "affiliations": [
                "Computer Center, Peking University",
                "Hunyuan, Tencent",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21802.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° SDE-ODE",
                    "desc": "MixGRPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ (SDE) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ (ODE) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. MixGRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DanceGRPO Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğ° 50%. Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ MixGRPO-Flash Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 71%."
                },
                "en": {
                    "title": "Streamlining Image Generation with MixGRPO",
                    "desc": "MixGRPO is a new framework that combines stochastic differential equations (SDE) and ordinary differential equations (ODE) to enhance flow matching models for generating images. It optimizes the process by using a sliding window approach, which allows for focused gradient updates and reduces the computational burden during training. This method confines randomness to a specific time-frame, improving efficiency and performance in human preference alignment tasks. Additionally, MixGRPO-Flash, a faster version of the framework, significantly cuts down training time while maintaining high performance levels."
                },
                "zh": {
                    "title": "MixGRPOï¼šé«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "MixGRPOæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å’Œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆä¸­çš„æµåŒ¹é…æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚é€šè¿‡åœ¨æ»‘åŠ¨çª—å£å†…è¿›è¡Œä¼˜åŒ–ï¼ŒMixGRPOå‡å°‘äº†ä¼˜åŒ–å¼€é”€ï¼Œä½¿å¾—æ¢¯åº¦æ›´æ–°æ›´åŠ é›†ä¸­ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—æå‡äº†äººç±»åå¥½å¯¹é½çš„æ•ˆæœï¼Œç›¸æ¯”äºDanceGRPOï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†è¿‘50%ã€‚æ­¤å¤–ï¼ŒMixGRPO-Flashè¿›ä¸€æ­¥å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†71%ï¼Œå±•ç°äº†æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22062",
            "title": "MetaCLIP 2: A Worldwide Scaling Recipe",
            "url": "https://huggingface.co/papers/2507.22062",
            "abstract": "MetaCLIP 2, trained on worldwide web-scale image-text pairs, improves zero-shot classification and multilingual benchmarks without system-level confounding factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.",
            "score": 0,
            "issue_id": 5112,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "989c6f6b088a6366",
            "authors": [
                "Yung-Sung Chuang",
                "Yang Li",
                "Dong Wang",
                "Ching-Feng Yeh",
                "Kehan Lyu",
                "Ramya Raghavendra",
                "James Glass",
                "Lifei Huang",
                "Jason Weston",
                "Luke Zettlemoyer",
                "Xinlei Chen",
                "Zhuang Liu",
                "Saining Xie",
                "Wen-tau Yih",
                "Shang-Wen Li",
                "Hu Xu"
            ],
            "affiliations": [
                "FAIR, Meta",
                "MIT",
                "New York University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22062.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#low_resource",
                    "#cv",
                    "#machine_translation",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "MetaCLIP 2: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MetaCLIP 2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ…. MetaCLIP 2 Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ \"Ğ¿Ñ€Ğ¾ĞºĞ»ÑÑ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸\", ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾Ñ‚ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "MetaCLIP 2: Bridging Multilingual Gaps in Image-Text Understanding",
                    "desc": "MetaCLIP 2 is a new model that enhances the performance of zero-shot classification and multilingual tasks by training on a vast amount of image-text pairs from the entire web. It addresses the challenges of previous models, particularly the 'curse of multilinguality', which caused poorer performance in non-English contexts. By using a carefully designed training approach, MetaCLIP 2 achieves better results than its English-only predecessors without introducing confounding factors like translation. This model sets new records in various benchmarks, demonstrating its effectiveness in both English and non-English data scenarios."
                },
                "zh": {
                    "title": "MetaCLIP 2ï¼šå…¨çƒæ•°æ®é©±åŠ¨çš„å¤šè¯­è¨€å›¾åƒ-æ–‡æœ¬æ¨¡å‹",
                    "desc": "MetaCLIP 2 æ˜¯ä¸€ç§æ–°å‹çš„å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å…¨çƒèŒƒå›´å†…çš„å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜é›¶-shot åˆ†ç±»å’Œå¤šè¯­è¨€åŸºå‡†çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰å¤šè¯­è¨€ CLIP åœ¨éè‹±è¯­æ•°æ®å¤„ç†ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶å…‹æœäº†å¤šè¯­è¨€æ¨¡å‹å¸¸è§çš„â€œå¤šè¯­è¨€è¯…å’’â€é—®é¢˜ã€‚é€šè¿‡ä¸¥æ ¼çš„æ¶ˆèå®éªŒï¼ŒMetaCLIP 2 åœ¨é›¶-shot ImageNet åˆ†ç±»ä¸­è¶…è¶Šäº†ä»…ä½¿ç”¨è‹±è¯­çš„æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªå¤šè¯­è¨€åŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›è®°å½•ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è‹±è¯­å’Œéè‹±è¯­æ•°æ®çš„äº’æƒ äº’åˆ©å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-30.html",
    "link_next": "2025-08-01.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.07",
        "en": "07/30",
        "zh": "7æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}