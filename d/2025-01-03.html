
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. January 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 января</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-02.html">⬅️ <span id="prev-date">02.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-06.html">➡️ <span id="next-date">06.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'};
        let feedDateNext = {'ru': '06.01', 'en': '01/06', 'zh': '1月6日'};
        let feedDatePrev = {'ru': '02.01', 'en': '01/02', 'zh': '1月2日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.01427', 'title': 'VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control', 'url': 'https://huggingface.co/papers/2501.01427', 'abstract': 'Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.', 'score': 15, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '4c67f688775a3eca', 'authors': ['Yuanpeng Tu', 'Hao Luo', 'Xi Chen', 'Sihui Ji', 'Xiang Bai', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group', 'HUST', 'Hupan Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.01427.jpg', 'data': {'categories': ['#diffusion', '#games', '#video'], 'emoji': '🎬', 'ru': {'title': 'Точная вставка объектов в видео с сохранением деталей', 'desc': 'В этой статье представлен VideoAnydoor - фреймворк для вставки объектов в видео без предварительного обучения. Он использует экстрактор идентификаторов и последовательность ограничивающих рамок для контроля движения объекта. Ключевым компонентом является пиксельный варпер, который сохраняет детали внешнего вида и позволяет точно управлять движением. Предложенная стратегия обучения с использованием видео и статических изображений улучшает качество вставки объектов.'}, 'en': {'title': 'Seamless Object Insertion in Videos with VideoAnydoor', 'desc': 'This paper introduces VideoAnydoor, a novel framework for zero-shot video object insertion that excels in maintaining high-fidelity details and precise motion control. The approach begins with a text-to-video model and incorporates an ID extractor to ensure consistent object identity while using a box sequence for motion management. A key innovation is the pixel warper, which adjusts pixel details based on key-point trajectories, enhancing both detail preservation and user control over motion. The proposed training strategy, which combines videos and static images with a reweighted reconstruction loss, significantly improves the quality of object insertion, making VideoAnydoor versatile for various applications without needing specific fine-tuning.'}, 'zh': {'title': '高保真视频对象插入的新突破', 'desc': '尽管视频生成技术取得了显著进展，但将特定对象插入视频仍然是一项具有挑战性的任务。本文提出了VideoAnydoor，这是一个零-shot视频对象插入框架，能够高保真地保留细节并精确控制运动。我们设计了一种像素变形器，能够根据关键点轨迹扭曲像素细节，并与扩散U-Net融合，从而提高细节保留能力。VideoAnydoor在现有方法中表现出显著优势，并支持多种下游应用，无需特定任务的微调。'}}}, {'id': 'https://huggingface.co/papers/2501.01264', 'title': 'ProgCo: Program Helps Self-Correction of Large Language Models', 'url': 'https://huggingface.co/papers/2501.01264', 'abstract': 'Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.', 'score': 11, 'issue_id': 1473, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'bda3f96e83319526', 'authors': ['Xiaoshuai Song', 'Yanan Wu', 'Weixun Wang', 'Jiaheng Liu', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Taobao & Tmall Group of Alibaba'], 'pdf_title_img': 'assets/pdf/title_img/2501.01264.jpg', 'data': {'categories': ['#training', '#math', '#reasoning', '#interpretability', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'ProgCo: Самокоррекция языковых моделей через программно-управляемую верификацию и уточнение', 'desc': 'Эта статья представляет новый подход к самокоррекции больших языковых моделей (LLM) под названием Program-driven Self-Correction (ProgCo). Метод включает в себя программно-управляемую верификацию (ProgVe), которая использует самогенерируемые и самовыполняющиеся псевдопрограммы для сложной логики проверки. Затем программно-управляемое уточнение (ProgRe) проводит двойную рефлексию и улучшение как ответов, так и программ верификации. Эксперименты показали, что ProgCo эффективен в самокоррекции и может дополнительно улучшить производительность при комбинировании с реальными программными инструментами.'}, 'en': {'title': 'Empowering LLMs with Program-Driven Self-Correction', 'desc': 'This paper introduces Program-driven Self-Correction (ProgCo) to improve the self-verification and self-refinement capabilities of large language models (LLMs). It addresses the common issue where LLMs struggle to provide accurate feedback, which can lead to incorrect refinements, particularly in complex reasoning tasks. ProgCo utilizes program-driven verification (ProgVe) to create self-executing verification pseudo-programs that enhance the verification process. Additionally, program-driven refinement (ProgRe) allows the model to reflect on and refine both its responses and the verification programs, leading to more reliable self-correction outcomes.'}, 'zh': {'title': '基于程序的自我纠正：提升语言模型的自我验证能力', 'desc': '自我纠正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs往往无法有效自我验证并生成正确的反馈，这会进一步误导其完善过程，尤其是在复杂推理任务中。本文提出了基于程序的自我纠正（ProgCo），通过自生成、自执行的验证伪程序实现复杂的验证逻辑和广泛的验证。实验结果表明，ProgCo在三个指令遵循和数学基准测试中实现了有效的自我纠正，并且与真实程序工具结合时可以进一步提升性能。'}}}, {'id': 'https://huggingface.co/papers/2501.00958', 'title': '2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining', 'url': 'https://huggingface.co/papers/2501.00958', 'abstract': 'Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}.', 'score': 10, 'issue_id': 1475, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': 'b10f0cd62f6334fc', 'authors': ['Wenqi Zhang', 'Hang Zhang', 'Xin Li', 'Jiashuo Sun', 'Yongliang Shen', 'Weiming Lu', 'Deli Zhao', 'Yueting Zhuang', 'Lidong Bing'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University', 'DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2501.00958.jpg', 'data': {'categories': ['#science', '#dataset', '#reasoning', '#multimodal', '#cv', '#video'], 'emoji': '📚', 'ru': {'title': 'Мультимодальный учебник: новый стандарт для обучения VLM', 'desc': 'Эта статья представляет новый подход к обучению моделей компьютерного зрения и обработки естественного языка (VLM) с использованием мультимодального учебного корпуса. Авторы создали базу данных из 22 000 часов обучающих видео, систематически собранных с помощью таксономии, предложенной языковой моделью (LLM). Этот корпус отличается более высокой плотностью знаний, лучшей связью между изображениями и текстом, а также логической согласованностью по сравнению с существующими наборами данных. Эксперименты показывают превосходную производительность предобучения на этом корпусе, особенно в задачах, требующих глубоких знаний и рассуждений.'}, 'en': {'title': 'Harnessing Instructional Videos for Superior Vision-Language Model Training', 'desc': 'This paper presents a new approach to training Vision-Language Models (VLMs) using a multimodal textbook corpus derived from instructional videos. Unlike traditional datasets that often suffer from low knowledge density and weak image-text relationships, this corpus offers a richer and more coherent context for VLM pretraining. The authors systematically extract visual, audio, and textual information from over 22,000 hours of instructional content, enhancing the alignment between images and text. Experiments show that VLMs trained on this video-centric dataset perform significantly better on knowledge-intensive tasks, demonstrating improved reasoning and context awareness.'}, 'zh': {'title': '视频教材：提升视觉语言模型的知识与推理能力', 'desc': '本文提出了一种高质量的多模态教材语料库，旨在为视觉语言模型（VLM）提供更丰富的基础知识。该语料库收集了超过2.5年的教学视频，总计22,000小时，系统性地提取了视频中的视觉、音频和文本知识。与现有的数据集相比，这种视频中心的教材提供了更连贯的上下文、更丰富的知识和更好的图像-文本对齐。实验结果表明，基于该教材预训练的VLM在知识和推理密集型任务中表现优异，尤其在ScienceQA和MathVista等任务中。'}}}, {'id': 'https://huggingface.co/papers/2501.00599', 'title': 'VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM', 'url': 'https://huggingface.co/papers/2501.00599', 'abstract': 'Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.', 'score': 9, 'issue_id': 1474, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': 'daee687ce36ef3db', 'authors': ['Yuqian Yuan', 'Hang Zhang', 'Wentong Li', 'Zesen Cheng', 'Boqiang Zhang', 'Long Li', 'Xin Li', 'Deli Zhao', 'Wenqiao Zhang', 'Yueting Zhuang', 'Jianke Zhu', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00599.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точное пространственно-временное понимание видео с помощью VideoRefer Suite', 'desc': 'Статья представляет VideoRefer Suite - комплексный подход к улучшению пространственно-временного понимания видео большими языковыми моделями. Авторы разработали масштабный набор данных VideoRefer-700K с инструкциями на уровне объектов, созданный с помощью мультиагентного движка. Они также представили модель VideoRefer с универсальным пространственно-временным кодировщиком объектов. Для оценки возможностей видео-LLM был создан бенчмарк VideoRefer-Bench, охватывающий различные аспекты понимания видео.'}, 'en': {'title': 'Empowering Video LLMs for Fine-Grained Understanding', 'desc': 'This paper introduces the VideoRefer Suite, which enhances Video Large Language Models (Video LLMs) for better understanding of videos by focusing on fine-grained spatial and temporal details. It addresses the limitations of existing models that primarily focus on overall comprehension and lack high-quality object-level instruction data. The suite includes a new dataset called VideoRefer-700K, a specialized VideoRefer model with a spatial-temporal object encoder, and a benchmark for evaluating video understanding capabilities. Experimental results show that the VideoRefer model significantly improves performance on video referring tasks while also enhancing general video comprehension.'}, 'zh': {'title': '提升视频理解，细致捕捉空间与时间', 'desc': '视频大型语言模型（Video LLMs）在视频理解方面展现了出色的能力，但在捕捉细粒度的空间和时间细节上存在困难。为了应对这些挑战，我们提出了VideoRefer Suite，以增强视频LLM在空间-时间视频理解方面的能力。我们开发了一个多代理数据引擎，创建了一个高质量的对象级视频指令数据集VideoRefer-700K，并提出了VideoRefer模型，配备了多功能的空间-时间对象编码器。最后，我们创建了VideoRefer-Bench，以全面评估视频LLM的空间-时间理解能力，实验结果表明我们的模型在视频引用基准上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2501.01149', 'title': 'A3: Android Agent Arena for Mobile GUI Agents', 'url': 'https://huggingface.co/papers/2501.01149', 'abstract': 'AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai.github.io/Android-Agent-Arena/.', 'score': 7, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '050f155aa526c100', 'authors': ['Yuxiang Chai', 'Hanhao Li', 'Jiayu Zhang', 'Liang Liu', 'Guozhi Wang', 'Shuai Ren', 'Siyuan Huang', 'Hongsheng Li'], 'affiliations': ['EE department @ CUHK', 'MMLab @ CUHK'], 'pdf_title_img': 'assets/pdf/title_img/2501.01149.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents'], 'emoji': '🤖', 'ru': {'title': 'A3: Арена для тестирования мобильных AI-агентов в реальном мире', 'desc': 'Статья представляет новую платформу для оценки мобильных GUI-агентов под названием Android Agent Arena (A3). A3 предлагает реалистичные задачи, широкое пространство действий и автоматизированную оценку на основе больших языковых моделей. Платформа включает 21 популярное стороннее приложение и 201 задачу, отражающую типичные пользовательские сценарии. A3 позволяет оценивать производительность агентов в реальных условиях, что отличает её от существующих статических наборов данных.'}, 'en': {'title': 'Revolutionizing Mobile GUI Agent Evaluation with A3', 'desc': 'This paper introduces the Android Agent Arena (A3), a new evaluation platform for mobile GUI agents that addresses limitations in existing datasets. A3 focuses on real-world tasks, providing a larger action space that accommodates agents trained on various datasets. It features 21 popular third-party apps and 201 tasks that reflect common user scenarios, enhancing the assessment of agent performance. Additionally, A3 incorporates an automated evaluation process using large language models, reducing the need for extensive human involvement and coding skills.'}, 'zh': {'title': 'Android Agent Arena：移动GUI代理的新评估平台', 'desc': '近年来，人工智能代理的应用越来越广泛，尤其是在大型语言模型（LLMs）领域的进步推动下。移动图形用户界面（GUI）代理是人工智能代理的一种，旨在自主执行移动设备上的任务。现有的研究虽然提出了许多代理、数据集和基准，但大多数数据集仅关注静态框架评估，无法全面评估真实世界中的任务表现。为了解决这一问题，我们提出了Android Agent Arena（A3），这是一个新颖的评估平台，提供了实际的任务和更灵活的操作空间，支持基于LLM的自动化评估过程。'}}}, {'id': 'https://huggingface.co/papers/2501.01054', 'title': 'Dynamic Scaling of Unit Tests for Code Reward Modeling', 'url': 'https://huggingface.co/papers/2501.01054', 'abstract': 'Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).', 'score': 3, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '33b9590f2acb0e48', 'authors': ['Zeyao Ma', 'Xiaokang Zhang', 'Jing Zhang', 'Jifan Yu', 'Sijia Luo', 'Jie Tang'], 'affiliations': ['Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, China', 'School of Information, Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01054.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#small_models', '#rlhf', '#optimization'], 'emoji': '🧪', 'ru': {'title': 'Масштабирование юнит-тестов для повышения точности LLM в сложных задачах', 'desc': 'Эта статья посвящена улучшению точности больших языковых моделей (LLM) в задачах сложного мышления, таких как генерация кода. Авторы предлагают метод масштабирования юнит-тестов для повышения качества сигналов вознаграждения при оценке решений. Они разработали легковесный генератор юнит-тестов CodeRM-8B и механизм динамического масштабирования, адаптирующийся к сложности задачи. Эксперименты показали значительное улучшение производительности различных моделей на нескольких тестовых наборах.'}, 'en': {'title': 'Enhancing LLM Performance through Scaled Unit Testing', 'desc': 'This paper addresses the limitations of large language models (LLMs) in generating accurate responses for complex tasks like code generation. It highlights the issue of unreliable reward signals from LLM-generated unit tests, which can lead to incorrect solutions. The authors propose a novel approach, CodeRM-8B, which generates a larger number of unit tests to improve the quality of these reward signals. Their experiments demonstrate that scaling unit tests enhances LLM performance, particularly for more challenging problems, leading to significant improvements across various models.'}, 'zh': {'title': '提升单元测试质量，增强模型性能', 'desc': '当前的大型语言模型（LLMs）在复杂推理任务（如代码生成）中，往往难以在第一次尝试时产生准确的响应。以往的研究通过生成多个候选解决方案并使用LLM生成的单元测试进行验证来应对这一挑战。单元测试的执行结果作为奖励信号，用于识别正确的解决方案。然而，由于LLMs常常自信地犯错，这些单元测试的可靠性不足，从而降低了奖励信号的质量。我们提出了CodeRM-8B，一个轻量级且有效的单元测试生成器，能够高效地扩展单元测试，并根据问题的难度动态调整单元测试的数量，从而进一步提高效率。'}}}, {'id': 'https://huggingface.co/papers/2501.00192', 'title': 'MLLM-as-a-Judge for Image Safety without Human Labeling', 'url': 'https://huggingface.co/papers/2501.00192', 'abstract': 'Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.', 'score': 3, 'issue_id': 1474, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': '2a62bcbb87c1b7a5', 'authors': ['Zhenting Wang', 'Shuming Hu', 'Shiyu Zhao', 'Xiaowen Lin', 'Felix Juefei-Xu', 'Zhuowei Li', 'Ligong Han', 'Harihar Subramanyam', 'Li Chen', 'Jianfa Chen', 'Nan Jiang', 'Lingjuan Lyu', 'Shiqing Ma', 'Dimitris N. Metaxas', 'Ankit Jain'], 'affiliations': ['GenAI @ Meta', 'Rutgers University', 'UMass Amherst', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00192.jpg', 'data': {'categories': ['#reasoning', '#training', '#ethics', '#cv', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Интеллектуальная защита: Zero-shot оценка безопасности изображений с помощью MLLM', 'desc': 'Статья представляет метод определения безопасности изображений с использованием мультимодальных больших языковых моделей (MLLM) в режиме zero-shot. Авторы предлагают подход, включающий объективизацию правил безопасности, оценку релевантности между правилами и изображениями, и быстрое принятие решений на основе дебиасированных вероятностей токенов. Метод также включает каскадные цепочки рассуждений для более глубокого анализа при необходимости. Эксперименты показывают высокую эффективность предложенного метода для задач оценки безопасности изображений без предварительного обучения.'}, 'en': {'title': 'Zero-Shot Image Safety Detection with MLLMs', 'desc': 'This paper addresses the challenge of identifying unsafe images in the context of AI-generated content using Multimodal Large Language Models (MLLMs). The authors propose a novel approach that allows for zero-shot detection of harmful images by utilizing predefined safety rules without the need for extensive human labeling. They highlight the limitations of traditional methods, such as the subjectivity of safety rules and the biases present in models. The proposed method enhances safety judgment by objectifying rules, assessing their relevance to images, and employing a reasoning process that simplifies complex safety guidelines.'}, 'zh': {'title': '利用MLLMs实现零样本图像安全判断', 'desc': '随着在线平台视觉媒体的兴起，图像内容安全成为一个重要挑战。许多图像生成模型能够产生有害内容，因此识别不安全图像变得至关重要。我们提出了一种基于预训练多模态大语言模型（MLLMs）的方法，通过查询这些模型来检测不安全图像，而无需依赖人工标注。实验结果表明，我们的方法在零样本图像安全判断任务中非常有效。'}}}, {'id': 'https://huggingface.co/papers/2501.01423', 'title': 'Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models', 'url': 'https://huggingface.co/papers/2501.01423', 'abstract': 'Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.', 'score': 2, 'issue_id': 1473, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '173fa21b6e47d04c', 'authors': ['Jingfeng Yao', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2501.01423.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#architecture', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Революция в латентных диффузионных моделях: быстрее, лучше, эффективнее', 'desc': 'Статья представляет новый подход к улучшению латентных диффузионных моделей с архитектурой Трансформер для генерации изображений высокого качества. Авторы предлагают метод VA-VAE, который выравнивает латентное пространство с предобученными моделями компьютерного зрения. Это позволяет значительно расширить границы реконструкции-генерации и ускорить сходимость Диффузионных Трансформеров в высокоразмерных латентных пространствах. На основе VA-VAE авторы создали улучшенную модель LightningDiT, достигающую современного уровня производительности на задаче генерации изображений ImageNet 256x256.'}, 'en': {'title': 'Accelerating Image Generation with Aligned Latent Spaces', 'desc': 'This paper discusses the challenges faced by latent diffusion models, particularly when using Transformer architectures for image generation. It highlights an optimization issue where increasing the feature dimensions in visual tokenizers can lead to larger models and longer training times, often resulting in sub-optimal image quality. The authors propose a solution by aligning the latent space with pre-trained vision models, introducing a new framework called VA-VAE to enhance the training process. Their improved model, LightningDiT, achieves state-of-the-art performance in image generation while significantly speeding up the training process.'}, 'zh': {'title': '优化潜在扩散模型，提升图像生成效率', 'desc': '本论文探讨了潜在扩散模型与变换器架构在生成高质量图像时的优化困境。研究表明，虽然增加视觉标记器中的每个标记特征维度可以提高重建质量，但这也导致需要更大的扩散模型和更多的训练迭代。为了解决这一问题，作者提出将潜在空间与预训练的视觉基础模型对齐，从而提高训练效率。最终，提出的VA-VAE模型显著提升了潜在扩散模型的重建生成能力，并在ImageNet数据集上实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.01245', 'title': 'SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization', 'url': 'https://huggingface.co/papers/2501.01245', 'abstract': 'Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model\'s predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.', 'score': 1, 'issue_id': 1475, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '30d94590a5c78569', 'authors': ['Yongle Huang', 'Haodong Chen', 'Zhenbang Xu', 'Zihan Jia', 'Haozhou Sun', 'Dian Shao'], 'affiliations': ['School of Automation, Northwestern Polytechnical University, Xian, China', 'School of Computer Science, Northwestern Polytechnical University, Xian, China', 'School of Software, Northwestern Polytechnical University, Xian, China', 'Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01245.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#multimodal', '#optimization', '#training'], 'emoji': '🤸', 'ru': {'title': 'SeFAR: Прорыв в распознавании детализированных действий с помощью полу-контролируемого обучения', 'desc': 'Статья представляет новый подход к задаче распознавания детализированных действий (Fine-grained Action Recognition, FAR) с использованием полу-контролируемого обучения. Авторы предлагают фреймворк SeFAR, который включает в себя двухуровневые временные элементы для более эффективного представления действий и новую стратегию аугментации данных. SeFAR также использует адаптивную регуляцию для стабилизации процесса обучения при работе с неопределенностью в предсказаниях модели-учителя. Эксперименты показывают, что SeFAR достигает лучших результатов на нескольких наборах данных FAR и классических наборах данных для распознавания действий.'}, 'en': {'title': 'SeFAR: Elevating Fine-grained Action Recognition with Semi-supervised Learning', 'desc': "This paper focuses on improving Fine-grained Action Recognition (FAR), which identifies specific actions in short time frames. The authors introduce a semi-supervised learning framework called SeFAR, which uses innovative techniques to enhance the learning process despite the challenges of limited labeled data. They develop Dual-level temporal elements for better visual representation and implement a strong augmentation strategy within a Teacher-Student learning setup. The results demonstrate that SeFAR achieves top performance on FAR datasets and enhances multimodal models' understanding of detailed actions."}, 'zh': {'title': '细粒度动作识别的新突破', 'desc': '人类动作理解对多模态系统的发展至关重要。本文提出了一种新的框架SeFAR，专注于细粒度动作识别（FAR），旨在处理短时间内的详细语义标签。我们采用半监督学习（SSL）来减少对大量标注数据的需求，并通过构建双层时间元素和新的强增强策略来提高模型的表现。实验结果表明，SeFAR在多个数据集上达到了最先进的性能，证明了我们设计的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.00658', 'title': 'Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing', 'url': 'https://huggingface.co/papers/2501.00658', 'abstract': "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.", 'score': 0, 'issue_id': 1476, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': '253304ea64defbe0', 'authors': ['Peihao Wang', 'Ruisi Cai', 'Yuehao Wang', 'Jiajun Zhu', 'Pragya Srivastava', 'Zhangyang Wang', 'Pan Li'], 'affiliations': ['Georgia Tech', 'Google DeepMind', 'University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00658.jpg', 'data': {'categories': ['#training', '#open_source', '#long_context', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений SSM: баланс между недавностью и сглаживанием', 'desc': 'Структурированные модели пространства состояний (SSM) рассматриваются как альтернатива трансформерам в обработке длинных последовательностей. Исследование показало, что SSM имеют существенное ограничение в виде сильного смещения к недавним данным, что затрудняет запоминание отдаленной информации. Увеличение глубины SSM улучшает обработку длинных контекстов, но приводит к проблеме чрезмерного сглаживания. Авторы предлагают метод поляризации каналов матриц перехода состояний для решения этих проблем, что улучшает точность ассоциативного извлечения дальних токенов.'}, 'en': {'title': 'Balancing Recency and Over-Smoothing in SSMs', 'desc': "This paper discusses Structured State Space Models (SSMs) as alternatives to transformers, highlighting their limitations due to strong recency bias. This bias affects the models' ability to remember distant information and creates robustness issues. The authors propose a solution by polarizing the state transition matrices, which helps mitigate both recency bias and over-smoothing that occurs with deeper architectures. Their experiments show that this new approach improves the accuracy of recalling long-range tokens, allowing SSMs to effectively utilize deeper structures."}, 'zh': {'title': '解决近期偏见与过平滑的双重挑战', 'desc': '结构状态空间模型（SSMs）作为变换器的替代方案，虽然在捕捉长序列依赖性方面表现出色，但存在强烈的近期偏见限制。我们的实证研究表明，这种偏见影响了模型对远程信息的回忆能力，并引入了鲁棒性问题。通过扩展实验，我们发现SSMs的深层结构可以促进长上下文的学习，但理论分析显示，随着深度增加，模型会出现过平滑的趋势，使得标记表示变得难以区分。我们提出的极化技术通过将状态转移矩阵的两个通道设置为零和一，解决了近期偏见和过平滑的问题，显著提高了长距离标记的关联回忆准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.01257', 'title': 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings', 'url': 'https://huggingface.co/papers/2501.01257', 'abstract': 'With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.', 'score': 0, 'issue_id': 1475, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'e31430bb6ba5dfc8', 'authors': ['Shanghaoran Quan', 'Jiaxi Yang', 'Bowen Yu', 'Bo Zheng', 'Dayiheng Liu', 'An Yang', 'Xuancheng Ren', 'Bofei Gao', 'Yibo Miao', 'Yunlong Feng', 'Zekun Wang', 'Jian Yang', 'Zeyu Cui', 'Yang Fan', 'Yichang Zhang', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2501.01257.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#reasoning', '#optimization', '#open_source'], 'emoji': '🏆', 'ru': {'title': 'CodeElo: новый стандарт оценки LLM в соревновательном программировании', 'desc': 'Статья представляет новый бенчмарк CodeElo для оценки способностей больших языковых моделей (LLM) в решении задач по программированию соревновательного уровня. CodeElo основан на платформе CodeForces и включает проблемы с детальной информацией о сложности и алгоритмических тегах. Авторы разработали систему расчета рейтинга Эло, сопоставимую с рейтингами человеческих участников. Результаты тестирования 33 LLM показали, что модели o1-mini и QwQ-32B-Preview значительно превосходят остальные, достигая рейтингов 1578 и 1261 соответственно.'}, 'en': {'title': 'CodeElo: Elevating Code Generation Benchmarks for LLMs', 'desc': 'This paper presents CodeElo, a new benchmark designed to evaluate the coding abilities of large language models (LLMs) in a competitive setting. Unlike existing benchmarks, CodeElo addresses limitations such as the lack of private test cases and misaligned execution environments by utilizing the CodeForces platform. The benchmark includes a unique judging method and an Elo rating system that allows for fair comparisons between LLMs and human participants. Results indicate that certain models, like o1-mini, perform significantly better than others, highlighting the varying capabilities of LLMs in code generation tasks.'}, 'zh': {'title': 'CodeElo：提升代码生成能力的标准化基准测试', 'desc': '随着大型语言模型（LLMs）在代码推理能力上的提升，开发更具挑战性和全面性的基准测试变得愈发重要。现有的基准测试如LiveCodeBench和USACO存在一些不足，例如缺乏私有测试用例和特殊评判支持。为了解决这些问题，我们提出了CodeElo，这是一个标准化的竞赛级代码生成基准，首次有效应对这些挑战。通过在CodeForces平台上编译最近六个月的竞赛问题，我们为30个流行的开源和3个专有LLMs提供了Elo评分，结果显示o1-mini和QwQ-32B-Preview表现突出。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture (2)', '#audio', '#benchmark (4)', '#cv (3)', '#data', '#dataset (5)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (2)', '#optimization (6)', '#plp', '#rag', '#reasoning (6)', '#rl', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-03 06:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-03 06:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-03 06:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    