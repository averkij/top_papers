
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. December 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-12-09.html">â¬…ï¸ <span id="prev-date">09.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-12-11.html">â¡ï¸ <span id="next-date">11.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'};
        let feedDateNext = {'ru': '11.12', 'en': '12/11', 'zh': '12æœˆ11æ—¥'};
        let feedDatePrev = {'ru': '09.12', 'en': '12/09', 'zh': '12æœˆ9æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.08765', 'title': 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance', 'url': 'https://huggingface.co/papers/2512.08765', 'abstract': "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", 'score': 105, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '35af41ed6b0c44fa', 'authors': ['Ruihang Chu', 'Yefei He', 'Zhekai Chen', 'Shiwei Zhang', 'Xiaogang Xu', 'Bin Xia', 'Dingdong Wang', 'Hongwei Yi', 'Xihui Liu', 'Hengshuang Zhao', 'Yu Liu', 'Yingya Zhang', 'Yujiu Yang'], 'affiliations': ['CUHK', 'HKU', 'Tongyi Lab, Alibaba Group', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08765.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Wan-Move â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ²Ğ´Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MoveBench Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Precision Motion Control for High-Quality Video Synthesis', 'desc': 'Wan-Move is a novel framework that enhances motion control in video generative models by incorporating motion-aware features into the latent space. This approach addresses the limitations of existing methods, which often struggle with coarse control and scalability. By utilizing dense point trajectories to represent object motions, Wan-Move allows for precise and high-quality motion guidance during video synthesis. The integration of these features into standard image-to-video models enables efficient fine-tuning and generates videos with impressive motion controllability, as demonstrated through extensive evaluations on the newly developed MoveBench benchmark.'}, 'zh': {'title': 'Wan-Moveï¼šç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„è¿åŠ¨', 'desc': 'Wan-Move æ˜¯ä¸€ä¸ªå¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨æ§åˆ¶çš„æ¡†æ¶ï¼Œé€šè¿‡å°†è¿åŠ¨æ„ŸçŸ¥ç‰¹å¾æ•´åˆåˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œå®ç°é«˜è´¨é‡å’Œå¯æ‰©å±•çš„è§†é¢‘åˆæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è¿åŠ¨æ§åˆ¶æ–¹æ³•åœ¨æ§åˆ¶ç²’åº¦ç²—ç³™å’Œå¯æ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ï¼Œæä¾›äº†ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å¯†é›†çš„ç‚¹è½¨è¿¹è¡¨ç¤ºç‰©ä½“è¿åŠ¨ï¼Œä½¿å¾—åœºæ™¯æ§åˆ¶æ›´åŠ ç»†è‡´ï¼Œå¹¶å°†è¿™äº›è½¨è¿¹æŠ•å½±åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œç”Ÿæˆå¯¹é½çš„æ—¶ç©ºç‰¹å¾å›¾ã€‚æœ€ç»ˆï¼ŒWan-Move ç”Ÿæˆçš„ 5 ç§’ 480p è§†é¢‘åœ¨è¿åŠ¨å¯æ§æ€§ä¸Šä¸å•†ä¸šäº§å“ç›¸åª²ç¾ï¼Œä¸”æä¾›äº†å…¬å¼€çš„ä»£ç å’ŒåŸºå‡†æ•°æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08478', 'title': 'Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform', 'url': 'https://huggingface.co/papers/2512.08478', 'abstract': 'Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.', 'score': 69, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '5bdc819571b85c24', 'authors': ['Yuning Gong', 'Yifei Liu', 'Yifan Zhan', 'Muyao Niu', 'Xueying Li', 'Yuanjun Liao', 'Jiaming Chen', 'Yuanyuan Gao', 'Jiaqi Chen', 'Minming Chen', 'Li Zhou', 'Yuning Zhang', 'Wei Wang', 'Xiaoqing Hou', 'Huaxi Huang', 'Shixiang Tang', 'Le Ma', 'Dingwen Zhang', 'Xue Yang', 'Junchi Yan', 'Yanchi Zhang', 'Yinqiang Zheng', 'Xiao Sun', 'Zhihang Zhong'], 'affiliations': ['Northwestern Polytechnical University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Sichuan University', 'The University of Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08478.jpg', 'data': {'categories': ['#inference', '#multimodal', '#3d', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ‘Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ 3D ÑÑ†ĞµĞ½', 'desc': 'Visionary â€” ÑÑ‚Ğ¾ Ğ²ĞµĞ±-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° 3D Gaussian Splatting Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ WebGPU Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ONNX Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° GPU Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Gaussian Generator, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ½Ğ° GPU Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ TypeScript API, Visionary Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 3DGS-ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ world models.'}, 'en': {'title': 'Revolutionizing 3D Rendering with Visionary', 'desc': 'Visionary is a web-native platform that allows for real-time rendering of 3D Gaussian Splatting (3DGS) and meshes using efficient GPU-based inference. It addresses the challenges of existing viewer solutions that are often heavy and limited in supporting dynamic content. By utilizing a standardized Gaussian Generator contract, Visionary enables the generation and updating of Gaussians in real-time, enhancing the rendering process. This platform also integrates seamlessly with existing web applications through a TypeScript API, making it easier to deploy and compare various 3DGS methods.'}, 'zh': {'title': 'Visionaryï¼šå®æ—¶æ¸²æŸ“3Dé«˜æ–¯ç‚¹äº‘çš„å¼€æ”¾å¹³å°', 'desc': 'Visionaryæ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç½‘ç»œåŸç”Ÿå¹³å°ï¼Œèƒ½å¤Ÿå®æ—¶æ¸²æŸ“3Dé«˜æ–¯ç‚¹äº‘å’Œç½‘æ ¼ï¼Œæ”¯æŒé«˜æ•ˆçš„åŸºäºGPUçš„æ¨ç†ã€‚è¯¥å¹³å°è§£å†³äº†ç°æœ‰æŸ¥çœ‹å™¨çš„ç¢ç‰‡åŒ–å’Œé‡è´Ÿæ‹…é—®é¢˜ï¼Œæä¾›äº†åŠ¨æ€å†…å®¹å’Œç”Ÿæˆæ¨¡å‹çš„æ”¯æŒã€‚Visionaryé€šè¿‡æ ‡å‡†åŒ–çš„é«˜æ–¯ç”Ÿæˆå™¨åˆçº¦ï¼Œå…è®¸æ¯å¸§ç”Ÿæˆæˆ–æ›´æ–°é«˜æ–¯ï¼Œç®€åŒ–äº†ç®—æ³•çš„é›†æˆã€‚å®éªŒè¡¨æ˜ï¼ŒVisionaryåœ¨ç›¸åŒçš„3Dé«˜æ–¯ç‚¹äº‘èµ„äº§ä¸‹ï¼Œæ¸²æŸ“æ•ˆç‡ä¼˜äºç°æœ‰çš„WebæŸ¥çœ‹å™¨ï¼Œæ˜¾è‘—é™ä½äº†3Dé«˜æ–¯ç‚¹äº‘æ–¹æ³•çš„é‡ç°å’Œéƒ¨ç½²é—¨æ§›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07951', 'title': 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality', 'url': 'https://huggingface.co/papers/2512.07951', 'abstract': "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", 'score': 44, 'issue_id': 2, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '2f09092cb73adc87', 'authors': ['Zekai Luo', 'Zongze Du', 'Zhouhang Zhu', 'Hao Zhong', 'Muzhi Zhu', 'Wen Wang', 'Yuling Xi', 'Chenchen Jing', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07951.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ğ° Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'LivingSwap â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ ÑÑÑ‹Ğ»ĞºÑƒ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Face2Face Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ†Ğ°, Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Seamless Video Face Swapping with LivingSwap', 'desc': "LivingSwap is a novel approach to video face swapping that utilizes keyframes and reference guidance to enhance identity preservation and visual fidelity across lengthy video sequences. By leveraging keyframes as conditioning signals, the model allows for flexible editing while maintaining temporal coherence. It introduces a paired dataset, Face2Face, to facilitate effective training and ensure high-quality results. The method significantly reduces manual effort in production, achieving state-of-the-art performance in integrating target identities with the source video's attributes."}, 'zh': {'title': 'LivingSwapï¼šé«˜ä¿çœŸè§†é¢‘é¢éƒ¨äº¤æ¢çš„æ–°æ–¹æ³•', 'desc': 'LivingSwap æ˜¯ä¸€ç§è§†é¢‘é¢éƒ¨äº¤æ¢æŠ€æœ¯ï¼Œé€šè¿‡ä½¿ç”¨å…³é”®å¸§å’Œå‚è€ƒå¼•å¯¼æ¥å¢å¼ºèº«ä»½å’Œä¿çœŸåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åºåˆ—ä¸­ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨å…³é”®å¸§ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œçµæ´»åœ°æ³¨å…¥ç›®æ ‡èº«ä»½ï¼Œä»è€Œå®ç°å¯æ§çš„ç¼–è¾‘ã€‚é€šè¿‡ç»“åˆå…³é”®å¸§æ¡ä»¶å’Œè§†é¢‘å‚è€ƒå¼•å¯¼ï¼Œæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ—¶é—´æ‹¼æ¥ï¼Œç¡®ä¿åœ¨é•¿è§†é¢‘åºåˆ—ä¸­ç¨³å®šåœ°ä¿æŒèº«ä»½å’Œé«˜ä¿çœŸé‡å»ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLivingSwap åœ¨å‡å°‘äººå·¥å·¥ä½œé‡çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07802', 'title': 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory', 'url': 'https://huggingface.co/papers/2512.07802', 'abstract': 'OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.', 'score': 34, 'issue_id': 2, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '441bb46d54effdc9', 'authors': ['Zhaochong An', 'Menglin Jia', 'Haonan Qiu', 'Zijian Zhou', 'Xiaoke Huang', 'Zhiheng Liu', 'Weiming Ren', 'Kumara Kahatapitiya', 'Ding Liu', 'Sen He', 'Chenyang Zhang', 'Tao Xiang', 'Fanny Yang', 'Serge Belongie', 'Tian Xie'], 'affiliations': ['Meta AI', 'University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07802.jpg', 'data': {'categories': ['#story_generation', '#dataset', '#open_source', '#long_context', '#synthetic', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑĞºĞ°Ğ·Ğ¾Ğ²', 'desc': 'OneStory Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Frame Selection Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸ Adaptive Conditioner Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ image-to-video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 60K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ¸Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'OneStory: Crafting Coherent Narratives in Multi-Shot Video Generation', 'desc': 'OneStory is a novel approach to generating coherent multi-shot videos by effectively modeling the context across different shots. It introduces a Frame Selection module that builds a global memory of relevant frames from previous shots, enhancing narrative continuity. Additionally, the Adaptive Conditioner uses importance-guided techniques to create a compact context for generating the next shot, improving the overall coherence of the video. By leveraging pretrained image-to-video models and a specially curated dataset, OneStory achieves superior performance in generating complex narratives.'}, 'zh': {'title': 'OneStoryï¼šè¿è´¯å™äº‹çš„å¤šé•œå¤´è§†é¢‘ç”Ÿæˆ', 'desc': 'OneStory æ˜¯ä¸€ç§ç”Ÿæˆè¿è´¯å¤šé•œå¤´è§†é¢‘çš„æ–¹æ³•ï¼Œé€šè¿‡å¸§é€‰æ‹©æ¨¡å—å’Œè‡ªé€‚åº”è°ƒèŠ‚å™¨æ¥å»ºæ¨¡å…¨å±€è·¨é•œå¤´ä¸Šä¸‹æ–‡ã€‚è¯¥æ–¹æ³•å°†å¤šé•œå¤´è§†é¢‘ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªé•œå¤´ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå¼ºè§†è§‰æ¡ä»¶åŒ–ã€‚OneStory å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå¸§é€‰æ‹©æ¨¡å—æ„å»ºåŸºäºå…ˆå‰é•œå¤´çš„ä¿¡æ¯å¸§çš„è¯­ä¹‰ç›¸å…³å…¨å±€è®°å¿†ï¼Œè‡ªé€‚åº”è°ƒèŠ‚å™¨åˆ™é€šè¿‡é‡è¦æ€§å¼•å¯¼çš„åˆ†å—ç”Ÿæˆç´§å‡‘çš„ä¸Šä¸‹æ–‡ã€‚ç»è¿‡åœ¨æˆ‘ä»¬ç­–åˆ’çš„ 60K æ•°æ®é›†ä¸Šå¾®è°ƒï¼ŒOneStory åœ¨å¤šæ ·åŒ–å’Œå¤æ‚åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å™äº‹è¿è´¯æ€§ï¼Œæ”¯æŒå¯æ§å’Œæ²‰æµ¸å¼çš„é•¿ç¯‡è§†é¢‘è®²è¿°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06776', 'title': 'From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2512.06776', 'abstract': 'Adapting autoregressive models to block-wise diffusion enables parallel generation and retains pretrained knowledge, achieving state-of-the-art performance among 7B-class diffusion language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.', 'score': 20, 'issue_id': 2, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': '87da6ba7e0ddb824', 'authors': ['Yuchuan Tian', 'Yuchen Liang', 'Jiacheng Sun', 'Shuo Zhang', 'Guangwen Yang', 'Yingte Shu', 'Sibo Fang', 'Tianyu Guo', 'Kai Han', 'Chao Xu', 'Hanting Chen', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ['Huawei Technologies', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06776.jpg', 'data': {'categories': ['#optimization', '#open_source', '#small_models', '#diffusion', '#architecture', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ (Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ»Ğ¾ĞºĞ° Ñ€Ğ°Ğ²ĞµĞ½ 1) Ğº Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NBDiff-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° ÑÑ€ĞµĞ´Ğ¸ 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Efficiently Bridging Autoregressive and Block-Diffusion Models', 'desc': 'This paper presents a method to adapt autoregressive (AR) models to block-wise diffusion, allowing for faster and more efficient text generation. By treating AR as a special case of block-diffusion, the authors create a pathway that retains the knowledge from pretrained models while enabling parallel processing. They introduce a context-causal attention mask and an auxiliary AR loss to enhance data utilization and maintain consistency during training and inference. The resulting model, NBDiff-7B, achieves state-of-the-art performance among 7B-class diffusion language models, excelling in various benchmarks.'}, 'zh': {'title': 'è‡ªå›å½’åˆ°å—çŠ¶æ‰©æ•£çš„é«˜æ•ˆé€‚åº”', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å°†è‡ªå›å½’æ¨¡å‹é€‚åº”äºå—çŠ¶æ‰©æ•£çš„æ–¹æ³•ï¼Œä»¥å®ç°å¹¶è¡Œç”Ÿæˆå¹¶ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚é€šè¿‡å°†è‡ªå›å½’è§†ä¸ºå—çŠ¶æ‰©æ•£ï¼ˆå—å¤§å°ä¸º1ï¼‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€‚åº”è·¯å¾„ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å› æœæ³¨æ„åŠ›æ©ç å’Œé«˜æ•ˆçš„å¹¶è¡Œé€‚åº”ç¨‹åºã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†ä¹‹é—´ä¿æŒä¸€è‡´æ€§ï¼Œå¹¶åœ¨7Bçº§æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§é€‚åº”æ–¹æ³•æ˜¯ä¸€ç§æœ‰æ•ˆä¸”è®¡ç®—é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¼˜äºä»å¤´è®­ç»ƒæ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07843', 'title': 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models', 'url': 'https://huggingface.co/papers/2512.07843', 'abstract': "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.", 'score': 19, 'issue_id': 2, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '48ba940c338c4183', 'authors': ['Long Lian', 'Sida Wang', 'Felix Juefei-Xu', 'Tsu-Jui Fu', 'Xiuyu Li', 'Adam Yala', 'Trevor Darrell', 'Alane Suhr', 'Yuandong Tian', 'Xi Victoria Lin'], 'affiliations': ['Meta Superintelligence Labs (MSL)', 'UC Berkeley', 'UCSF'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07843.jpg', 'data': {'categories': ['#inference', '#reasoning', '#optimization', '#benchmark', '#training', '#rlhf', '#math'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ThreadWeaver â€” ÑÑ‚Ğ¾æ¡†æ¶ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning. Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ trie-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ reinforcement learning Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¿Ğ»Ğ¾Ñ‚ÑŒ Ğ´Ğ¾ 1,53 Ñ€Ğ°Ğ·Ğ°.'}, 'en': {'title': 'ThreadWeaver: Speeding Up Reasoning Without Sacrificing Accuracy', 'desc': 'ThreadWeaver is a novel framework designed to enhance adaptive parallel reasoning in machine learning models. It achieves high accuracy similar to traditional sequential models while significantly reducing the time taken for inference. The framework utilizes a two-stage parallel trajectory generator, a trie-based training-inference co-design, and a reinforcement learning approach that focuses on balancing accuracy and parallelization. This results in improved performance on complex reasoning tasks, demonstrating a new balance between efficiency and accuracy in large language models.'}, 'zh': {'title': 'ThreadWeaverï¼šæå‡æ¨ç†æ•ˆç‡çš„æ–°æ¡†æ¶', 'desc': 'ThreadWeaveræ˜¯ä¸€ä¸ªè‡ªé€‚åº”å¹¶è¡Œæ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿æŒä¸ä¼ ç»Ÿé¡ºåºæ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡å¹¶è¡Œè½¨è¿¹ç”Ÿæˆã€åŸºäºå­—å…¸æ ‘çš„è®­ç»ƒ-æ¨ç†ååŒè®¾è®¡ä»¥åŠå…³æ³¨å¹¶è¡ŒåŒ–çš„å¼ºåŒ–å­¦ä¹ ç­‰ä¸‰é¡¹åˆ›æ–°æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ThreadWeaveråœ¨å…­ä¸ªå¤æ‚çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡ä¸å…ˆè¿›çš„é¡ºåºæ¨ç†æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨æ¨ç†å»¶è¿Ÿä¸Šå®ç°äº†é«˜è¾¾1.53å€çš„åŠ é€Ÿã€‚è¯¥æ¡†æ¶ä¸ºå‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´å»ºç«‹äº†æ–°çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06864', 'title': 'Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training', 'url': 'https://huggingface.co/papers/2512.06864', 'abstract': 'AutoQ-VIS achieves state-of-the-art results in unsupervised Video Instance Segmentation using quality-guided self-training to bridge the synthetic-to-real domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.', 'score': 12, 'issue_id': 2, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': '39be88a1bc10da6c', 'authors': ['Kaixuan Lu', 'Mehmet Onurcan Kaya', 'Dim P. Papadopoulos'], 'affiliations': ['Pioneer Centre for AI', 'Technical University of Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06864.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#video', '#training', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'AutoQ-VIS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ Ğ¿ĞµÑ‚Ğ»Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ 52.6 AP Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ YouTubeVIS-2019, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ Ñ€ĞµĞºĞ¾Ñ€Ğ´ Ğ½Ğ° 4.4% Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging the Gap: Quality-Guided Self-Training for Unsupervised Video Segmentation', 'desc': 'AutoQ-VIS is an innovative framework for unsupervised Video Instance Segmentation (VIS) that addresses the challenges of annotation by using quality-guided self-training. It effectively bridges the gap between synthetic and real video data, which is crucial for improving model performance. The method creates a closed-loop system that generates pseudo-labels and assesses their quality, allowing for continuous improvement in segmentation accuracy. With a reported performance of 52.6 AP_{50} on the YouTubeVIS-2019 validation set, AutoQ-VIS outperforms previous methods without the need for human annotations, showcasing the potential of quality-aware self-training in VIS tasks.'}, 'zh': {'title': 'è´¨é‡å¼•å¯¼è‡ªæˆ‘è®­ç»ƒï¼Œçªç ´è§†é¢‘å®ä¾‹åˆ†å‰²çš„ç•Œé™', 'desc': 'AutoQ-VISæ˜¯ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´çš„å·®è·ã€‚è¯¥æ–¹æ³•é€šè¿‡è´¨é‡å¼•å¯¼çš„è‡ªæˆ‘è®­ç»ƒï¼Œå»ºç«‹äº†ä¼ªæ ‡ç­¾ç”Ÿæˆä¸è‡ªåŠ¨è´¨é‡è¯„ä¼°ä¹‹é—´çš„é—­ç¯ç³»ç»Ÿï¼Œä»è€Œå®ç°äº†ä»åˆæˆè§†é¢‘åˆ°çœŸå®è§†é¢‘çš„é€æ­¥é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoQ-VISåœ¨YouTubeVIS-2019éªŒè¯é›†ä¸Šè¾¾åˆ°äº†52.6çš„AP_{50}ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•VideoCutLERã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†è´¨é‡æ„ŸçŸ¥è‡ªæˆ‘è®­ç»ƒåœ¨æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²ä¸­çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06628', 'title': 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment', 'url': 'https://huggingface.co/papers/2512.06628', 'abstract': 'MIND-V generates long-horizon robotic manipulation videos by integrating semantic reasoning, domain-invariant representations, and physical plausibility through a hierarchical framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.', 'score': 12, 'issue_id': 2, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': 'b20d67c0287a6a3c', 'authors': ['Ruicheng Zhang', 'Mingyang Zhang', 'Jun Zhou', 'Zhangrui Guo', 'Xiaofan Liu', 'Zunnan Xu', 'Zhizhou Zhong', 'Puxin Yan', 'Haocheng Luo', 'Xiu Li'], 'affiliations': ['Central South University', 'China University of Geosciences', 'Hong Kong University of Science and Technology', 'Sun Yat-sen University', 'Tsinghua University', 'X Square Robot'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06628.jpg', 'data': {'categories': ['#reasoning', '#robotics', '#optimization', '#rl', '#multimodal', '#synthetic', '#video', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼', 'desc': 'MIND-V â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° V-JEPA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹.'}, 'en': {'title': 'MIND-V: Bridging Reasoning and Realism in Robotic Video Generation', 'desc': 'MIND-V is a new framework that generates long videos of robots performing complex tasks by combining reasoning about actions, consistent representations, and realistic physical behavior. It addresses the challenge of limited data for robotic manipulation by synthesizing videos that are both logically coherent and physically plausible. The framework includes components for planning tasks, translating instructions into representations, and rendering videos, all while ensuring the actions follow physical laws. By using advanced techniques like reinforcement learning and a world model, MIND-V achieves impressive results in creating detailed robotic manipulation videos.'}, 'zh': {'title': 'MIND-Vï¼šç”Ÿæˆé•¿æ—¶é—´è·¨åº¦çš„æœºå™¨äººæ“ä½œè§†é¢‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'MIND-V æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆé•¿æ—¶é—´è·¨åº¦çš„æœºå™¨äººæ“ä½œè§†é¢‘ã€‚å®ƒé€šè¿‡ç»“åˆè¯­ä¹‰æ¨ç†ã€é¢†åŸŸä¸å˜è¡¨ç¤ºå’Œç‰©ç†åˆç†æ€§ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åªèƒ½ç”ŸæˆçŸ­è§†é¢‘çš„é™åˆ¶ã€‚MIND-V åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè¯­ä¹‰æ¨ç†ä¸­å¿ƒã€è¡Œä¸ºè¯­ä¹‰æ¡¥å’Œè¿åŠ¨è§†é¢‘ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿå°†é«˜å±‚æ¬¡æ¨ç†ä¸åƒç´ çº§åˆæˆç›¸ç»“åˆã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†åŸºäºç‰©ç†å‰ç»ä¸€è‡´æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†è§„å¾‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08186', 'title': 'Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation', 'url': 'https://huggingface.co/papers/2512.08186', 'abstract': 'DualVLN integrates high-level reasoning and low-level action execution to improve vision-language navigation in dynamic environments, achieving robust real-time control and long-horizon planning.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.', 'score': 7, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '507c84e420909927', 'authors': ['Meng Wei', 'Chenyang Wan', 'Jiaqi Peng', 'Xiqian Yu', 'Yuqiang Yang', 'Delin Feng', 'Wenzhe Cai', 'Chenming Zhu', 'Tai Wang', 'Jiangmiao Pang', 'Xihui Liu'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08186.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#benchmark', '#diffusion', '#interpretability', '#architecture'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ + Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'desc': 'DualVLN â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° 2 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° 1 Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'DualVLN: Navigating the Future with Dual Intelligence', 'desc': 'DualVLN is a novel approach to vision-language navigation that combines high-level reasoning with low-level action execution. It features a dual-system architecture where System 2 acts as a global planner using a vision-language model to set mid-term goals, while System 1 executes actions quickly using a lightweight Diffusion Transformer. This design allows for smoother and more accurate navigation, particularly in dynamic environments where obstacles may change. By separating the training processes, DualVLN maintains the generalization capabilities of the vision-language model while enhancing local decision-making and real-time control.'}, 'zh': {'title': 'åŒç³»ç»Ÿæ¨¡å‹ï¼Œæå‡è§†è§‰-è¯­è¨€å¯¼èˆªèƒ½åŠ›', 'desc': 'DualVLNæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰-è¯­è¨€å¯¼èˆªæ¨¡å‹ï¼Œå®ƒç»“åˆäº†é«˜å±‚æ¬¡æ¨ç†å’Œä½å±‚æ¬¡åŠ¨ä½œæ‰§è¡Œï¼Œä»¥æé«˜åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªç³»ç»ŸååŒå·¥ä½œï¼šç³»ç»Ÿ2è´Ÿè´£å…¨å±€è§„åˆ’ï¼Œåˆ©ç”¨å›¾åƒåŸºç¡€æ¨ç†é¢„æµ‹ä¸­æœŸç›®æ ‡ï¼›ç³»ç»Ÿ1åˆ™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ç­–ç•¥ï¼Œå¿«é€Ÿç”Ÿæˆå¹³æ»‘çš„è½¨è¿¹ã€‚é€šè¿‡è¿™ç§åŒç³»ç»Ÿè®¾è®¡ï¼ŒDualVLNèƒ½å¤Ÿåœ¨å¤æ‚çš„åŠ¨æ€ç¯å¢ƒä¸­å®ç°å®æ—¶æ§åˆ¶å’Œè‡ªé€‚åº”å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDualVLNåœ¨æ‰€æœ‰è§†è§‰-è¯­è¨€å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹é•¿æ—¶é—´è§„åˆ’å’ŒåŠ¨æ€ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07921', 'title': 'DeepCode: Open Agentic Coding', 'url': 'https://huggingface.co/papers/2512.07921', 'abstract': 'DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.', 'score': 7, 'issue_id': 2, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '2209d884eee8a8b2', 'authors': ['Zongwei Li', 'Zhonghang Li', 'Zirui Guo', 'Xubin Ren', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07921.jpg', 'data': {'categories': ['#optimization', '#plp', '#agents', '#long_context', '#benchmark', '#rag', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¸ÑˆĞµÑ‚ ĞºĞ¾Ğ´ Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹', 'desc': 'DeepCode â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ° ÑĞ²ÑĞ·Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PaperBench DeepCode Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ PhD-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': "Transforming Documents into Code with DeepCode's Autonomous Framework", 'desc': 'DeepCode is an innovative framework designed to improve the process of converting documents into codebases by managing information flow effectively. It tackles the challenges of high-fidelity synthesis by optimizing how information is processed, using techniques like source compression and structured indexing. The framework employs retrieval-augmented generation for knowledge injection and implements error correction to enhance accuracy. Evaluations show that DeepCode not only outperforms existing commercial coding agents but also exceeds the capabilities of human experts in producing high-quality code from scientific papers.'}, 'zh': {'title': 'DeepCodeï¼šæ–‡æ¡£åˆ°ä»£ç çš„æ™ºèƒ½åˆæˆæ–°çºªå…ƒ', 'desc': 'DeepCodeæ˜¯ä¸€ä¸ªå®Œå…¨è‡ªä¸»çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æ¡£åˆ°ä»£ç åº“åˆæˆçš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡ä¼˜åŒ–ä¿¡æ¯æµï¼Œé‡‡ç”¨æºå‹ç¼©ã€ç»“æ„åŒ–ç´¢å¼•ã€çŸ¥è¯†æ³¨å…¥å’Œé”™è¯¯ä¿®æ­£ç­‰å››ä¸ªä¿¡æ¯æ“ä½œï¼Œæ˜¾è‘—æé«˜äº†åˆæˆçš„å‡†ç¡®æ€§ã€‚DeepCodeå°†ä»£ç åº“åˆæˆè§†ä¸ºä¸€ä¸ªé€šé“ä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œåœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡é¢„ç®—ä¸‹æœ€å¤§åŒ–ä»»åŠ¡ç›¸å…³ä¿¡å·ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒDeepCodeåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†é¢†å…ˆçš„å•†ä¸šä»£ç†å’Œé¡¶å°–äººç±»ä¸“å®¶ï¼Œå¼€åˆ›äº†è‡ªä¸»ç§‘å­¦å†ç°çš„æ–°åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08153', 'title': 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models', 'url': 'https://huggingface.co/papers/2512.08153', 'abstract': 'TreeGRPO, a novel RL framework, enhances training efficiency for generative models by using a tree-structured denoising process, leading to faster training and better performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.', 'score': 4, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '38c97e4f2def5493', 'authors': ['Zheng Ding', 'Weirui Ye'], 'affiliations': ['MIT', 'UC San Diego'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08153.jpg', 'data': {'categories': ['#optimization', '#rl', '#alignment', '#benchmark', '#diffusion', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TreeGRPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ†ĞµĞ»Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 2.4 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'TreeGRPO: Efficient Reinforcement Learning for Generative Models', 'desc': 'TreeGRPO is a new reinforcement learning (RL) framework designed to improve the training efficiency of generative models. It utilizes a tree-structured denoising process that allows for better sample efficiency and faster training times. By branching from shared initial noise samples, it generates multiple candidate trajectories while reusing common prefixes, which enhances performance. This method also allows for fine-grained credit assignment and amortized computation, leading to significant improvements over traditional trajectory-based methods.'}, 'zh': {'title': 'TreeGRPOï¼šé«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ–°æ–¹æ³•', 'desc': 'TreeGRPOæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ ‘çŠ¶å»å™ªè¿‡ç¨‹æé«˜ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚å®ƒå°†å»å™ªè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºæœç´¢æ ‘ï¼Œä»å…±äº«çš„åˆå§‹å™ªå£°æ ·æœ¬ä¸­æˆ˜ç•¥æ€§åœ°åˆ†æ”¯ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰è½¨è¿¹ï¼Œå¹¶æœ‰æ•ˆé‡ç”¨å®ƒä»¬çš„å…¬å…±å‰ç¼€ã€‚è¯¥æ–¹æ³•å…·æœ‰ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šé«˜æ ·æœ¬æ•ˆç‡ã€ç»†ç²’åº¦çš„å¥–åŠ±åå‘ä¼ æ’­ä»¥åŠæ‘Šé”€è®¡ç®—ï¼Œèƒ½å¤Ÿåœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­è¿›è¡Œå¤šæ¬¡ç­–ç•¥æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeGRPOåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«2.4å€ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸å¥–åŠ±çš„æƒè¡¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08309', 'title': 'Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation', 'url': 'https://huggingface.co/papers/2512.08309', 'abstract': 'Terrain Diffusion uses diffusion models and a novel algorithm called InfiniteDiffusion to generate realistic, seamless, and boundless procedural worlds with constant-time random access.  \t\t\t\t\tAI-generated summary \t\t\t\t For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.', 'score': 2, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '19d0c940763ad27f', 'authors': ['Alexander Goslin'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08309.jpg', 'data': {'categories': ['#diffusion', '#3d', '#training', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Terrain Diffusion â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ ÑˆÑƒĞ¼Ğ° ĞŸĞµÑ€Ğ»Ğ¸Ğ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ InfiniteDiffusion, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ, ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ»ÑĞ±Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‚ĞµĞº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸, Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ñ†ĞµĞ»Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºÑƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Endless Realism: Infinite Worlds with Terrain Diffusion', 'desc': 'Terrain Diffusion is a new approach that combines diffusion models with a unique algorithm called InfiniteDiffusion to create realistic and endless virtual worlds. Unlike traditional methods like Perlin noise, which can lack realism, this method ensures high fidelity while maintaining the ability to generate infinite landscapes. The system uses a hierarchical structure of diffusion models to balance large-scale planetary features with intricate local details. Additionally, it incorporates an open-source framework for efficient memory management and quick generation, making it a powerful tool for creating coherent and expansive procedural environments.'}, 'zh': {'title': 'æ— ç¼ç”Ÿæˆæ— é™ä¸–ç•Œçš„æ‰©æ•£æ¨¡å‹', 'desc': 'Terrain Diffusion æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œåä¸º InfiniteDiffusion çš„æ–°ç®—æ³•ç”Ÿæˆé€¼çœŸã€æ— ç¼ä¸”æ— é™çš„ç¨‹åºåŒ–ä¸–ç•Œçš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç¨‹åºå™ªå£°å‡½æ•°ï¼ˆå¦‚ Perlin å™ªå£°ï¼‰ç›¸æ¯”ï¼Œå®ƒåœ¨ä¿æŒå¿«é€Ÿå’Œæ— é™çš„åŒæ—¶ï¼Œæå‡äº†çœŸå®æ„Ÿå’Œå¤§è§„æ¨¡ä¸€è‡´æ€§ã€‚è¯¥ç®—æ³•é€šè¿‡å±‚æ¬¡åŒ–çš„æ‰©æ•£æ¨¡å‹ç»“åˆäº†è¡Œæ˜ŸèƒŒæ™¯å’Œå±€éƒ¨ç»†èŠ‚ï¼Œç¡®ä¿äº†è¾“å‡ºçš„ç¨³å®šæ€§ã€‚æ•´ä½“ä¸Šï¼Œè¿™äº›æŠ€æœ¯ä½¿å¾—æ‰©æ•£æ¨¡å‹æˆä¸ºç¨‹åºåŒ–ä¸–ç•Œç”Ÿæˆçš„å®ç”¨åŸºç¡€ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åˆæˆæ•´ä¸ªæ˜Ÿçƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06531', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'url': 'https://huggingface.co/papers/2512.06531', 'abstract': 'Two novel deep learning architectures, SAETCN and SAS-Net, achieve high accuracy in classifying and segmenting brain tumors from MRI scans.  \t\t\t\t\tAI-generated summary \t\t\t\t Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.', 'score': 2, 'issue_id': 2, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '8d34cff6bbc1e65a', 'authors': ['Sayan Das', 'Arghadip Biswas'], 'affiliations': ['IIIT Delhi', 'Jadavpur University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06531.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#cv', '#science'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ½Ğ° ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…. ĞŸĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, SAETCN, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ (Ğ³Ğ»Ğ¸Ğ¾Ğ¼Ğ°, Ğ¼ĞµĞ½Ğ¸Ğ½Ğ³Ğ¸Ğ¾Ğ¼Ğ°, Ğ¿Ğ¸Ñ‚ÑƒĞ¸Ñ‚Ğ°Ñ€Ğ½Ğ°Ñ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ÑŒ) Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 99.38% Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, SAS-Net, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 99.23%. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ….'}, 'en': {'title': 'Revolutionizing Brain Tumor Detection with Deep Learning', 'desc': 'This paper presents two innovative deep learning architectures, SAETCN and SAS-Net, designed for the classification and segmentation of brain tumors from MRI scans. SAETCN utilizes self-attention mechanisms to classify various types of brain tumors, achieving an impressive accuracy of 99.38% on the validation dataset. Meanwhile, SAS-Net focuses on accurately segmenting brain tumors, reaching a pixel accuracy of 99.23%. These models address the challenges of manual detection and aim to enhance early diagnosis and treatment of brain tumors through automated processes.'}, 'zh': {'title': 'æ·±åº¦å­¦ä¹ åŠ©åŠ›è„‘è‚¿ç˜¤æ—©æœŸæ£€æµ‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ï¼ŒSAETCNå’ŒSAS-Netï¼Œæ—¨åœ¨ä»MRIæ‰«æä¸­é«˜æ•ˆåˆ†ç±»å’Œåˆ†å‰²è„‘è‚¿ç˜¤ã€‚è„‘è‚¿ç˜¤çš„æ—©æœŸå‡†ç¡®æ£€æµ‹å¯¹äºæ”¹å–„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„æ‰‹åŠ¨æ£€æµ‹æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥å¤„ç†å¤§é‡æ•°æ®ã€‚SAETCNåœ¨éªŒè¯æ•°æ®é›†ä¸Šè¾¾åˆ°äº†99.38%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè€ŒSAS-Netåœ¨åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†99.23%çš„åƒç´ å‡†ç¡®ç‡ã€‚è¿™äº›æ¨¡å‹å±•ç¤ºäº†äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿä¸ºè„‘è‚¿ç˜¤çš„æ—©æœŸæ£€æµ‹æä¾›æœ‰æ•ˆæ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08730', 'title': 'SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images', 'url': 'https://huggingface.co/papers/2512.08730', 'abstract': "A preliminary exploration of using SAM 3 for remote sensing open-vocabulary semantic segmentation demonstrates promising results through a mask fusion strategy and presence score filtering.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.", 'score': 1, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': 'd06af54aaee7980f', 'authors': ['Kaiyu Li', 'Shengqi Zhang', 'Yupeng Deng', 'Zhi Wang', 'Deyu Meng', 'Xiangyong Cao'], 'affiliations': ['Chinese Academy of Sciences', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08730.jpg', 'data': {'categories': ['#science', '#multimodal', '#cv', '#open_source'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'SAM 3 Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Segment Anything Model 3 (SAM 3) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼ Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¸Ğ· Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ SAM 3 Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Transformer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² ÑÑ†ĞµĞ½Ğµ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unlocking Remote Sensing with SAM 3: A New Era in Open-Vocabulary Segmentation', 'desc': "This paper explores the use of the Segment Anything Model 3 (SAM 3) for open-vocabulary semantic segmentation in remote sensing without requiring any training. It introduces a mask fusion strategy that combines outputs from both the semantic segmentation head and the instance head of SAM 3 to improve land coverage accuracy. Additionally, the authors implement presence score filtering to eliminate irrelevant categories, which helps reduce false positives in complex geospatial scenes. The results from extensive evaluations indicate that this approach shows promising performance, highlighting SAM 3's potential in remote sensing applications."}, 'zh': {'title': 'åˆ©ç”¨SAM 3æå‡é¥æ„Ÿè¯­ä¹‰åˆ†å‰²çš„æ½œåŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨SAM 3è¿›è¡Œé¥æ„Ÿå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„åˆæ­¥æ¢ç´¢ã€‚é€šè¿‡æ©è†œèåˆç­–ç•¥å’Œå­˜åœ¨åˆ†æ•°è¿‡æ»¤ï¼Œå–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚æˆ‘ä»¬ç»“åˆäº†SAM 3çš„è¯­ä¹‰åˆ†å‰²å¤´å’Œå®ä¾‹å¤´çš„è¾“å‡ºï¼Œä»¥æé«˜åœŸåœ°è¦†ç›–çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å­˜åœ¨å¤´çš„å­˜åœ¨åˆ†æ•°è¿‡æ»¤æ‰åœºæ™¯ä¸­ä¸å­˜åœ¨çš„ç±»åˆ«ï¼Œå‡å°‘äº†ç”±äºè¯æ±‡é‡åºå¤§å’Œåœ°ç†åœºæ™¯å¤„ç†é€ æˆçš„è¯¯æŠ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.08923', 'title': 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs', 'url': 'https://huggingface.co/papers/2512.08923', 'abstract': 'Two new benchmarks assess cross-modal inconsistency in multimodal large language models, showing significant variability and impact of visual characteristics on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.', 'score': 0, 'issue_id': 2, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '10f41aa1ced2ff48', 'authors': ['Angela van Sprang', 'Laurens Samson', 'Ana Lucic', 'Erman Acar', 'Sennay Ghebreab', 'Yuki M. Asano'], 'affiliations': ['City of Amsterdam', 'University of Amsterdam', 'University of Technology Nuremberg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08923.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#interpretability'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ÑÑ‚ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² MLLM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° REST Ğ¸ REST+ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ†Ğ²ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Evaluating Cross-Modal Inconsistency in Multimodal Language Models', 'desc': 'This paper introduces two benchmarks, REST and REST+, designed to evaluate cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs aim to process both visual and textual data in a unified way, but they struggle to perform consistently across different modalities. The authors tested 15 MLLMs and discovered significant variability in their performance, influenced by visual characteristics like text color and resolution. The findings suggest that even with accurate text recognition, MLLMs still face challenges in reasoning across modalities, indicating a deeper issue related to the modality gap.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ä¸€è‡´æ€§æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸¤ä¸ªæ–°çš„åŸºå‡†RESTå’ŒREST+ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§ã€‚å°½ç®¡MLLMsè¢«è®­ç»ƒåœ¨åŒä¸€åµŒå…¥ç©ºé—´ä¸­è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ï¼Œä½†å®ƒä»¬åœ¨ä¸åŒæ¨¡æ€ä¸‹æ‰§è¡Œç›¸åŒä»»åŠ¡çš„èƒ½åŠ›å´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…å«åœ¨ä¸‰ç§æ¨¡æ€ï¼ˆå›¾åƒã€æ–‡æœ¬ã€æ··åˆï¼‰ä¸­å…·æœ‰ç›¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ ·æœ¬ï¼Œç»“æœæ˜¾ç¤ºæœ€å…ˆè¿›çš„MLLMsåœ¨è¿™äº›æ¨¡æ€ä¸Šæ— æ³•ä¿æŒä¸€è‡´çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°è§†è§‰ç‰¹å¾ï¼ˆå¦‚æ–‡æœ¬é¢œè‰²å’Œåˆ†è¾¨ç‡ï¼‰ä»¥åŠè§†è§‰æ ‡è®°çš„æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒäº†è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§çš„æœºåˆ¶è§£é‡Šã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (6)', '#cv (3)', '#data', '#dataset (3)', '#diffusion (4)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (6)', '#open_source (8)', '#optimization (5)', '#plp (1)', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science (3)', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (6)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-12-10 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-10 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-10 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    