{
    "date": {
        "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 13",
        "zh": "1æœˆ13æ—¥"
    },
    "time_utc": "2025-01-13 05:11",
    "weekday": 0,
    "issue_id": 1628,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.05874",
            "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
            "url": "https://huggingface.co/papers/2501.05874",
            "abstract": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines.",
            "score": 17,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "a6a86d4d49a42b4d",
            "authors": [
                "Soyeong Jeong",
                "Kangsan Kim",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05874.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#interpretability",
                    "#hallucinations",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VideoRAG: ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "VideoRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ¸Ñ…. VideoRAG Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ’Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ÑÑ… (LVLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VideoRAG Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Generation with Dynamic Video Retrieval",
                    "desc": "This paper presents VideoRAG, a new framework that enhances the Retrieval-Augmented Generation (RAG) approach by incorporating video content into the generation process. Unlike previous methods that primarily focused on text or predefined videos, VideoRAG dynamically retrieves relevant videos based on the user's query. It leverages both visual and textual information from the videos, allowing for a richer and more accurate output generation. The framework utilizes Large Video Language Models (LVLMs) to effectively process and integrate video content, demonstrating superior performance compared to existing methods."
                },
                "zh": {
                    "title": "è§†é¢‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæå‡å¤šæ¨¡æ€çŸ¥è¯†çš„åˆ©ç”¨",
                    "desc": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç­–ç•¥ï¼Œç”¨äºè§£å†³åŸºç¡€æ¨¡å‹ç”Ÿæˆäº‹å®ä¸å‡†ç¡®è¾“å‡ºçš„é—®é¢˜ã€‚ç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¿¡æ¯ä¸Šï¼Œæœ€è¿‘çš„ä¸€äº›è¿›å±•å¼€å§‹è€ƒè™‘å›¾åƒï¼Œä½†å¤§å¤šæ•°å¿½è§†äº†è§†é¢‘è¿™ä¸€ä¸°å¯Œçš„å¤šæ¨¡æ€çŸ¥è¯†æºã€‚æˆ‘ä»¬æå‡ºäº†VideoRAGæ¡†æ¶ï¼Œå®ƒä¸ä»…æ ¹æ®æŸ¥è¯¢åŠ¨æ€æ£€ç´¢ç›¸å…³è§†é¢‘ï¼Œè¿˜åˆ©ç”¨è§†é¢‘çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œè¾“å‡ºç”Ÿæˆã€‚å®éªŒç»“æœéªŒè¯äº†VideoRAGçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå…¶ä¼˜äºç›¸å…³åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05727",
            "title": "Enabling Scalable Oversight via Self-Evolving Critic",
            "url": "https://huggingface.co/papers/2501.05727",
            "abstract": "Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs, SCRIT achieves up to a 10.3\\% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component.",
            "score": 12,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "5a9e3b95b6aa1312",
            "authors": [
                "Zhengyang Tang",
                "Ziniu Li",
                "Zhenyang Xiao",
                "Tian Ding",
                "Ruoyu Sun",
                "Benyou Wang",
                "Dayiheng Liu",
                "Fei Huang",
                "Tianyu Liu",
                "Bowen Yu",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc., Beijing, China",
                "Shenzhen Research Institute of Big Data, Shenzhen, China",
                "The Chinese University of Hong Kong, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05727.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#rlhf",
                    "#synthetic"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "SCRIT: Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ LLM",
                    "desc": "SCRIT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-72B-Instruct, SCRIT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸-ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ SCRIT Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Evolving Critique",
                    "desc": "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) in tasks where human evaluation is difficult. It introduces SCRIT (Self-evolving CRITic), a framework that enhances the critique capabilities of LLMs without relying on external supervision. SCRIT utilizes synthetic data generated by a contrastive-based self-critic and incorporates a self-validation mechanism to ensure the quality of critiques. The results show that SCRIT significantly improves critique-correction and error identification benchmarks, demonstrating its effectiveness as LLMs scale in size and data."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–ï¼Œæå‡æ‰¹è¯„èƒ½åŠ›ï¼",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¯æ‰©å±•ç›‘ç£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥è¿›è¡Œäººç±»è¯„ä¼°çš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†SCRITï¼ˆè‡ªæˆ‘è¿›åŒ–æ‰¹è¯„è€…ï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„è‡ªæˆ‘æ‰¹è¯„èƒ½åŠ›ã€‚SCRITé€šè¿‡å¯¹æ¯”è‡ªæˆ‘æ‰¹è¯„ç”Ÿæˆåˆæˆæ•°æ®ï¼Œå¹¶åˆ©ç”¨è‡ªæˆ‘éªŒè¯æœºåˆ¶ç¡®ä¿æ‰¹è¯„è´¨é‡ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCRITåœ¨æ‰¹è¯„çº æ­£å’Œé”™è¯¯è¯†åˆ«åŸºå‡†ä¸Šæé«˜äº†10.3%çš„æ€§èƒ½ï¼Œä¸”å…¶è¡¨ç°éšç€æ•°æ®å’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03841",
            "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
            "url": "https://huggingface.co/papers/2501.03841",
            "abstract": "The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.",
            "score": 5,
            "issue_id": 1628,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "c2dc8cc20b9b990a",
            "authors": [
                "Mingjie Pan",
                "Jiyao Zhang",
                "Tianshu Wu",
                "Yinghao Zhao",
                "Wenlong Gao",
                "Hao Dong"
            ],
            "affiliations": [
                "AgiBot",
                "CFCS, School of CS, Peking University",
                "PKU-AgiBot Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03841.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#robotics",
                    "#3d",
                    "#transfer_learning",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞµ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ†Ğ¸ĞºĞ»Ğ°: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VLM Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 6D-Ğ¿Ğ¾Ğ·Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging High-Level Reasoning and Low-Level Manipulation in Robotics",
                    "desc": "This paper addresses the challenge of enabling robots to manipulate objects in unpredictable environments by enhancing Vision-Language Models (VLM) with a new approach. The authors propose a Vision-Language-Action Model (VLA) that utilizes an object-centric representation, focusing on an object's canonical space defined by its functional affordances. This representation helps translate high-level reasoning from VLM into specific 3D spatial actions needed for manipulation tasks. The proposed dual closed-loop system allows for effective planning and execution without the need for extensive fine-tuning, demonstrating strong performance in various robotic tasks."
                },
                "zh": {
                    "title": "æ‰“ç ´é«˜å±‚æ¨ç†ä¸ä½å±‚æ“ä½œçš„å£å’",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ“ä½œçš„é€šç”¨æœºå™¨äººç³»ç»Ÿçš„å¼€å‘æŒ‘æˆ˜ã€‚è™½ç„¶è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨é«˜å±‚æ¬¡çš„å¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ç²¾ç»†çš„ä¸‰ç»´ç©ºé—´ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨å¼¥åˆVLMçš„é«˜å±‚æ¨ç†ä¸æ“ä½œæ‰€éœ€çš„ä½å±‚ç²¾åº¦ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡å¼•å…¥åŒé—­ç¯ã€å¼€æ”¾è¯æ±‡çš„æœºå™¨äººæ“ä½œç³»ç»Ÿï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆçš„å®æ—¶æ§åˆ¶ï¼Œä¸”æ— éœ€å¯¹VLMè¿›è¡Œå¾®è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06186",
            "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2501.06186",
            "abstract": "Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\\% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.",
            "score": 3,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "40e1a0d2c562cda5",
            "authors": [
                "Omkar Thawakar",
                "Dinura Dissanayake",
                "Ketan More",
                "Ritesh Thawkar",
                "Ahmed Heakl",
                "Noor Ahsan",
                "Yuhao Li",
                "Mohammed Zumri",
                "Jean Lahoud",
                "Rao Muhammad Anwer",
                "Hisham Cholakkal",
                "Ivan Laptev",
                "Mubarak Shah",
                "Fahad Shahbaz Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Australian National University",
                "LinkÃ¶ping University",
                "Mohamed bin Zayed University of AI",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06186.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¨Ğ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LlamaV-o1, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LlamaV-o1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Advancing Step-by-Step Visual Reasoning in LLMs",
                    "desc": "This paper introduces a new framework to enhance visual reasoning in large language models (LLMs) by focusing on step-by-step problem-solving. It presents a visual reasoning benchmark with over 4,000 reasoning steps across eight categories, allowing for thorough evaluation of LLMs' multi-step reasoning capabilities. Additionally, a novel metric is proposed to assess the quality of visual reasoning at each step, providing insights beyond traditional accuracy measures. The authors also introduce LlamaV-o1, a multimodal model trained with a curriculum learning approach, which shows significant performance improvements over existing models."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„å…¨æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§†è§‰æ¨ç†ä¸­çš„é€æ­¥æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè§†è§‰æ¨ç†åŸºå‡†ï¼ŒåŒ…å«å¤šè¾¾4000ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ¶µç›–å¤æ‚çš„è§†è§‰æ„ŸçŸ¥å’Œç§‘å­¦æ¨ç†ç­‰å…«ä¸ªç±»åˆ«ï¼Œä»¥ä¾¿å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†ï¼Œä¸“æ³¨äºé€æ­¥æ¨ç†çš„æ­£ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œæä¾›æ¯”ä¼ ç»Ÿçš„ä»»åŠ¡å‡†ç¡®ç‡æ›´æ·±å…¥çš„æ´å¯Ÿã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†åä¸ºLlamaV-o1çš„å¤šæ¨¡æ€è§†è§‰æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡é€æ­¥è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-10.html",
    "link_next": "2025-01-14.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "10.01",
        "en": "01/10",
        "zh": "1æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« åé©³äº†â€œGANéš¾ä»¥è®­ç»ƒâ€çš„è¯´æ³•ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ã€è§„èŒƒçš„GANæŸå¤±å‡½æ•°ï¼Œè§£å†³äº†ä¹‹å‰é€šè¿‡ä¸´æ—¶æŠ€å·§å¤„ç†çš„æ¨¡å¼ä¸¢å¤±å’Œä¸æ”¶æ•›é—®é¢˜ã€‚ä½œè€…è¯æ˜äº†è¿™ä¸ªæŸå¤±å‡½æ•°çš„å±€éƒ¨æ”¶æ•›æ€§ï¼Œå¹¶ç”¨å®ƒæ›¿æ¢äº†å¸¸è§GANä¸­çš„è¿‡æ—¶æ¶æ„ã€‚é€šè¿‡StyleGAN2çš„ä¾‹å­ï¼Œä½œè€…å±•ç¤ºäº†ç®€åŒ–å’Œç°ä»£åŒ–çš„è·¯çº¿å›¾ï¼Œå¾—åˆ°äº†ä¸€ä¸ªæ–°çš„æœ€å°åŒ–åŸºå‡†R3GANã€‚å°½ç®¡ç®€å•ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†StyleGAN2ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„GANå’Œæ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚",
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "pinyin": "è¿™ç¯‡æ–‡ç« åé©³äº†â€œGANéš¾ä»¥è®­ç»ƒâ€çš„è¯´æ³•ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng fÇnbÃ³ le â€œGAN nÃ¡nyÇ xÃ¹nliÃ nâ€ de shuÅfÇ.\n\nä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ã€è§„èŒƒçš„GANæŸå¤±å‡½æ•°ï¼Œè§£å†³äº†ä¹‹å‰é€šè¿‡ä¸´æ—¶æŠ€å·§å¤„ç†çš„æ¨¡å¼ä¸¢å¤±å’Œä¸æ”¶æ•›é—®é¢˜ã€‚\nZuÃ²zhÄ› tÃ­chÅ« le yÄ« zhÇ’ng xÄ«n de, guÄ«fÃ n de GAN sÇ”nshÄ« hÃ¡nshÃ¹, jiÄ›juÃ© le zhÄ«qiÃ¡n tÅngguÃ² lÃ­nshÃ­ jÃ¬qiÇo chÇ”lÇ de mÃ³shÃ¬ diÅ«shÄ« hÃ© bÃ¹ shÅuliÇn wÃ¨ntÃ­.\n\nä½œè€…è¯æ˜äº†è¿™ä¸ªæŸå¤±å‡½æ•°çš„å±€éƒ¨æ”¶æ•›æ€§ï¼Œå¹¶ç”¨å®ƒæ›¿æ¢äº†å¸¸è§GANä¸­çš„è¿‡æ—¶æ¶æ„ã€‚\nZuÃ²zhÄ› zhÃ¨ngmÃ­ng le zhÃ¨ ge sÇ”nshÄ« hÃ¡nshÃ¹ de jÃºbÃ¹ shÅuliÇn xÃ¬ng, bÃ¬ng yÃ²ng tÄ tÃ¬huÃ n le chÃ¡ngjiÃ n GAN zhÅng de guÃ²shÃ­ jiÃ gÃ²u.\n\né€šè¿‡StyleGAN2çš„ä¾‹å­ï¼Œä½œè€…å±•ç¤ºäº†ç®€åŒ–å’Œç°ä»£åŒ–çš„è·¯çº¿å›¾ï¼Œå¾—åˆ°äº†ä¸€ä¸ªæ–°çš„æœ€å°åŒ–åŸºå‡†R3GANã€‚\nTÅngguÃ² StyleGAN2 de lÃ¬zi, zuÃ²zhÄ› zhÇnshÃ¬ le jiÇnhuÃ  hÃ© xiÃ ndÃ ihuÃ  de lÃ¹xiÃ n tÃº, dÃ©dÃ o le yÄ« ge xÄ«n de zuÃ¬xiÇohuÃ  jÄ«zhÇ”n R3GAN.\n\nå°½ç®¡ç®€å•ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†StyleGAN2ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„GANå’Œæ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚\nJÇnguÇn jiÇndÄn, dÃ n zhÃ¨ zhÇ’ng fÄngfÇ zÃ i duÅ ge shÃ¹jÃ¹jÃ­ shÃ ng chÄoyuÃ¨ le StyleGAN2, bÃ¬ng yÇ” zuÃ¬ xiÄnjÃ¬n de GAN hÃ© kuÃ²sÃ n mÃ³xÃ­ng xiÄng bÇmÄ›i.",
        "vocab": "[{'word': 'åé©³', 'pinyin': 'fÇn bÃ³', 'trans': 'refute'}, {'word': 'GAN', 'pinyin': 'GAN', 'trans': 'Generative Adversarial Network'}, {'word': 'éš¾ä»¥', 'pinyin': 'nÃ¡n yÇ', 'trans': 'difficult to'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'train'}, {'word': 'è¯´æ³•', 'pinyin': 'shuÅ fÇ', 'trans': 'saying'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'è§„èŒƒ', 'pinyin': 'guÄ« fÃ n', 'trans': 'standard'}, {'word': 'æŸå¤±å‡½æ•°', 'pinyin': 'sÇ”n shÄ« hÃ¡n shÃ¹', 'trans': 'loss function'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'ä¹‹å‰', 'pinyin': 'zhÄ« qiÃ¡n', 'trans': 'before'}, {'word': 'ä¸´æ—¶', 'pinyin': 'lÃ­n shÃ­', 'trans': 'temporary'}, {'word': 'æŠ€å·§', 'pinyin': 'jÃ¬ qiÇo', 'trans': 'skill'}, {'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'handle'}, {'word': 'æ¨¡å¼', 'pinyin': 'mÃ³ shÃ¬', 'trans': 'pattern'}, {'word': 'ä¸¢å¤±', 'pinyin': 'diÅ« shÄ«', 'trans': 'lose'}, {'word': 'ä¸æ”¶æ•›', 'pinyin': 'bÃ¹ shÅu liÇn', 'trans': 'not converge'}, {'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'}, {'word': 'è¯æ˜', 'pinyin': 'zhÃ¨ng mÃ­ng', 'trans': 'prove'}, {'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'}, {'word': 'æ”¶æ•›æ€§', 'pinyin': 'shÅu liÇn xÃ¬ng', 'trans': 'convergence'}, {'word': 'æ›¿æ¢', 'pinyin': 'tÃ¬ huÃ n', 'trans': 'replace'}, {'word': 'å¸¸è§', 'pinyin': 'chÃ¡ng jiÃ n', 'trans': 'common'}, {'word': 'è¿‡æ—¶', 'pinyin': 'guÃ² shÃ­', 'trans': 'outdated'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅng guÃ²', 'trans': 'through'}, {'word': 'StyleGAN2', 'pinyin': 'StyleGAN2', 'trans': 'StyleGAN2'}, {'word': 'ä¾‹å­', 'pinyin': 'lÃ¬ zi', 'trans': 'example'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'display'}, {'word': 'ç®€åŒ–', 'pinyin': 'jiÇn huÃ ', 'trans': 'simplify'}, {'word': 'ç°ä»£åŒ–', 'pinyin': 'xiÃ n dÃ i huÃ ', 'trans': 'modernize'}, {'word': 'è·¯çº¿å›¾', 'pinyin': 'lÃ¹ xiÃ n tÃº', 'trans': 'roadmap'}, {'word': 'å¾—åˆ°', 'pinyin': 'dÃ© dÃ o', 'trans': 'obtain'}, {'word': 'æœ€å°åŒ–', 'pinyin': 'zuÃ¬ xiÇo huÃ ', 'trans': 'minimize'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'R3GAN', 'pinyin': 'R3GAN', 'trans': 'R3GAN'}, {'word': 'å°½ç®¡', 'pinyin': 'jÃ¬n guÇn', 'trans': 'although'}, {'word': 'ä½†', 'pinyin': 'dÃ n', 'trans': 'but'}, {'word': 'è¿™ç§', 'pinyin': 'zhÃ¨ zhÇ’ng', 'trans': 'this'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'å¤šä¸ª', 'pinyin': 'duÅ gÃ¨', 'trans': 'multiple'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'}, {'word': 'è¶…è¶Š', 'pinyin': 'chÄo yuÃ¨', 'trans': 'surpass'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'ç›¸åª²ç¾', 'pinyin': 'xiÄng pÃ¬ mÄ›i', 'trans': 'compare favorably'}]",
        "trans": "This article refutes the claim that \"GANs are difficult to train.\" The author proposes a new, regularized GAN loss function that addresses the issues of mode collapse and non-convergence, which were previously handled through ad-hoc tricks. The author demonstrates the local convergence of this loss function and uses it to replace outdated architectures in common GANs. Through the example of StyleGAN2, the author presents a simplified and modernized roadmap, resulting in a new minimized benchmark, R3GAN. Despite its simplicity, this method outperforms StyleGAN2 on multiple datasets and is comparable to state-of-the-art GANs and diffusion models.",
        "update_ts": "2025-01-12 12:39"
    }
}