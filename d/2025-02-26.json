{
    "date": {
        "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 26",
        "zh": "2æœˆ26æ—¥"
    },
    "time_utc": "2025-02-26 21:09",
    "weekday": 2,
    "issue_id": 2427,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.18411",
            "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
            "url": "https://huggingface.co/papers/2502.18411",
            "abstract": "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.",
            "score": 52,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "a37015745aae1e1d",
            "authors": [
                "Xiangyu Zhao",
                "Shengyuan Ding",
                "Zicheng Zhang",
                "Haian Huang",
                "Maosong Cao",
                "Weiyun Wang",
                "Jiaqi Wang",
                "Xinyu Fang",
                "Wenhai Wang",
                "Guangtao Zhai",
                "Haodong Duan",
                "Hua Yang",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18411.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniAlign-V - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MM-AlignBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ MLLM Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° OmniAlign-V Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ."
                },
                "en": {
                    "title": "Aligning MLLMs with Human Preferences through OmniAlign-V",
                    "desc": "This paper presents OmniAlign-V, a new dataset containing 200,000 high-quality training samples that include diverse images and complex questions to help multi-modal large language models (MLLMs) better align with human preferences. The authors also introduce MM-AlignBench, a benchmark for evaluating how well MLLMs reflect human values. By fine-tuning MLLMs with the OmniAlign-V dataset using techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), the models show improved alignment with human preferences while maintaining their performance on standard Visual Question Answering (VQA) tasks. The resources, including datasets and benchmarks, are made publicly available to support further research in this area."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«20ä¸‡ä¸ªé«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚æ•°æ®é›†ä¸­åŒ…å«å¤šæ ·çš„å›¾åƒã€å¤æ‚çš„é—®é¢˜å’Œå¤šç§å“åº”æ ¼å¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MM-AlignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„äººç±»æ ‡æ³¨åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨OmniAlign-Vè¿›è¡Œå¾®è°ƒæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡äº†åœ¨æ ‡å‡†è§†è§‰é—®ç­”åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18137",
            "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
            "url": "https://huggingface.co/papers/2502.18137",
            "abstract": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.",
            "score": 38,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "1029ef0dffc41bba",
            "authors": [
                "Jintao Zhang",
                "Chendong Xiang",
                "Haofeng Huang",
                "Jia Wei",
                "Haocheng Xi",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18137.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#video",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpargeAttn Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. SpargeAttn ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Accelerating Attention with SpargeAttn: Speed Meets Efficiency",
                    "desc": "This paper introduces SpargeAttn, a novel approach to implementing sparse attention in machine learning models. It addresses the challenge of quadratic time complexity in attention mechanisms by leveraging the inherent sparsity of attention maps. The proposed method employs a two-stage online filtering process to efficiently predict and optimize the attention map, allowing for the omission of unnecessary computations. Experimental results demonstrate that SpargeAttn accelerates various models across different domains, such as language and image processing, while maintaining high performance metrics."
                },
                "zh": {
                    "title": "é€šç”¨ç¨€ç–æ³¨æ„åŠ›ï¼Œæå‡æ¨¡å‹è®¡ç®—æ•ˆç‡ï¼",
                    "desc": "åœ¨å¤§å‹æ¨¡å‹ä¸­ï¼Œé«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°è‡³å…³é‡è¦ï¼Œå› ä¸ºå…¶æ—¶é—´å¤æ‚åº¦ä¸ºå¹³æ–¹çº§ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ³¨æ„åŠ›é€šå¸¸è¡¨ç°å‡ºç¨€ç–æ€§ï¼Œå³æ³¨æ„åŠ›å›¾ä¸­çš„è®¸å¤šå€¼æ¥è¿‘äºé›¶ï¼Œè¿™ä½¿å¾—å¯ä»¥çœç•¥ç›¸åº”çš„è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSpargeAttnçš„é€šç”¨ç¨€ç–å’Œé‡åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ é€Ÿå„ç§æ¨¡å‹çš„è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒç«¯åˆ°ç«¯çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µçš„åœ¨çº¿è¿‡æ»¤å™¨æ¥å®ç°ï¼Œç¬¬ä¸€é˜¶æ®µå¿«é€Ÿå‡†ç¡®åœ°é¢„æµ‹æ³¨æ„åŠ›å›¾ï¼Œç¬¬äºŒé˜¶æ®µè®¾è®¡äº†ä¸€ä¸ªåœ¨çº¿çš„softmaxæ„ŸçŸ¥è¿‡æ»¤å™¨ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18449",
            "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
            "url": "https://huggingface.co/papers/2502.18449",
            "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
            "score": 28,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "938af9b1ea2398d8",
            "authors": [
                "Yuxiang Wei",
                "Olivier Duchenne",
                "Jade Copet",
                "Quentin Carbonneaux",
                "Lingming Zhang",
                "Daniel Fried",
                "Gabriel Synnaeve",
                "Rishabh Singh",
                "Sida I. Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "FAIR at Meta",
                "GenAI at Meta",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18449.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#dataset",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SWE-RL: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-RL - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», SWE-RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Llama3-SWE-RL-70B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama 3, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ LLM ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞŸĞ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Software Engineering Reasoning",
                    "desc": "This paper presents SWE-RL, a novel approach that applies reinforcement learning (RL) to enhance the reasoning abilities of large language models (LLMs) specifically for software engineering tasks. By utilizing a lightweight rule-based reward system, SWE-RL allows LLMs to learn from extensive open-source software evolution data, which includes various stages of software development. The resulting model, Llama3-SWE-RL-70B, achieves impressive performance on real-world GitHub issues, outperforming other medium-sized LLMs and even rivaling larger proprietary models. Additionally, this approach not only improves software-related reasoning but also demonstrates generalized reasoning skills across various tasks, indicating its broad applicability."
                },
                "zh": {
                    "title": "é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SWE-RLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºçœŸå®è½¯ä»¶å·¥ç¨‹çš„æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨è½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒSWE-RLèƒ½å¤Ÿä»å¤§é‡å¼€æºè½¯ä»¶æ¼”å˜æ•°æ®ä¸­å­¦ä¹ ï¼Œè‡ªåŠ¨æ¢å¤å¼€å‘è€…çš„æ¨ç†è¿‡ç¨‹å’Œè§£å†³æ–¹æ¡ˆã€‚è®­ç»ƒåçš„æ¨¡å‹Llama3-SWE-RL-70Båœ¨çœŸå®çš„GitHubé—®é¢˜ä¸Šè¾¾åˆ°äº†41.0%çš„è§£å†³ç‡ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–ä¸­å‹è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡ä»…åœ¨è½¯ä»¶æ¼”å˜æ•°æ®ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒLlama3-SWE-RLä»å±•ç°å‡ºå¹¿æ³›çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä»»åŠ¡ä¸­å–å¾—è‰¯å¥½ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17363",
            "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
            "url": "https://huggingface.co/papers/2502.17363",
            "abstract": "Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit",
            "score": 26,
            "issue_id": 2409,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "d1e7a717a0d2e56e",
            "authors": [
                "Tianrui Zhu",
                "Shiyi Zhang",
                "Jiawei Shao",
                "Yansong Tang"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence (TeleAI), China Telecom",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "KV-Edit: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°",
                    "desc": "KV-Edit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² DiT-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ„Ğ¾Ğ½Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ O(1), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ KV-Edit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Seamless Image Editing with KV-Edit: No Training Needed!",
                    "desc": "The paper introduces KV-Edit, a novel method for image editing that addresses the challenge of maintaining background consistency. Unlike traditional methods that require extensive training, KV-Edit utilizes a KV cache in Denoising Transformers (DiTs) to preserve background tokens, allowing for seamless integration of new content. This approach simplifies the editing process by avoiding complex mechanisms and optimizing memory usage to O(1) without sacrificing quality. Experimental results show that KV-Edit outperforms existing techniques, including those that rely on training, in both background consistency and overall image quality."
                },
                "zh": {
                    "title": "KV-Editï¼šæ— è®­ç»ƒçš„èƒŒæ™¯ä¸€è‡´æ€§å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "èƒŒæ™¯ä¸€è‡´æ€§åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿æŒä¸åŸå§‹å›¾åƒç›¸ä¼¼æ€§å’Œç”Ÿæˆç¬¦åˆç›®æ ‡å†…å®¹ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†KV-Editï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨KVç¼“å­˜æ¥ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§ï¼Œä¿ç•™èƒŒæ™¯æ ‡è®°è€Œä¸æ˜¯é‡æ–°ç”Ÿæˆï¼Œä»è€Œç®€åŒ–äº†å¤æ‚æœºåˆ¶å’Œé«˜æˆæœ¬è®­ç»ƒçš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒKV-Editåœ¨èƒŒæ™¯å’Œå›¾åƒè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18364",
            "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
            "url": "https://huggingface.co/papers/2502.18364",
            "abstract": "Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.",
            "score": 21,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "23632a98b9252831",
            "authors": [
                "Yifan Pu",
                "Yiming Zhao",
                "Zhicong Tang",
                "Ruihong Yin",
                "Haoxing Ye",
                "Yuhui Yuan",
                "Dong Chen",
                "Jianmin Bao",
                "Sirui Zhang",
                "Yanbin Wang",
                "Lin Liang",
                "Lijuan Wang",
                "Ji Li",
                "Xiu Li",
                "Zhouhui Lian",
                "Gao Huang",
                "Baining Guo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18364.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ART: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Anonymous Region Transformer (ART) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ART Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Multi-Layer Image Generation with ART",
                    "desc": "This paper presents the Anonymous Region Transformer (ART), a novel approach for generating multi-layer transparent images using a global text prompt. ART allows the model to autonomously match visual tokens to text tokens through an anonymous region layout, improving upon traditional semantic layouts. The layer-wise region crop mechanism enhances efficiency by reducing attention computation costs, enabling the generation of images with many distinct layers quickly. Overall, ART introduces a new paradigm for interactive content creation, allowing for precise control and scalable image generation."
                },
                "zh": {
                    "title": "åŒ¿ååŒºåŸŸå˜æ¢å™¨ï¼šå¤šå±‚å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "å¤šå±‚å›¾åƒç”Ÿæˆæ˜¯ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿéš”ç¦»ã€é€‰æ‹©å’Œç¼–è¾‘ç‰¹å®šçš„å›¾åƒå±‚ï¼Œä»è€Œæ”¹å˜ä¸ç”Ÿæˆæ¨¡å‹çš„äº¤äº’æ–¹å¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºåŒ¿ååŒºåŸŸå˜æ¢å™¨ï¼ˆARTï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ ¹æ®å…¨å±€æ–‡æœ¬æç¤ºå’ŒåŒ¿ååŒºåŸŸå¸ƒå±€ç›´æ¥ç”Ÿæˆå¯å˜çš„å¤šå±‚é€æ˜å›¾åƒã€‚è¯¥æ–¹æ³•å…è®¸ç”Ÿæˆæ¨¡å‹è‡ªä¸»å†³å®šå“ªäº›è§†è§‰æ ‡è®°ä¸å“ªäº›æ–‡æœ¬æ ‡è®°å¯¹é½ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡ï¼Œå¹¶å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¼•å…¥é«˜è´¨é‡çš„å¤šå±‚é€æ˜å›¾åƒè‡ªç¼–ç å™¨ï¼ŒARTä¸ºäº¤äº’å¼å†…å®¹åˆ›ä½œå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17262",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "url": "https://huggingface.co/papers/2502.17262",
            "abstract": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the \"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.",
            "score": 14,
            "issue_id": 2410,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "246c4bb39b0f6996",
            "authors": [
                "Chengyin Xu",
                "Kaiyuan Chen",
                "Xiao Li",
                "Ke Shen",
                "Chenggang Li"
            ],
            "affiliations": [
                "Seed-LLM, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17262.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Clustering-On-Difficulty (COD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. COD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² 70-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ LLM Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Predicting LLM Performance with Clustering-On-Difficulty",
                    "desc": "This paper addresses the challenges of predicting the performance of Large Language Models (LLMs) before they are fully trained. It introduces a new framework called Clustering-On-Difficulty (COD), which clusters tasks based on their difficulty to create a predictable subset of tasks. By focusing on this subset, the framework allows for more accurate predictions of how well the LLM will perform on a broader set of tasks. The method has shown significant accuracy improvements, achieving a mean deviation of just 1.36% when predicting the performance of a 70 billion parameter LLM."
                },
                "zh": {
                    "title": "åŸºäºéš¾åº¦èšç±»çš„ä¸‹æ¸¸æ€§èƒ½é¢„æµ‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¦‚ä½•åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹å‰å‡†ç¡®é¢„æµ‹å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç”±äºâ€œæ¶Œç°ç°è±¡â€å’Œä»»åŠ¡éš¾åº¦åˆ†å¸ƒä¸å‡ï¼Œç°æœ‰çš„æ€§èƒ½é¢„æµ‹æ–¹æ³•å¾€å¾€ä¸å¤Ÿå‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºéš¾åº¦èšç±»çš„ä¸‹æ¸¸æ€§èƒ½é¢„æµ‹æ¡†æ¶ï¼ˆCODï¼‰ï¼Œé€šè¿‡èšç±»ä»»åŠ¡å¹¶æ’é™¤ä¸é€‚åˆçš„é›†ç¾¤æ¥æ„å»ºå¯é¢„æµ‹çš„æ”¯æŒå­é›†ã€‚è¯¥æ–¹æ³•åœ¨70B LLMçš„æ€§èƒ½é¢„æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æœ‰æ•ˆçš„èµ„æºåˆ†é…å»ºè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15499",
            "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
            "url": "https://huggingface.co/papers/2502.15499",
            "abstract": "Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing gradient explosion and dissipation. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
            "score": 12,
            "issue_id": 2410,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "3fb4636aad6d75b4",
            "authors": [
                "Ya Wang",
                "Zhijian Zhuo",
                "Yutao Zeng",
                "Xun Zhou",
                "Jian Yang",
                "Xiaoqing Li"
            ],
            "affiliations": [
                "Capital University of Economics and Business",
                "School of Mathematical Sciences, Peking University",
                "Seed-Foundation-Model, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15499.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Scale-Distribution Decoupling (SDD) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SDD Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ·Ñ€Ñ‹Ğ² Ğ¸ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Stabilizing Large Language Model Training with SDD",
                    "desc": "This paper addresses the issue of training stability in large language models, especially those using Post-Norm Transformers, which often face problems like gradient explosion and dissipation. The authors introduce a new method called Scale-Distribution Decoupling (SDD), which separates the scale and distribution of weight matrices in fully-connected layers to enhance training stability. By implementing a normalization mechanism and a learnable scaling vector, SDD ensures well-conditioned gradients and improves optimization efficiency in deep networks. Experimental results show that SDD not only stabilizes training across various architectures but also outperforms existing normalization techniques while being lightweight and compatible with current frameworks."
                },
                "zh": {
                    "title": "è§„æ¨¡-åˆ†å¸ƒè§£è€¦ï¼šç¨³å®šå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒä¸­ï¼Œè®­ç»ƒç¨³å®šæ€§ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºåå½’ä¸€åŒ–å˜æ¢å™¨æ¶æ„ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆæ•£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè§„æ¨¡-åˆ†å¸ƒè§£è€¦ï¼ˆSDDï¼‰ï¼Œé€šè¿‡æ˜ç¡®è§£è€¦å…¨è¿æ¥å±‚ä¸­æƒé‡çŸ©é˜µçš„è§„æ¨¡å’Œåˆ†å¸ƒæ¥ç¨³å®šè®­ç»ƒã€‚SDDé‡‡ç”¨å½’ä¸€åŒ–æœºåˆ¶æ¥è°ƒèŠ‚æ¿€æ´»å€¼ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾å‘é‡æ¥ä¿æŒè‰¯å¥½çš„æ¢¯åº¦æ¡ä»¶ï¼Œæœ‰æ•ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆæ•£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§LLMæ¶æ„ä¸­ç¨³å®šäº†è®­ç»ƒï¼Œå¹¶åœ¨ä¸åŒçš„å½’ä¸€åŒ–é…ç½®ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16069",
            "title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents",
            "url": "https://huggingface.co/papers/2502.16069",
            "abstract": "Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4times improvement in correctly answering experimental questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.",
            "score": 8,
            "issue_id": 2424,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 22",
                "zh": "2æœˆ22æ—¥"
            },
            "hash": "fca1ff11dd417d30",
            "authors": [
                "Patrick Tser Jern Kon",
                "Jiachen Liu",
                "Qiuyi Ding",
                "Yiming Qiu",
                "Zhenning Yang",
                "Yibo Huang",
                "Jayanth Srinivasa",
                "Myungjin Lee",
                "Mosharaf Chowdhury",
                "Ang Chen"
            ],
            "affiliations": [
                "Cisco Systems",
                "Department of Computer Science and Engineering, University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16069.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training",
                    "#science",
                    "#interpretability",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Curie: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Curie - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Curie ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¸, Ğ¼ĞµĞ¶Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ± ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 46 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Curie: Enhancing Rigor in Scientific Experimentation with AI",
                    "desc": "This paper introduces Curie, an AI agent framework aimed at improving the rigor of scientific experimentation. Curie incorporates three main components: an intra-agent rigor module for enhancing reliability, an inter-agent rigor module for ensuring methodical control, and an experiment knowledge module for boosting interpretability. The framework was evaluated using a new benchmark consisting of 46 questions from various computer science domains, demonstrating a significant performance improvement over existing methods. The results indicate that Curie can effectively automate rigorous experimentation, making it a valuable tool for researchers."
                },
                "zh": {
                    "title": "Curieï¼šæå‡ç§‘å­¦å®éªŒä¸¥è°¨æ€§çš„AIæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCurieçš„äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç§‘å­¦å®éªŒçš„ä¸¥è°¨æ€§ã€‚Curieé€šè¿‡ä¸‰ä¸ªå…³é”®æ¨¡å—å®ç°è¿™ä¸€ç›®æ ‡ï¼šå†…éƒ¨ä»£ç†ä¸¥è°¨æ¨¡å—å¢å¼ºå¯é æ€§ï¼Œå¤–éƒ¨ä»£ç†ä¸¥è°¨æ¨¡å—ä¿æŒæ–¹æ³•æ§åˆ¶ï¼Œä»¥åŠå®éªŒçŸ¥è¯†æ¨¡å—æå‡å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°çš„å®éªŒåŸºå‡†ï¼ŒåŒ…å«46ä¸ªé—®é¢˜ï¼Œæ¶µç›–å››ä¸ªè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œä»¥è¯„ä¼°Curieçš„æ€§èƒ½ã€‚ä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒCurieåœ¨æ­£ç¡®å›ç­”å®éªŒé—®é¢˜æ–¹é¢å®ç°äº†3.4å€çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18461",
            "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
            "url": "https://huggingface.co/papers/2502.18461",
            "abstract": "Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties of LoRA can effectively guide diffusion models in merging learned subject and style. Building on this insight, we propose K-LoRA, a simple yet effective training-free LoRA fusion approach. In each attention layer, K-LoRA compares the Top-K elements in each LoRA to be fused, determining which LoRA to select for optimal fusion. This selection mechanism ensures that the most representative features of both subject and style are retained during the fusion process, effectively balancing their contributions. Experimental results demonstrate that the proposed method effectively integrates the subject and style information learned by the original LoRAs, outperforming state-of-the-art training-based approaches in both qualitative and quantitative results.",
            "score": 8,
            "issue_id": 2412,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "db8a7da9e8b3af17",
            "authors": [
                "Ziheng Ouyang",
                "Zhen Li",
                "Qibin Hou"
            ],
            "affiliations": [
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18461.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "K-LoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ K-LoRA Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LoRA (Low-Rank Adaptation) Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. K-LoRA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Top-K ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ÑÑ‚Ğ¸Ğ»Ğµ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞºĞ°Ğº Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ K-LoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "K-LoRA: Effortless Fusion of Style and Subject in LoRA Models",
                    "desc": "This paper introduces K-LoRA, a novel approach for fusing different Low-Rank Adaptation (LoRA) models without the need for additional training. The method leverages the intrinsic properties of LoRA to guide diffusion models in effectively merging learned subject and style. By comparing the Top-K elements in each attention layer, K-LoRA selects the most representative features from each LoRA, ensuring that both subject and style are preserved during fusion. Experimental results show that K-LoRA outperforms existing training-based methods in integrating subject and style information, achieving superior qualitative and quantitative outcomes."
                },
                "zh": {
                    "title": "K-LoRAï¼šæ— è®­ç»ƒçš„LoRAèåˆæ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆç»“åˆä¸åŒçš„LoRAï¼Œä»¥åŒæ—¶ç”Ÿæˆå­¦ä¹ åˆ°çš„é£æ ¼å’Œå†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºK-LoRAçš„æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ— è®­ç»ƒLoRAèåˆæ–¹æ³•ã€‚K-LoRAåœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¸­æ¯”è¾ƒè¦èåˆçš„LoRAä¸­çš„Top-Kå…ƒç´ ï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜çš„LoRAè¿›è¡Œèåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™ä¸»é¢˜å’Œé£æ ¼ä¿¡æ¯æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18356",
            "title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
            "url": "https://huggingface.co/papers/2502.18356",
            "abstract": "We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents.",
            "score": 6,
            "issue_id": 2410,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "54dae5eb2e25ce92",
            "authors": [
                "George Thomas",
                "Alex J. Chan",
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Fraser Greenlee",
                "Andy Toulis",
                "Marvin Purtorab"
            ],
            "affiliations": [
                "Clusterfudge Ltd.",
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18356.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "WebGames: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…",
                    "desc": "WebGames - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 50 Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4o, Claude Computer-Use, Gemini-1.5-Pro Ğ¸ Qwen2-VL, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WebGames Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the AI Performance Gap in Web Browsing",
                    "desc": "WebGames is a new benchmark suite designed to test web-browsing AI agents with over 50 interactive challenges. These challenges are easy for humans but are meant to expose the weaknesses of AI in tasks like browser interactions and cognitive processing. The testing environment is self-contained, allowing for consistent evaluations with clear correct answers. When tested, top AI models showed a significant performance gap compared to humans, achieving only 43.1% success versus 95.7% for humans, indicating that current AI struggles with intuitive web tasks."
                },
                "zh": {
                    "title": "WebGamesï¼šè¯„ä¼°ç½‘é¡µæµè§ˆAIçš„å…¨æ–°åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†WebGamesï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨é€šè¿‡50å¤šä¸ªäº’åŠ¨æŒ‘æˆ˜è¯„ä¼°é€šç”¨ç½‘é¡µæµè§ˆAIä»£ç†ã€‚è¿™äº›æŒ‘æˆ˜è®¾è®¡å¾—å¯¹äººç±»æ¥è¯´ç®€å•æ˜äº†ï¼Œä½†ç³»ç»Ÿåœ°æµ‹è¯•å½“å‰AIç³»ç»Ÿåœ¨åŸºæœ¬æµè§ˆå™¨äº¤äº’ã€å…ˆè¿›è¾“å…¥å¤„ç†ã€è®¤çŸ¥ä»»åŠ¡ã€å·¥ä½œæµè‡ªåŠ¨åŒ–å’Œäº’åŠ¨å¨±ä¹ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸€ä¸ªå°é—­çš„æµ‹è¯•ç¯å¢ƒæ¶ˆé™¤äº†å¤–éƒ¨ä¾èµ–ï¼Œç¡®ä¿å¯é‡å¤çš„è¯„ä¼°å’Œå¯éªŒè¯çš„çœŸå®è§£å†³æ–¹æ¡ˆã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³AIç³»ç»Ÿçš„æˆåŠŸç‡ä»…ä¸º43.1%ï¼Œè€Œäººç±»çš„è¡¨ç°ä¸º95.7%ï¼Œçªæ˜¾äº†å½“å‰AIç³»ç»Ÿåœ¨å¤„ç†äººç±»ç›´è§‚çš„å¸¸è§ç½‘é¡µäº¤äº’æ¨¡å¼æ–¹é¢çš„åŸºæœ¬å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17425",
            "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2502.17425",
            "abstract": "To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken",
            "score": 5,
            "issue_id": 2414,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "0e916367fd15b42f",
            "authors": [
                "Runpeng Yu",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17425.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¢Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ­Ñ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ MLLM Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: Ğ¢Ğ¾ĞºĞµĞ½ Ğ’Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¸ Ğ¢Ğ¾ĞºĞµĞ½ ĞŸĞµÑ€ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ—Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ MLLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering MLLMs with Visual Perception Tokens for Enhanced Understanding",
                    "desc": "This paper introduces the Visual Perception Token concept to enhance the capabilities of Multimodal Large Language Models (MLLMs) in visual perception. The proposed tokens allow MLLMs to autonomously control their visual perception processes, enabling them to focus on specific image regions or object categories. Two types of tokens are designed: the Region Selection Token for identifying areas needing further analysis, and the Vision Re-Encoding Token for guiding additional perception actions. Experimental results show that these tokens significantly improve spatial reasoning and fine-grained understanding, leading to a performance increase of 23.6% in a 2B model compared to previous methods."
                },
                "zh": {
                    "title": "èµ‹èƒ½MLLMçš„è§†è§‰æ„ŸçŸ¥æ§åˆ¶æœºåˆ¶",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¾èµ–è§†è§‰ç¼–ç å™¨çš„æ„ŸçŸ¥è¿‡ç¨‹æ¥åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚è§†è§‰æ„ŸçŸ¥çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§å¯¹ç©ºé—´æ¨ç†å’Œç»†ç²’åº¦ç†è§£ç­‰ä»»åŠ¡çš„ç²¾åº¦æœ‰é‡è¦å½±å“ã€‚æœ¬æ–‡æå‡ºäº†è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œçš„æ¦‚å¿µï¼Œä»¥å¢å¼ºMLLMå¯¹å…¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†åŒºåŸŸé€‰æ‹©ä»¤ç‰Œå’Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œä¸¤ç§ç±»å‹çš„è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œï¼Œå®éªŒè¡¨æ˜è¿™äº›ä»¤ç‰Œåœ¨å¤„ç†ç©ºé—´æ¨ç†å’Œæé«˜ç»†ç²’åº¦ç†è§£æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17535",
            "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
            "url": "https://huggingface.co/papers/2502.17535",
            "abstract": "Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods.",
            "score": 4,
            "issue_id": 2412,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "38076575c5631707",
            "authors": [
                "Zhenheng Tang",
                "Xiang Liu",
                "Qian Wang",
                "Peijie Dong",
                "Bingsheng He",
                "Xiaowen Chu",
                "Bo Li"
            ],
            "affiliations": [
                "CSE, The Hong Kong University of Science and Technology",
                "DSA, The Hong Kong University of Science and Technology (Guangzhou)",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17535.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#training"
                ],
                "emoji": "ğŸŸï¸",
                "ru": {
                    "title": "Ğ›Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºÑÑˆĞ° KV. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ LLM Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ°Ñ 'Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ğ°Ñ' LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° ÑÑ‚Ğ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ LLM Ğ¸ ĞºÑÑˆĞ° KV."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Lottery LLM Hypothesis",
                    "desc": "This paper discusses the challenges of reducing the computational and storage costs of large language models (LLMs) while maintaining their performance. It reviews recent advancements in techniques like retrieval-augmented generation and multi-step reasoning that improve LLM capabilities. The authors introduce the 'lottery LLM' hypothesis, suggesting that a smaller model can achieve similar performance as a larger one by leveraging external tools and reasoning strategies. They also highlight important features that current compression methods often neglect, which are crucial for the success of both lottery LLMs and KV cache compression."
                },
                "zh": {
                    "title": "å‹ç¼©æ¨¡å‹ï¼Œæå‡æ€§èƒ½çš„å…³é”®ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡å‹å‹ç¼©å’ŒKVç¼“å­˜å‹ç¼©æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚ç ”ç©¶è€…ä»¬å…³æ³¨å¦‚ä½•åœ¨é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒå‹ç¼©åæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªâ€œå½©ç¥¨LLMå‡è®¾â€ï¼Œå³å¯¹äºç‰¹å®šçš„LLMå’Œä»»åŠ¡ï¼Œå­˜åœ¨ä¸€ä¸ªæ›´å°çš„LLMèƒ½å¤Ÿé€šè¿‡å¤šæ­¥æ¨ç†å’Œå¤–éƒ¨å·¥å…·å®ç°ä¸åŸå§‹æ¨¡å‹ç›¸åŒçš„æ€§èƒ½ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å½©ç¥¨LLMå’ŒKVç¼“å­˜å‹ç¼©æ‰€éœ€çš„å…³é”®èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›åœ¨ç°æœ‰æ–¹æ³•ä¸­å¸¸å¸¸è¢«å¿½è§†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16794",
            "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
            "url": "https://huggingface.co/papers/2502.16794",
            "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",
            "score": 4,
            "issue_id": 2410,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "680b3da301440d2b",
            "authors": [
                "Xilin Jiang",
                "Sukru Samet Dindar",
                "Vishal Choudhari",
                "Stephan Bickel",
                "Ashesh Mehta",
                "Guy M McKhann",
                "Adeen Flinker",
                "Daniel Friedman",
                "Nima Mesgarani"
            ],
            "affiliations": [
                "Department of Electrical Engineering, Columbia University, USA",
                "Department of Neurological Surgery, Columbia University, USA",
                "Hofstra Northwell School of Medicine, USA",
                "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, USA",
                "Neurology Department, New York University, USA",
                "The Feinstein Institutes for Medical Research, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16794.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#alignment",
                    "#audio"
                ],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº: Ğ˜Ğ˜ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ AAD-LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğ° ĞºĞ°ĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…, Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Listening with Intention: Enhancing Machine Hearing through Attention",
                    "desc": "This paper introduces a new approach to auditory processing in machine learning called Intention-Informed Auditory Scene Understanding (II-ASU). It presents a prototype system, Auditory Attention-Driven LLM (AAD-LLM), which uses brain signals to determine which speaker a listener is focusing on in complex sound environments. By integrating intracranial electroencephalography (iEEG) data, the model can tailor its responses based on the listener's attention, enhancing the relevance of its outputs. The evaluation shows that AAD-LLM significantly improves performance in tasks like speaker description and question answering, aligning better with human auditory perception."
                },
                "zh": {
                    "title": "æ„å›¾é©±åŠ¨çš„å¬è§‰ç†è§£ï¼Œæå‡æœºå™¨å¬è§‰èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¬è§‰åœºæ™¯ç†è§£æ¨¡å‹ï¼Œç§°ä¸ºæ„å›¾é©±åŠ¨çš„å¬è§‰åœºæ™¯ç†è§£ï¼ˆII-ASUï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ†æè„‘ç”µå›¾ä¿¡å·ï¼Œè¯†åˆ«å¬ä¼—å…³æ³¨çš„ç‰¹å®šè¯´è¯è€…ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆå¬ä¼—æ„å›¾çš„å“åº”ã€‚ä¸ä¼ ç»Ÿçš„å¬è§‰å¤§è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒAAD-LLMèƒ½å¤Ÿæ ¹æ®å¬ä¼—çš„æ³¨æ„åŠ›çŠ¶æ€è°ƒæ•´å…¶è¾“å‡ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šè¯´è¯è€…åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”å¬ä¼—çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16825",
            "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization",
            "url": "https://huggingface.co/papers/2502.16825",
            "abstract": "Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to scale up the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a decline in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 (C_7^2) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position mu - 2sigma rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.",
            "score": 3,
            "issue_id": 2421,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "174509953671dacd",
            "authors": [
                "Yao Xiao",
                "Hai Ye",
                "Linyao Chen",
                "Hwee Tou Ng",
                "Lidong Bing",
                "Xiaoli Li",
                "Roy Ka-wei Lee"
            ],
            "affiliations": [
                "Institute for Infocomm Research, A*Star, Singapore",
                "National University of Singapore",
                "Shanda AI Research Institute",
                "Singapore University of Technology and Design",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16825.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Direct Preference Optimization (DPO), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ mu - 2sigma Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing LLM Alignment with Scalable Preference Data Construction",
                    "desc": "This paper discusses improving the alignment of large language models (LLMs) through iterative data generation and model retraining. It introduces Direct Preference Optimization (DPO), which uses preference pairs of responses to enhance training data selection. The authors find that traditional methods of selecting responses based on extreme rewards can hinder performance as sample sizes grow. They propose a new strategy for constructing preference data that leverages the normal distribution of sample rewards, leading to better model performance with larger datasets."
                },
                "zh": {
                    "title": "ä¼˜åŒ–åå¥½æ•°æ®æ„å»ºï¼Œæå‡æ¨¡å‹å¯¹é½æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡è¿­ä»£æ•°æ®ç”Ÿæˆå’Œæ¨¡å‹é‡è®­ç»ƒæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åå¥½æ•°æ®æ„å»ºç­–ç•¥ï¼Œé€šè¿‡å¯¹æ ·æœ¬å¥–åŠ±çš„æ­£æ€åˆ†å¸ƒè¿›è¡Œåˆ†æï¼Œä¼˜åŒ–äº†é€‰æ‹©å’Œæ‹’ç»å“åº”çš„è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œé€‰æ‹©å¥–åŠ±ä½ç½®åœ¨ mu - 2sigma çš„æ‹’ç»å“åº”ï¼Œè€Œä¸æ˜¯æœ€ä½å¥–åŠ±ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç­–ç•¥åœ¨æ ·æœ¬è§„æ¨¡å¢åŠ æ—¶ï¼Œèƒ½å¤ŸæŒç»­æ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14855",
            "title": "Prompt-to-Leaderboard",
            "url": "https://huggingface.co/papers/2502.14855",
            "abstract": "Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l.",
            "score": 2,
            "issue_id": 2422,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "1aff9de655abd821",
            "authors": [
                "Evan Frick",
                "Connor Chen",
                "Joseph Tennyson",
                "Tianle Li",
                "Wei-Lin Chiang",
                "Anastasios N. Angelopoulos",
                "Ion Stoica"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14855.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Prompt-to-Leaderboard (P2L) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². P2L Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ‘Ñ€ÑĞ´Ğ»Ğ¸-Ğ¢ĞµÑ€Ñ€Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ P2L Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ñƒ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¸Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Personalized Evaluation with Prompt-to-Leaderboard (P2L)",
                    "desc": "This paper introduces a new method called Prompt-to-Leaderboard (P2L) for evaluating large language models (LLMs) based on specific prompts rather than averaged metrics. P2L generates leaderboards that reflect how well models perform on individual prompts by using a trained LLM to output Bradley-Terry coefficients, which predict human preferences. This approach allows for more nuanced evaluations, optimal model routing, and insights into model strengths and weaknesses. The results indicate that P2L captures the complexities of model performance better than traditional methods, achieving notable success in the Chatbot Arena."
                },
                "zh": {
                    "title": "æç¤ºç‰¹å®šæ’è¡Œæ¦œï¼šæå‡è¯­è¨€æ¨¡å‹è¯„ä¼°çš„ç²¾å‡†åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œç§°ä¸ºPrompt-to-Leaderboardï¼ˆP2Lï¼‰ã€‚P2Lé€šè¿‡ç”Ÿæˆç‰¹å®šäºæç¤ºçš„æ’è¡Œæ¦œï¼Œè§£å†³äº†ä¼ ç»Ÿè¯„ä¼°ä¸­ç”¨æˆ·å’Œæç¤ºç‰¹å®šæ€§èƒ½å˜åŒ–è¢«å¹³å‡åŒ–çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è®­ç»ƒLLMä»¥è‡ªç„¶è¯­è¨€æç¤ºä¸ºè¾“å…¥ï¼Œè¾“å‡ºBradley-Terryç³»æ•°å‘é‡ï¼Œä»è€Œé¢„æµ‹äººç±»åå¥½æŠ•ç¥¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒP2Lèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶åœ¨Chatbot Arenaä¸­å–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15612",
            "title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models",
            "url": "https://huggingface.co/papers/2502.15612",
            "abstract": "State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns.",
            "score": 2,
            "issue_id": 2419,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "04a878da4ff4475d",
            "authors": [
                "Hugo Pitorro",
                "Marcos Treviso"
            ],
            "affiliations": [
                "Instituto de TelecomunicaÃ§Ãµes, Lisbon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15612.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#multimodal",
                    "#machine_translation"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ Mamba: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ SSM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ (SSM), Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Mamba, Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ LaTIM - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Mamba-1 Ğ¸ Mamba-2. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LaTIM Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mamba."
                },
                "en": {
                    "title": "Unlocking Interpretability in State Space Models with LaTIM",
                    "desc": "This paper presents LaTIM, a new method designed to improve the interpretability of state space models (SSMs) like Mamba. While SSMs are efficient for handling long sequences, they lack tools to understand how individual tokens contribute to the model's decisions. LaTIM allows researchers to break down and analyze the interactions between tokens at a granular level, enhancing our understanding of Mamba's processing across different layers. The effectiveness of LaTIM is validated through various tasks, showcasing its ability to reveal intricate token relationships within the model."
                },
                "zh": {
                    "title": "æå‡Mambaæ¨¡å‹çš„å¯è§£é‡Šæ€§",
                    "desc": "çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œå¦‚Mambaï¼Œæˆä¸ºäº†é•¿åºåˆ—å»ºæ¨¡ä¸­æ¯”å˜å‹å™¨æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒSSMsä»ç„¶ç¼ºä¹ç†è§£å’Œæ”¹è¿›åŸºäºæ³¨æ„åŠ›æ¶æ„æ‰€éœ€çš„å¯è§£é‡Šæ€§å·¥å…·ã€‚æœ€è¿‘çš„ç ”ç©¶è™½ç„¶æä¾›äº†å¯¹Mambaå†…éƒ¨æœºåˆ¶çš„è§è§£ï¼Œä½†å¹¶æ²¡æœ‰æ˜ç¡®åˆ†è§£æ¯ä¸ªtokençš„è´¡çŒ®ï¼Œå¯¼è‡´å¯¹Mambaå¦‚ä½•åœ¨å„å±‚ä¸­é€‰æ‹©æ€§å¤„ç†åºåˆ—çš„ç†è§£å­˜åœ¨ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºäº†LaTIMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„tokençº§åˆ†è§£æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºMamba-1å’ŒMamba-2æä¾›ç»†ç²’åº¦çš„å¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17092",
            "title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI",
            "url": "https://huggingface.co/papers/2502.17092",
            "abstract": "We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks.",
            "score": 2,
            "issue_id": 2412,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "5433ca3c66d5e184",
            "authors": [
                "Syed Abdul Gaffar Shakhadri",
                "Kruthika KR",
                "Kartik Basavaraj Angadi"
            ],
            "affiliations": [
                "SandLogic Technologies Pvt Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17092.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#reasoning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Shakti VLM Ñ 1B Ğ¸ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ QK-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Shakti VLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ OCR Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficient Multimodal Learning with Shakti VLM",
                    "desc": "Shakti VLM is a new family of vision-language models that come in sizes of 1 billion and 4 billion parameters, aimed at improving data efficiency in multimodal learning. Unlike other models that rely heavily on large datasets, Shakti utilizes innovative architectural features to achieve strong performance with fewer training tokens. Key improvements include QK-Normalization for stable attention mechanisms, hybrid normalization methods, and better positional encoding. The model's three-stage training approach enhances learning efficiency, demonstrating that effective design and training can lead to high performance without needing vast amounts of data."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ çš„æ–°é€‰æ‹©ï¼šShakti VLM",
                    "desc": "Shakti VLMæ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰10äº¿å’Œ40äº¿å‚æ•°ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡é—®é¢˜ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒShaktié€šè¿‡æ¶æ„åˆ›æ–°ï¼Œèƒ½å¤Ÿç”¨æ›´å°‘çš„æ ‡è®°å®ç°ç«äº‰åŠ›çš„ç»“æœã€‚å…¶å…³é”®è¿›å±•åŒ…æ‹¬QKå½’ä¸€åŒ–ä»¥æé«˜æ³¨æ„åŠ›ç¨³å®šæ€§ã€æ··åˆå½’ä¸€åŒ–æŠ€æœ¯å’Œå¢å¼ºçš„ä½ç½®ç¼–ç ã€‚æ­¤å¤–ï¼Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–äº†å­¦ä¹ æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17422",
            "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs",
            "url": "https://huggingface.co/papers/2502.17422",
            "abstract": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.",
            "score": 1,
            "issue_id": 2426,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "3fd784a7d93488b3",
            "authors": [
                "Jiarui Zhang",
                "Mahyar Khayatkhoei",
                "Prateek Chhikara",
                "Filip Ilievski"
            ],
            "affiliations": [
                "University of Southern California, USA",
                "Vrije Universiteit Amsterdam, The Netherlands"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17422.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°ÑÑ‚, ĞºÑƒĞ´Ğ° ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ, Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°ÑÑ‚ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Visual Detail Recognition in MLLMs",
                    "desc": "This paper investigates the visual perception capabilities of Multimodal Large Language Models (MLLMs), particularly focusing on their ability to recognize small visual details compared to larger ones. The authors find that MLLMs' performance is significantly affected by the size of the visual subject in questions, and they establish a causal relationship through an intervention study. They also analyze the attention patterns of MLLMs, revealing that these models can identify relevant areas in images even when they answer incorrectly. To address the limitations in recognizing small details, the paper proposes training-free visual intervention methods that utilize the models' internal attention and gradient maps, demonstrating improved accuracy on various benchmarks without additional training."
                },
                "zh": {
                    "title": "æå‡è§†è§‰ç»†èŠ‚æ„ŸçŸ¥ï¼Œçªç ´MLLMsçš„é™åˆ¶",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚æœ¬æ–‡ç ”ç©¶äº†MLLMsåœ¨å›ç­”å›¾åƒé—®é¢˜æ—¶ï¼Œèƒ½å¦æœ‰æ•ˆæ„ŸçŸ¥å°çš„è§†è§‰ç»†èŠ‚ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„è¡¨ç°å¯¹è§†è§‰ä¸»é¢˜çš„å¤§å°éå¸¸æ•æ„Ÿï¼Œå¹¶é€šè¿‡å¹²é¢„ç ”ç©¶è¯æ˜äº†è¿™ä¸€å› æœå…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„è§†è§‰å¹²é¢„æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹å†…éƒ¨çš„æ³¨æ„åŠ›å’Œæ¢¯åº¦å›¾æ¥å¢å¼ºå…¶å¯¹å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17814",
            "title": "An Overview of Large Language Models for Statisticians",
            "url": "https://huggingface.co/papers/2502.17814",
            "abstract": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.",
            "score": 1,
            "issue_id": 2424,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "bc5cbd3359161d3e",
            "authors": [
                "Wenlong Ji",
                "Weizhe Yuan",
                "Emily Getzen",
                "Kyunghyun Cho",
                "Michael I. Jordan",
                "Song Mei",
                "Jason E Weston",
                "Weijie J. Su",
                "Jing Xu",
                "Linjun Zhang"
            ],
            "affiliations": [
                "INRIA",
                "Meta FAIR",
                "New York University",
                "Rutgers University",
                "Stanford University",
                "UC Berkeley",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17814.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#ethics",
                    "#interpretability",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) ÑÑ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ½ĞµÑÑ‚Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ LLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ĞµĞ¼ÑÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ˜Ğ˜ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºĞ°Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "Bridging AI and Statistics for Trustworthy LLMs",
                    "desc": "This paper discusses the intersection of Large Language Models (LLMs) and statistics, highlighting the need for statistical methods to enhance the reliability and transparency of LLMs. It identifies key areas where statisticians can contribute, such as uncertainty quantification, interpretability, and fairness, which are crucial for building trust in AI systems. The authors argue that addressing these statistical challenges can improve decision-making and causal inference in LLM applications. Ultimately, the paper advocates for a collaborative approach between AI and statistics to tackle complex societal issues effectively."
                },
                "zh": {
                    "title": "ç»Ÿè®¡å­¦ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦èåˆ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€æ¨ç†å’Œå†³ç­–ç­‰å¤šç§ä»»åŠ¡ã€‚å°½ç®¡å…¶æˆåŠŸä¸»è¦ä¾èµ–äºè®¡ç®—èƒ½åŠ›å’Œæ·±åº¦å­¦ä¹ æ¶æ„çš„è¿›æ­¥ï¼Œä½†åœ¨ä¸ç¡®å®šæ€§é‡åŒ–ã€å†³ç­–åˆ¶å®šã€å› æœæ¨æ–­å’Œåˆ†å¸ƒå˜åŒ–ç­‰é¢†åŸŸï¼Œä»éœ€æ›´æ·±å…¥çš„ç»Ÿè®¡å­¦å‚ä¸ã€‚æœ¬æ–‡æ¢è®¨äº†ç»Ÿè®¡å­¦å®¶åœ¨LLMså‘å±•ä¸­çš„é‡è¦è´¡çŒ®ï¼Œç‰¹åˆ«æ˜¯åœ¨æå‡æ¨¡å‹çš„å¯ä¿¡åº¦å’Œé€æ˜åº¦æ–¹é¢ã€‚æˆ‘ä»¬å…³æ³¨çš„ä¸ä»…æ˜¯ä¸ç¡®å®šæ€§é‡åŒ–ã€å¯è§£é‡Šæ€§ã€å…¬å¹³æ€§ã€éšç§ä¿æŠ¤å’Œæ¨¡å‹é€‚åº”æ€§ï¼Œè¿˜è€ƒè™‘äº†LLMsåœ¨ç»Ÿè®¡åˆ†æä¸­çš„æ½œåœ¨è§’è‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18316",
            "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging",
            "url": "https://huggingface.co/papers/2502.18316",
            "abstract": "We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with \"None of the above\", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.",
            "score": 0,
            "issue_id": 2422,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "875de4e64c13e56b",
            "authors": [
                "Ahmed Elhady",
                "Eneko Agirre",
                "Mikel Artetxe"
            ],
            "affiliations": [
                "HiTZ Center, University of the Basque Country (UPV/EHU)",
                "Reka AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18316.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WiCkeD: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ WiCkeD, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 'ĞĞ¸ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ²Ñ‹ÑˆĞµĞ¿ĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ…'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ WiCkeD Ğº 6 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 18 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 12.1 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ WiCkeD Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMLU."
                },
                "en": {
                    "title": "WiCkeD: Elevating Benchmark Complexity for Enhanced Model Evaluation",
                    "desc": "WiCkeD is a novel method designed to enhance the difficulty of existing multiple-choice benchmarks by introducing a 'None of the above' option randomly. This approach can be applied automatically to any benchmark, increasing the challenge for machine learning models. In experiments with 18 open-weight large language models (LLMs) across 6 popular benchmarks, we observed an average performance drop of 12.1 points when using the WiCkeD variant. Additionally, the method reveals varying sensitivities among models to the increased reasoning demands, providing deeper insights into their capabilities."
                },
                "zh": {
                    "title": "WiCkeDï¼šæå‡å¤šé¡¹é€‰æ‹©åŸºå‡†çš„æŒ‘æˆ˜æ€§",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºWiCkeDçš„æ–¹æ³•ï¼Œé€šè¿‡éšæœºå°†é€‰é¡¹æ›¿æ¢ä¸ºâ€œä»¥ä¸Šçš†éâ€æ¥å¢åŠ ç°æœ‰å¤šé¡¹é€‰æ‹©åŸºå‡†çš„å¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•å¸¸ç”¨äºæ•™è‚²æµ‹è¯•ï¼Œå¯ä»¥è‡ªåŠ¨åº”ç”¨äºä»»ä½•ç°æœ‰åŸºå‡†ï¼Œä½¿å…¶æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å°†WiCkeDåº”ç”¨äºå…­ä¸ªæµè¡Œçš„åŸºå‡†ï¼Œå¹¶è¯„ä¼°äº†18ä¸ªå¼€æ”¾æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹çš„æ€§èƒ½å¹³å‡ä¸‹é™äº†12.1ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜WiCkeDå¯¹å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ¨¡å‹åŒæ ·å…·æœ‰æŒ‘æˆ˜æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-25.html",
    "link_next": "2025-02-27.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 7,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 14,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 6,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚",
        "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
        "pinyin": "æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚\n\nZuÃ¬jÃ¬n de kÄiyuÇn duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) zhÇ”yÃ o jÃ­zhÅng zÃ i zÄ“ngqiÃ¡ng jÄ«chÇ” nÃ©nglÃ¬, dÃ n zÃ i yÇ” rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­ fÄngmiÃ n rÃ©ng yÇ’u hÄ›n dÃ  chÄjÃ¹. BÄ›nwÃ©n jiÃ¨shÃ o le OmniAlign-V, yÄ«gÃ¨ bÄohÃ¡n 20 wÃ n gÄo zhÃ¬liÃ ng xÃ¹nliÃ n yÃ ngbÄ›n de quÃ¡nmiÃ n shÃ¹jÃ¹jÃ­, hÃ¡njiÄ“ duÅyÃ nghuÃ  tÃºxiÃ ng, fÃ¹zÃ¡ xuÃ¡nzhÃ²ng wÃ¨ntÃ­ hÃ© duÅzhÇ’ng huÃ­dÃ¡ gÃ©shÃ¬, yÇ tÃ­shÄ“ng MLLMs yÇ” rÃ©nlÃ¨i piÄnhÇo de duÃ¬qÃ­. WÇ’men hÃ¡i tuÄ«chÅ« le MM-AlignBench, yÄ«gÃ¨ zhuÄnmÃ©n shÃ¨jÃ¬ yÃ²ngyÃº pÃ­nggÇ” MLLMs yÇ” rÃ©nlÃ¨i jiÃ zhÃ­ duÃ¬qÃ­ de rÃ©ngÅng biÄozhÃ¹ jÄ«zhÇ”n. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, shÇyÃ²ng jiÃ ndÅ« wÄ“itiÃ¡o (SFT) huÃ² zhÃ­jiÄ“ piÄnhÇo yÅuhuÃ  (DPO) duÃ¬ MLLMs jÃ¬nxÃ­ng wÄ“itiÃ¡o, xiÇnzhÃ¹ tÃ­gÄo le rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­, tÃ³ngshÃ­ zÃ i biÄozhÇ”n VQA jÄ«zhÇ”n shÃ ng bÇochÃ­ huÃ² tÃ­shÄ“ng xÃ¬ngnÃ©ng, bÇoliÃº qÃ­ jÄ«bÄ›n nÃ©nglÃ¬. WÇ’men de shÃ¹jÃ¹jÃ­, jÄ«zhÇ”n, dÃ imÇ hÃ© jiÇnchÃ¡diÇn yÇ fÄbÃ¹ zÃ i https://github.com/PhoenixZ810/OmniAlign-V.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"åŸºç¡€èƒ½åŠ›\", \"pinyin\": \"jÄ« chÇ” nÃ©ng lÃ¬\", \"trans\": \"basic capabilities\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æ ·æœ¬\", \"pinyin\": \"yÃ ng bÄ›n\", \"trans\": \"sample\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡n gÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"å¤šæ ·åŒ–\", \"pinyin\": \"duÅ yÃ ng huÃ \", \"trans\": \"diversified\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ ¼å¼\", \"pinyin\": \"gÃ© shÃ¬\", \"trans\": \"format\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improve\"},\n    {\"word\": \"äººå·¥æ ‡æ³¨\", \"pinyin\": \"rÃ©n gÅng biÄo zhÃ¹\", \"trans\": \"manual annotation\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"ç›‘ç£å¾®è°ƒ\", \"pinyin\": \"jiÃ n dÅ« wÄ“i tiÃ¡o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"ç›´æ¥åå¥½ä¼˜åŒ–\", \"pinyin\": \"zhÃ­ jiÄ“ piÄn hÃ o yÅu huÃ \", \"trans\": \"direct preference optimization\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇo chÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄo zhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"VQA\", \"pinyin\": \"VQA\", \"trans\": \"Visual Question Answering\"},\n    {\"word\": \"æ£€æŸ¥ç‚¹\", \"pinyin\": \"jiÇn chÃ¡ diÇn\", \"trans\": \"checkpoint\"}\n]",
        "trans": "Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V.",
        "update_ts": "2025-02-26 09:11"
    }
}