{
    "date": {
        "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 19",
        "zh": "12æœˆ19æ—¥"
    },
    "time_utc": "2025-12-19 03:25",
    "weekday": 4,
    "issue_id": 139,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.16776",
            "title": "Kling-Omni Technical Report",
            "url": "https://huggingface.co/papers/2512.16776",
            "abstract": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
            "score": 7,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "d9dc39f81fe6fbef",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yuanzheng Ci",
                "Xiangyu Du",
                "Zipeng Feng",
                "Kun Gai",
                "Sainan Guo",
                "Feng Han",
                "Jingbin He",
                "Kang He",
                "Xiao Hu",
                "Xiaohua Hu",
                "Boyuan Jiang",
                "Fangyuan Kong",
                "Hang Li",
                "Jie Li",
                "Qingyu Li",
                "Shen Li",
                "Xiaohan Li",
                "Yan Li",
                "Jiajun Liang",
                "Borui Liao",
                "Yiqiao Liao",
                "Weihong Lin",
                "Quande Liu",
                "Xiaokun Liu",
                "Yilun Liu",
                "Yuliang Liu",
                "Shun Lu",
                "Hangyu Mao",
                "Yunyao Mao",
                "Haodong Ouyang",
                "Wenyu Qin",
                "Wanqi Shi",
                "Xiaoyu Shi",
                "Lianghao Su",
                "Haozhi Sun",
                "Peiqin Sun",
                "Pengfei Wan",
                "Chao Wang",
                "Chenyu Wang",
                "Meng Wang",
                "Qiulin Wang",
                "Runqi Wang",
                "Xintao Wang",
                "Xuebo Wang",
                "Zekun Wang",
                "Min Wei",
                "Tiancheng Wen",
                "Guohao Wu",
                "Xiaoshi Wu",
                "Zhenhua Wu",
                "Da Xie",
                "Yingtong Xiong",
                "Yulong Xu",
                "Sile Yang",
                "Zikang Yang",
                "Weicai Ye",
                "Ziyang Yuan",
                "Shenglong Zhang",
                "Shuaiyu Zhang",
                "Yuanxing Zhang",
                "Yufan Zhang",
                "Wenzheng Zhao",
                "Ruiliang Zhou",
                "Yan Zhou",
                "Guosheng Zhu",
                "Yongjie Zhu"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16776.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ»ÑĞ±Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Kling-Omni â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. Kling-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Kling-Omni: Unifying Video Creation through Multimodal Inputs",
                    "desc": "Kling-Omni is a cutting-edge generative framework that creates high-quality videos from various types of inputs, such as text and images. It combines video generation, editing, and reasoning into a single, cohesive system, unlike traditional methods that treat these tasks separately. By processing multimodal inputs into a unified representation, Kling-Omni can produce cinematic-quality videos that intelligently respond to user instructions. This framework is built on a robust data system and enhanced by large-scale pre-training, making it a significant step towards advanced multimodal content creation and interaction."
                },
                "zh": {
                    "title": "Kling-Omniï¼šå¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆçš„æœªæ¥",
                    "desc": "Kling-Omniæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥ä»å¤šæ¨¡æ€è¾“å…¥åˆæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚å®ƒå°†è§†é¢‘ç”Ÿæˆã€ç¼–è¾‘å’Œæ¨ç†ä»»åŠ¡æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„è§†è§’ã€‚ä¸ä¼ ç»Ÿçš„åˆ†ç¦»å¼æµç¨‹ä¸åŒï¼ŒKling-Omniæ”¯æŒå¤šç§ç”¨æˆ·è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬æŒ‡ä»¤ã€å‚è€ƒå›¾åƒå’Œè§†é¢‘ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿç”Ÿæˆç”µå½±çº§åˆ«çš„æ™ºèƒ½è§†é¢‘å†…å®¹ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„å¤§è§„æ¨¡é¢„è®­ç»ƒç­–ç•¥å’ŒåŸºç¡€è®¾æ–½ä¼˜åŒ–ï¼Œå±•ç¤ºäº†åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆã€åŸºäºæ¨ç†çš„ç¼–è¾‘å’Œå¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16913",
            "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
            "url": "https://huggingface.co/papers/2512.16913",
            "abstract": "A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\_website/}",
            "score": 5,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9da6c70ec001e519",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#architecture",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° UE5 Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ‚Ğ¾ Ñ Ğ²ĞµĞ±. Ğ§Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ domain gap Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸/ÑƒĞ»Ğ¸Ñ†ĞµĞ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸/Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ ground truth. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ DINOv3-Large ĞºĞ°Ğº backbone Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ range mask head, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Robust Depth Prediction Across Diverse Scenes with DINOv3-Large",
                    "desc": "This paper introduces a panoramic metric depth foundation model that effectively handles various real-world scenes using DINOv3-Large. The authors utilize a data-in-the-loop approach, combining public datasets, synthetic data, and real images to create a comprehensive dataset. To address the challenges of domain gaps, they implement a three-stage pseudo-label pipeline that generates reliable ground truth for unlabeled images. The model demonstrates strong performance and zero-shot generalization across multiple benchmarks, showcasing its robustness in predicting depth metrics in diverse environments."
                },
                "zh": {
                    "title": "å…¨æ™¯æ·±åº¦æ¨¡å‹ï¼šè·¨åœºæ™¯çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ™¯åº¦é‡æ·±åº¦åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨DINOv3-Largeå’Œä¸‰é˜¶æ®µä¼ªæ ‡ç­¾ç®¡é“ï¼Œåœ¨å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨æ•°æ®å¾ªç¯çš„æ–¹å¼ï¼Œç»“åˆå…¬å…±æ•°æ®é›†ã€é«˜è´¨é‡åˆæˆæ•°æ®å’ŒçœŸå®å…¨æ™¯å›¾åƒï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†å‡å°‘å®¤å†…/å®¤å¤–å’Œåˆæˆ/çœŸå®æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é˜¶æ®µä¼ªæ ‡ç­¾ç­–åˆ’ç®¡é“ï¼Œä»¥ç”Ÿæˆå¯é çš„æ— æ ‡ç­¾å›¾åƒçš„çœŸå®æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„çœŸå®åœºæ™¯ä¸­æä¾›ç¨³å®šçš„åº¦é‡é¢„æµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16625",
            "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
            "url": "https://huggingface.co/papers/2512.16625",
            "abstract": "DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.",
            "score": 5,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "de2aaa1b8be6e0b0",
            "authors": [
                "Linghui Shen",
                "Mingyue Cui",
                "Xingyi Yang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#security",
                    "#diffusion",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DeContext Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ½ĞµÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ÑÑ Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑÑÑ‚ ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DeContext ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DeContext: Safeguarding Images from Unauthorized Edits",
                    "desc": "DeContext is a method designed to protect images from unauthorized editing by disrupting the flow of information in multimodal attention layers. It does this by applying small perturbations that weaken cross-attention pathways, which are crucial for linking input images to their edited outputs. This approach not only prevents unwanted modifications but also maintains the visual quality of the images. The research demonstrates that focusing on specific transformer blocks and early denoising steps enhances the effectiveness of this defense against image manipulation."
                },
                "zh": {
                    "title": "DeContextï¼šä¿æŠ¤å›¾åƒå…å—æœªç»æˆæƒç¼–è¾‘çš„æœ‰æ•ˆé˜²å¾¡",
                    "desc": "DeContextæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¿æŠ¤å›¾åƒå…å—æœªç»æˆæƒçš„ç¼–è¾‘ã€‚å®ƒé€šè¿‡å‰Šå¼±å¤šæ¨¡æ€æ³¨æ„åŠ›å±‚ä¸­çš„äº¤å‰æ³¨æ„åŠ›é€šé“ï¼Œæ¥é˜»æ­¢ä¸å¿…è¦çš„ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒè§†è§‰è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å…¥å°çš„ã€æœ‰é’ˆå¯¹æ€§çš„æ‰°åŠ¨ï¼Œæ‰“ç ´è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„è”ç³»ï¼Œä»è€Œæœ‰æ•ˆåœ°é˜²æ­¢å›¾åƒè¢«æ¶æ„ç¯¡æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeContextåœ¨é˜»æ­¢ä¸å½“å›¾åƒç¼–è¾‘çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒå›¾åƒçš„è§†è§‰æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16912",
            "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
            "url": "https://huggingface.co/papers/2512.16912",
            "abstract": "Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
            "score": 3,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "af4ba1587be600c7",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Ziniu Li",
                "Wotao Yin",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "affiliations": [
                "CUHK SZ",
                "Columbia",
                "DAMO, Alibaba US",
                "NYU Stern"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16912.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞµÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ RLVR."
                },
                "en": {
                    "title": "Unlocking LLM Reasoning: The Power of Spurious Rewards and Entropy Minimization",
                    "desc": "This paper explores how reinforcement learning with verifiable rewards (RLVR) can enhance the reasoning abilities of Large Language Models (LLMs). It identifies two key mechanisms: spurious rewards, which can mislead the model by rewarding incorrect outcomes, and entropy minimization, which encourages the model to produce more confident responses. The study reveals that both mechanisms, while seemingly contradictory, can lead to improved reasoning performance by reducing clipping bias and policy entropy. Ultimately, the research provides insights into how these dynamics work together and offers guidelines for optimizing RLVR training."
                },
                "zh": {
                    "title": "å¯éªŒè¯å¥–åŠ±æå‡LLMæ¨ç†èƒ½åŠ›çš„å¥¥ç§˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRé€šè¿‡ä¸¤ç§çœ‹ä¼¼çŸ›ç›¾çš„æœºåˆ¶æ¥å¢å¼ºLLMsçš„æ•°å­¦æ¨ç†ï¼šè™šå‡å¥–åŠ±å’Œç†µæœ€å°åŒ–ã€‚è™šå‡å¥–åŠ±é€šè¿‡å¥–åŠ±ä¸çœŸå®ç»“æœæ— å…³çš„ç»“æœæ¥æŠ‘åˆ¶åˆ©ç”¨ï¼Œè€Œç†µæœ€å°åŒ–åˆ™é€šè¿‡æ¨åŠ¨æ¨¡å‹æœå‘æ›´è‡ªä¿¡å’Œç¡®å®šçš„è¾“å‡ºï¼ŒæŠ‘åˆ¶æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†è™šå‡å¥–åŠ±å¦‚ä½•é€šè¿‡å‡å°‘ç­–ç•¥ç†µæ¥æé«˜æ¨ç†æ€§èƒ½ï¼Œå¹¶æå‡ºäº†å¥–åŠ±ä¸ä¸€è‡´æ¨¡å‹ï¼Œä»¥è§£é‡Šè™šå‡å¥–åŠ±åœ¨å—æ±¡æŸ“ç¯å¢ƒä¸­ä»èƒ½æå‡æ€§èƒ½çš„åŸå› ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15745",
            "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
            "url": "https://huggingface.co/papers/2512.15745",
            "abstract": "LLaDA2.0 converts auto-regressive models into discrete diffusion large language models using a block-level training scheme, improving efficiency and performance at large scales.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
            "score": 2,
            "issue_id": 139,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "1ebc58b9d434a25e",
            "authors": [
                "Tiwei Bie",
                "Maosong Cao",
                "Kun Chen",
                "Lun Du",
                "Mingliang Gong",
                "Zhuochen Gong",
                "Yanmei Gu",
                "Jiaqi Hu",
                "Zenan Huang",
                "Zhenzhong Lan",
                "Chengxi Li",
                "Chongxuan Li",
                "Jianguo Li",
                "Zehuan Li",
                "Huabin Liu",
                "Ling Liu",
                "Guoshan Lu",
                "Xiaocheng Lu",
                "Yuxin Ma",
                "Jianfeng Tan",
                "Lanning Wei",
                "Ji-Rong Wen",
                "Yipeng Xing",
                "Xiaolu Zhang",
                "Junbo Zhao",
                "Da Zheng",
                "Jun Zhou",
                "Junlin Zhou",
                "Zhanchao Zhou",
                "Liwang Zhu",
                "Yihong Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "HongKong University of Science and Technology",
                "Renmin University of China",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15745.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#transfer_learning",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LLaDA2.0 â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ´Ğ¾ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ñ‘Ñ…Ñ„Ğ°Ğ·Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaDA2.0-mini Ğ¸ LLaDA2.0-flash Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT Ğ¸ DPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Transforming AR Models for Efficient Large-Scale Language Processing",
                    "desc": "LLaDA2.0 is a new approach that transforms auto-regressive (AR) models into discrete diffusion large language models (dLLMs) to enhance their efficiency and performance. It uses a unique block-level training method that involves three phases: warming up with small blocks, stabilizing with full-sequence diffusion, and then decaying back to compact blocks. This method allows for the retention of knowledge from pre-trained AR models, making the training process more efficient. The resulting models, LLaDA2.0-mini and LLaDA2.0-flash, are optimized for practical use and demonstrate improved performance at large scales while being open-sourced for wider accessibility."
                },
                "zh": {
                    "title": "LLaDA2.0ï¼šé«˜æ•ˆè½¬æ¢è‡ªå›å½’æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "LLaDA2.0 æ˜¯ä¸€ç§å°†è‡ªå›å½’æ¨¡å‹è½¬æ¢ä¸ºç¦»æ•£æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ° 1000 äº¿ä¸ªå‚æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„è½¬æ¢è¿‡ç¨‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œä¿ç•™äº†çŸ¥è¯†ç»§æ‰¿å’Œé«˜æ•ˆè®¾è®¡åŸåˆ™ã€‚LLaDA2.0 é‡‡ç”¨ä¸‰é˜¶æ®µçš„å—çº§è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬çƒ­èº«é˜¶æ®µã€ç¨³å®šé˜¶æ®µå’Œè¡°å‡é˜¶æ®µï¼Œä»¥å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚æœ€ç»ˆï¼ŒLLaDA2.0-mini å’Œ LLaDA2.0-flash ä¸¤ä¸ªä¼˜åŒ–ç‰ˆæœ¬è¢«å¼€æºï¼Œå±•ç°äº†åœ¨å‰æ²¿è§„æ¨¡ä¸‹çš„å“è¶Šæ€§èƒ½å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16915",
            "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
            "url": "https://huggingface.co/papers/2512.16915",
            "abstract": "StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",
            "score": 1,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9098462f35e68746",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ StereoPilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UniStereo Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ° ÑÑ‚ĞµÑ€ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StereoPilot Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing Stereo Video Synthesis with StereoPilot",
                    "desc": "StereoPilot is a novel feed-forward model designed for synthesizing high-quality stereo video without the need for depth maps. It utilizes a learnable domain switcher and cycle consistency loss to enhance adaptability across different stereo formats. This approach addresses common issues in traditional Monocular-to-Stereo conversion methods, such as error propagation and depth ambiguity. Extensive testing shows that StereoPilot surpasses existing techniques in both visual quality and computational efficiency, making it a significant advancement in stereo video generation."
                },
                "zh": {
                    "title": "StereoPilotï¼šé«˜æ•ˆåˆæˆç«‹ä½“è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "StereoPilotæ˜¯ä¸€ç§å‰é¦ˆæ¨¡å‹ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„é¢†åŸŸåˆ‡æ¢å™¨å’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±ï¼Œèƒ½å¤Ÿç›´æ¥åˆæˆé«˜è´¨é‡çš„ç«‹ä½“è§†é¢‘ï¼Œè€Œæ— éœ€æ·±åº¦å›¾ã€‚è¯¥æ¨¡å‹åœ¨è§†è§‰ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿå•ç›®åˆ°ç«‹ä½“è½¬æ¢ä¸­çš„æ·±åº¦æ¨¡ç³Šå’Œæ ¼å¼ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥UniStereoæ•°æ®é›†ï¼ŒStereoPilotèƒ½å¤Ÿåœ¨ä¸åŒçš„ç«‹ä½“æ ¼å¼ä¹‹é—´æ— ç¼é€‚åº”ï¼Œå¹¶å®ç°æ›´å¥½çš„ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoPilotåœ¨è§†è§‰è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16301",
            "title": "Adaptation of Agentic AI",
            "url": "https://huggingface.co/papers/2512.16301",
            "abstract": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
            "score": 1,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "e95fabb54f043168",
            "authors": [
                "Pengcheng Jiang",
                "Jiacheng Lin",
                "Zhiyi Shi",
                "Zifeng Wang",
                "Luxi He",
                "Yichen Wu",
                "Ming Zhong",
                "Peiyang Song",
                "Qizheng Zhang",
                "Heng Wang",
                "Xueqiang Xu",
                "Hanwen Xu",
                "Pengrui Han",
                "Dylan Zhang",
                "Jiashuo Sun",
                "Chaoqi Yang",
                "Kun Qian",
                "Tian Wang",
                "Changran Hu",
                "Manling Li",
                "Quanzheng Li",
                "Hao Peng",
                "Sheng Wang",
                "Jingbo Shang",
                "Chao Zhang",
                "Jiaxuan You",
                "Liyuan Liu",
                "Pan Lu",
                "Yu Zhang",
                "Heng Ji",
                "Yejin Choi",
                "Dawn Song",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "affiliations": [
                "Harvard",
                "Northwestern",
                "TAMU",
                "UCSD",
                "UIUC",
                "UW",
                "Unity"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16301.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° (Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°) Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ°Ñ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞµĞ³Ğ¾ supervision). ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ trade-offs Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Agentic AI: A Framework for Adaptation Strategies",
                    "desc": "This paper introduces a framework for enhancing agentic AI systems through effective adaptation of both agents and tools. It categorizes adaptations into two main types: those signaled by tool execution and those signaled by agent output, as well as distinguishing between agent-agnostic and agent-supervised tool adaptations. By clarifying the design space and trade-offs of these adaptation strategies, the framework provides practical guidance for improving AI performance and reliability. The paper also reviews existing approaches, highlighting their strengths and challenges, while identifying future research opportunities in the field."
                },
                "zh": {
                    "title": "æ„å»ºæ›´å¼ºå¤§çš„æ™ºèƒ½ä»£ç†AIç³»ç»Ÿçš„é€‚åº”æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæ™ºèƒ½ä»£ç†ç³»ç»Ÿä¸­ä»£ç†å’Œå·¥å…·é€‚åº”çš„æ¡†æ¶ï¼Œæ˜ç¡®äº†è®¾è®¡ç­–ç•¥å¹¶è¯†åˆ«äº†æé«˜AIèƒ½åŠ›çš„å¼€æ”¾æŒ‘æˆ˜ã€‚éšç€è¿™äº›ç³»ç»Ÿèƒ½åŠ›å’ŒèŒƒå›´çš„å¢é•¿ï¼Œé€‚åº”æ€§æˆä¸ºæé«˜æ€§èƒ½ã€å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒæœºåˆ¶ã€‚æˆ‘ä»¬å°†å¿«é€Ÿæ‰©å±•çš„ç ”ç©¶é¢†åŸŸç»Ÿä¸€ä¸ºä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œæ¶µç›–ä»£ç†é€‚åº”å’Œå·¥å…·é€‚åº”ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå·¥å…·æ‰§è¡Œä¿¡å·å’Œä»£ç†è¾“å‡ºä¿¡å·çš„é€‚åº”å½¢å¼ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªæ¦‚å¿µåŸºç¡€å’Œå®ç”¨è·¯çº¿å›¾ï¼Œä»¥æ„å»ºæ›´å¼ºå¤§ã€é«˜æ•ˆå’Œå¯é çš„æ™ºèƒ½ä»£ç†AIç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15907",
            "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
            "url": "https://huggingface.co/papers/2512.15907",
            "abstract": "TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
            "score": 0,
            "issue_id": 139,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "20df4be45558984f",
            "authors": [
                "Tejas Anvekar",
                "Juhna Park",
                "Aparna Garimella",
                "Vivek Gupta"
            ],
            "affiliations": [
                "Adobe",
                "Arizona State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15907.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "TabReX â€” ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. TabReX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‡ĞµĞµĞº."
                },
                "en": {
                    "title": "Revolutionizing Table Quality Evaluation with TabReX",
                    "desc": "TabReX is a novel framework designed to assess the quality of tables produced by large language models (LLMs) without relying on predefined references. It utilizes graph-based reasoning to convert both the input text and the generated tables into canonical knowledge graphs, allowing for a more structured evaluation. By aligning these graphs through a matching process guided by LLMs, TabReX computes scores that reflect both structural and factual accuracy. The framework also introduces TabReX-Bench, a comprehensive benchmark for testing the robustness of the evaluation metrics across various domains and perturbation types, demonstrating high correlation with expert assessments."
                },
                "zh": {
                    "title": "TabReXï¼šæ— å‚è€ƒçš„è¡¨æ ¼ç”Ÿæˆè´¨é‡è¯„ä¼°æ–°æ¡†æ¶",
                    "desc": "TabReXæ˜¯ä¸€ä¸ªæ— å‚è€ƒçš„æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºå›¾çš„æ¨ç†æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„è¡¨æ ¼è´¨é‡ã€‚å®ƒå°†æºæ–‡æœ¬å’Œç”Ÿæˆçš„è¡¨æ ¼è½¬æ¢ä¸ºè§„èŒƒçŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡LLMå¼•å¯¼çš„åŒ¹é…è¿‡ç¨‹è¿›è¡Œå¯¹é½ï¼Œå¹¶è®¡ç®—å¯è§£é‡Šçš„ã€ç¬¦åˆè¯„åˆ†æ ‡å‡†çš„åˆ†æ•°ï¼Œä»¥é‡åŒ–ç»“æ„å’Œäº‹å®çš„å‡†ç¡®æ€§ã€‚TabReXæä¾›äº†çµæ•åº¦å’Œç‰¹å¼‚æ€§ä¹‹é—´çš„å¯æ§æƒè¡¡ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸äººç±»åˆ¤æ–­ä¸€è‡´çš„è¯„ä¼°ç»“æœã€‚é€šè¿‡å¼•å…¥TabReX-BenchåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°äº†è¯¥æŒ‡æ ‡çš„ç¨³å¥æ€§ï¼Œæ˜¾ç¤ºTabReXåœ¨ä¸“å®¶æ’åä¸­å…·æœ‰æœ€é«˜çš„ç›¸å…³æ€§ï¼Œå¹¶åœ¨æ›´å¤æ‚çš„æ‰°åŠ¨ä¸‹ä¿æŒç¨³å®šã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-18.html",
    "link_next": "2025-12-22.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.12",
        "en": "12/22",
        "zh": "12æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}