
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. September 1.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-29.html">â¬…ï¸ <span id="prev-date">29.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-02.html">â¡ï¸ <span id="next-date">02.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'};
        let feedDateNext = {'ru': '02.09', 'en': '09/02', 'zh': '9æœˆ2æ—¥'};
        let feedDatePrev = {'ru': '29.08', 'en': '08/29', 'zh': '8æœˆ29æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.21113', 'title': 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning', 'url': 'https://huggingface.co/papers/2508.21113', 'abstract': "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.", 'score': 76, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'e3b0726caba25eb1', 'authors': ['Jie Jiang', 'Qi Yang', 'Bolin Ni', 'Shiming Xiang', 'Han Hu', 'Houwen Peng'], 'affiliations': ['Institute of Automation, CAS', 'Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.21113.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#benchmark', '#dataset', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'R-4B: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'R-4B - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. R-4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 25 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'R-4B: Smart Thinking for Efficient Problem Solving', 'desc': 'R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models.'}, 'zh': {'title': 'R-4Bï¼šæ™ºèƒ½æ€è€ƒä¸é«˜æ•ˆè§£å†³çš„ç»“åˆ', 'desc': 'R-4Bæ˜¯ä¸€ç§è‡ªåŠ¨æ€è€ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œæ€è€ƒã€‚å®ƒé‡‡ç”¨åŒæ¨¡é€€ç«å’ŒåŒæ¨¡ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨è§£å†³é—®é¢˜æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒR-4Bèƒ½å¤Ÿåœ¨ç®€å•é—®é¢˜ä¸Šé¿å…å†—ä½™çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-4Båœ¨25ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21112', 'title': 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control', 'url': 'https://huggingface.co/papers/2508.21112', 'abstract': 'EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.', 'score': 45, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '5bbbfa48bbd5fb7c', 'authors': ['Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Zhaoqing Chen', 'Xianqiang Gao', 'Xinyi Ye', 'Qi Lv', 'Modi Shi', 'Guanghui Ren', 'Cheng Ruan', 'Maoqing Yao', 'Haoran Yang', 'Jiacheng Bao', 'Bin Zhao', 'Dong Wang'], 'affiliations': ['AgiBot', 'Fudan University', 'Northwestern Polytechnical University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.21112.jpg', 'data': {'categories': ['#architecture', '#training', '#agents', '#multimodal', '#agi', '#reasoning', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: EO-Robotics Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'desc': 'EO-Robotics Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EO-1 Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° EO-Data1.5M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ EO-1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EO-Data1.5M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering Robots with Multimodal Reasoning and Control', 'desc': "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."}, 'zh': {'title': 'æå‡æœºå™¨äººæ§åˆ¶çš„å¤šæ¨¡æ€æ¨ç†æ–°çªç ´', 'desc': 'EO-Roboticsæ˜¯ä¸€ä¸ªæ–°æ¨¡å‹ï¼ŒåŒ…å«EO-1æ¨¡å‹å’ŒEO-Data1.5Mæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡äº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œé¢„è®­ç»ƒæ¥æå‡å¤šæ¨¡æ€çš„å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶èƒ½åŠ›ã€‚EO-1æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘å’ŒåŠ¨ä½œç­‰å¤šç§è¾“å…¥ï¼Œå±•ç°å‡ºåœ¨å¤šæ¨¡æ€å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒä¾èµ–äºä¸€ä¸ªåŒ…å«è¶…è¿‡150ä¸‡æ ·æœ¬çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¼ºè°ƒäº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œç†è§£ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†äº¤æ›¿å­¦ä¹ åœ¨å¼€æ”¾ä¸–ç•Œç†è§£å’Œæ³›åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†æ„å»ºå…ˆè¿›å…·èº«åŸºç¡€æ¨¡å‹çš„å®è´µè§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.18106', 'title': 'A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code', 'url': 'https://huggingface.co/papers/2508.18106', 'abstract': "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.", 'score': 43, 'issue_id': 5638, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 25', 'zh': '8æœˆ25æ—¥'}, 'hash': '7b691aaa7b52bcfd', 'authors': ['Keke Lian', 'Bin Wang', 'Lei Zhang', 'Libo Chen', 'Junjie Wang', 'Ziming Zhao', 'Yujiu Yang', 'Haotong Duan', 'Haoran Zhao', 'Shuang Liao', 'Mingda Guo', 'Jiazheng Quan', 'Yilu Zhong', 'Chenhao He', 'Zichuan Chen', 'Jie Wu', 'Haoling Li', 'Zhaoxuan Li', 'Jiongchi Yu', 'Hui Li', 'Dong Zhang'], 'affiliations': ['Fudan University', 'Institute of Information Engineering, Chinese Academy of Sciences', 'Peking University', 'Shanghai Jiao Tong University', 'Singapore Management University', 'Tencent', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18106.jpg', 'data': {'categories': ['#open_source', '#security', '#benchmark', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'A.S.E: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'A.S.E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. A.S.E ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Claude-3.7-Sonnet Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½ĞµĞ²ĞµĞ»Ğ¸Ğº.'}, 'en': {'title': 'A.S.E: Elevating Security Evaluation for AI-Generated Code', 'desc': 'The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches.'}, 'zh': {'title': 'A.S.Eï¼šæå‡ä»£ç ç”Ÿæˆå®‰å…¨æ€§çš„åŸºå‡†è¯„ä¼°', 'desc': 'A.S.Eæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†ï¼Œåˆ©ç”¨çœŸå®ä¸–ç•Œçš„ä»£ç åº“å’Œä¸“å®¶å®šä¹‰çš„è§„åˆ™ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¿æ¥è¾“å…¥ä¸Šä¸‹æ–‡çš„è´¨é‡ä¸è¾“å‡ºçš„å®‰å…¨æ€§ã€‚A.S.Eé€šè¿‡æ„å»ºçœŸå®ä»£ç åº“ä¸­çš„ä»»åŠ¡ï¼Œä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›å¯é‡å¤çš„å®‰å…¨è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-Sonnetåœ¨æ•´ä½“è¡¨ç°ä¸Šæœ€ä½³ï¼Œè€ŒQwen3-235B-A22B-Instructåœ¨å®‰å…¨æ€§è¯„åˆ†ä¸Šè¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20470', 'title': 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation', 'url': 'https://huggingface.co/papers/2508.20470', 'abstract': 'Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.', 'score': 26, 'issue_id': 5642, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '2bbd88d7f14f5a78', 'authors': ['Xiaochuan Li', 'Guoguang Du', 'Runze Zhang', 'Liang Jin', 'Qi Jia', 'Lihua Lu', 'Zhenhua Guo', 'Yaqian Zhao', 'Haiyang Liu', 'Tianqi Wang', 'Changsheng Li', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20470.jpg', 'data': {'categories': ['#dataset', '#3d', '#multimodal', '#open_source', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Droplet3D-4M Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Droplet3D, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 3D Asset Generation with Video Commonsense Priors', 'desc': 'This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation.'}, 'zh': {'title': 'åˆ©ç”¨è§†é¢‘æ•°æ®æå‡3Dç”Ÿæˆçš„ç©ºé—´ä¸è¯­ä¹‰ä¸€è‡´æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è§†é¢‘æ•°æ®æ¥å¢å¼º3Dèµ„äº§ç”Ÿæˆï¼Œæä¾›ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰åˆç†æ€§ã€‚ç”±äº3Dé¢†åŸŸçš„æ•°æ®ç¨€ç¼ºï¼Œè§†é¢‘ä¸­çš„å¸¸è¯†å…ˆéªŒæˆä¸ºäº†ä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£ç›‘ç£ä¿¡å·ã€‚è§†é¢‘æ•æ‰çš„å¤šè§†è§’ä¿¡æ¯ä¸º3Dç”Ÿæˆæä¾›äº†ç©ºé—´ä¸€è‡´æ€§ï¼Œè€Œä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯åˆ™ä½¿ç”Ÿæˆçš„å†…å®¹æ›´ç¬¦åˆæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬ä»‹ç»äº†Droplet3D-4Mæ•°æ®é›†å’ŒDroplet3Dç”Ÿæˆæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Då†…å®¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13618', 'title': 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis', 'url': 'https://huggingface.co/papers/2508.13618', 'abstract': 'TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid', 'score': 13, 'issue_id': 5639, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': '8baf01eb014bc50c', 'authors': ['Shunian Chen', 'Hejin Huang', 'Yexin Liu', 'Zihan Ye', 'Pengcheng Chen', 'Chenghao Zhu', 'Michael Guan', 'Rongsheng Wang', 'Junying Chen', 'Guanbin Li', 'Ser-Nam Lim', 'Harry Yang', 'Benyou Wang'], 'affiliations': ['Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.13618.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#transfer_learning', '#dataset', '#cv', '#data'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'TalkVid: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1244 Ñ‡Ğ°ÑĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ 7729 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° TalkVid, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ….'}, 'en': {'title': 'Bridging the Diversity Gap in AI with TalkVid', 'desc': 'The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research.'}, 'zh': {'title': 'TalkVidï¼šæå‡è™šæ‹Ÿäººå¤´åˆæˆçš„å¤šæ ·æ€§ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'TalkVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºéŸ³é¢‘çš„è™šæ‹Ÿäººå¤´åˆæˆæŠ€æœ¯ã€‚ç°æœ‰çš„æ¨¡å‹åœ¨å¤„ç†ä¸åŒç§æ—ã€è¯­è¨€å’Œå¹´é¾„ç¾¤ä½“æ—¶å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œå¤šæ ·æ€§ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTalkVidåŒ…å«äº†1244å°æ—¶æ¥è‡ª7729ä¸ªç‹¬ç‰¹è¯´è¯è€…çš„è§†é¢‘ï¼Œç»è¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè‡ªåŠ¨åŒ–ç­›é€‰ï¼Œç¡®ä¿äº†æ•°æ®çš„ç¨³å®šæ€§å’Œç¾å­¦è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºTalkVidè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†ä¸åŒå­ç¾¤ä½“ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21767', 'title': 'UItron: Foundational GUI Agent with Advanced Perception and Planning', 'url': 'https://huggingface.co/papers/2508.21767', 'abstract': 'UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '4e413654a21d562e', 'authors': ['Zhixiong Zeng', 'Jing Huang', 'Liming Zheng', 'Wenkang Han', 'Yufeng Zhong', 'Lei Chen', 'Longrong Yang', 'Yingjie Chu', 'Yuzhi He', 'Lin Ma'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2508.21767.jpg', 'data': {'categories': ['#benchmark', '#rl', '#open_source', '#training', '#optimization', '#reasoning', '#agi', '#dataset', '#data', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'UItron: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'UItron - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. UItron Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UItron: Advancing GUI Agents for Real-World Applications', 'desc': 'UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents.'}, 'zh': {'title': 'UItronï¼šæ¨åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†çš„æœªæ¥', 'desc': 'UItronæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†è®¾è®¡ï¼Œæå‡äº†è§†è§‰ç†è§£å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å…ˆè¿›çš„æ„ŸçŸ¥ã€å®šä½å’Œè§„åˆ’åŠŸèƒ½ï¼Œåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚UItronå¼ºè°ƒäº†ç³»ç»Ÿæ•°æ®å·¥ç¨‹å’Œäº¤äº’åŸºç¡€è®¾æ–½åœ¨GUIä»£ç†å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUItronåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿GUIä»£ç†æ›´æ¥è¿‘å®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21365', 'title': 'Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2508.21365', 'abstract': 'Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': 'cbcbd196468063e9', 'authors': ['Yi Liao', 'Yu Gu', 'Yuan Sui', 'Zining Zhu', 'Yifan Lu', 'Guohua Tang', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.21365.jpg', 'data': {'categories': ['#training', '#games', '#optimization', '#reasoning', '#interpretability', '#multimodal', '#rl', '#rlhf'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹: Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº ÑƒĞ¼ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Think in Games (TiG), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. TiG Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, TiG Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Empowering Language Models to Learn by Playing', 'desc': 'The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable.'}, 'zh': {'title': 'é€šè¿‡æ¸¸æˆæ€ç»´æå‡AIçš„å­¦ä¹ èƒ½åŠ›', 'desc': 'Think in Games (TiG) æ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡äº’åŠ¨æ¸¸æˆç¯å¢ƒå‘å±•ç¨‹åºæ€§çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTiG åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¯­è¨€æŒ‡å¯¼çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiG æˆåŠŸå¼¥è¡¥äº†å£°æ˜æ€§çŸ¥è¯†ä¸ç¨‹åºæ€§çŸ¥è¯†ä¹‹é—´çš„å·®è·ï¼Œæå‡äº†å¤æ‚äº’åŠ¨ä»»åŠ¡çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17677', 'title': 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training', 'url': 'https://huggingface.co/papers/2508.17677', 'abstract': "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.", 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 25', 'zh': '8æœˆ25æ—¥'}, 'hash': '8c118ab21cea1eb2', 'authors': ['Yifan Wang', 'Binbin Liu', 'Fengze Liu', 'Yuanfan Guo', 'Jiyao Deng', 'Xuecheng Wu', 'Weidong Zhou', 'Xiaohuan Zhou', 'Taifeng Wang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.17677.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TiKMiX - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Group Influence Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. TiKMiX Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TiKMiX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Dynamic Data Mixing for Enhanced Language Model Performance', 'desc': "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTiKMiXçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ¨¡å‹çš„å­¦ä¹ åå¥½åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„é™æ€æ•°æ®æ··åˆç­–ç•¥æ— æ³•é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–çš„åå¥½ã€‚TiKMiXå¼•å…¥äº†Group Influenceè¿™ä¸€é«˜æ•ˆæŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ•°æ®é¢†åŸŸå¯¹æ¨¡å‹çš„å½±å“ï¼Œä»è€Œä¼˜åŒ–æ•°æ®æ··åˆçš„åˆ†å¸ƒã€‚é€šè¿‡TiKMiX-Då’ŒTiKMiX-Mä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†åœ¨è®¡ç®—èµ„æºä½¿ç”¨ä¸Šæ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21290', 'title': 'Efficient Code Embeddings from Code Generation Models', 'url': 'https://huggingface.co/papers/2508.21290', 'abstract': 'Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.', 'score': 3, 'issue_id': 5640, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '484a770fa8f460fc', 'authors': ['Daria Kryvosheieva', 'Saba Sturua', 'Michael GÃ¼nther', 'Scott Martens', 'Han Xiao'], 'affiliations': ['Jina AI GmbH', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.21290.jpg', 'data': {'categories': ['#multilingual', '#transfer_learning', '#data', '#dataset', '#games', '#plp', '#small_models', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼', 'desc': 'Jina-code-embeddings - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ĞºĞ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ´Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Code Retrieval with Smart Embeddings', 'desc': 'Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.'}, 'zh': {'title': 'åˆ›æ–°ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæå‡ä»£ç æ£€ç´¢ä¸é—®ç­”èƒ½åŠ›', 'desc': 'Jina-code-embeddings æ˜¯ä¸€ç§æ–°å‹çš„ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢ä»£ç ã€è¿›è¡ŒæŠ€æœ¯é—®ç­”ä»¥åŠè¯†åˆ«ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç ç‰‡æ®µã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ä½¿ç”¨äº†ä¸€ä¸ªåœ¨æ–‡æœ¬å’Œä»£ç ä¸Šé¢„è®­ç»ƒçš„è‡ªå›å½’éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡æœ€åä¸€ä¸ªæ ‡è®°çš„æ± åŒ–ç”ŸæˆåµŒå…¥ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å°½ç®¡æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œä½†ä»èƒ½å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™éªŒè¯äº†è¿™ç§ä»£ç åµŒå…¥æ¨¡å‹æ„å»ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21148', 'title': 'A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers', 'url': 'https://huggingface.co/papers/2508.21148', 'abstract': 'Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.', 'score': 3, 'issue_id': 5645, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '3097c905f2f36541', 'authors': ['Ming Hu', 'Chenglong Ma', 'Wei Li', 'Wanghan Xu', 'Jiamin Wu', 'Jucheng Hu', 'Tianbin Li', 'Guohang Zhuang', 'Jiaqi Liu', 'Yingzhou Lu', 'Ying Chen', 'Chaoyang Zhang', 'Cheng Tan', 'Jie Ying', 'Guocheng Wu', 'Shujian Gao', 'Pengcheng Chen', 'Jiashi Lin', 'Haitao Wu', 'Lulu Chen', 'Fengxiang Wang', 'Yuanyuan Zhang', 'Xiangyu Zhao', 'Feilong Tang', 'Encheng Su', 'Junzhi Ning', 'Xinyao Liu', 'Ye Du', 'Changkai Ji', 'Cheng Tang', 'Huihui Xu', 'Ziyang Chen', 'Ziyan Huang', 'Jiyao Liu', 'Pengfei Jiang', 'Yizhou Wang', 'Chen Tang', 'Jianyu Wu', 'Yuchen Ren', 'Siyuan Yan', 'Zhonghua Wang', 'Zhongxing Xu', 'Shiyan Su', 'Shangquan Sun', 'Runkai Zhao', 'Zhisheng Zhang', 'Yu Liu', 'Fudi Wang', 'Yuanfeng Ji', 'Yanzhou Su', 'Hongming Shan', 'Chunmei Feng', 'Jiahao Xu', 'Jiangtao Yan', 'Wenhao Tang', 'Diping Song', 'Lihao Liu', 'Yanyan Huang', 'Lequan Yu', 'Bin Fu', 'Shujun Wang', 'Xiaomeng Li', 'Xiaowei Hu', 'Yun Gu', 'Ben Fei', 'Zhongying Deng', 'Benyou Wang', 'Yuewen Cao', 'Minjie Shen', 'Haodong Duan', 'Jie Xu', 'Yirong Chen', 'Fang Yan', 'Hongxia Hao', 'Jielan Li', 'Jiajun Du', 'Yanbo Wang', 'Imran Razzak', 'Chi Zhang', 'Lijun Wu', 'Conghui He', 'Zhaohui Lu', 'Jinhai Huang', 'Yihao Liu', 'Fenghua Ling', 'Yuqiang Li', 'Aoran Wang', 'Qihao Zheng', 'Nanqing Dong', 'Tianfan Fu', 'Dongzhan Zhou', 'Yan Lu', 'Wenlong Zhang', 'Jin Ye', 'Jianfei Cai', 'Wanli Ouyang', 'Yu Qiao', 'Zongyuan Ge', 'Shixiang Tang', 'Junjun He', 'Chunfeng Song', 'Lei Bai', 'Bowen Zhou'], 'affiliations': ['Beijing Institute of Heart, Lung and Blood Vessel Diseases', 'China Pharmaceutical University', 'Chinese Academy of Sciences', 'Fudan University', 'Fuzhou University', 'Monash University', 'Purdue University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'South China University', 'Stanford University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'UNC-Chapel Hill', 'University College Dublin', 'University College London', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.21148.jpg', 'data': {'categories': ['#benchmark', '#agents', '#survey', '#multimodal', '#data', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Sci-LLMs: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sci-LLMs) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ‚ĞµÑĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Sci-LLMs, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 270 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Sci-LLMs, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Science with Autonomous Language Models', 'desc': 'This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base.'}, 'zh': {'title': 'ç§‘å­¦ç ”ç©¶ä¸­çš„æ™ºèƒ½åˆä½œä¼™ä¼´', 'desc': 'ç§‘å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSci-LLMsï¼‰æ­£åœ¨é€šè¿‡ä¸ç§‘å­¦æ•°æ®çš„å…±åŒå‘å±•è€Œä¸æ–­æ¼”å˜ï¼Œè§£å†³å¤šæ¨¡æ€å’Œç‰¹å®šé¢†åŸŸä¿¡æ¯ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç»¼åˆæ¡†æ¶ï¼Œå°†Sci-LLMsçš„å‘å±•è§†ä¸ºæ¨¡å‹ä¸å…¶åŸºç¡€æ•°æ®ä¹‹é—´çš„å…±åŒè¿›åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†ç§‘å­¦æ•°æ®çš„ç»Ÿä¸€åˆ†ç±»æ³•å’Œç§‘å­¦çŸ¥è¯†çš„å±‚æ¬¡æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç§‘å­¦è¯­æ–™åº“ä¸ä¸€èˆ¬è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ä¹‹é—´çš„åŒºåˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬å±•æœ›äº†å‘é—­ç¯ç³»ç»Ÿçš„è½¬å˜ï¼Œå¼ºè°ƒåŸºäºSci-LLMsçš„è‡ªä¸»ä»£ç†å¦‚ä½•ç§¯æå®éªŒã€éªŒè¯å¹¶ä¸ºä¸æ–­å‘å±•çš„çŸ¥è¯†åº“åšå‡ºè´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21456', 'title': 'Morae: Proactively Pausing UI Agents for User Choices', 'url': 'https://huggingface.co/papers/2508.21456', 'abstract': 'Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.', 'score': 2, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '3d7bd580c525eaa6', 'authors': ['Yi-Hao Peng', 'Dingzeyu Li', 'Jeffrey P. Bigham', 'Amy Pavel'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.21456.jpg', 'data': {'categories': ['#ethics', '#agi', '#multimodal', '#healthcare', '#agents'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Morae: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Morae - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Morae Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Morae Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Morae Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Empowering BLV Users with Interactive Decision-Making', 'desc': 'Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression.'}, 'zh': {'title': 'Moraeï¼šè®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–çš„æ™ºèƒ½ä»£ç†', 'desc': 'Moraeæ˜¯ä¸€ç§ç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡è®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–è¿‡ç¨‹æ¥æé«˜å¯è®¿é—®æ€§ã€‚å®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¥ç†è§£ç”¨æˆ·æŸ¥è¯¢å’Œç”¨æˆ·ç•Œé¢å…ƒç´ ï¼Œå¹¶åœ¨ä»»åŠ¡æ‰§è¡Œä¸­è‡ªåŠ¨è¯†åˆ«å†³ç­–ç‚¹ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åšå‡ºé€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„å…¨è‡ªåŠ¨ä»£ç†ä¸åŒï¼ŒMoraeåœ¨å…³é”®æ—¶åˆ»æš‚åœï¼Œæç¤ºç”¨æˆ·è¿›è¡Œæ¾„æ¸…ï¼Œä»è€Œå¢å¼ºç”¨æˆ·çš„è‡ªä¸»æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMoraeå¸®åŠ©ç”¨æˆ·å®Œæˆæ›´å¤šä»»åŠ¡ï¼Œå¹¶é€‰æ‹©æ›´ç¬¦åˆä»–ä»¬åå¥½çš„é€‰é¡¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21376', 'title': 'AHELM: A Holistic Evaluation of Audio-Language Models', 'url': 'https://huggingface.co/papers/2508.21376', 'abstract': 'AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.', 'score': 1, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': 'fccdd3f91aa4aebd', 'authors': ['Tony Lee', 'Haoqin Tu', 'Chi Heem Wong', 'Zijun Wang', 'Siwei Yang', 'Yifan Mai', 'Yuyin Zhou', 'Cihang Xie', 'Percy Liang'], 'affiliations': ['Hitachi America, Ltd.', 'Stanford University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2508.21376.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#reasoning', '#multimodal', '#audio', '#dataset'], 'emoji': 'ğŸ§', 'ru': {'title': 'AHELM: Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'AHELM - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ALM). ĞĞ½ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Gemini 2.5 Pro Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² 5 Ğ¸Ğ· 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ½ĞµÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ASR.'}, 'en': {'title': 'AHELM: A Holistic Benchmark for Evaluating Audio-Language Models', 'desc': 'AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness.'}, 'zh': {'title': 'AHELMï¼šéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°åŸºå‡†', 'desc': 'AHELMæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰ï¼Œæ¶µç›–å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œæ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†æ•´åˆäº†å¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°çš„åˆæˆéŸ³é¢‘-æ–‡æœ¬æ•°æ®é›†PARADEå’ŒCoRe-Benchï¼Œä»¥å…¨é¢æµ‹é‡ALMsåœ¨éŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ç­‰10ä¸ªé‡è¦æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æ ‡å‡†åŒ–æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼ŒAHELMç¡®ä¿äº†æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Gemini 2.5 Proåœ¨10ä¸ªæ–¹é¢ä¸­æœ‰5ä¸ªæ’åç¬¬ä¸€ï¼Œä½†åœ¨ASRä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¾¤ä½“ä¸å…¬å¹³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20085', 'title': 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2508.20085', 'abstract': 'HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.', 'score': 1, 'issue_id': 5640, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 27', 'zh': '8æœˆ27æ—¥'}, 'hash': 'ad1271c7a1097c84', 'authors': ['Zhecheng Yuan', 'Tianming Wei', 'Langzhe Gu', 'Pu Hua', 'Tianhai Liang', 'Yuanpei Chen', 'Huazhe Xu'], 'affiliations': ['Peking University', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20085.jpg', 'data': {'categories': ['#rl', '#agents', '#optimization', '#transfer_learning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº ÑƒĞ¼ĞµĞ»Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°', 'desc': 'HERMES - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². HERMES ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Bridging Human Motion and Robotic Dexterity with HERMES', 'desc': 'HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.'}, 'zh': {'title': 'HERMESï¼šäººæœºåä½œçš„çµå·§æ“ä½œæ–°æ¡†æ¶', 'desc': 'HERMESæ˜¯ä¸€ä¸ªäººæœºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†äººç±»æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººè¡Œä¸ºã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œä»¿çœŸåˆ°ç°å®çš„è½¬ç§»æŠ€æœ¯ï¼Œå¸®åŠ©æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„æ“ä½œã€‚HERMESèƒ½å¤Ÿå°†æ¥è‡ªå¤šä¸ªæ¥æºçš„äººç±»æ‰‹éƒ¨åŠ¨ä½œç»Ÿä¸€è½¬åŒ–ä¸ºå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºï¼Œå¹¶é€šè¿‡æ·±åº¦å›¾åƒå®ç°æ›´å¥½çš„ç°å®é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHERMESåœ¨å„ç§å¤æ‚çš„ç§»åŠ¨åŒæ‰‹çµå·§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.14197', 'title': 'CLIPSym: Delving into Symmetry Detection with CLIP', 'url': 'https://huggingface.co/papers/2508.14197', 'abstract': "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.", 'score': 0, 'issue_id': 5642, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': 'fe61d1db34c28a8d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#architecture', '#optimization', '#multimodal', '#cv', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'CLIPSym: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'CLIPSym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLIP. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° CLIP Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ G-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. CLIPSym Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Symmetry Detection with CLIPSym', 'desc': 'CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches.'}, 'zh': {'title': 'CLIPSymï¼šæå‡å¯¹ç§°æ€§æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'CLIPSymæ˜¯ä¸€ç§åŸºäºCLIPçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ç§°æ€§æ£€æµ‹çš„èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ—‹è½¬ç­‰å˜è§£ç å™¨å’Œè¯­ä¹‰æ„ŸçŸ¥æç¤ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è‡ªç„¶å›¾åƒæè¿°ä¸­çš„å¯¹ç§°æ€§çº¿ç´¢ã€‚é€šè¿‡ç»“åˆTransformerå’ŒG-å·ç§¯çš„æ··åˆç»“æ„ï¼ŒCLIPSymèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹æ—‹è½¬å’Œåå°„å¯¹ç§°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIPSymåœ¨ä¸‰ä¸ªæ ‡å‡†å¯¹ç§°æ€§æ£€æµ‹æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi (3)', '#alignment', '#architecture (2)', '#audio (1)', '#benchmark (6)', '#cv (2)', '#data (5)', '#dataset (10)', '#diffusion', '#ethics (3)', '#games (3)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (8)', '#open_source (5)', '#optimization (6)', '#plp (1)', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (6)', '#transfer_learning (3)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-09-01 11:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-01 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-01 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    