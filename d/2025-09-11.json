{
    "date": {
        "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 11",
        "zh": "9æœˆ11æ—¥"
    },
    "time_utc": "2025-09-11 03:27",
    "weekday": 3,
    "issue_id": 5830,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.08827",
            "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
            "url": "https://huggingface.co/papers/2509.08827",
            "abstract": "Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "score": 25,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "ea01091be58aef4b",
            "authors": [
                "Kaiyan Zhang",
                "Yuxin Zuo",
                "Bingxiang He",
                "Youbang Sun",
                "Runze Liu",
                "Che Jiang",
                "Yuchen Fan",
                "Kai Tian",
                "Guoli Jia",
                "Pengfei Li",
                "Yu Fu",
                "Xingtai Lv",
                "Yuchen Zhang",
                "Sihang Zeng",
                "Shang Qu",
                "Haozhan Li",
                "Shijie Wang",
                "Yuru Wang",
                "Xinwei Long",
                "Fangfu Liu",
                "Xiang Xu",
                "Jiaze Ma",
                "Xuekai Zhu",
                "Ermo Hua",
                "Yihao Liu",
                "Zonglin Li",
                "Huayu Chen",
                "Xiaoye Qu",
                "Yafu Li",
                "Weize Chen",
                "Zhenzhao Yuan",
                "Junqi Gao",
                "Dong Li",
                "Zhiyuan Ma",
                "Ganqu Cui",
                "Zhiyuan Liu",
                "Biqing Qi",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huazhong University of Science and Technology",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University College London",
                "University of Science and Technology of China",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08827.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#survey",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑ…Ğ¸ RL Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RL Ğ´Ğ»Ñ LRM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Scaling Reinforcement Learning for Advanced Reasoning in Language Models",
                    "desc": "This paper reviews the integration of Reinforcement Learning (RL) with Large Language Models (LLMs) to improve their reasoning capabilities. It highlights the success of RL in enhancing LLMs for complex tasks like mathematics and coding, positioning RL as a key method for evolving LLMs into more advanced reasoning models (LRMs). The authors discuss the challenges of scaling RL, including the need for better computational resources, algorithm design, and training data. They aim to identify future research directions to further develop RL applications in reasoning tasks, especially in the context of achieving Artificial SuperIntelligence (ASI)."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡",
                    "desc": "æœ¬è®ºæ–‡è°ƒæŸ¥äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æå‡LLMèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³å¤æ‚çš„é€»è¾‘ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒRLåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•é¢ä¸´ç€è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰åŸºç¡€æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹ç»¼è¿°ä¿ƒè¿›æœªæ¥åœ¨æ›´å¹¿æ³›æ¨ç†æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08826",
            "title": "RewardDance: Reward Scaling in Visual Generation",
            "url": "https://huggingface.co/papers/2509.08826",
            "abstract": "RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
            "score": 15,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "773c62b4801465a6",
            "authors": [
                "Jie Wu",
                "Yu Gao",
                "Zilyu Ye",
                "Ming Li",
                "Liang Li",
                "Hanzhong Guo",
                "Jie Liu",
                "Zeyue Xue",
                "Xiaoxia Hou",
                "Wei Liu",
                "Yan Zeng",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.08826.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ’ƒ",
                "ru": {
                    "title": "RewardDance: Ğ¢Ğ°Ğ½Ñ†ÑƒÑ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RewardDance - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ°Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ VLM. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. RewardDance Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ 'Ğ´Ğ°', Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RewardDance Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "RewardDance: Scaling Reward Models for Better AI Generation",
                    "desc": "RewardDance is a new framework designed to improve reward modeling in visual generation tasks by aligning with Vision-Language Models (VLMs). It addresses the limitations of existing reward models, which often struggle with architectural constraints and misalignment with next-token predictions. The framework introduces a novel generative reward paradigm that reformulates reward scoring, allowing for effective scaling of reward models up to 26 billion parameters. Importantly, RewardDance mitigates the issue of reward hacking, ensuring that models produce diverse and high-quality outputs during reinforcement learning fine-tuning."
                },
                "zh": {
                    "title": "RewardDanceï¼šè§£å†³å¥–åŠ±é»‘å®¢çš„å¯æ‰©å±•å¥–åŠ±å»ºæ¨¡æ¡†æ¶",
                    "desc": "RewardDanceæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°æ‰©å±•å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¹¶è§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­çš„æ‰©å±•æ€§å—åˆ°æ¶æ„å’Œè¾“å…¥æ¨¡æ€çš„é™åˆ¶ï¼Œè€Œæµè¡Œçš„Bradley-TerryæŸå¤±ä¸VLMçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ä¸åŒ¹é…ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ‰©å±•ã€‚é€šè¿‡å°†å¥–åŠ±åˆ†æ•°é‡æ–°å®šä¹‰ä¸ºæ¨¡å‹é¢„æµ‹â€œæ˜¯â€æ ‡è®°çš„æ¦‚ç‡ï¼ŒRewardDanceä½¿å¥–åŠ±ç›®æ ‡ä¸VLMæ¶æ„å†…åœ¨å¯¹é½ï¼Œä»è€Œåœ¨æ¨¡å‹å’Œä¸Šä¸‹æ–‡ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°æ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒRewardDanceåœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆè§£å†³äº†å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08755",
            "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.08755",
            "abstract": "AgentGym-RL is a modular RL framework for training LLM agents in diverse environments without supervised fine-tuning, featuring ScalingInter-RL for balanced exploration-exploitation.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.",
            "score": 4,
            "issue_id": 5830,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "ed8314c7d1c6d19b",
            "authors": [
                "Zhiheng Xi",
                "Jixuan Huang",
                "Chenyang Liao",
                "Baodai Huang",
                "Honglin Guo",
                "Jiaqi Liu",
                "Rui Zheng",
                "Junjie Ye",
                "Jiazheng Zhang",
                "Wenxiang Chen",
                "Wei He",
                "Yiwen Ding",
                "Guanyu Li",
                "Zehui Chen",
                "Zhengyin Du",
                "Xuesong Yao",
                "Yufei Xu",
                "Jiecao Chen",
                "Tao Gui",
                "Zuxuan Wu",
                "Qi Zhang",
                "Xuanjing Huang",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08755.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#agents",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "AgentGym-RL: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "AgentGym-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ RL. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ScalingInter-RL Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 27 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLM Agents with AgentGym-RL: Explore, Learn, Succeed!",
                    "desc": "AgentGym-RL is a new modular reinforcement learning (RL) framework designed to train large language model (LLM) agents in various environments without the need for supervised fine-tuning. It allows agents to learn through exploration and interaction, similar to human cognitive development, enabling them to make intelligent decisions in complex tasks. The framework includes a unique training method called ScalingInter-RL, which balances exploration and exploitation to optimize learning stability. Extensive experiments show that agents trained with this framework perform competitively against commercial models across multiple tasks, and the framework will be open-sourced to support further research in intelligent agent development."
                },
                "zh": {
                    "title": "AgentGym-RLï¼šæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä»£ç†è®­ç»ƒæ¡†æ¶",
                    "desc": "AgentGym-RLæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ScalingInter-RLæ–¹æ³•ï¼Œä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œç¡®ä¿ä»£ç†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è·å–çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚é€šè¿‡é™åˆ¶æ—©æœŸçš„äº¤äº’æ¬¡æ•°ï¼Œæ¡†æ¶å¼ºè°ƒåˆ©ç”¨ï¼Œéšåé€æ­¥å¢åŠ æ¢ç´¢çš„èŒƒå›´ï¼Œä»è€Œé¼“åŠ±å¤šæ ·åŒ–çš„é—®é¢˜è§£å†³ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentGym-RLæ¡†æ¶åŠå…¶è®­ç»ƒæ–¹æ³•åœ¨27ä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå•†ä¸šæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05209",
            "title": "Hunyuan-MT Technical Report",
            "url": "https://huggingface.co/papers/2509.05209",
            "abstract": "Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are multilingual translation models that outperform existing models, especially in translating between Mandarin and minority languages, through a combination of pre-training, supervised fine-tuning, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic.",
            "score": 4,
            "issue_id": 5830,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "1bc53ae7f9d89dff",
            "authors": [
                "Mao Zheng",
                "Zheng Li",
                "Bingxin Qu",
                "Mingyang Song",
                "Yang Du",
                "Mingrui Sun",
                "Di Wang"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05209.jpg",
            "data": {
                "categories": [
                    "#translation",
                    "#open_source",
                    "#low_resource",
                    "#multilingual",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Hunyuan-MT-7B Ğ¸ Hunyuan-MT-Chimera-7B - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞĞ½Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ WMT2025 Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñƒ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ 30 Ğ¸Ğ· 31 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€."
                },
                "en": {
                    "title": "Revolutionizing Multilingual Translation with Hunyuan Models",
                    "desc": "Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are advanced multilingual translation models designed to excel in translating between Mandarin and various minority languages. They utilize a comprehensive training approach that includes pre-training, supervised fine-tuning, and reinforcement learning to enhance translation accuracy. The Chimera model innovatively combines multiple outputs from the Hunyuan-MT-7B to improve performance beyond traditional models. Experimental results show that these models achieve state-of-the-art performance in multilingual translation tasks, particularly excelling in low-resource language pairs."
                },
                "zh": {
                    "title": "è¶…è¶Šä¼ ç»Ÿçš„å¤šè¯­è¨€ç¿»è¯‘æ–°æ¨¡å‹",
                    "desc": "Hunyuan-MT-7Bå’ŒHunyuan-MT-Chimera-7Bæ˜¯å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œç‰¹åˆ«æ“…é•¿äºæ™®é€šè¯ä¸å°‘æ•°æ°‘æ—è¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ã€‚è¿™äº›æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ç¿»è¯‘æ€§èƒ½ã€‚Hunyuan-MT-Chimera-7Bé‡‡ç”¨æ…¢æ€ç»´æ¨¡å¼ï¼Œæ•´åˆäº†å¤šç§è¾“å‡ºï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨¡å‹ã€‚ç»è¿‡å…¨é¢å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨WMT2025å…±äº«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨31ä¸ªè¯­è¨€å¯¹ä¸­æ’åç¬¬ä¸€ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·è¯­è¨€ç¯å¢ƒä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06784",
            "title": "P3-SAM: Native 3D Part Segmentation",
            "url": "https://huggingface.co/papers/2509.06784",
            "abstract": "P3-SAM, a native 3D point-promptable part segmentation model, achieves precise and robust segmentation of complex 3D objects using a feature extractor, multiple segmentation heads, and an IoU predictor.  \t\t\t\t\tAI-generated summary \t\t\t\t Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon.",
            "score": 3,
            "issue_id": 5830,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "c461d5e9a3042a90",
            "authors": [
                "Changfeng Ma",
                "Yang Li",
                "Xinhao Yan",
                "Jiachen Xu",
                "Yunhan Yang",
                "Chunshi Wang",
                "Zibo Zhao",
                "Yanwen Guo",
                "Zhuo Chen",
                "Chunchao Guo"
            ],
            "affiliations": [
                "HKU",
                "NJU",
                "ShanghaiTech",
                "Tencent Hunyuan",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06784.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#dataset",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "P3-SAM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ IoU. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 3,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. P3-SAM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Automating 3D Part Segmentation with P3-SAM",
                    "desc": "P3-SAM is a novel model designed for segmenting 3D objects into their individual parts using point prompts. It utilizes a feature extractor and multiple segmentation heads, along with an Intersection over Union (IoU) predictor, to enhance segmentation accuracy and robustness. The model aims to automate the segmentation process, addressing limitations of existing methods that struggle with complex shapes. Trained on a large dataset of 3.7 million models, P3-SAM demonstrates state-of-the-art performance in part instance segmentation."
                },
                "zh": {
                    "title": "P3-SAMï¼šå®ç°3Dç‰©ä½“çš„è‡ªåŠ¨åŒ–ç²¾ç¡®åˆ†å‰²",
                    "desc": "P3-SAMæ˜¯ä¸€ç§åŸç”Ÿçš„3Dç‚¹æç¤ºéƒ¨ä»¶åˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿç²¾ç¡®ä¸”ç¨³å¥åœ°å¯¹å¤æ‚3Dç‰©ä½“è¿›è¡Œåˆ†å‰²ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç‰¹å¾æå–å™¨ã€å¤šé‡åˆ†å‰²å¤´å’ŒIoUé¢„æµ‹å™¨ï¼Œæ—¨åœ¨å®ç°3Dç‰©ä½“çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒP3-SAMåœ¨å¤„ç†å¤æ‚ç‰©ä½“æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶æ”¯æŒç”¨æˆ·è¿›è¡Œäº¤äº’å¼åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªåŒ…å«è¿‘370ä¸‡ä¸ªæ¨¡å‹çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒäº†è¯¥æ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨åˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08088",
            "title": "EnvX: Agentize Everything with Agentic AI",
            "url": "https://huggingface.co/papers/2509.08088",
            "abstract": "EnvX leverages Agentic AI to transform GitHub repositories into intelligent agents capable of natural language interaction and collaboration, automating the entire process of understanding, initializing, and operationalizing repository functionality.  \t\t\t\t\tAI-generated summary \t\t\t\t The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.",
            "score": 2,
            "issue_id": 5830,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "0afcc93155a2cf60",
            "authors": [
                "Linyao Chen",
                "Zimian Peng",
                "Yingxuan Yang",
                "Yikun Wang",
                "Wenzheng Tom Tang",
                "Hiroki H. Kobayashi",
                "Weinan Zhang"
            ],
            "affiliations": [
                "EnvX Team",
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Tokyo",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08088.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#open_source",
                    "#benchmark",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "EnvX: ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ GitHub-Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "EnvX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ GitHub-Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€ĞµÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. EnvX Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° GitTaskBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ EnvX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Transforming Repositories into Intelligent Agents for Seamless Collaboration",
                    "desc": "EnvX is a framework that transforms GitHub repositories into intelligent agents using Agentic AI, allowing for natural language interaction and collaboration. It automates the understanding, initialization, and operationalization of repository functionality, addressing the challenges developers face with manual and error-prone processes. The framework operates in three phases: initializing the environment, enabling autonomous task performance, and facilitating collaboration between agents. By leveraging large language models and structured tool integration, EnvX significantly improves the efficiency of software reuse and collaboration in open-source projects."
                },
                "zh": {
                    "title": "å°†ä»£ç åº“è½¬å˜ä¸ºæ™ºèƒ½ä»£ç†çš„é©å‘½æ€§æ¡†æ¶",
                    "desc": "EnvXæ˜¯ä¸€ä¸ªåˆ©ç”¨Agentic AIçš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†GitHubä»£ç åº“è½¬å˜ä¸ºæ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’å’Œåä½œã€‚å®ƒé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹å®ç°è¿™ä¸€ç›®æ ‡ï¼šé¦–å…ˆæ˜¯ç¯å¢ƒåˆå§‹åŒ–ï¼Œè®¾ç½®å¿…è¦çš„ä¾èµ–å’Œæ•°æ®ï¼›å…¶æ¬¡æ˜¯äººç±»å¯¹é½çš„è‡ªåŠ¨åŒ–ï¼Œä½¿å¾—ç‰¹å®šä»£ç åº“çš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå®é™…ä»»åŠ¡ï¼›æœ€åæ˜¯ä»£ç†é—´åè®®ï¼Œå…è®¸å¤šä¸ªä»£ç†è¿›è¡Œåä½œã€‚EnvXä¸ä»…è‡ªåŠ¨ç”Ÿæˆä»£ç ï¼Œè¿˜è‡ªåŠ¨åŒ–ç†è§£ã€åˆå§‹åŒ–å’Œæ“ä½œä»£ç åº“åŠŸèƒ½çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†è½¯ä»¶é‡ç”¨çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07996",
            "title": "3D and 4D World Modeling: A Survey",
            "url": "https://huggingface.co/papers/2509.07996",
            "abstract": "This survey provides a comprehensive review of 3D and 4D world modeling and generation, establishing definitions, taxonomy, datasets, and evaluation metrics, and discussing applications and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey",
            "score": 2,
            "issue_id": 5830,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "1cdd6619184c7e1e",
            "authors": [
                "Lingdong Kong",
                "Wesley Yang",
                "Jianbiao Mei",
                "Youquan Liu",
                "Ao Liang",
                "Dekai Zhu",
                "Dongyue Lu",
                "Wei Yin",
                "Xiaotao Hu",
                "Mingkai Jia",
                "Junyuan Deng",
                "Kaiwen Zhang",
                "Yang Wu",
                "Tianyi Yan",
                "Shenyuan Gao",
                "Song Wang",
                "Linfeng Li",
                "Liang Pan",
                "Yong Liu",
                "Jianke Zhu",
                "Wei Tsang Ooi",
                "Steven C. H. Hoi",
                "Ziwei Liu"
            ],
            "affiliations": [
                "CNRS@CREATE, Singapore",
                "HKUST",
                "Horizon Robotics",
                "HyperGAI",
                "Nanjing University of Science and Technology",
                "Nanyang Technological University, Singapore",
                "National University of Singapore",
                "Shanghai AI Laboratory",
                "Technical University of Munich",
                "Tsinghua University",
                "University of Macau",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07996.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¸ 4D Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¸ 4D Ğ¼Ğ¸Ñ€Ğ¾Ğ². Ğ’ Ğ½ĞµĞ¼ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ (VideoGen), Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° (OccGen) Ğ¸ LiDAR (LiDARGen). ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unifying 3D and 4D World Modeling for AI Understanding",
                    "desc": "This paper reviews the field of 3D and 4D world modeling and generation, focusing on how AI can understand and predict environments. It highlights the importance of using native 3D and 4D data types, like RGB-D images and LiDAR point clouds, which are often overlooked in favor of 2D methods. The authors propose a clear taxonomy for world models, categorizing them into video-based, occupancy-based, and LiDAR-based approaches. Additionally, the survey outlines datasets, evaluation metrics, and discusses applications and challenges in the field, aiming to unify and advance research in 3D and 4D modeling."
                },
                "zh": {
                    "title": "3Dä¸4Dä¸–ç•Œå»ºæ¨¡çš„å…¨é¢ç»¼è¿°",
                    "desc": "è¿™ç¯‡ç»¼è¿°æ–‡ç« å…¨é¢å›é¡¾äº†3Då’Œ4Dä¸–ç•Œå»ºæ¨¡ä¸ç”Ÿæˆçš„ç ”ç©¶ï¼Œå»ºç«‹äº†ç›¸å…³çš„å®šä¹‰ã€åˆ†ç±»ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Då›¾åƒå’Œè§†é¢‘æ•°æ®çš„ç”Ÿæˆæ–¹æ³•ä¸Šï¼Œä½†å¯¹3Då’Œ4Dè¡¨ç¤ºï¼ˆå¦‚RGB-Då›¾åƒã€å ç”¨ç½‘æ ¼å’ŒLiDARç‚¹äº‘ï¼‰çš„ç ”ç©¶æ­£åœ¨å¿«é€Ÿå¢é•¿ã€‚ä¸ºäº†å¡«è¡¥æ–‡çŒ®ä¸­ç¼ºä¹æ ‡å‡†åŒ–å®šä¹‰å’Œåˆ†ç±»çš„ç©ºç™½ï¼Œæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†3Då’Œ4Dä¸–ç•Œå»ºæ¨¡ä¸ç”Ÿæˆçš„ç›¸å…³å·¥ä½œã€‚æœ€åï¼Œæ–‡ç« è¿˜è®¨è®ºäº†å®é™…åº”ç”¨ã€è¯†åˆ«å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„è¿›æ­¥æä¾›ä¸€ä¸ªè¿è´¯çš„åŸºç¡€å‚è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08494",
            "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
            "url": "https://huggingface.co/papers/2509.08494",
            "abstract": "A benchmark evaluates human agency in AI assistants using large language models, finding varying support across systems and dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.",
            "score": 0,
            "issue_id": 5830,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "f9f3a3cdf8cd2c8d",
            "authors": [
                "Benjamin Sturgeon",
                "Daniel Samuelson",
                "Jacob Haimes",
                "Jacy Reese Anthis"
            ],
            "affiliations": [
                "AI Safety Cape Town",
                "Apart Research",
                "Sentience Institute",
                "Stanford University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08494.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#alignment",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HumanAgencyBench (HAB) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). HAB Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¸ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ…, Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Empowering Human Agency in AI: A New Benchmark Approach",
                    "desc": "This paper introduces a benchmark called HumanAgencyBench (HAB) to evaluate how well AI assistants support human agency across various dimensions. It combines philosophical and scientific theories of agency with AI evaluation methods, specifically using large language models (LLMs) to assess AI responses to user queries. The study finds that current LLM-based assistants provide low-to-moderate support for human agency, with significant differences among systems and dimensions. The authors suggest that improving agency support should not solely rely on enhancing LLM capabilities but also focus on establishing better safety and alignment standards."
                },
                "zh": {
                    "title": "æå‡AIåŠ©æ‰‹ä¸­çš„äººç±»ä»£ç†æƒ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åŠ©æ‰‹ä¸­äººç±»ä»£ç†æƒçš„è¯„ä¼°ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¨¡æ‹Ÿå’ŒéªŒè¯ç”¨æˆ·æŸ¥è¯¢ã€‚æˆ‘ä»¬æå‡ºäº†HumanAgencyBenchï¼ˆHABï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç»´åº¦çš„äººç±»ä»£ç†æƒï¼Œæ—¨åœ¨è¯„ä¼°AIåŠ©æ‰‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰åŸºäºLLMçš„åŠ©æ‰‹åœ¨äººç±»ä»£ç†æƒæ”¯æŒæ–¹é¢è¡¨ç°ä½è‡³ä¸­ç­‰ï¼Œå¹¶ä¸”åœ¨ä¸åŒç³»ç»Ÿå¼€å‘è€…å’Œç»´åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬å»ºè®®åœ¨AIç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯¹é½ç›®æ ‡ä¸Šè¿›è¡Œæ›´å¼ºæœ‰åŠ›çš„æ”¹è¿›ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒäººç±»ä»£ç†æƒã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-10.html",
    "link_next": "2025-09-12.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1,
        "#translation": 1
    }
}