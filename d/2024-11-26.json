{
    "date": {
        "ru": "26 ноября",
        "en": "November 26",
        "zh": "11月26日"
    },
    "time_utc": "2024-11-26 21:09",
    "weekday": 1,
    "issue_id": 798,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.15138",
            "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
            "url": "https://huggingface.co/papers/2411.15138",
            "abstract": "We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.",
            "score": 32,
            "issue_id": 776,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "34b8f6718115f1e3",
            "authors": [
                "Xin Huang",
                "Tengfei Wang",
                "Ziwei Liu",
                "Qing Wang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15138.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная генерация материалов для 3D-объектов с помощью диффузии",
                    "desc": "В статье представлен Material Anything - полностью автоматизированный унифицированный фреймворк диффузии для генерации физически корректных материалов для 3D-объектов. В отличие от существующих методов, он предлагает надежное сквозное решение, адаптируемое к объектам в различных условиях освещения. Подход использует предобученную модель диффузии изображений с тройной архитектурой и функцией потерь рендеринга для улучшения стабильности и качества материалов. Также вводятся маски уверенности как динамический переключатель в модели диффузии, позволяющий эффективно обрабатывать объекты с текстурами и без них в различных условиях освещения."
                },
                "en": {
                    "title": "Automating Realistic Material Generation for 3D Objects",
                    "desc": "Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions."
                },
                "zh": {
                    "title": "全自动材料生成，适应多种光照条件",
                    "desc": "本文介绍了一种名为Material Anything的全自动统一扩散框架，旨在为3D物体生成基于物理的材料。与现有方法依赖复杂流程或特定优化不同，Material Anything提供了一种稳健的端到端解决方案，适应不同光照条件下的物体。我们的方法利用了预训练的图像扩散模型，并通过三头架构和渲染损失来提高稳定性和材料质量。此外，我们引入了置信掩码作为扩散模型中的动态切换器，使其能够有效处理有纹理和无纹理的物体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15466",
            "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
            "url": "https://huggingface.co/papers/2411.15466",
            "abstract": "Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/",
            "score": 27,
            "issue_id": 777,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "288600e8c54930f4",
            "authors": [
                "Chaehun Shin",
                "Jooyoung Choi",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15466.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Diptych Prompting: точная генерация изображений без дополнительного обучения",
                    "desc": "Статья представляет новый метод генерации изображений под названием Diptych Prompting. Этот подход использует свойство диптиха в крупномасштабных моделях text-to-image для точного воспроизведения субъекта в желаемом контексте. Метод интерпретирует задачу как инпейнтинг, размещая исходное изображение в левой части диптиха и генерируя правую часть на основе текстового промпта. Diptych Prompting превосходит существующие zero-shot методы и поддерживает различные приложения генерации изображений."
                },
                "en": {
                    "title": "Diptych Prompting: Zero-Shot Image Generation with Subject Precision",
                    "desc": "This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing."
                },
                "zh": {
                    "title": "Diptych Prompting：精准的主题驱动图像生成新方法",
                    "desc": "本文提出了一种新的零-shot方法，称为Diptych Prompting，旨在实现主题驱动的文本到图像生成。该方法通过将生成任务重新解释为图像修补，确保了主题的精确对齐。Diptych Prompting利用大型文本到图像模型的双联生成特性，左侧面板展示参考图像，右侧面板进行文本条件的修补。实验结果表明，该方法在视觉效果上优于传统的零-shot图像提示方法，且支持多种图像生成应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16594",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "url": "https://huggingface.co/papers/2411.16594",
            "abstract": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.",
            "score": 17,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "56883eb77dcb5fa3",
            "authors": [
                "Dawei Li",
                "Bohan Jiang",
                "Liangjie Huang",
                "Alimohammad Beigi",
                "Chengshuai Zhao",
                "Zhen Tan",
                "Amrita Bhattacharjee",
                "Yuxuan Jiang",
                "Canyu Chen",
                "Tianhao Wu",
                "Kai Shu",
                "Lu Cheng",
                "Huan Liu"
            ],
            "affiliations": [
                "Arizona State University",
                "Emory University",
                "Illinois Institute of Technology",
                "University of California, Berkeley",
                "University of Illinois Chicago",
                "University of Maryland, Baltimore County"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16594.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "LLM как судья: новая парадигма оценки в AI и NLP",
                    "desc": "Статья представляет собой обзор использования больших языковых моделей (LLM) в качестве судей для оценки и ранжирования в задачах искусственного интеллекта и обработки естественного языка. Авторы предлагают подробную таксономию подхода 'LLM-as-a-judge', рассматривая что, как и где оценивать. В работе также представлены бенчмарки для оценки эффективности LLM в роли судей. Статья завершается обсуждением ключевых проблем и перспективных направлений исследований в этой области."
                },
                "en": {
                    "title": "Harnessing LLMs for Enhanced AI Evaluation",
                    "desc": "This paper discusses the challenges of assessment and evaluation in artificial intelligence and natural language processing, particularly focusing on the limitations of traditional methods. It introduces the innovative concept of using Large Language Models (LLMs) as judges to score, rank, or select outputs in various tasks. The authors provide a detailed framework that categorizes LLM-based judgment into three key areas: what to judge, how to judge, and where to judge. Additionally, the paper compiles benchmarks for evaluating these models and identifies future research directions to enhance the effectiveness of LLMs in assessment tasks."
                },
                "zh": {
                    "title": "大型语言模型：评判的新力量",
                    "desc": "本论文探讨了大型语言模型（LLM）在评估和判断中的应用，提出了“LLM作为评判者”的新范式。传统的评估方法往往无法有效判断细微的属性，而LLM能够在多种任务中进行打分、排名和选择。我们从输入和输出的角度详细定义了评判的概念，并建立了一个全面的分类法，探讨了评判的内容、方式和场所。最后，我们编制了评估LLM作为评判者的基准，并强调了关键挑战和未来研究的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16205",
            "title": "MH-MoE:Multi-Head Mixture-of-Experts",
            "url": "https://huggingface.co/papers/2411.16205",
            "abstract": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.",
            "score": 15,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "b684b6d745cb66ff",
            "authors": [
                "Shaohan Huang",
                "Xun Wu",
                "Shuming Ma",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16205.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Мультиголовая смесь экспертов: эффективность без компромиссов",
                    "desc": "Статья представляет новую реализацию мультиголовой смеси экспертов (MH-MoE), которая сохраняет паритет по FLOP и параметрам с разреженными моделями смеси экспертов. Эксперименты на языковых моделях показывают, что новая реализация улучшает качество по сравнению с обычными MoE и мелкозернистыми MoE моделями. MH-MoE использует механизм мультиголовности для совместного внимания к информации из различных пространств представлений в разных экспертах. Исследование также демонстрирует совместимость MH-MoE с 1-битными большими языковыми моделями, такими как BitNet."
                },
                "en": {
                    "title": "Unlocking Performance with Multi-Head Mixture-of-Experts",
                    "desc": "The Multi-Head Mixture-of-Experts (MH-MoE) model enhances performance by allowing multiple heads to focus on different aspects of data from various experts. This paper introduces a new way to implement MH-MoE that keeps the same computational cost and number of parameters as traditional sparse Mixture of Experts models. Experiments conducted on language models reveal that this new approach provides better results compared to standard MoE and fine-grained MoE models. Furthermore, the findings indicate that MH-MoE can effectively work with 1-bit Large Language Models like BitNet."
                },
                "zh": {
                    "title": "多头混合专家：提升模型性能的新方法",
                    "desc": "多头混合专家模型（MH-MoE）通过多头机制，能够同时关注来自不同专家的多种表示空间的信息，从而展现出优越的性能。本文提出了一种新颖的MH-MoE实现，能够在计算量（FLOPs）和参数数量上与稀疏混合专家模型保持一致。实验结果表明，该新实现相较于传统的MoE和细粒度MoE模型在语言模型上有显著的质量提升。此外，我们的实验还表明MH-MoE与1位大语言模型（如BitNet）兼容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15611",
            "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
            "url": "https://huggingface.co/papers/2411.15611",
            "abstract": "We present a way to learn novel concepts by only using their textual description. We call this method Knowledge Transfer. Similarly to human perception, we leverage cross-modal interaction to introduce new concepts. We hypothesize that in a pre-trained visual encoder there are enough low-level features already learned (e.g. shape, appearance, color) that can be used to describe previously unknown high-level concepts. Provided with a textual description of the novel concept, our method works by aligning the known low-level features of the visual encoder to its high-level textual description. We show that Knowledge Transfer can successfully introduce novel concepts in multimodal models, in a very efficient manner, by only requiring a single description of the target concept. Our approach is compatible with both separate textual and visual encoders (e.g. CLIP) and shared parameters across modalities. We also show that, following the same principle, Knowledge Transfer can improve concepts already known by the model. Leveraging Knowledge Transfer we improve zero-shot performance across different tasks such as classification, segmentation, image-text retrieval, and captioning.",
            "score": 13,
            "issue_id": 784,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "117dd5d3b47d2583",
            "authors": [
                "Carlo Alberto Barbano",
                "Luca Molinaro",
                "Emanuele Aiello",
                "Marco Grangetto"
            ],
            "affiliations": [
                "Politecnico di Torino",
                "University of Turin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15611.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение новым концепциям через текст: революция в машинном обучении",
                    "desc": "Статья представляет метод под названием 'Передача знаний' (Knowledge Transfer), позволяющий обучать модели машинного обучения новым концепциям, используя только их текстовое описание. Авторы предполагают, что в предобученном визуальном энкодере уже содержатся низкоуровневые признаки, которые можно использовать для описания ранее неизвестных высокоуровневых концепций. Метод работает путем выравнивания известных низкоуровневых признаков визуального энкодера с их высокоуровневым текстовым описанием. Исследователи показывают, что этот подход эффективен для мультимодальных моделей и применим в различных задачах, таких как классификация, сегментация, поиск изображений по тексту и генерация подписей."
                },
                "en": {
                    "title": "Learn New Concepts with Just Words!",
                    "desc": "This paper introduces a method called Knowledge Transfer, which allows machine learning models to learn new concepts using only their textual descriptions. The approach utilizes a pre-trained visual encoder that has already learned basic features like shape and color, which can be aligned with high-level textual descriptions of new concepts. By doing this, the model can efficiently incorporate novel concepts into its understanding without needing extensive retraining. Additionally, Knowledge Transfer enhances the model's performance on existing concepts and improves its zero-shot capabilities across various tasks such as classification and image-text retrieval."
                },
                "zh": {
                    "title": "知识迁移：通过文本描述学习新概念",
                    "desc": "我们提出了一种通过文本描述学习新概念的方法，称为知识迁移。该方法利用跨模态交互，类似于人类的感知，来引入新概念。我们假设预训练的视觉编码器中已经学习了足够的低级特征（如形状、外观、颜色），可以用来描述之前未知的高级概念。通过对目标概念的单一文本描述进行对齐，我们的方法能够高效地在多模态模型中引入新概念，并提升模型在分类、分割、图像-文本检索和标题生成等任务上的零-shot性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16489",
            "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
            "url": "https://huggingface.co/papers/2411.16489",
            "abstract": "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.",
            "score": 13,
            "issue_id": 780,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "9d7613aad6cae404",
            "authors": [
                "Zhen Huang",
                "Haoyang Zou",
                "Xuefeng Li",
                "Yixiu Liu",
                "Yuxiang Zheng",
                "Ethan Chern",
                "Shijie Xia",
                "Yiwei Qin",
                "Weizhe Yuan",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "NYU",
                "SII",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16489.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#data",
                    "#math",
                    "#benchmark",
                    "#transfer_learning",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Дистилляция знаний: скрытый путь к репликации передовых языковых моделей",
                    "desc": "Эта статья представляет критический анализ текущих подходов к репликации возможностей модели O1 от OpenAI, уделяя особое внимание широко распространенному, но часто нераскрываемому использованию методов дистилляции знаний. Исследование показывает, как простая дистилляция из API O1 в сочетании с контролируемой тонкой настройкой может достичь превосходной производительности в сложных математических задачах. Эксперименты демонстрируют, что базовая модель, настроенная на десятках тысяч образцов дистиллированных из O1 цепочек рассуждений, превосходит O1-preview на американском математическом экзамене AIME при минимальной технической сложности. Авторы также исследуют возможности обобщения моделей, дистиллированных из O1, на различные задачи, включая галлюцинации, безопасность и открытые вопросно-ответные системы."
                },
                "en": {
                    "title": "Unlocking AI Potential: The Power of Transparent Distillation",
                    "desc": "This paper critically analyzes the methods used to replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. The authors demonstrate that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks compared to the original O1 model. Their experiments reveal that models trained on O1-distilled data not only excel in math but also generalize well to other tasks, showing reduced biases and improved safety. The study advocates for transparency in AI research and highlights the importance of foundational understanding in developing advanced AI systems."
                },
                "zh": {
                    "title": "知识蒸馏：提升AI模型性能的关键",
                    "desc": "本文对当前复制OpenAI O1模型能力的方法进行了深入分析，特别关注知识蒸馏技术的广泛使用。研究表明，通过简单的从O1 API进行蒸馏，并结合监督微调，可以在复杂的数学推理任务上实现优越的性能。我们的实验显示，经过微调的基础模型在美国邀请数学考试（AIME）中表现优于O1预览，且技术复杂性较低。此外，尽管模型仅在数学问题解决数据上进行训练，但在开放式问答任务中也展现出强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16657",
            "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
            "url": "https://huggingface.co/papers/2411.16657",
            "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.",
            "score": 13,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "02cf3312e8d1f6ca",
            "authors": [
                "Zun Wang",
                "Jialu Li",
                "Han Lin",
                "Jaehong Yoon",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16657.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "DreamRunner: От сценария к видео с помощью ИИ",
                    "desc": "DreamRunner - это новый метод генерации видео по текстовому сценарию, который использует большую языковую модель для структурирования входных данных. Он применяет адаптацию на основе извлечения информации для захвата целевых движений объектов в каждой сцене. DreamRunner также предлагает новый модуль пространственно-временного 3D-внимания и внедрения приоров для точного связывания объектов и движений. Метод демонстрирует передовые результаты в согласованности персонажей, соответствии тексту и плавных переходах."
                },
                "en": {
                    "title": "DreamRunner: Crafting Seamless Storytelling Videos from Text",
                    "desc": "This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts."
                },
                "zh": {
                    "title": "DreamRunner：创新的故事视频生成方法",
                    "desc": "故事视频生成（SVG）是一项新兴任务，旨在根据输入文本脚本创建长篇、多动作、多场景的视频。该方法面临着多个挑战，包括对象需要展现复杂的细微动作，以及多个对象在不同场景中的一致性。为了解决这些问题，我们提出了DreamRunner，这是一种新颖的故事到视频生成方法，利用大型语言模型（LLM）进行场景规划和对象布局。DreamRunner还引入了空间-时间区域基础的3D注意力机制，能够实现细粒度的对象动作绑定和逐帧语义控制，展现出在角色一致性和文本对齐方面的先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14522",
            "title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI",
            "url": "https://huggingface.co/papers/2411.14522",
            "abstract": "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.",
            "score": 11,
            "issue_id": 778,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "eb0e262f1661d5c8",
            "authors": [
                "Tianbin Li",
                "Yanzhou Su",
                "Wei Li",
                "Bin Fu",
                "Zhe Chen",
                "Ziyan Huang",
                "Guoan Wang",
                "Chenglong Ma",
                "Ying Chen",
                "Ming Hu",
                "Yanjun Li",
                "Pengcheng Chen",
                "Xiaowei Hu",
                "Zhongying Deng",
                "Yuanfeng Ji",
                "Jin Ye",
                "Yu Qiao",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "Monash University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences",
                "Stanford University",
                "University of Cambridge",
                "University of Washington",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14522.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#multimodal",
                    "#optimization",
                    "#science",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "GMAI-VL: Мощная мультимодальная модель для медицинского ИИ",
                    "desc": "Исследователи представили GMAI-VL-5.5M - обширный мультимодальный медицинский датасет, созданный путем преобразования сотен специализированных медицинских наборов данных в пары изображение-текст. На основе этого датасета была разработана модель GMAI-VL - общая медицинская модель компьютерного зрения и обработки естественного языка, обученная по трехэтапной стратегии. GMAI-VL достигает передовых результатов в широком спектре мультимодальных медицинских задач, таких как визуальные вопросно-ответные системы и диагностика медицинских изображений. Работа вносит вклад в развитие искусственного интеллекта в медицине, предоставляя новый датасет, модель и бенчмарки."
                },
                "en": {
                    "title": "Empowering Medical AI with Multimodal Learning",
                    "desc": "This paper introduces GMAI-VL-5.5M, a new multimodal medical dataset designed to enhance general medical AI (GMAI) by combining various specialized medical datasets into image-text pairs. The dataset supports a wide range of medical tasks and includes high-quality data that improves the model's understanding of both visual and textual information. The authors propose a three-stage training strategy for the GMAI-VL model, which significantly boosts its performance in tasks like visual question answering and medical image diagnosis. Experimental results show that GMAI-VL sets new benchmarks in the medical domain, demonstrating its effectiveness in aiding clinical decision-making."
                },
                "zh": {
                    "title": "医学领域的多模态智能突破",
                    "desc": "尽管通用人工智能（如GPT-4）取得了显著进展，但在医学领域的有效性仍然受到限制，因为缺乏专业的医学知识。为了解决这个问题，我们提出了GMAI-VL-5.5M，这是一个通过将数百个专业医学数据集转换为精心构建的图像-文本对而创建的综合多模态医学数据集。基于这个多模态数据集，我们提出了GMAI-VL，一个具有逐步三阶段训练策略的通用医学视觉-语言模型。实验评估表明，GMAI-VL在视觉问答和医学图像诊断等多种多模态医学任务中达到了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16318",
            "title": "One Diffusion to Generate Them All",
            "url": "https://huggingface.co/papers/2411.16318",
            "abstract": "We introduce OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion",
            "score": 10,
            "issue_id": 782,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "2719da6b8a88249f",
            "authors": [
                "Duong H. Le",
                "Tuan Pham",
                "Sangho Lee",
                "Christopher Clark",
                "Aniruddha Kembhavi",
                "Stephan Mandt",
                "Ranjay Krishna",
                "Jiasen Lu"
            ],
            "affiliations": [
                "AI2",
                "University of California, Irvine",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16318.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная диффузионная модель для синтеза и анализа изображений",
                    "desc": "OneDiffusion - это универсальная крупномасштабная диффузионная модель, поддерживающая двунаправленный синтез и понимание изображений для различных задач. Модель позволяет выполнять условную генерацию на основе текста, глубины, позы, макета и семантических карт, а также решать задачи устранения размытия, увеличения масштаба и обратные процессы. OneDiffusion использует единый подход к обучению, рассматривая все задачи как последовательности кадров с различными уровнями шума. Экспериментальные результаты демонстрируют конкурентоспособную производительность модели в задачах генерации и предсказания, несмотря на относительно небольшой набор данных для обучения."
                },
                "en": {
                    "title": "OneDiffusion: Unifying Image Synthesis and Understanding with Versatile Diffusion",
                    "desc": "OneDiffusion is a powerful diffusion model designed for various image synthesis and understanding tasks. It can generate images based on different inputs like text and depth, and also perform tasks such as image deblurring and segmentation. The model treats all tasks as sequences of frames with different noise levels, allowing flexibility during inference. Its unified training approach enhances scalability and generalization, achieving strong performance across multiple tasks with a relatively small dataset."
                },
                "zh": {
                    "title": "OneDiffusion：多任务图像生成与理解的统一模型",
                    "desc": "OneDiffusion是一种多功能的大规模扩散模型，能够支持双向图像合成和理解。它可以根据文本、深度、姿态、布局和语义图等输入进行条件生成，同时处理图像去模糊、放大和深度估计等任务。该模型通过将所有任务视为具有不同噪声尺度的帧序列进行训练，简化了训练过程，并允许在推理时使用任意帧作为条件图像。实验结果表明，OneDiffusion在生成和预测任务中表现出色，尽管训练数据集相对较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16034",
            "title": "VisualLens: Personalization through Visual History",
            "url": "https://huggingface.co/papers/2411.16034",
            "abstract": "We hypothesize that a user's visual history with images reflecting their daily life, offers valuable insights into their interests and preferences, and can be leveraged for personalization. Among the many challenges to achieve this goal, the foremost is the diversity and noises in the visual history, containing images not necessarily related to a recommendation task, not necessarily reflecting the user's interest, or even not necessarily preference-relevant. Existing recommendation systems either rely on task-specific user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. We propose a novel approach, VisualLens, that extracts, filters, and refines image representations, and leverages these signals for personalization. We created two new benchmarks with task-agnostic visual histories, and show that our method improves over state-of-the-art recommendations by 5-10% on Hit@3, and improves over GPT-4o by 2-5%. Our approach paves the way for personalized recommendations in scenarios where traditional methods fail.",
            "score": 10,
            "issue_id": 781,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "5eece5978da5fe1d",
            "authors": [
                "Wang Bill Zhu",
                "Deqing Fu",
                "Kai Sun",
                "Yi Lu",
                "Zhaojiang Lin",
                "Seungwhan Moon",
                "Kanika Narang",
                "Mustafa Canim",
                "Yue Liu",
                "Anuj Kumar",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "Meta",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16034.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Персонализированные рекомендации через призму визуальной истории пользователя",
                    "desc": "Статья представляет новый подход VisualLens для персонализации рекомендаций на основе визуальной истории пользователя. Авторы предлагают метод извлечения, фильтрации и уточнения представлений изображений для улучшения рекомендаций. Они создали два новых бенчмарка с задаче-агностичными визуальными историями. Результаты показывают улучшение на 5-10% по метрике Hit@3 по сравнению с современными методами рекомендаций."
                },
                "en": {
                    "title": "Unlocking Personalization Through Visual History",
                    "desc": "This paper introduces VisualLens, a new method for enhancing personalized recommendations by utilizing a user's visual history of images. The authors argue that these images, which reflect daily life, can provide insights into user interests and preferences, despite challenges like noise and irrelevant content. VisualLens processes and refines image representations to filter out unhelpful data, improving the quality of recommendations. The results show that this approach outperforms existing systems, achieving a 5-10% increase in recommendation accuracy on new benchmarks."
                },
                "zh": {
                    "title": "利用视觉历史实现个性化推荐",
                    "desc": "我们假设用户的视觉历史可以提供关于他们兴趣和偏好的重要信息，这些信息可以用于个性化推荐。当前的推荐系统通常依赖于特定任务的用户交互记录，或者关注文本信号，而忽视了视觉信息的潜力。我们提出了一种新方法，VisualLens，通过提取、过滤和优化图像表示，利用这些信号进行个性化推荐。我们的实验表明，该方法在无任务视觉历史的基准测试中，推荐效果比现有技术提高了5-10%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16681",
            "title": "Factorized Visual Tokenization and Generation",
            "url": "https://huggingface.co/papers/2411.16681",
            "abstract": "Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN",
            "score": 10,
            "issue_id": 780,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "966d673404fb7a77",
            "authors": [
                "Zechen Bai",
                "Jianxiong Gao",
                "Ziteng Gao",
                "Pichao Wang",
                "Zheng Zhang",
                "Tong He",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Amazon",
                "Fudan University",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16681.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Факторизованная квантизация: новый шаг в масштабируемой токенизации изображений",
                    "desc": "Статья представляет новый подход к визуальным токенизаторам для генерации изображений - Факторизованную Квантизацию (FQ). Этот метод разбивает большой кодбук на несколько независимых под-кодбуков, что позволяет эффективно масштабировать процесс токенизации. Авторы предлагают регуляризацию для уменьшения избыточности и повышения разнообразия между под-кодбуками. Интеграция предобученных моделей компьютерного зрения, таких как CLIP и DINO, обогащает семантическое представление токенов."
                },
                "en": {
                    "title": "Revitalizing Image Generation with Factorized Quantization",
                    "desc": "This paper presents a new method called Factorized Quantization (FQ) to improve visual tokenizers used in image generation. Traditional VQ-based tokenizers struggle with limited vocabulary sizes, which can hinder performance and scalability. FQ addresses this by breaking down a large codebook into smaller, independent sub-codebooks, reducing complexity and enhancing efficiency. Additionally, the method incorporates disentanglement regularization and representation learning to ensure diverse and rich semantic representations, leading to better image generation results."
                },
                "zh": {
                    "title": "因子化量化：提升视觉标记器的效率与表现",
                    "desc": "视觉标记器是图像生成的基础，它将视觉数据转换为离散的标记，使基于变换器的模型在图像生成方面表现出色。尽管VQGAN等基于VQ的标记器取得了一定成功，但由于词汇量有限，它们面临着显著的局限性。我们提出了一种新方法——因子化量化（FQ），通过将大型代码本分解为多个独立的子代码本，来解决这一问题，从而提高了视觉标记化的效率和可扩展性。实验表明，FQGAN模型显著提高了视觉标记器的重建质量，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14525",
            "title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation",
            "url": "https://huggingface.co/papers/2411.14525",
            "abstract": "Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation.",
            "score": 6,
            "issue_id": 778,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "eb68ad946d69ba56",
            "authors": [
                "Jin Ye",
                "Ying Chen",
                "Yanjun Li",
                "Haoyu Wang",
                "Zhongying Deng",
                "Ziyan Huang",
                "Yanzhou Su",
                "Chenglong Ma",
                "Yuanfeng Ji",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University",
                "Shanghai AI Laboratory",
                "Stanford University",
                "University of Cambridge",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14525.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#healthcare",
                    "#open_source",
                    "#training",
                    "#transfer_learning",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальность предобученных КТ-моделей в медицинской сегментации",
                    "desc": "Статья посвящена исследованию переноса обучения моделей, предобученных на полноразмерных КТ-изображениях, на другие задачи сегментации медицинских изображений. Авторы создали масштабный бенчмарк из 87 публичных датасетов для оценки эффективности такого переноса. Результаты показывают, что предобученные модели хорошо адаптируются к другим модальностям (например, МРТ) и различным целевым задачам. Исследование выявило эффект 'бутылочного горлышка' в зависимости от размера датасета при тонкой настройке моделей."
                },
                "en": {
                    "title": "Unlocking Transfer Learning in Medical Imaging with CT Datasets",
                    "desc": "This paper investigates the transferability of models pre-trained on full-body CT images for various medical segmentation tasks. It highlights the effectiveness of the STU-Net model in adapting to different imaging modalities, such as MRI, and diverse anatomical targets. The study reveals that dataset size impacts fine-tuning performance, with both small and large datasets yielding better results than medium-sized ones. Overall, the findings emphasize the potential of using large-scale CT datasets to enhance model performance in medical image segmentation tasks."
                },
                "zh": {
                    "title": "全身CT预训练模型的迁移学习能力研究",
                    "desc": "计算机断层扫描（CT）是医学成像中最常用的技术之一。本文探讨了在不同条件下，基于全身CT预训练模型的迁移学习能力，特别是在其他成像模态和多样化目标的分割任务中。我们收集了87个公共数据集进行评估，结果表明，数据集大小对微调有瓶颈效应，且全身CT预训练模型在迁移到其他模态（如MRI）时表现良好。我们的研究希望为未来的体积医学图像分割研究提供指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16341",
            "title": "From CISC to RISC: language-model guided assembly transpilation",
            "url": "https://huggingface.co/papers/2411.16341",
            "abstract": "The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM's energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86's CISC-based and ARM's RISC-based computing paradigms while preserving program semantics and optimizing performance. We evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73times speedup compared to Apple's Rosetta 2 virtualization engine, while delivering 2.41times memory efficiency and 1.47times better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide and generates correctly executable RISC code despite machine ``language'' barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/.",
            "score": 6,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "1d28eaae89074f91",
            "authors": [
                "Ahmed Heakl",
                "Chaimaa Abi",
                "Rania Hossam",
                "Abdulrahman Mahmoud"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16341.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Преодоление барьера CISC/RISC: автоматическая трансляция x86 в ARM с помощью ИИ",
                    "desc": "Статья представляет CRT - лёгкий транспилятор на основе большой языковой модели, который автоматически конвертирует ассемблерный код x86 в ассемблер ARM. Этот подход преодолевает фундаментальный архитектурный разрыв между вычислительными парадигмами CISC (x86) и RISC (ARM), сохраняя семантику программ и оптимизируя производительность. Авторы оценивают CRT на различных реальных приложениях, достигая 79.25% точности перевода с x86 на ARMv5. В практических развертываниях на оборудовании Apple M2 (ARMv8) транспилированный код достигает ускорения в 1.73 раза по сравнению с виртуализационным движком Apple Rosetta 2."
                },
                "en": {
                    "title": "Bridging the CISC/RISC Divide with CRT Transpiler",
                    "desc": "This paper presents CRT, a lightweight LLM-based transpiler designed to convert x86 assembly code into ARM assembly code. The transition from x86 to ARM architecture is challenging due to the differences in their instruction set architectures (ISAs), specifically x86's Complex Instruction Set Computing (CISC) and ARM's Reduced Instruction Set Computing (RISC). CRT effectively bridges this gap while maintaining the original program's functionality and optimizing performance. The evaluation shows that CRT achieves high translation accuracy and outperforms existing solutions in terms of speed, memory efficiency, and energy consumption on ARM hardware."
                },
                "zh": {
                    "title": "轻松跨越架构鸿沟，提升性能与效率",
                    "desc": "本论文介绍了一种名为CRT的轻量级LLM基础的转译器，能够自动将x86汇编代码转换为ARM汇编代码。该方法解决了x86的复杂指令集（CISC）与ARM的精简指令集（RISC）之间的架构差异，同时保持程序语义并优化性能。通过在真实应用上的评估，CRT在x86到ARMv5的转换准确率达到79.25%，在x86到RISC-V的转换准确率达到88.68%。在Apple M2硬件上的实际部署中，转译后的代码相比于Apple的Rosetta 2虚拟化引擎实现了1.73倍的速度提升和2.41倍的内存效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12814",
            "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
            "url": "https://huggingface.co/papers/2411.12814",
            "abstract": "Interactive Medical Image Segmentation (IMIS) has long been constrained by the limited availability of large-scale, diverse, and densely annotated datasets, which hinders model generalization and consistent evaluation across different models. In this paper, we introduce the IMed-361M benchmark dataset, a significant advancement in general IMIS research. First, we collect and standardize over 6.4 million medical images and their corresponding ground truth masks from multiple data sources. Then, leveraging the strong object recognition capabilities of a vision foundational model, we automatically generated dense interactive masks for each image and ensured their quality through rigorous quality control and granularity management. Unlike previous datasets, which are limited by specific modalities or sparse annotations, IMed-361M spans 14 modalities and 204 segmentation targets, totaling 361 million masks-an average of 56 masks per image. Finally, we developed an IMIS baseline network on this dataset that supports high-quality mask generation through interactive inputs, including clicks, bounding boxes, text prompts, and their combinations. We evaluate its performance on medical image segmentation tasks from multiple perspectives, demonstrating superior accuracy and scalability compared to existing interactive segmentation models. To facilitate research on foundational models in medical computer vision, we release the IMed-361M and model at https://github.com/uni-medical/IMIS-Bench.",
            "score": 5,
            "issue_id": 789,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "693c4e9e00a65f7c",
            "authors": [
                "Junlong Cheng",
                "Bin Fu",
                "Jin Ye",
                "Guoan Wang",
                "Tianbin Li",
                "Haoyu Wang",
                "Ruoyu Li",
                "He Yao",
                "Junren Chen",
                "Jingwen Li",
                "Yanzhou Su",
                "Min Zhu",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University, School of computer science and technology",
                "Monash University",
                "Shanghai AI Laboratory, General Medical Artificial Intelligence",
                "Shanghai Jiao Tong University, School of biomedical engineering",
                "Sichuan University, School of Computer Science",
                "Xinjiang University, School of Computer Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12814.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#science",
                    "#benchmark",
                    "#healthcare",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Революция в сегментации медицинских изображений: IMed-361M открывает новые горизонты",
                    "desc": "Статья представляет новый набор данных IMed-361M для интерактивной сегментации медицинских изображений. Этот датасет содержит 6,4 миллиона изображений с 361 миллионом масок для 14 модальностей и 204 целей сегментации. Авторы использовали фундаментальную модель компьютерного зрения для автоматической генерации интерактивных масок. На основе этого датасета разработана базовая сеть для IMIS, превосходящая существующие модели по точности и масштабируемости."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Segmentation with IMed-361M",
                    "desc": "This paper presents the IMed-361M benchmark dataset, which addresses the challenges in Interactive Medical Image Segmentation (IMIS) caused by the lack of large and diverse annotated datasets. The dataset includes over 6.4 million medical images and 361 million dense interactive masks, covering 14 different modalities and 204 segmentation targets. By utilizing a vision foundational model, the authors generated high-quality masks and established a baseline network that supports various interactive input methods for improved segmentation accuracy. The results show that this new dataset and model significantly enhance the performance and scalability of medical image segmentation tasks compared to existing methods."
                },
                "zh": {
                    "title": "IMIS研究的新里程碑：IMed-361M数据集",
                    "desc": "本文介绍了IMed-361M基准数据集，这是互动医学图像分割（IMIS）研究的重要进展。我们收集并标准化了超过640万张医学图像及其对应的真实掩膜，涵盖14种模态和204个分割目标，共计3.61亿个掩膜。通过利用视觉基础模型的强大物体识别能力，我们自动生成了密集的互动掩膜，并通过严格的质量控制确保其质量。我们还在该数据集上开发了IMIS基线网络，支持通过点击、边界框和文本提示等互动输入生成高质量掩膜，展示了优于现有模型的准确性和可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16443",
            "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
            "url": "https://huggingface.co/papers/2411.16443",
            "abstract": "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.",
            "score": 5,
            "issue_id": 779,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "8447f5d110a89105",
            "authors": [
                "Hyojun Go",
                "Byeongjun Park",
                "Jiho Jang",
                "Jin-Young Kim",
                "Soonwoo Kwon",
                "Changick Kim"
            ],
            "affiliations": [
                "KAIST",
                "Twelve Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16443.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная система для интуитивного создания и редактирования 3D-сцен",
                    "desc": "SplatFlow - это комплексная система для генерации и редактирования 3D-сцен на основе текстовых запросов. Она состоит из двух основных компонентов: многоракурсной модели выпрямленного потока и декодера гауссовского сплаттинга. Система позволяет генерировать многоракурсные изображения, карты глубины и положения камер одновременно, а затем эффективно преобразовывать их в 3D-представления. SplatFlow поддерживает широкий спектр задач 3D-моделирования, включая редактирование объектов, синтез новых ракурсов и оценку положения камеры, в рамках единой системы."
                },
                "en": {
                    "title": "SplatFlow: Unifying 3D Scene Generation and Editing",
                    "desc": "This paper presents SplatFlow, a unified framework for generating and editing 3D scenes using 3D Gaussian Splatting (3DGS). It features a multi-view rectified flow model that generates images, depths, and camera poses from text prompts, addressing challenges in scene diversity and camera movement. The framework also includes a Gaussian Splatting Decoder that converts latent outputs into 3DGS representations efficiently. SplatFlow supports various 3D tasks like object editing and novel view synthesis, demonstrating its effectiveness on multiple datasets without the need for complex pipelines."
                },
                "zh": {
                    "title": "SplatFlow：统一的3D生成与编辑框架",
                    "desc": "本文介绍了一种名为SplatFlow的综合框架，旨在简化3D场景的生成和编辑。SplatFlow结合了多视角整流流模型和高斯点云解码器，能够根据文本提示同时生成多视角图像、深度和相机姿态。该框架通过无训练反演和修补技术，实现了无缝的3D高斯点云编辑，支持多种3D任务，如物体编辑和新视角合成。我们在MVImgNet和DL3DV-7K数据集上验证了SplatFlow的能力，展示了其在多种3D生成和编辑任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16035",
            "title": "Predicting Emergent Capabilities by Finetuning",
            "url": "https://huggingface.co/papers/2411.16035",
            "abstract": "A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., \"emergence laws\"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.",
            "score": 5,
            "issue_id": 779,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "a3e818d78a8bea58",
            "authors": [
                "Charlie Snell",
                "Eric Wallace",
                "Dan Klein",
                "Sergey Levine"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16035.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#agi",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Предсказание будущего ИИ: раскрытие тайн эмерджентности в языковых моделях",
                    "desc": "Статья посвящена проблеме предсказания возникновения новых способностей у крупномасштабных языковых моделей (LLM). Авторы предлагают метод предсказания эмерджентности, основанный на дообучении моделей на конкретных задачах. Они обнаружили, что дообучение может сдвинуть точку возникновения эмерджентности в сторону менее мощных моделей. Используя этот подход, исследователи смогли точно предсказать эмерджентность для моделей, обученных с использованием в 4 раза больше вычислительных ресурсов."
                },
                "en": {
                    "title": "Predicting Emergence: Unlocking Future LLM Capabilities",
                    "desc": "This paper addresses the challenge of predicting emergent capabilities in large language models (LLMs) as they scale. While the pretraining loss of LLMs is predictable based on compute resources, their downstream performance can show unexpected jumps in capability. The authors introduce the concept of emergence prediction, which involves fine-tuning LLMs on specific tasks to determine when these emergent capabilities will appear. They validate their approach using standard NLP benchmarks and demonstrate that it is possible to predict the emergence of capabilities in future models based on the performance of smaller models."
                },
                "zh": {
                    "title": "预测语言模型能力的突破",
                    "desc": "本研究探讨了大型语言模型（LLM）在扩展时出现的能力预测问题。我们发现，通过对特定任务进行微调，可以提前预测未来模型在该任务上的表现。研究表明，微调可以将能力出现的临界点向能力较低的模型移动。我们在多个自然语言处理基准上验证了这一方法，并展示了如何利用这一预测能力进行实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16085",
            "title": "Cautious Optimizers: Improving Training with One Line of Code",
            "url": "https://huggingface.co/papers/2411.16085",
            "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim",
            "score": 5,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "48a2e1454fdde298",
            "authors": [
                "Kaizhao Liang",
                "Lizhang Chen",
                "Bo Liu",
                "Qiang Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.16085.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Осторожная оптимизация: простое изменение, большой результат",
                    "desc": "Статья представляет модификацию оптимизаторов на основе импульса, названную Cautious Optimizer. Авторы предлагают простое изменение в коде PyTorch, которое сохраняет функцию Гамильтона для Adam и гарантирует сходимость по анализу Ляпунова. Теоретический анализ раскрывает целое семейство новых оптимизаторов. Эмпирические эксперименты показывают ускорение предобучения Llama и MAE до 1.47 раз."
                },
                "en": {
                    "title": "Cautious Optimizer: Speeding Up Transformer Pretraining!",
                    "desc": "This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners."
                },
                "zh": {
                    "title": "提升变换器预训练速度的新优化器",
                    "desc": "本文提出了一种新的优化器，称为Cautious Optimizer，旨在提高变换器预训练的速度和稳定性。通过对现有的动量优化器进行简单的修改，我们的理论分析表明，这种修改保持了Adam的哈密顿函数，并且在Lyapunov分析下不破坏收敛性保证。我们还揭示了一系列新的优化器，并选择了其中最简单的进行实证实验，结果显示在Llama和MAE预训练中速度提升可达1.47倍。相关代码已在GitHub上发布。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14486",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
            "url": "https://huggingface.co/papers/2411.14486",
            "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
            "score": 5,
            "issue_id": 777,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "c30a94b30cace49a",
            "authors": [
                "David Noever",
                "Forrest McKee"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14486.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#interpretability",
                    "#agi",
                    "#dataset"
                ],
                "emoji": "🤔",
                "ru": {
                    "title": "Признание незнания: ключевой аспект оценки искусственного интеллекта",
                    "desc": "Это исследование представляет новую систему оценки способности больших языковых моделей (LLM) признавать неопределенность на 675 принципиально нерешаемых проблемах. Двенадцать современных LLM были оценены на их склонность признавать незнание, а не генерировать правдоподобные, но неверные ответы. Лучшие модели показали точность 62-68% в признании неизвестности решения проблемы в различных областях. Исследование выявило обратную зависимость между сложностью проблемы и точностью модели, а также значительные различия между категориями проблем."
                },
                "en": {
                    "title": "Evaluating Uncertainty: A New Benchmark for Language Models",
                    "desc": "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "承认不确定性：评估大型语言模型的新视角",
                    "desc": "本研究提出了一种新的评估框架，用于评估大型语言模型（LLMs）在675个根本无法解决的问题上承认不确定性的能力。我们使用了一组经过精心挑选的研究生级别的重大挑战问题数据集，评估了包括开源和闭源模型在内的十二个最先进的LLMs，观察它们承认无知的倾向。结果显示，最佳模型在承认问题解决方案未知的准确率范围为62%到68%，并且在更具挑战性的问题上（如生物学、哲学和数学）表现出更高的不确定性承认率。研究还发现，不同问题类别之间存在显著差异，模型在承认发明和NP难题的不确定性时表现较差，而在哲学和心理学挑战中表现相对较好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15862",
            "title": "LLMs Do Not Think Step-by-step In Implicit Reasoning",
            "url": "https://huggingface.co/papers/2411.15862",
            "abstract": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs' performance on complex tasks. However, because it also introduces slower inference speeds and higher computational costs, many researches have attempted to use implicit CoT, which does not need LLMs to explicitly generate the intermediate steps. But there is still gap between their efficacy and typical explicit CoT methods. This leaves us a doubt that, does implicit CoT really equal to explicit CoT? Therefore, in this study, we address this question through experiments. We probe the information of intermediate steps from the model's hidden states when it is performing implicit CoT. The results surprisingly indicate that LLMs hardly think about intermediate steps, suggesting they may just rely on experience rather than strict step-by-step reasoning. Moreover, we find LLMs' implicit reasoning capabilities are susceptible and unstable, reaffirming the necessity of explicit CoT to effectively support complex tasks.",
            "score": 3,
            "issue_id": 788,
            "pub_date": "2024-11-24",
            "pub_date_card": {
                "ru": "24 ноября",
                "en": "November 24",
                "zh": "11月24日"
            },
            "hash": "8b819b67a91e3ad7",
            "authors": [
                "Yijiong Yu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15862.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#inference",
                    "#rl"
                ],
                "emoji": "🤔",
                "ru": {
                    "title": "Неявное мышление ИИ: иллюзия или реальность?",
                    "desc": "Исследователи изучили эффективность метода 'неявной цепочки рассуждений' (implicit Chain-of-Thought) в крупных языковых моделях. Они обнаружили, что при использовании этого метода модели не генерируют промежуточные шаги рассуждений, а полагаются на накопленный опыт. Результаты показали, что способности моделей к неявным рассуждениям нестабильны и подвержены влиянию. Авторы подчеркивают необходимость использования явной цепочки рассуждений для эффективного решения сложных задач."
                },
                "en": {
                    "title": "Implicit vs. Explicit CoT: Unveiling the True Thinking of LLMs",
                    "desc": "This paper investigates the effectiveness of implicit Chain-of-Thought (CoT) reasoning in large language models (LLMs) compared to explicit CoT methods. The authors conduct experiments to analyze the hidden states of LLMs during implicit CoT tasks, revealing that these models often do not engage in detailed intermediate reasoning steps. Instead, they tend to rely on prior experiences, which leads to inconsistent and unstable performance. The findings highlight the importance of explicit CoT for enhancing LLMs' capabilities in handling complex tasks effectively."
                },
                "zh": {
                    "title": "显式思维链是复杂任务的关键",
                    "desc": "本研究探讨了隐式思维链（implicit CoT）与显式思维链（explicit CoT）在大型语言模型（LLMs）中的表现差异。尽管隐式思维链可以减少计算成本和推理时间，但实验结果显示，LLMs在隐式推理时几乎不考虑中间步骤。相反，它们更依赖于经验，而不是严格的逐步推理。研究还发现，LLMs的隐式推理能力不稳定，进一步强调了显式思维链在支持复杂任务中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15671",
            "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
            "url": "https://huggingface.co/papers/2411.15671",
            "abstract": "Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations.",
            "score": 3,
            "issue_id": 779,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "540358181ae0274e",
            "authors": [
                "Ali Behrouz",
                "Ali Parviz",
                "Mahdi Karami",
                "Clayton Sanford",
                "Bryan Perozzi",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research",
                "New Jersey Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15671.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#architecture",
                    "#benchmark",
                    "#graphs"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Унифицированный подход к обработке графов с помощью последовательностных моделей",
                    "desc": "Статья представляет унифицированный фреймворк Graph Sequence Model (GSM) для применения последовательностных моделей к графовым данным. GSM состоит из трех этапов: токенизация графа в набор последовательностей, локальное кодирование окрестностей узлов и глобальное кодирование для захвата дальних зависимостей. Авторы теоретически оценивают репрезентативную мощность трансформеров и современных рекуррентных моделей для графовых задач. На основе этого анализа предлагается гибридная модель GSM++, использующая иерархическую кластеризацию для токенизации и архитектуру трансформера для кодирования."
                },
                "en": {
                    "title": "Unifying Sequence Models for Enhanced Graph Learning",
                    "desc": "This paper introduces the Graph Sequence Model (GSM), a framework that integrates sequence models with graph-structured data. It consists of three steps: tokenization of graphs into sequences, local encoding of node neighborhoods, and global encoding to capture long-range dependencies. The authors evaluate the strengths and weaknesses of different sequence models, particularly Transformers and recurrent models, in graph tasks. They also propose GSM++, a hybrid model that enhances performance by using hierarchical tokenization and a Transformer architecture, demonstrating superior results in benchmark tests."
                },
                "zh": {
                    "title": "图序列模型：结合序列与图的力量",
                    "desc": "现代序列模型（如变换器和线性递归神经网络）在深度学习框架中占据主导地位，因其高效性、表示能力和捕捉长距离依赖的能力。将这些序列模型应用于图结构数据的研究逐渐受到关注，作为消息传递神经网络（MPNNs）的替代方案。本文提出了图序列模型（GSM），这是一个统一框架，包含图的标记化、局部编码和全局编码三个主要步骤。我们还提出了GSM++，一种快速混合模型，利用层次亲和聚类算法对图进行分层序列标记，并采用变换器的混合架构进行编码，实验结果表明GSM++在大多数基准评估中优于其他模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16665",
            "title": "Edge Weight Prediction For Category-Agnostic Pose Estimation",
            "url": "https://huggingface.co/papers/2411.16665",
            "abstract": "Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse object categories with a single model, using one or a few annotated support images. Recent works have shown that using a pose graph (i.e., treating keypoints as nodes in a graph rather than isolated points) helps handle occlusions and break symmetry. However, these methods assume a static pose graph with equal-weight edges, leading to suboptimal results. We introduce EdgeCape, a novel framework that overcomes these limitations by predicting the graph's edge weights which optimizes localization. To further leverage structural priors, we propose integrating Markovian Structural Bias, which modulates the self-attention interaction between nodes based on the number of hops between them. We show that this improves the model's ability to capture global spatial dependencies. Evaluated on the MP-100 benchmark, which includes 100 categories and over 20K images, EdgeCape achieves state-of-the-art results in the 1-shot setting and leads among similar-sized methods in the 5-shot setting, significantly improving keypoint localization accuracy. Our code is publicly available.",
            "score": 2,
            "issue_id": 791,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "f14fbd2f53f85526",
            "authors": [
                "Or Hirschorn",
                "Shai Avidan"
            ],
            "affiliations": [
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16665.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🔑",
                "ru": {
                    "title": "EdgeCape: точное определение ключевых точек объектов с минимумом данных",
                    "desc": "EdgeCape - это новая система для определения ключевых точек объектов разных категорий на основе одного или нескольких образцов. Она улучшает существующие методы, предсказывая веса рёбер в графе позы и используя марковское структурное смещение. Это позволяет лучше учитывать глобальные пространственные зависимости между точками. EdgeCape достигает лучших результатов на бенчмарке MP-100 в сценарии с одним образцом."
                },
                "en": {
                    "title": "Dynamic Edge Weights for Enhanced Pose Estimation",
                    "desc": "The paper presents EdgeCape, a new framework for Category-Agnostic Pose Estimation (CAPE) that enhances keypoint localization across various object categories using minimal annotated images. It addresses the limitations of previous methods that used static pose graphs with equal-weight edges by introducing dynamic edge weight predictions. Additionally, EdgeCape incorporates Markovian Structural Bias to improve the interaction between keypoints based on their spatial relationships. The results demonstrate that EdgeCape achieves state-of-the-art performance on the MP-100 benchmark, particularly excelling in one-shot and five-shot scenarios."
                },
                "zh": {
                    "title": "EdgeCape：优化关键点定位的新方法",
                    "desc": "这篇论文介绍了一种新的框架，名为EdgeCape，用于在不同物体类别中进行无类别姿态估计。它通过预测图的边权重来优化关键点的定位，克服了传统方法中静态姿态图的局限性。论文还提出了马尔可夫结构偏置，增强了节点之间的自注意力交互，从而更好地捕捉全局空间依赖关系。经过在MP-100基准上的评估，EdgeCape在1-shot和5-shot设置中均取得了最先进的结果，显著提高了关键点定位的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15221",
            "title": "Reflections from the 2024 Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry",
            "url": "https://huggingface.co/papers/2411.15221",
            "abstract": "Here, we present the outcomes from the second Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry, which engaged participants across global hybrid locations, resulting in 34 team submissions. The submissions spanned seven key application areas and demonstrated the diverse utility of LLMs for applications in (1) molecular and material property prediction; (2) molecular and material design; (3) automation and novel interfaces; (4) scientific communication and education; (5) research data management and automation; (6) hypothesis generation and evaluation; and (7) knowledge extraction and reasoning from scientific literature. Each team submission is presented in a summary table with links to the code and as brief papers in the appendix. Beyond team results, we discuss the hackathon event and its hybrid format, which included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable local and virtual collaboration. Overall, the event highlighted significant improvements in LLM capabilities since the previous year's hackathon, suggesting continued expansion of LLMs for applications in materials science and chemistry research. These outcomes demonstrate the dual utility of LLMs as both multipurpose models for diverse machine learning tasks and platforms for rapid prototyping custom applications in scientific research.",
            "score": 1,
            "issue_id": 797,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "3984b5ab3fb11866",
            "authors": [
                "Yoel Zimmermann",
                "Adib Bazgir",
                "Zartashia Afzal",
                "Fariha Agbere",
                "Qianxiang Ai",
                "Nawaf Alampara",
                "Alexander Al-Feghali",
                "Mehrad Ansari",
                "Dmytro Antypov",
                "Amro Aswad",
                "Jiaru Bai",
                "Viktoriia Baibakova",
                "Devi Dutta Biswajeet",
                "Erik Bitzek",
                "Joshua D. Bocarsly",
                "Anna Borisova",
                "Andres M Bran",
                "L. Catherine Brinson",
                "Marcel Moran Calderon",
                "Alessandro Canalicchio",
                "Victor Chen",
                "Yuan Chiang",
                "Defne Circi",
                "Benjamin Charmes",
                "Vikrant Chaudhary",
                "Zizhang Chen",
                "Min-Hsueh Chiu",
                "Judith Clymo",
                "Kedar Dabhadkar",
                "Nathan Daelman",
                "Archit Datar",
                "Matthew L. Evans",
                "Maryam Ghazizade Fard",
                "Giuseppe Fisicaro",
                "Abhijeet Sadashiv Gangan",
                "Janine George",
                "Jose D. Cojal Gonzalez",
                "Michael Götte",
                "Ankur K. Gupta",
                "Hassan Harb",
                "Pengyu Hong",
                "Abdelrahman Ibrahim",
                "Ahmed Ilyas",
                "Alishba Imran",
                "Kevin Ishimwe",
                "Ramsey Issa",
                "Kevin Maik Jablonka",
                "Colin Jones",
                "Tyler R. Josephson",
                "Greg Juhasz",
                "Sarthak Kapoor",
                "Rongda Kang",
                "Ghazal Khalighinejad",
                "Sartaaj Khan",
                "Sascha Klawohn",
                "Suneel Kuman",
                "Alvin Noe Ladines",
                "Sarom Leang",
                "Magdalena Lederbauer",
                "Sheng-Lun Mark Liao",
                "Hao Liu",
                "Xuefeng Liu",
                "Stanley Lo",
                "Sandeep Madireddy",
                "Piyush Ranjan Maharana",
                "Shagun Maheshwari",
                "Soroush Mahjoubi",
                "José A. Márquez",
                "Rob Mills",
                "Trupti Mohanty",
                "Bernadette Mohr",
                "Seyed Mohamad Moosavi",
                "Alexander Moßhammer",
                "Amirhossein D. Naghdi",
                "Aakash Naik",
                "Oleksandr Narykov",
                "Hampus Näsström",
                "Xuan Vu Nguyen",
                "Xinyi Ni",
                "Dana O'Connor",
                "Teslim Olayiwola",
                "Federico Ottomano",
                "Aleyna Beste Ozhan",
                "Sebastian Pagel",
                "Chiku Parida",
                "Jaehee Park",
                "Vraj Patel",
                "Elena Patyukova",
                "Martin Hoffmann Petersen",
                "Luis Pinto",
                "José M. Pizarro",
                "Dieter Plessers",
                "Tapashree Pradhan",
                "Utkarsh Pratiush",
                "Charishma Puli",
                "Andrew Qin",
                "Mahyar Rajabi",
                "Francesco Ricci",
                "Elliot Risch",
                "Martiño Ríos-García",
                "Aritra Roy",
                "Tehseen Rug",
                "Hasan M Sayeed",
                "Markus Scheidgen",
                "Mara Schilling-Wilhelmi",
                "Marcel Schloz",
                "Fabian Schöppach",
                "Julia Schumann",
                "Philippe Schwaller",
                "Marcus Schwarting",
                "Samiha Sharlin",
                "Kevin Shen",
                "Jiale Shi",
                "Pradip Si",
                "Jennifer D'Souza",
                "Taylor Sparks",
                "Suraj Sudhakar",
                "Leopold Talirz",
                "Dandan Tang",
                "Olga Taran",
                "Carla Terboven",
                "Mark Tropin",
                "Anastasiia Tsymbal",
                "Katharina Ueltzen",
                "Pablo Andres Unzueta",
                "Archit Vasan",
                "Tirtha Vinchurkar",
                "Trung Vo",
                "Gabriel Vogel",
                "Christoph Völker",
                "Jan Weinreich",
                "Faradawn Yang",
                "Mohd Zaki",
                "Chi Zhang",
                "Sylvester Zhang",
                "Weijie Zhang",
                "Ruijie Zhu",
                "Shang Zhu",
                "Jan Janssen",
                "Ian Foster",
                "Ben Blaiszik"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.15221.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "LLM открывают новые горизонты в материаловедении и химии",
                    "desc": "Статья описывает результаты второго хакатона по применению больших языковых моделей (LLM) в материаловедении и химии. Участники из разных стран представили 34 проекта в семи ключевых областях применения, включая предсказание свойств материалов, автоматизацию исследований и извлечение знаний из научной литературы. Хакатон проходил в гибридном формате с физическими хабами в шести городах и глобальным онлайн-хабом. Результаты показали значительный прогресс в возможностях LLM по сравнению с прошлым годом, демонстрируя их потенциал как универсальных моделей для различных задач машинного обучения в научных исследованиях."
                },
                "en": {
                    "title": "Unlocking the Power of LLMs in Materials Science and Chemistry",
                    "desc": "This paper discusses the results of the second Large Language Model (LLM) Hackathon focused on materials science and chemistry, which attracted 34 teams from around the world. The submissions showcased the versatility of LLMs in various applications, including predicting molecular properties, designing materials, and automating research processes. The event also emphasized the advancements in LLM capabilities compared to the previous year, indicating their growing importance in scientific research. Overall, the hackathon served as a platform for collaboration and innovation, highlighting LLMs as powerful tools for both general machine learning tasks and specialized scientific applications."
                },
                "zh": {
                    "title": "大型语言模型在材料科学与化学中的应用潜力",
                    "desc": "本文介绍了第二届大型语言模型（LLM）黑客马拉松在材料科学和化学领域的成果，吸引了全球参与者，共提交了34个团队项目。这些项目涵盖了七个关键应用领域，展示了LLM在分子和材料属性预测、设计、自动化、科学传播、数据管理、假设生成和知识提取等方面的多样化应用。每个团队的提交结果都以摘要表的形式呈现，并附有代码链接和简要论文。此次活动突显了LLM能力的显著提升，表明其在材料科学和化学研究中的应用前景广阔。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.13550",
            "title": "Find Any Part in 3D",
            "url": "https://huggingface.co/papers/2411.13550",
            "abstract": "We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/",
            "score": 1,
            "issue_id": 794,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "d8b296940d5af71d",
            "authors": [
                "Ziqi Ma",
                "Yisong Yue",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "California Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.13550.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Find3D: Революция в сегментации частей 3D-объектов с открытым миром",
                    "desc": "Исследователи представили модель Find3D для сегментации частей 3D-объектов с открытым миром на основе текстовых запросов. Модель обучается на большом наборе 3D-данных из интернета без ручной разметки, используя контрастивное обучение и фундаментальные модели для аннотирования. Find3D демонстрирует значительное улучшение производительности и обобщающей способности по сравнению с существующими методами на различных наборах данных. Авторы также представили новый бенчмарк для оценки моделей сегментации частей 3D-объектов общего назначения."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with Find3D",
                    "desc": "This paper presents Find3D, a novel model for open-world part segmentation in 3D, allowing segmentation of any object part based on text queries. Unlike previous methods that are restricted to specific object categories and part vocabularies, Find3D utilizes a general-category point embedding model trained on extensive 3D data from the internet without human annotations. The model employs a combination of a data engine and contrastive training, resulting in significant performance improvements, achieving up to 3x higher mean Intersection over Union (mIoU) compared to existing methods. Additionally, Find3D operates at a much faster speed, being 6x to over 300x quicker than current baselines, and introduces a benchmark to promote further research in this area."
                },
                "zh": {
                    "title": "开放世界3D部件分割的新突破",
                    "desc": "我们研究了开放世界的3D部件分割，能够根据文本查询对任何物体的任何部件进行分割。以往的方法在物体类别和部件词汇上存在局限性。我们提出了一种名为Find3D的开放世界直接预测模型，能够在没有任何人工标注的情况下，对来自互联网的大规模3D资产进行训练。该模型结合了数据引擎和对比训练方法，在多个数据集上实现了强大的性能和泛化能力，mIoU提升达到3倍以上，同时速度比现有基线快6倍到300倍。"
                }
            }
        }
    ],
    "link_prev": "2024-11-25.html",
    "link_next": "2024-11-27.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11月25日"
    },
    "short_date_next": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 12,
        "#agents": 0,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 7,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 4,
        "#training": 8,
        "#robotics": 0,
        "#agi": 3,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 3,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们展示了Material Anything，一个完全自动化、统一的扩散框架，旨在为3D物体生成基于物理的材质。与依赖复杂流水线或特定案例优化的现有方法不同，Material Anything提供了一个适应多种光照条件下物体的健壮、端到端的解决方案。我们的方法利用了预训练的图像扩散模型，结合三头架构和渲染损失，以提高稳定性和材质质量。此外，我们引入了置信掩码作为扩散模型内的动态开关，使其能够有效处理有纹理和无纹理的物体。通过使用这些置信掩码指导的渐进材质生成策略，以及UV空间材质精炼器，我们的方法确保了一致的、UV准备好的材质输出。广泛的实验表明，我们的方法在各种物体类别和光照条件下优于现有方法。",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "pinyin": "Wǒmen zhǎnshìle Material Anything, yīgè wánquán zìdònghuà, tǒngyī de kuòsàn kuàngjià, zhǐzài wèi 3D wùtǐ shēngchéng jīyǐ wùlǐ de cáizhì. Yǔ yīlài fùzá xiùshuǐxiàn huò tèdìng ànliào yōuhuà de xiàn yǒu fāngfǎ bùtóng, Material Anything tígōngle yīgè shìyìng duōzhǒng guāngzhào tiáojiàn xià wùtǐ de jiànkāng, duān dào duān de jiějué fāng'àn. Wǒmen de fāngfǎ lìyòngle yù xùnliàn de túxiàng kuòsàn móxíng, jiéhé sān tóu jiàgòu hé xuànshēi sǔnshī, yǐ tígāo wěndìngxìng hé cáizhì zhìliàng. Cǐwài, wǒmen yǐn rùle zhìxìn mózhào zuòwéi kuòsàn móxíng nèi de dòngtài kāiguān, shǐ qí nénggòu yǒuxiào chǔlǐ yǒu wénlǐ hé wú wénlǐ de wùtǐ. Tōngguò shǐyòng zhèxiē zhìxìn mózhào zhǐdǎo de jiànjìn cáizhì shēngchéng cèlüè, yǐjiǎ UV kōngjiān cáizhì jīngliànqì, wǒmen de fāngfǎ quèbǎole yīzhì de, UV zhǔnbèi hǎo de cáizhì shūchū. Guǎngfàn de shìyàn biǎomíng, wǒmen de fāngfǎ zài gèzhǒng wùtǐ lèibié hé guāngzhào tiáojiàn xià yōu yú xiàn yǒu fāngfǎ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'},\n{'word': 'Material', 'pinyin': 'Méitèriǎl', 'trans': 'Material'},\n{'word': 'Anything', 'pinyin': 'Ēnìthìng', 'trans': 'Anything'},\n{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '统一', 'pinyin': 'tǒngyī', 'trans': 'unified'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '物理', 'pinyin': 'wùlǐ', 'trans': 'physics'},\n{'word': '材质', 'pinyin': 'cáizhì', 'trans': 'material'},\n{'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely on'},\n{'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'},\n{'word': '流水线', 'pinyin': 'liúshuǐxiàn', 'trans': 'pipeline'},\n{'word': '特定', 'pinyin': 'tèdìng', 'trans': 'specific'},\n{'word': '案例', 'pinyin': 'ànlì', 'trans': 'case'},\n{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'},\n{'word': '健壮', 'pinyin': 'jiànzhuàng', 'trans': 'robust'},\n{'word': '端到端', 'pinyin': 'duāndàoduān', 'trans': 'end-to-end'},\n{'word': '解决方案', 'pinyin': 'jiějué fāngàn', 'trans': 'solution'},\n{'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'},\n{'word': '预训练', 'pinyin': 'yùxùnliàn', 'trans': 'pre-trained'},\n{'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'},\n{'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'},\n{'word': '三头架构', 'pinyin': 'sāntóu jiàgòu', 'trans': 'three-headed architecture'},\n{'word': '渲染', 'pinyin': 'xuànrán', 'trans': 'rendering'},\n{'word': '损失', 'pinyin': 'sǔnshī', 'trans': 'loss'},\n{'word': '稳定性', 'pinyin': 'wěndìngxìng', 'trans': 'stability'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '置信', 'pinyin': 'zhìxìn', 'trans': 'confidence'},\n{'word': '掩码', 'pinyin': 'yǎnmǎ', 'trans': 'mask'},\n{'word': '动态', 'pinyin': 'dòngtài', 'trans': 'dynamic'},\n{'word': '开关', 'pinyin': 'kāiguān', 'trans': 'switch'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '纹理', 'pinyin': 'wénlǐ', 'trans': 'texture'},\n{'word': '渐进', 'pinyin': 'jiànjìn', 'trans': 'progressive'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': 'UV空间', 'pinyin': 'UV kōngjiān', 'trans': 'UV space'},\n{'word': '精炼器', 'pinyin': 'jīngliànqì', 'trans': 'refiner'},\n{'word': '确保', 'pinyin': 'quèbǎo', 'trans': 'ensure'},\n{'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'},\n{'word': '准备好', 'pinyin': 'zhǔnbèi hǎo', 'trans': 'ready'},\n{'word': '广泛', 'pinyin': 'guǎngfàn', 'trans': 'extensive'},\n{'word': '类别', 'pinyin': 'lèibié', 'trans': 'category'},\n{'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}]",
        "trans": "We presented Material Anything, a fully automated, unified diffusion framework aimed at generating physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to various lighting conditions for objects. Our approach leverages pre-trained image diffusion models, combined with a tri-head architecture and rendering loss, to enhance stability and material quality. Additionally, we introduced confidence masks as dynamic switches within the diffusion model, enabling it to effectively handle both textured and non-textured objects. By employing a progressive material generation strategy guided by these confidence masks, along with a UV space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate that our method outperforms existing approaches across various object categories and lighting conditions.",
        "update_ts": "2024-11-26 09:11"
    }
}