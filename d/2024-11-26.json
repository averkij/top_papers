{
    "date": {
        "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 26",
        "zh": "11æœˆ26æ—¥"
    },
    "time_utc": "2024-11-26 03:25",
    "weekday": 1,
    "issue_id": 777,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.15466",
            "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
            "url": "https://huggingface.co/papers/2411.15466",
            "abstract": "Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/",
            "score": 9,
            "issue_id": 777,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "288600e8c54930f4",
            "authors": [
                "Chaehun Shin",
                "Jooyoung Choi",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15466.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Diptych Prompting: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diptych Prompting. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-image Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ»ĞµĞ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°Ğ²ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Diptych Prompting Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ zero-shot Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Diptych Prompting: Zero-Shot Image Generation with Subject Precision",
                    "desc": "This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing."
                },
                "zh": {
                    "title": "Diptych Promptingï¼šç²¾å‡†çš„ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶-shotæ–¹æ³•ï¼Œç§°ä¸ºDiptych Promptingï¼Œæ—¨åœ¨å®ç°ä¸»é¢˜é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆä»»åŠ¡é‡æ–°è§£é‡Šä¸ºå›¾åƒä¿®è¡¥ï¼Œç¡®ä¿äº†ä¸»é¢˜çš„ç²¾ç¡®å¯¹é½ã€‚Diptych Promptingåˆ©ç”¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŒè”ç”Ÿæˆç‰¹æ€§ï¼Œå·¦ä¾§é¢æ¿å±•ç¤ºå‚è€ƒå›¾åƒï¼Œå³ä¾§é¢æ¿è¿›è¡Œæ–‡æœ¬æ¡ä»¶çš„ä¿®è¡¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æ•ˆæœä¸Šä¼˜äºä¼ ç»Ÿçš„é›¶-shotå›¾åƒæç¤ºæ–¹æ³•ï¼Œä¸”æ”¯æŒå¤šç§å›¾åƒç”Ÿæˆåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16657",
            "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
            "url": "https://huggingface.co/papers/2411.16657",
            "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.",
            "score": 4,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "02cf3312e8d1f6ca",
            "authors": [
                "Zun Wang",
                "Jialu Li",
                "Han Lin",
                "Jaehong Yoon",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16657.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DreamRunner: ĞÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DreamRunner - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ. DreamRunner Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ñ…."
                },
                "en": {
                    "title": "DreamRunner: Crafting Seamless Storytelling Videos from Text",
                    "desc": "This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts."
                },
                "zh": {
                    "title": "DreamRunnerï¼šåˆ›æ–°çš„æ•…äº‹è§†é¢‘ç”Ÿæˆæ–¹æ³•",
                    "desc": "æ•…äº‹è§†é¢‘ç”Ÿæˆï¼ˆSVGï¼‰æ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥æ–‡æœ¬è„šæœ¬åˆ›å»ºé•¿ç¯‡ã€å¤šåŠ¨ä½œã€å¤šåœºæ™¯çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é¢ä¸´ç€å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹è±¡éœ€è¦å±•ç°å¤æ‚çš„ç»†å¾®åŠ¨ä½œï¼Œä»¥åŠå¤šä¸ªå¯¹è±¡åœ¨ä¸åŒåœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamRunnerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ•…äº‹åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåœºæ™¯è§„åˆ’å’Œå¯¹è±¡å¸ƒå±€ã€‚DreamRunnerè¿˜å¼•å…¥äº†ç©ºé—´-æ—¶é—´åŒºåŸŸåŸºç¡€çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„å¯¹è±¡åŠ¨ä½œç»‘å®šå’Œé€å¸§è¯­ä¹‰æ§åˆ¶ï¼Œå±•ç°å‡ºåœ¨è§’è‰²ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15138",
            "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
            "url": "https://huggingface.co/papers/2411.15138",
            "abstract": "We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.",
            "score": 3,
            "issue_id": 776,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "34b8f6718115f1e3",
            "authors": [
                "Xin Huang",
                "Tengfei Wang",
                "Ziwei Liu",
                "Qing Wang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15138.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Material Anything - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ€Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¼Ğ°ÑĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Automating Realistic Material Generation for 3D Objects",
                    "desc": "Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions."
                },
                "zh": {
                    "title": "å…¨è‡ªåŠ¨ææ–™ç”Ÿæˆï¼Œé€‚åº”å¤šç§å…‰ç…§æ¡ä»¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMaterial Anythingçš„å…¨è‡ªåŠ¨ç»Ÿä¸€æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º3Dç‰©ä½“ç”ŸæˆåŸºäºç‰©ç†çš„ææ–™ã€‚ä¸ç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æµç¨‹æˆ–ç‰¹å®šä¼˜åŒ–ä¸åŒï¼ŒMaterial Anythingæä¾›äº†ä¸€ç§ç¨³å¥çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œé€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„ç‰©ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸‰å¤´æ¶æ„å’Œæ¸²æŸ“æŸå¤±æ¥æé«˜ç¨³å®šæ€§å’Œææ–™è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡æ©ç ä½œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„åŠ¨æ€åˆ‡æ¢å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœ‰çº¹ç†å’Œæ— çº¹ç†çš„ç‰©ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16085",
            "title": "Cautious Optimizers: Improving Training with One Line of Code",
            "url": "https://huggingface.co/papers/2411.16085",
            "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim",
            "score": 1,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "48a2e1454fdde298",
            "authors": [
                "Kaizhao Liang",
                "Lizhang Chen",
                "Bo Liu",
                "Qiang Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.16085.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Cautious Optimizer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ´Ğµ PyTorch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ“Ğ°Ğ¼Ğ¸Ğ»ÑŒÑ‚Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ Adam Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ›ÑĞ¿ÑƒĞ½Ğ¾Ğ²Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Llama Ğ¸ MAE Ğ´Ğ¾ 1.47 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Cautious Optimizer: Speeding Up Transformer Pretraining!",
                    "desc": "This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners."
                },
                "zh": {
                    "title": "æå‡å˜æ¢å™¨é¢„è®­ç»ƒé€Ÿåº¦çš„æ–°ä¼˜åŒ–å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œç§°ä¸ºCautious Optimizerï¼Œæ—¨åœ¨æé«˜å˜æ¢å™¨é¢„è®­ç»ƒçš„é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚é€šè¿‡å¯¹ç°æœ‰çš„åŠ¨é‡ä¼˜åŒ–å™¨è¿›è¡Œç®€å•çš„ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§ä¿®æ”¹ä¿æŒäº†Adamçš„å“ˆå¯†é¡¿å‡½æ•°ï¼Œå¹¶ä¸”åœ¨Lyapunovåˆ†æä¸‹ä¸ç ´åæ”¶æ•›æ€§ä¿è¯ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†ä¸€ç³»åˆ—æ–°çš„ä¼˜åŒ–å™¨ï¼Œå¹¶é€‰æ‹©äº†å…¶ä¸­æœ€ç®€å•çš„è¿›è¡Œå®è¯å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨Llamaå’ŒMAEé¢„è®­ç»ƒä¸­é€Ÿåº¦æå‡å¯è¾¾1.47å€ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14486",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
            "url": "https://huggingface.co/papers/2411.14486",
            "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
            "score": 1,
            "issue_id": 777,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "c30a94b30cace49a",
            "authors": [
                "David Noever",
                "Forrest McKee"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14486.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#interpretability",
                    "#agi",
                    "#dataset"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ: ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ°ÑĞ¿ĞµĞºÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 675 Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµÑ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ…. Ğ”Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¸Ñ… ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 62-68% Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "Evaluating Uncertainty: A New Benchmark for Language Models",
                    "desc": "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è§†è§’",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨675ä¸ªæ ¹æœ¬æ— æ³•è§£å†³çš„é—®é¢˜ä¸Šæ‰¿è®¤ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç»„ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç ”ç©¶ç”Ÿçº§åˆ«çš„é‡å¤§æŒ‘æˆ˜é—®é¢˜æ•°æ®é›†ï¼Œè¯„ä¼°äº†åŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹åœ¨å†…çš„åäºŒä¸ªæœ€å…ˆè¿›çš„LLMsï¼Œè§‚å¯Ÿå®ƒä»¬æ‰¿è®¤æ— çŸ¥çš„å€¾å‘ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨æ‰¿è®¤é—®é¢˜è§£å†³æ–¹æ¡ˆæœªçŸ¥çš„å‡†ç¡®ç‡èŒƒå›´ä¸º62%åˆ°68%ï¼Œå¹¶ä¸”åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¸Šï¼ˆå¦‚ç”Ÿç‰©å­¦ã€å“²å­¦å’Œæ•°å­¦ï¼‰è¡¨ç°å‡ºæ›´é«˜çš„ä¸ç¡®å®šæ€§æ‰¿è®¤ç‡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒé—®é¢˜ç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ¨¡å‹åœ¨æ‰¿è®¤å‘æ˜å’ŒNPéš¾é¢˜çš„ä¸ç¡®å®šæ€§æ—¶è¡¨ç°è¾ƒå·®ï¼Œè€Œåœ¨å“²å­¦å’Œå¿ƒç†å­¦æŒ‘æˆ˜ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-25.html",
    "link_next": "2024-11-27.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éš¾ä»¥å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ä½³ã€‚ä½œè€…æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒæ—¶å°†ä¿¡å™ªæ¯”åˆ†å¸ƒåå‘æ›´é«˜çš„å™ªå£°æ°´å¹³ï¼Œä»¥æ•æ‰ç‹¬ç‰¹é£æ ¼ã€‚è¿™ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ç”Ÿæˆå…·æœ‰é«˜é£æ ¼ä¸€è‡´æ€§çš„å›¾åƒï¼Œå¹¶å…è®¸åˆ›å»ºå’Œå…±äº«æ–°çš„é£æ ¼æ¨¡æ¿ã€‚ä½œè€…å±•ç¤ºäº†ç”Ÿæˆæ°´å½©ç”»ã€ç®€ç¬”ç”»ã€3Dæ¸²æŸ“ç­‰å¤šç§é£æ ¼çš„èƒ½åŠ›ã€‚",
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éš¾ä»¥å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ä½³ã€‚ä½œè€…æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒæ—¶å°†ä¿¡å™ªæ¯”åˆ†å¸ƒåå‘æ›´é«˜çš„å™ªå£°æ°´å¹³ï¼Œä»¥æ•æ‰ç‹¬ç‰¹é£æ ¼ã€‚è¿™ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ç”Ÿæˆå…·æœ‰é«˜é£æ ¼ä¸€è‡´æ€§çš„å›¾åƒï¼Œå¹¶å…è®¸åˆ›å»ºå’Œå…±äº«æ–°çš„é£æ ¼æ¨¡æ¿ã€‚ä½œè€…å±•ç¤ºäº†ç”Ÿæˆæ°´å½©ç”»ã€ç®€ç¬”ç”»ã€3Dæ¸²æŸ“ç­‰å¤šç§é£æ ¼çš„èƒ½åŠ›ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  xÃ­ng kuÃ² sÃ n mÃ³ xÃ­ng shÄ“ng chÃ©ng gÄo zhÃ¬ liÃ ng tÃº xiÃ ng, dÃ n nÃ¡n yÇ xuÃ© xÃ­ xÄ«n de gÃ¨ xÃ¬ng huÃ  yÃ¬ shÃ¹ fÄ“ng gÃ©. xiÃ n yÇ’u fÄng fÇ zÃ i wÄ“i tiÃ¡o shÃ­ mÄng mÃ¹ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n mÃ¹ biÄo hÃ© zÃ o shÄ“ng shuÇ pÃ­ng fÄ“n bÃ¹, dÇo zhÃ¬ fÄ“ng gÃ© duÃ¬ qÃ­ bÃ¹ jiÄ. zuÃ² zhÄ› tÃ­ chÅ« le fÄ“ng gÃ© yÇ’u hÇo de xÃ¬n zÃ o bÇ cÇi yÇng qÃ¬, zÃ i wÄ“i tiÃ¡o shÃ­ jiÄng xÃ¬n zÃ o bÇ fÄ“n bÃ¹ piÄn xiÃ ng gÃ¨ng gÄo de zÃ o shÄ“ng shuÇ pÃ­ng, yÇ bÇ” zhuÅ dÃº tÃ¨ fÄ“ng gÃ©. zhÃ¨ shÇ mÃ³ xÃ­ng nÃ©ng gÃ¨ng hÇo de shÄ“ng chÃ©ng jÃ¹ yÇ’u gÄo fÄ“ng gÃ© yÄ« zhÃ¬ xÃ¬ng de tÃº xiÃ ng, bÃ¬ng yÇ”n xÇ” chuÃ ng jiÃ n hÃ© gÃ²ng xiÇng xÄ«n de fÄ“ng gÃ© mÃº bÇn. zuÃ² zhÄ› zhÇn shÃ¬ le shÄ“ng chÃ©ng shuÇ cÇi huÃ , jiÇn bÇ huÃ , 3D xuÃ n rÃ¡n dÄ›ng duÅ zhÇ’ng fÄ“ng gÃ© de nÃ©ng lÃ¬.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'ä¸ªæ€§åŒ–', 'pinyin': 'gÃ¨ xÃ¬ng huÃ ', 'trans': 'personalized'}, {'word': 'è‰ºæœ¯', 'pinyin': 'yÃ¬ shÃ¹', 'trans': 'art'}, {'word': 'é£æ ¼', 'pinyin': 'fÄ“ng gÃ©', 'trans': 'style'}, {'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'}, {'word': 'ç›²ç›®', 'pinyin': 'mÃ¡ng mÃ¹', 'trans': 'blindly'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-trained'}, {'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'target'}, {'word': 'å™ªå£°', 'pinyin': 'zÃ o shÄ“ng', 'trans': 'noise'}, {'word': 'æ°´å¹³', 'pinyin': 'shuÇ pÃ­ng', 'trans': 'level'}, {'word': 'åˆ†å¸ƒ', 'pinyin': 'fÄ“n bÃ¹', 'trans': 'distribution'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ä¿¡å™ªæ¯”', 'pinyin': 'xÃ¬n zÃ o bÇ', 'trans': 'signal-to-noise ratio'}, {'word': 'é‡‡æ ·å™¨', 'pinyin': 'cÇi yÃ ng qÃ¬', 'trans': 'sampler'}, {'word': 'åå‘', 'pinyin': 'piÄn xiÃ ng', 'trans': 'bias towards'}, {'word': 'æ•æ‰', 'pinyin': 'bÇ” zhuÅ', 'trans': 'capture'}, {'word': 'ç‹¬ç‰¹', 'pinyin': 'dÃº tÃ¨', 'trans': 'unique'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'æ¨¡æ¿', 'pinyin': 'mÃº bÇn', 'trans': 'template'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'}, {'word': 'æ°´å½©ç”»', 'pinyin': 'shuÇ cÇi huÃ ', 'trans': 'watercolor painting'}, {'word': 'ç®€ç¬”ç”»', 'pinyin': 'jiÇn bÇ huÃ ', 'trans': 'line drawing'}, {'word': '3Dæ¸²æŸ“', 'pinyin': '3D xuÃ n rÃ¡n', 'trans': '3D rendering'}]",
        "trans": "This article discusses the ability of large diffusion models to generate high-quality images but their difficulty in learning new personalized artistic styles. Existing methods blindly use pre-trained objectives and noise level distributions during fine-tuning, leading to poor style alignment. The authors propose a style-friendly signal-to-noise ratio sampler that biases the signal-to-noise ratio distribution towards higher noise levels during fine-tuning to capture unique styles. This allows the model to generate images with higher style consistency and enables the creation and sharing of new style templates. The authors demonstrate the capability to generate various styles such as watercolor, sketch, and 3D rendering.",
        "update_ts": "2024-11-25 09:06"
    }
}