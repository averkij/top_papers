{
    "date": {
        "ru": "27 ноября",
        "en": "November 27",
        "zh": "11月27日"
    },
    "time_utc": "2024-11-27 02:21",
    "weekday": 2,
    "issue_id": 802,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17116",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "url": "https://huggingface.co/papers/2411.17116",
            "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.",
            "score": 3,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "12194d270104d50f",
            "authors": [
                "Shantanu Acharya",
                "Fei Jia",
                "Boris Ginsburg"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17116.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "⭐",
                "ru": {
                    "title": "Звездное внимание: ускорение LLM без потери точности",
                    "desc": "Статья представляет метод Star Attention для улучшения эффективности вычислений в трансформерных моделях большого языка (LLM) при работе с длинными последовательностями. Метод использует двухфазное блочно-разреженное приближение, распределяя внимание между несколькими узлами и минимизируя накладные расходы на коммуникацию. Star Attention сочетает локальное блочное внимание на первой фазе и глобальное внимание на второй фазе. Этот подход позволяет снизить требования к памяти и время вывода до 11 раз при сохранении 95-100% точности."
                },
                "en": {
                    "title": "Boosting Efficiency in Long Sequence Processing with Star Attention",
                    "desc": "This paper presents Star Attention, a new method designed to enhance the efficiency of Transformer-based Large Language Models (LLMs) when processing long sequences. It addresses the high computational cost and slow inference times caused by the traditional self-attention mechanism's quadratic complexity. Star Attention operates in two phases: first, it uses blockwise-local attention to process context in parallel across multiple hosts, and then it applies sequence-global attention for query and response tokens. This approach significantly reduces memory usage and inference time by up to 11 times while maintaining a high level of accuracy, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "星际注意力：提升长序列推理效率的创新方法",
                    "desc": "本文介绍了一种名为星际注意力（Star Attention）的新方法，旨在提高基于变换器的大型语言模型（LLM）在长序列上的推理效率。该方法通过在多个主机之间分片注意力，采用两阶段的块稀疏近似，显著降低了计算成本。第一阶段使用块局部注意力并行处理上下文，第二阶段则通过全局注意力处理查询和响应标记。星际注意力与大多数使用全局注意力训练的变换器模型无缝集成，能够将内存需求和推理时间减少多达11倍，同时保持95-100%的准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17223",
            "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
            "url": "https://huggingface.co/papers/2411.17223",
            "abstract": "Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce a disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github.com/mycfhs/DreamMix.",
            "score": 1,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "56eb0ccceda40f61",
            "authors": [
                "Yicheng Yang",
                "Pengxiang Li",
                "Lu Zhang",
                "Liqian Ma",
                "Ping Hu",
                "Siyu Du",
                "Yunzhi Zhuge",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "University of Electronic Science and Technology of China",
                "ZMO AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17223.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "DreamMix: Вставка и редактирование объектов на изображениях с помощью текста",
                    "desc": "DreamMix - это генеративная модель на основе диффузии для вставки объектов в изображения. Она позволяет не только сохранять идентичность объектов, но и редактировать их атрибуты с помощью текстовых запросов. Модель использует усовершенствованный подход к инпейнтингу, сочетающий локальную и глобальную обработку. Также в DreamMix применяется механизм разделения атрибутов и модуль текстовой подстановки атрибутов для улучшения разнообразия и точности управления свойствами объектов."
                },
                "en": {
                    "title": "DreamMix: Seamless Object Insertion and Attribute Editing in Images",
                    "desc": "This paper presents DreamMix, a novel diffusion-based generative model designed for subject-driven image inpainting. Unlike previous methods that prioritize identity preservation, DreamMix allows users to insert objects into images at specified locations while also enabling modifications to their attributes through text prompts. The model employs a disentangled local-global inpainting framework to ensure that inserted objects blend well with the overall scene. Additionally, it introduces mechanisms for attribute decoupling and textual attribute substitution to enhance the diversity and effectiveness of text-based guidance for editing."
                },
                "zh": {
                    "title": "DreamMix：智能图像修复与属性编辑的完美结合",
                    "desc": "本文介绍了一种名为DreamMix的扩散生成模型，旨在实现目标对象在指定场景中的插入，同时允许用户对其属性进行文本驱动的修改。与以往方法不同，DreamMix不仅关注身份保留，还能保持插入对象的可编辑性。我们采用了先进的基础修复模型，并引入了局部-全局分离修复框架，以平衡精确的局部对象插入和有效的全局视觉一致性。此外，本文还提出了属性解耦机制和文本属性替换模块，以提高基于文本的属性指导的多样性和区分能力。"
                }
            }
        }
    ],
    "link_prev": "2024-11-26.html",
    "link_next": "2024-11-28.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "short_date_next": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11月28日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们展示了Material Anything，一个完全自动化、统一的扩散框架，旨在为3D物体生成基于物理的材质。与依赖复杂流水线或特定案例优化的现有方法不同，Material Anything提供了一个适应多种光照条件下物体的健壮、端到端的解决方案。我们的方法利用了预训练的图像扩散模型，结合三头架构和渲染损失，以提高稳定性和材质质量。此外，我们引入了置信掩码作为扩散模型内的动态开关，使其能够有效处理有纹理和无纹理的物体。通过使用这些置信掩码指导的渐进材质生成策略，以及UV空间材质精炼器，我们的方法确保了一致的、UV准备好的材质输出。广泛的实验表明，我们的方法在各种物体类别和光照条件下优于现有方法。",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "pinyin": "Wǒmen zhǎnshìle Material Anything, yīgè wánquán zìdònghuà, tǒngyī de kuòsàn kuàngjià, zhǐzài wèi 3D wùtǐ shēngchéng jīyǐ wùlǐ de cáizhì. Yǔ yīlài fùzá xiùshuǐxiàn huò tèdìng ànliào yōuhuà de xiàn yǒu fāngfǎ bùtóng, Material Anything tígōngle yīgè shìyìng duōzhǒng guāngzhào tiáojiàn xià wùtǐ de jiànkāng, duān dào duān de jiějué fāng'àn. Wǒmen de fāngfǎ lìyòngle yù xùnliàn de túxiàng kuòsàn móxíng, jiéhé sān tóu jiàgòu hé xuànshēi sǔnshī, yǐ tígāo wěndìngxìng hé cáizhì zhìliàng. Cǐwài, wǒmen yǐn rùle zhìxìn mózhào zuòwéi kuòsàn móxíng nèi de dòngtài kāiguān, shǐ qí nénggòu yǒuxiào chǔlǐ yǒu wénlǐ hé wú wénlǐ de wùtǐ. Tōngguò shǐyòng zhèxiē zhìxìn mózhào zhǐdǎo de jiànjìn cáizhì shēngchéng cèlüè, yǐjiǎ UV kōngjiān cáizhì jīngliànqì, wǒmen de fāngfǎ quèbǎole yīzhì de, UV zhǔnbèi hǎo de cáizhì shūchū. Guǎngfàn de shìyàn biǎomíng, wǒmen de fāngfǎ zài gèzhǒng wùtǐ lèibié hé guāngzhào tiáojiàn xià yōu yú xiàn yǒu fāngfǎ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'},\n{'word': 'Material', 'pinyin': 'Méitèriǎl', 'trans': 'Material'},\n{'word': 'Anything', 'pinyin': 'Ēnìthìng', 'trans': 'Anything'},\n{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '统一', 'pinyin': 'tǒngyī', 'trans': 'unified'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '物理', 'pinyin': 'wùlǐ', 'trans': 'physics'},\n{'word': '材质', 'pinyin': 'cáizhì', 'trans': 'material'},\n{'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely on'},\n{'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'},\n{'word': '流水线', 'pinyin': 'liúshuǐxiàn', 'trans': 'pipeline'},\n{'word': '特定', 'pinyin': 'tèdìng', 'trans': 'specific'},\n{'word': '案例', 'pinyin': 'ànlì', 'trans': 'case'},\n{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'},\n{'word': '健壮', 'pinyin': 'jiànzhuàng', 'trans': 'robust'},\n{'word': '端到端', 'pinyin': 'duāndàoduān', 'trans': 'end-to-end'},\n{'word': '解决方案', 'pinyin': 'jiějué fāngàn', 'trans': 'solution'},\n{'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'},\n{'word': '预训练', 'pinyin': 'yùxùnliàn', 'trans': 'pre-trained'},\n{'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'},\n{'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'},\n{'word': '三头架构', 'pinyin': 'sāntóu jiàgòu', 'trans': 'three-headed architecture'},\n{'word': '渲染', 'pinyin': 'xuànrán', 'trans': 'rendering'},\n{'word': '损失', 'pinyin': 'sǔnshī', 'trans': 'loss'},\n{'word': '稳定性', 'pinyin': 'wěndìngxìng', 'trans': 'stability'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '置信', 'pinyin': 'zhìxìn', 'trans': 'confidence'},\n{'word': '掩码', 'pinyin': 'yǎnmǎ', 'trans': 'mask'},\n{'word': '动态', 'pinyin': 'dòngtài', 'trans': 'dynamic'},\n{'word': '开关', 'pinyin': 'kāiguān', 'trans': 'switch'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '纹理', 'pinyin': 'wénlǐ', 'trans': 'texture'},\n{'word': '渐进', 'pinyin': 'jiànjìn', 'trans': 'progressive'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': 'UV空间', 'pinyin': 'UV kōngjiān', 'trans': 'UV space'},\n{'word': '精炼器', 'pinyin': 'jīngliànqì', 'trans': 'refiner'},\n{'word': '确保', 'pinyin': 'quèbǎo', 'trans': 'ensure'},\n{'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'},\n{'word': '准备好', 'pinyin': 'zhǔnbèi hǎo', 'trans': 'ready'},\n{'word': '广泛', 'pinyin': 'guǎngfàn', 'trans': 'extensive'},\n{'word': '类别', 'pinyin': 'lèibié', 'trans': 'category'},\n{'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}]",
        "trans": "We presented Material Anything, a fully automated, unified diffusion framework aimed at generating physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to various lighting conditions for objects. Our approach leverages pre-trained image diffusion models, combined with a tri-head architecture and rendering loss, to enhance stability and material quality. Additionally, we introduced confidence masks as dynamic switches within the diffusion model, enabling it to effectively handle both textured and non-textured objects. By employing a progressive material generation strategy guided by these confidence masks, along with a UV space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate that our method outperforms existing approaches across various object categories and lighting conditions.",
        "update_ts": "2024-11-26 09:11"
    }
}