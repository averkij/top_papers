{
    "date": {
        "ru": "27 ноября",
        "en": "November 27",
        "zh": "11月27日"
    },
    "time_utc": "2024-11-27 03:27",
    "weekday": 2,
    "issue_id": 803,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17116",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "url": "https://huggingface.co/papers/2411.17116",
            "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.",
            "score": 4,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "12194d270104d50f",
            "authors": [
                "Shantanu Acharya",
                "Fei Jia",
                "Boris Ginsburg"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17116.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "⭐",
                "ru": {
                    "title": "Звездное внимание: ускорение LLM без потери точности",
                    "desc": "Статья представляет метод Star Attention для улучшения эффективности вычислений в трансформерных моделях большого языка (LLM) при работе с длинными последовательностями. Метод использует двухфазное блочно-разреженное приближение, распределяя внимание между несколькими узлами и минимизируя накладные расходы на коммуникацию. Star Attention сочетает локальное блочное внимание на первой фазе и глобальное внимание на второй фазе. Этот подход позволяет снизить требования к памяти и время вывода до 11 раз при сохранении 95-100% точности."
                },
                "en": {
                    "title": "Boosting Efficiency in Long Sequence Processing with Star Attention",
                    "desc": "This paper presents Star Attention, a new method designed to enhance the efficiency of Transformer-based Large Language Models (LLMs) when processing long sequences. It addresses the high computational cost and slow inference times caused by the traditional self-attention mechanism's quadratic complexity. Star Attention operates in two phases: first, it uses blockwise-local attention to process context in parallel across multiple hosts, and then it applies sequence-global attention for query and response tokens. This approach significantly reduces memory usage and inference time by up to 11 times while maintaining a high level of accuracy, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "星际注意力：提升长序列推理效率的创新方法",
                    "desc": "本文介绍了一种名为星际注意力（Star Attention）的新方法，旨在提高基于变换器的大型语言模型（LLM）在长序列上的推理效率。该方法通过在多个主机之间分片注意力，采用两阶段的块稀疏近似，显著降低了计算成本。第一阶段使用块局部注意力并行处理上下文，第二阶段则通过全局注意力处理查询和响应标记。星际注意力与大多数使用全局注意力训练的变换器模型无缝集成，能够将内存需求和推理时间减少多达11倍，同时保持95-100%的准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17465",
            "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
            "url": "https://huggingface.co/papers/2411.17465",
            "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI.",
            "score": 2,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "372a78043d62af12",
            "authors": [
                "Kevin Qinghong Lin",
                "Linjie Li",
                "Difei Gao",
                "Zhengyuan Yang",
                "Shiwei Wu",
                "Zechen Bai",
                "Weixian Lei",
                "Lijuan Wang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Microsoft",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17465.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#games",
                    "#optimization",
                    "#cv",
                    "#agents",
                    "#graphs",
                    "#dataset"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "ShowUI: Революция в создании интеллектуальных графических интерфейсов",
                    "desc": "Статья представляет ShowUI - модель для создания графических пользовательских интерфейсов (GUI) с использованием искусственного интеллекта. Модель использует инновационный подход к выбору визуальных токенов на основе структуры интерфейса, что позволяет снизить вычислительные затраты. ShowUI объединяет зрение, язык и действия в потоковом режиме, что делает ее эффективной для различных задач GUI. Модель достигает точности 75.1% при нулевом обучении в задаче локализации элементов на скриншотах, используя всего 256 тысяч примеров для обучения."
                },
                "en": {
                    "title": "Empowering GUI Assistants with Visual Intelligence",
                    "desc": "This paper introduces ShowUI, a vision-language-action model designed to improve GUI assistant capabilities by better understanding visual elements. It innovates with UI-Guided Visual Token Selection to optimize computational efficiency by treating screenshots as connected graphs, which helps in selecting relevant visual tokens. Additionally, it employs Interleaved Vision-Language-Action Streaming to manage visual-action history effectively, enhancing the model's ability to handle complex GUI tasks. The model demonstrates strong performance with a 75.1% accuracy in zero-shot screenshot grounding and significantly reduces redundant visual tokens during training, showcasing its potential in advancing GUI visual agents."
                },
                "zh": {
                    "title": "提升GUI助手效率的视觉-语言-行动模型",
                    "desc": "本研究提出了一种新的视觉-语言-行动模型，名为ShowUI，旨在提升图形用户界面（GUI）助手的效率。该模型通过UI引导的视觉标记选择，减少了计算成本，并在自注意力模块中优化了标记选择过程。ShowUI还实现了交错的视觉-语言-行动流，灵活处理GUI任务中的多样需求，提高了训练效率。通过精心的数据整理和重采样策略，ShowUI在零样本截图定位中达到了75.1%的准确率，展示了其在GUI视觉代理领域的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17223",
            "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
            "url": "https://huggingface.co/papers/2411.17223",
            "abstract": "Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce a disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github.com/mycfhs/DreamMix.",
            "score": 2,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "56eb0ccceda40f61",
            "authors": [
                "Yicheng Yang",
                "Pengxiang Li",
                "Lu Zhang",
                "Liqian Ma",
                "Ping Hu",
                "Siyu Du",
                "Yunzhi Zhuge",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "University of Electronic Science and Technology of China",
                "ZMO AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17223.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "DreamMix: Вставка и редактирование объектов на изображениях с помощью текста",
                    "desc": "DreamMix - это генеративная модель на основе диффузии для вставки объектов в изображения. Она позволяет не только сохранять идентичность объектов, но и редактировать их атрибуты с помощью текстовых запросов. Модель использует усовершенствованный подход к инпейнтингу, сочетающий локальную и глобальную обработку. Также в DreamMix применяется механизм разделения атрибутов и модуль текстовой подстановки атрибутов для улучшения разнообразия и точности управления свойствами объектов."
                },
                "en": {
                    "title": "DreamMix: Seamless Object Insertion and Attribute Editing in Images",
                    "desc": "This paper presents DreamMix, a novel diffusion-based generative model designed for subject-driven image inpainting. Unlike previous methods that prioritize identity preservation, DreamMix allows users to insert objects into images at specified locations while also enabling modifications to their attributes through text prompts. The model employs a disentangled local-global inpainting framework to ensure that inserted objects blend well with the overall scene. Additionally, it introduces mechanisms for attribute decoupling and textual attribute substitution to enhance the diversity and effectiveness of text-based guidance for editing."
                },
                "zh": {
                    "title": "DreamMix：智能图像修复与属性编辑的完美结合",
                    "desc": "本文介绍了一种名为DreamMix的扩散生成模型，旨在实现目标对象在指定场景中的插入，同时允许用户对其属性进行文本驱动的修改。与以往方法不同，DreamMix不仅关注身份保留，还能保持插入对象的可编辑性。我们采用了先进的基础修复模型，并引入了局部-全局分离修复框架，以平衡精确的局部对象插入和有效的全局视觉一致性。此外，本文还提出了属性解耦机制和文本属性替换模块，以提高基于文本的属性指导的多样性和区分能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15411",
            "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
            "url": "https://huggingface.co/papers/2411.15411",
            "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions.   However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning.   Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "54aae052c8dc586b",
            "authors": [
                "Hang Hua",
                "Qing Liu",
                "Lingzhi Zhang",
                "Jing Shi",
                "Zhifei Zhang",
                "Yilin Wang",
                "Jianming Zhang",
                "Jiebo Luo"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15411.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#games",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый подход к композиционному описанию изображений с помощью усовершенствованных мультимодальных языковых моделей",
                    "desc": "В статье представлена новая модель FINECAPTION, способная распознавать произвольные маски и обрабатывать изображения высокого разрешения для композиционного описания изображений на разных уровнях детализации. Авторы также представили новый датасет COMPOSITIONCAP для многоуровневого композиционного описания регионов изображений. Эмпирические результаты демонстрируют эффективность предложенной модели по сравнению с другими современными мультимодальными языковыми моделями. В работе также проанализированы возможности существующих моделей в распознавании различных визуальных подсказок для композиционного описания регионов изображений."
                },
                "en": {
                    "title": "Enhancing Compositional Understanding in Vision-Language Models",
                    "desc": "This paper introduces FINECAPTION, a new Vision-Language Model (VLM) designed to enhance compositional image captioning by accurately recognizing and processing arbitrary segmentation masks. The model addresses the challenge of aligning image regions with their corresponding semantics, which is crucial for generating coherent captions. To facilitate this, the authors present COMPOSITIONCAP, a dataset that focuses on multi-grained region compositional image captioning, allowing for a deeper understanding of visual attributes. Empirical results show that FINECAPTION outperforms existing VLMs, while also identifying areas for further improvement in VLM capabilities."
                },
                "zh": {
                    "title": "提升视觉语言模型的组合能力",
                    "desc": "本文介绍了一种新的视觉语言模型FINECAPTION，旨在提高多模态任务中的图像区域组合信息感知能力。尽管现有的大型视觉语言模型在图像和视频描述等任务中表现出色，但它们在精确对齐分割掩码和语义方面存在困难。FINECAPTION能够识别任意掩码作为参考输入，并处理高分辨率图像，以实现不同粒度级别的组合图像描述。我们还提出了一个新数据集COMPOSITIONCAP，以支持多粒度区域组合图像描述任务，实验结果表明该模型在性能上优于其他先进的视觉语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17467",
            "title": "Learning 3D Representations from Procedural 3D Programs",
            "url": "https://huggingface.co/papers/2411.17467",
            "abstract": "Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations.   Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current self-supervised learning methods primarily capture geometric structures rather than high-level semantics.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "de7061420b319a85",
            "authors": [
                "Xuweiyi Chen",
                "Zezhou Cheng"
            ],
            "affiliations": [
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17467.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Самообучение 3D-представлениям без семантики: геометрия важнее смысла",
                    "desc": "Статья представляет новый подход к самообучению для получения трехмерных представлений из немаркированных облаков точек. Авторы предлагают использовать процедурные 3D-программы для автоматической генерации форм из простых примитивов. Несмотря на отсутствие семантического содержания, полученные представления показывают результаты на уровне современных методов. Исследование также указывает, что текущие методы самообучения в основном фиксируют геометрические структуры, а не высокоуровневую семантику."
                },
                "en": {
                    "title": "Unlocking 3D Learning with Procedural Generation",
                    "desc": "This paper discusses a new method for self-supervised learning to create useful 3D representations from unlabeled 3D point clouds. The authors highlight the difficulty of obtaining 3D data due to the need for specialized equipment and the associated copyright issues. They propose using procedural 3D programs that generate shapes from basic building blocks, allowing for scalable data generation. The results show that these representations, although generated without semantic meaning, perform comparably to those derived from labeled 3D models in various tasks like shape classification and segmentation."
                },
                "zh": {
                    "title": "自监督学习：从程序化生成中获取3D表示",
                    "desc": "自监督学习是一种有前景的方法，可以从未标记的3D点云中获取可转移的3D表示。与2D图像不同，获取3D资产需要专业知识或专业的3D扫描设备，这使得其难以扩展并引发版权问题。为了解决这些挑战，我们提出从程序化3D程序中学习3D表示，这些程序使用简单的原始体和增强技术自动生成3D形状。值得注意的是，尽管缺乏语义内容，但从合成数据集中学习的3D表示在各种下游3D任务中表现与从语义可识别的3D模型（如飞机）学习的最先进表示相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14740",
            "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
            "url": "https://huggingface.co/papers/2411.14740",
            "abstract": "While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis. Project page is at http://cvmi-lab.github.io/TEXGen/.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "79b09429e72c3c18",
            "authors": [
                "Xin Yu",
                "Ze Yuan",
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "JianHui Liu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Ding Liang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, Hong Kong",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14740.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в генерации 3D-текстур: обучение диффузионной модели прямо в UV-пространстве",
                    "desc": "Статья представляет новый подход к генерации текстурных карт для 3D-объектов. Авторы обучили крупную диффузионную модель, способную напрямую создавать высококачественные текстуры в UV-пространстве. Предложена масштабируемая архитектура нейронной сети, сочетающая свёрточные слои на UV-картах с слоями внимания на облаках точек. Обученная модель может генерировать текстуры на основе текстовых подсказок и изображений с одного ракурса, а также поддерживает задачи инпейнтинга и дополнения текстур."
                },
                "en": {
                    "title": "Revolutionizing 3D Textures: Direct Learning in UV Space",
                    "desc": "This paper presents a novel approach to generating high-quality texture maps for 3D assets by directly learning in the UV texture space. Unlike traditional methods that use pre-trained 2D models, the authors introduce a large diffusion model that generates textures in a feed-forward manner. They propose a scalable network architecture that combines convolutional operations on UV maps with attention mechanisms on point clouds, enabling efficient learning. The resulting model, with 700 million parameters, can create UV texture maps based on text prompts and single-view images, and supports various applications like texture inpainting and synthesis."
                },
                "zh": {
                    "title": "直接在UV纹理空间中生成高质量纹理图",
                    "desc": "本研究提出了一种新的方法，直接在UV纹理空间中学习高质量的纹理图。我们训练了一个大型扩散模型，能够以前馈方式生成高分辨率的纹理图，而不是依赖于预训练的2D扩散模型。该模型具有7亿个参数，能够根据文本提示和单视图图像生成UV纹理图。我们的架构设计结合了卷积和注意力层，使得在高分辨率UV空间中的学习更加高效，并支持多种扩展应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17383",
            "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
            "url": "https://huggingface.co/papers/2411.17383",
            "abstract": "The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Additionally, we introduce the HOI-region reweighting loss, a training objective that enhances the learning of object details. Extensive experiments demonstrate that our proposed system outperforms existing methods in preserving object appearance and shape awareness, while simultaneously maintaining consistency in human appearance and motion. Project page: https://cangcz.github.io/Anchor-Crafter/",
            "score": 0,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "8deec5510490c901",
            "authors": [
                "Ziyi Xu",
                "Ziyao Huang",
                "Juan Cao",
                "Yong Zhang",
                "Xiaodong Cun",
                "Qing Shuai",
                "Yuchen Wang",
                "Linchao Bao",
                "Jintao Li",
                "Fan Tang"
            ],
            "affiliations": [
                "Great Bay University",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "Meituan",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17383.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "AnchorCrafter: ИИ создает реалистичные промо-видео с взаимодействием человека и товара",
                    "desc": "Статья представляет AnchorCrafter - новую систему на основе диффузии для генерации видео с взаимодействием человека и объекта. Система решает проблему интеграции взаимодействий человека с объектами в генерацию видео на основе поз. AnchorCrafter использует инновационные подходы HOI-appearance perception и HOI-motion injection для улучшения распознавания внешнего вида объекта и управления сложными взаимодействиями. Эксперименты показывают, что система превосходит существующие методы в сохранении внешнего вида объекта при поддержании согласованности внешнего вида и движений человека."
                },
                "en": {
                    "title": "Revolutionizing Product Promotion with AnchorCrafter!",
                    "desc": "This paper presents AnchorCrafter, a new system for generating promotional videos that feature a person interacting with a product. It focuses on improving how humans and objects are represented together in videos by using advanced techniques in pose-guided video generation. The system introduces two main innovations: one that improves how objects are recognized from different angles and another that allows for realistic interactions between humans and objects. The results show that AnchorCrafter produces videos with better object appearance and human motion consistency compared to existing methods."
                },
                "zh": {
                    "title": "锚点风格视频生成的新突破",
                    "desc": "本论文提出了一种名为AnchorCrafter的新型扩散系统，旨在自动生成锚点风格的产品推广视频。该系统通过整合人-物交互（HOI）来提升基于姿态的人类视频生成效果，解决了复杂的人-物交互问题。我们提出了HOI-外观感知和HOI-运动注入两个关键创新，前者改善了物体外观的识别，后者则增强了人-物交互的复杂性。实验结果表明，AnchorCrafter在物体外观和形状保持方面优于现有方法，同时保持了人类外观和运动的一致性。"
                }
            }
        }
    ],
    "link_prev": "2024-11-26.html",
    "link_next": "2024-11-28.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "short_date_next": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11月28日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们展示了Material Anything，一个完全自动化、统一的扩散框架，旨在为3D物体生成基于物理的材质。与依赖复杂流水线或特定案例优化的现有方法不同，Material Anything提供了一个适应多种光照条件下物体的健壮、端到端的解决方案。我们的方法利用了预训练的图像扩散模型，结合三头架构和渲染损失，以提高稳定性和材质质量。此外，我们引入了置信掩码作为扩散模型内的动态开关，使其能够有效处理有纹理和无纹理的物体。通过使用这些置信掩码指导的渐进材质生成策略，以及UV空间材质精炼器，我们的方法确保了一致的、UV准备好的材质输出。广泛的实验表明，我们的方法在各种物体类别和光照条件下优于现有方法。",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "pinyin": "Wǒmen zhǎnshìle Material Anything, yīgè wánquán zìdònghuà, tǒngyī de kuòsàn kuàngjià, zhǐzài wèi 3D wùtǐ shēngchéng jīyǐ wùlǐ de cáizhì. Yǔ yīlài fùzá xiùshuǐxiàn huò tèdìng ànliào yōuhuà de xiàn yǒu fāngfǎ bùtóng, Material Anything tígōngle yīgè shìyìng duōzhǒng guāngzhào tiáojiàn xià wùtǐ de jiànkāng, duān dào duān de jiějué fāng'àn. Wǒmen de fāngfǎ lìyòngle yù xùnliàn de túxiàng kuòsàn móxíng, jiéhé sān tóu jiàgòu hé xuànshēi sǔnshī, yǐ tígāo wěndìngxìng hé cáizhì zhìliàng. Cǐwài, wǒmen yǐn rùle zhìxìn mózhào zuòwéi kuòsàn móxíng nèi de dòngtài kāiguān, shǐ qí nénggòu yǒuxiào chǔlǐ yǒu wénlǐ hé wú wénlǐ de wùtǐ. Tōngguò shǐyòng zhèxiē zhìxìn mózhào zhǐdǎo de jiànjìn cáizhì shēngchéng cèlüè, yǐjiǎ UV kōngjiān cáizhì jīngliànqì, wǒmen de fāngfǎ quèbǎole yīzhì de, UV zhǔnbèi hǎo de cáizhì shūchū. Guǎngfàn de shìyàn biǎomíng, wǒmen de fāngfǎ zài gèzhǒng wùtǐ lèibié hé guāngzhào tiáojiàn xià yōu yú xiàn yǒu fāngfǎ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'},\n{'word': 'Material', 'pinyin': 'Méitèriǎl', 'trans': 'Material'},\n{'word': 'Anything', 'pinyin': 'Ēnìthìng', 'trans': 'Anything'},\n{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '统一', 'pinyin': 'tǒngyī', 'trans': 'unified'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '物理', 'pinyin': 'wùlǐ', 'trans': 'physics'},\n{'word': '材质', 'pinyin': 'cáizhì', 'trans': 'material'},\n{'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely on'},\n{'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'},\n{'word': '流水线', 'pinyin': 'liúshuǐxiàn', 'trans': 'pipeline'},\n{'word': '特定', 'pinyin': 'tèdìng', 'trans': 'specific'},\n{'word': '案例', 'pinyin': 'ànlì', 'trans': 'case'},\n{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'},\n{'word': '健壮', 'pinyin': 'jiànzhuàng', 'trans': 'robust'},\n{'word': '端到端', 'pinyin': 'duāndàoduān', 'trans': 'end-to-end'},\n{'word': '解决方案', 'pinyin': 'jiějué fāngàn', 'trans': 'solution'},\n{'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'},\n{'word': '预训练', 'pinyin': 'yùxùnliàn', 'trans': 'pre-trained'},\n{'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'},\n{'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'},\n{'word': '三头架构', 'pinyin': 'sāntóu jiàgòu', 'trans': 'three-headed architecture'},\n{'word': '渲染', 'pinyin': 'xuànrán', 'trans': 'rendering'},\n{'word': '损失', 'pinyin': 'sǔnshī', 'trans': 'loss'},\n{'word': '稳定性', 'pinyin': 'wěndìngxìng', 'trans': 'stability'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '置信', 'pinyin': 'zhìxìn', 'trans': 'confidence'},\n{'word': '掩码', 'pinyin': 'yǎnmǎ', 'trans': 'mask'},\n{'word': '动态', 'pinyin': 'dòngtài', 'trans': 'dynamic'},\n{'word': '开关', 'pinyin': 'kāiguān', 'trans': 'switch'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '纹理', 'pinyin': 'wénlǐ', 'trans': 'texture'},\n{'word': '渐进', 'pinyin': 'jiànjìn', 'trans': 'progressive'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': 'UV空间', 'pinyin': 'UV kōngjiān', 'trans': 'UV space'},\n{'word': '精炼器', 'pinyin': 'jīngliànqì', 'trans': 'refiner'},\n{'word': '确保', 'pinyin': 'quèbǎo', 'trans': 'ensure'},\n{'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'},\n{'word': '准备好', 'pinyin': 'zhǔnbèi hǎo', 'trans': 'ready'},\n{'word': '广泛', 'pinyin': 'guǎngfàn', 'trans': 'extensive'},\n{'word': '类别', 'pinyin': 'lèibié', 'trans': 'category'},\n{'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}]",
        "trans": "We presented Material Anything, a fully automated, unified diffusion framework aimed at generating physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to various lighting conditions for objects. Our approach leverages pre-trained image diffusion models, combined with a tri-head architecture and rendering loss, to enhance stability and material quality. Additionally, we introduced confidence masks as dynamic switches within the diffusion model, enabling it to effectively handle both textured and non-textured objects. By employing a progressive material generation strategy guided by these confidence masks, along with a UV space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate that our method outperforms existing approaches across various object categories and lighting conditions.",
        "update_ts": "2024-11-26 09:11"
    }
}