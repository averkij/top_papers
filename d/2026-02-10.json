{
    "date": {
        "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 10",
        "zh": "2æœˆ10æ—¥"
    },
    "time_utc": "2026-02-10 04:27",
    "weekday": 1,
    "issue_id": 980,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.07026",
            "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2602.07026",
            "abstract": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
            "score": 32,
            "issue_id": 980,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "56c55d551e82f8ed",
            "authors": [
                "Xiaomin Yu",
                "Yi Xin",
                "Wenjie Zhang",
                "Chonghan Liu",
                "Hanzhen Zhao",
                "Xiaoxing Hu",
                "Xinlei Yu",
                "Ziyue Qiao",
                "Hao Tang",
                "Xue Yang",
                "Xiaobin Hu",
                "Chengwei Qin",
                "Hui Xiong",
                "Yu Qiao",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "GBU",
                "HKUST(GZ)",
                "NUS",
                "PKU",
                "SII",
                "SJTU",
                "UCLA",
                "sh AILab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07026.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReAlign, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReAlign Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ReVision Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Modality Gap for Efficient Multimodal Learning",
                    "desc": "This paper tackles the challenge of the Modality Gap in multimodal learning, which occurs when different types of data (like images and text) that mean the same thing are not aligned properly in their representation spaces. The authors introduce a new theory called Fixed-frame Modality Gap Theory, which helps to understand and decompose the geometric misalignment between these modalities. They propose a method called ReAlign that aligns text and image representations without needing extensive training, using unpaired data instead. Finally, they present ReVision, a scalable approach for training Multimodal Large Language Models that leverages this alignment to improve learning efficiency without relying on large sets of paired data."
                },
                "zh": {
                    "title": "è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å›ºå®šæ¡†æ¶ç†è®ºå’Œæ— è®­ç»ƒå¯¹é½æ–¹æ³•ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·é—®é¢˜ã€‚æ¨¡æ€å·®è·æ˜¯æŒ‡ä¸åŒæ¨¡æ€çš„åµŒå…¥åœ¨å‡ ä½•ä¸Šå­˜åœ¨ç³»ç»Ÿæ€§åç§»ï¼Œå¯¼è‡´ç›¸åŒè¯­ä¹‰çš„è¡¨ç¤ºä¸åœ¨åŒä¸€åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡ç²¾ç¡®æè¿°æ¨¡æ€å·®è·çš„å‡ ä½•å½¢çŠ¶ï¼Œæå‡ºäº†ReAlignæ–¹æ³•ï¼Œåˆ©ç”¨å¤§é‡æœªé…å¯¹æ•°æ®è¿›è¡Œæ¨¡æ€å¯¹é½ã€‚åŸºäºReAlignï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ReVisionè®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§è§„æ¨¡é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ‰©å±•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06422",
            "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
            "url": "https://huggingface.co/papers/2602.06422",
            "abstract": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
            "score": 16,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "92898f90dd66cb60",
            "authors": [
                "Yunze Tong",
                "Mushui Liu",
                "Canyu Zhao",
                "Wanggui He",
                "Shiyi Zhang",
                "Hongwei Zhang",
                "Peng Zhang",
                "Jinlong Liu",
                "Ju Huang",
                "Jiamang Wang",
                "Hao Jiang",
                "Pipei Huang"
            ],
            "affiliations": [
                "Alibaba Group, Hangzhou, China",
                "Tsinghua University, Beijing, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06422.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "TP-GRPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ â€” Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ³Ğ´Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ´ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ â€” Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¸Ğ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TP-GRPO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Reward Signals for Better Denoising in Flow Matching",
                    "desc": "TP-GRPO is a novel framework designed to improve reward mechanisms in flow matching models, particularly for text-to-image generation tasks. It introduces step-level incremental rewards that provide a more detailed learning signal, allowing for better isolation of the effects of individual denoising actions. Additionally, TP-GRPO identifies turning points in the reward trend, which are critical steps that influence future rewards, thus capturing long-term dependencies in the denoising process. This approach enhances the efficiency of reward utilization and leads to consistent improvements in generation quality."
                },
                "zh": {
                    "title": "TP-GRPOï¼šæå‡å»å™ªæ¨¡å‹çš„å¥–åŠ±åˆ©ç”¨æ•ˆç‡",
                    "desc": "TP-GRPOæ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æµåŒ¹é…æ¨¡å‹ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥é€æ­¥å¢é‡å¥–åŠ±å’Œè¯†åˆ«è½¬æŠ˜ç‚¹æ¥æ•æ‰å»å™ªè½¨è¿¹ä¸­çš„é•¿æœŸå½±å“ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTP-GRPOä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤æä¾›äº†æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«é‚£äº›å½±å“åç»­å¥–åŠ±çš„å…³é”®æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTP-GRPOåœ¨ç”Ÿæˆä»»åŠ¡ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07962",
            "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
            "url": "https://huggingface.co/papers/2602.07962",
            "abstract": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
            "score": 11,
            "issue_id": 980,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "483ad401f8702de9",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07962.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#long_context",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LOCA-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ€Ğ¾ÑÑ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ (ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Â«ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ğ´Ğ°Â»). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, LOCA-bench Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Language Agents in Dynamic Long-Context Scenarios",
                    "desc": "LOCA-bench is a new benchmark designed to evaluate language agents in complex, long-context scenarios where they must manage their environment effectively. It addresses the issue of 'context rot', where the performance of large language models declines as the context length increases. Unlike existing benchmarks that focus on single-step tasks, LOCA-bench allows for dynamic context management, enabling agents to handle tasks that require exploration and decision-making over extended interactions. By using advanced context management strategies, LOCA-bench aims to improve the reliability and success rates of language agents in real-world applications."
                },
                "zh": {
                    "title": "LOCA-benchï¼šé•¿ä¸Šä¸‹æ–‡ä»£ç†çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "LOCA-benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†åœ¨é•¿ä¸Šä¸‹æ–‡å’Œä»£ç†åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„ç¯å¢ƒçŠ¶æ€æ§åˆ¶ï¼Œè°ƒèŠ‚ä»£ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥åº”å¯¹å¤æ‚çš„ä»»åŠ¡ã€‚å°½ç®¡ç¯å¢ƒçŠ¶æ€çš„å¤æ‚æ€§ä¼šå¯¼è‡´ä»£ç†æ€§èƒ½ä¸‹é™ï¼Œä½†å…ˆè¿›çš„ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æˆåŠŸç‡ã€‚LOCA-benchä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾çš„å¹³å°ï¼Œä»¥è¯„ä¼°åœ¨é•¿ä¸Šä¸‹æ–‡ä»£ç†åœºæ™¯ä¸­çš„æ¨¡å‹å’Œæ”¯æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06454",
            "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2602.06454",
            "abstract": "RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
            "score": 10,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "5323131100597ece",
            "authors": [
                "Jiwon Song",
                "Yoongon Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06454.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "RelayGen â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° (Ğ´Ğ¾ 2.2 Ñ€Ğ°Ğ·) Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. RelayGen Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Dynamic Model Switching for Efficient Reasoning",
                    "desc": "RelayGen is a novel framework designed to enhance the efficiency of large reasoning models (LRMs) by dynamically switching between large and small models based on the difficulty of reasoning tasks. It identifies transitions in difficulty at the segment level, allowing for faster inference without significant loss in accuracy. Unlike traditional methods that either overlook difficulty variations or require complex supervised routing, RelayGen operates without additional training, making it simpler and more effective. By leveraging token probability margins to analyze generation uncertainty, RelayGen achieves substantial speed improvements while maintaining high performance across various reasoning benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆæ¨ç†",
                    "desc": "RelayGen æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ‡æ¢å¤§æ¨¡å‹å’Œå°æ¨¡å‹ï¼Œæ¥è¯†åˆ«æ®µçº§åˆ«çš„éš¾åº¦å˜åŒ–ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ä¸”å‡ ä¹ä¸æŸå¤±å‡†ç¡®æ€§ã€‚å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†æ—¶çš„æ‰©å±•ä¼šå¸¦æ¥é«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ã€‚RelayGen é€šè¿‡ç¦»çº¿åˆ†æç”Ÿæˆä¸ç¡®å®šæ€§ï¼Œåˆ©ç”¨ç²—ç²’åº¦çš„æ®µçº§æ§åˆ¶æ¥æ•æ‰æ¨ç†è½¨è¿¹ä¸­çš„éš¾åº¦å˜åŒ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹ç‰¹å®šçš„åˆ‡æ¢ä¿¡å·ï¼Œå°†ä½éš¾åº¦æ®µçš„æ¨ç†äº¤ç»™å°æ¨¡å‹å¤„ç†ï¼ŒåŒæ—¶ä¿æŒé«˜éš¾åº¦æ¨ç†åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07075",
            "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
            "url": "https://huggingface.co/papers/2602.07075",
            "abstract": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
            "score": 10,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "8c5ebd4aa9539a12",
            "authors": [
                "Xinwu Ye",
                "Yicheng Mao",
                "Jia Zhang",
                "Yimeng Liu",
                "Li Hao",
                "Fang Wu",
                "Zhiwei Li",
                "Yuxuan Liao",
                "Zehong Wang",
                "Zhiyuan Liu",
                "Zhenfei Yin",
                "Li Yuan",
                "Philip Torr",
                "Huan Sun",
                "Xiangxiang Zeng",
                "Mengdi Wang",
                "Le Cong",
                "Shenghua Gao",
                "Xiangru Tang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.07075.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning"
                ],
                "emoji": "âš—ï¸",
                "ru": {
                    "title": "Ğ¥Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸",
                    "desc": "LatentChem Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 59.88% Ğ¿Ğ¾Ğ±ĞµĞ´Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 10.84-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Chemical Reasoning with Continuous Latent Dynamics",
                    "desc": "LatentChem is a novel approach that enhances chemical reasoning by utilizing continuous latent space computations instead of relying on discrete textual tokens. Traditional methods, which use Chain-of-Thought (CoT) reasoning, often struggle with the inherent continuous nature of chemical data, leading to inefficiencies. By allowing models to perform multi-step reasoning directly in latent space, LatentChem improves both performance and speed, achieving a significant win rate over existing CoT-based methods. This research demonstrates that chemical reasoning benefits from a continuous representation, resulting in faster and more effective computations."
                },
                "zh": {
                    "title": "åŒ–å­¦æ¨ç†çš„æ–°æ–¹æ³•ï¼šæ½œåœ¨ç©ºé—´è®¡ç®—",
                    "desc": "LatentChem æ˜¯ä¸€ç§æ–°çš„åŒ–å­¦æ¨ç†æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿ç»­çš„æ½œåœ¨ç©ºé—´è®¡ç®—æ¥å®ç°ï¼Œè€Œä¸æ˜¯ä¾èµ–äºç¦»æ•£çš„æ–‡æœ¬ç¬¦å·ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿé“¾å¼æ€ç»´åœ¨åŒ–å­¦æ¨ç†ä¸­å­˜åœ¨çš„è¡¨ç¤ºä¸åŒ¹é…é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹ä»…ä¼˜åŒ–ä»»åŠ¡æˆåŠŸæ—¶ï¼Œå®ƒä»¬ä¼šè‡ªå‘åœ°å†…åŒ–æ¨ç†ï¼Œé€æ¸æ”¾å¼ƒå†—é•¿çš„æ–‡æœ¬æ¨å¯¼ï¼Œè½¬è€Œä½¿ç”¨éšå¼çš„æ½œåœ¨è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLatentChem åœ¨åŒ–å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06540",
            "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
            "url": "https://huggingface.co/papers/2602.06540",
            "abstract": "AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
            "score": 5,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "99c9091242c48388",
            "authors": [
                "Yishan Li",
                "Wentong Chen",
                "Yukun Yan",
                "Mingwei Li",
                "Sen Mei",
                "Xiaorong Wang",
                "Kunpeng Liu",
                "Xin Cong",
                "Shuo Wang",
                "Zhong Zhang",
                "Yaxi Lu",
                "Zhenghao Liu",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06540.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "AgentCPM-Report Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Writing As Reasoning Policy Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ²Ğ¸Ğ·Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RL, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Small Models for Insightful Research Reports",
                    "desc": "AgentCPM-Report introduces a novel approach for generating deep research reports using a lightweight local model that enhances reasoning and outline development. It employs a Writing As Reasoning Policy (WARP) that allows the model to iteratively refine its outline while drafting the report. This method addresses the limitations of traditional plan-then-write strategies by integrating evidence-based drafting with reasoning-driven enhancements. The Multi-Stage Agentic Training strategy further empowers smaller models to achieve performance levels comparable to larger, closed-source systems, while ensuring user data privacy and safety."
                },
                "zh": {
                    "title": "è½»é‡çº§æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæå‡æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›",
                    "desc": "AgentCPM-Report æ˜¯ä¸€ç§è½»é‡çº§çš„æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å†™ä½œä½œä¸ºæ¨ç†æ”¿ç­–æ¡†æ¶å’Œå¤šé˜¶æ®µä»£ç†è®­ç»ƒæ¥ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šæ—¶é¢ä¸´çš„ä¿¡æ¯è·å–å’Œåˆ†æåˆæˆçš„æŒ‘æˆ˜ã€‚é€šè¿‡åŠ¨æ€ä¿®è®¢å¤§çº²ï¼ŒAgentCPM-Report æé«˜äº†å°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¤§çº²æ¼”å˜èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„é—­æºç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æ´å¯ŸåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21363",
            "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
            "url": "https://huggingface.co/papers/2601.21363",
            "abstract": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "2f5a5c620860610c",
            "authors": [
                "Weidong Huang",
                "Zhehan Li",
                "Hangxin Liu",
                "Biao Hou",
                "Yao Su",
                "Jingwen Zhang"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Xidian University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Soft Actor-Critic Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (UTD) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ğ° ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞºĞ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Humanoid Control: Bridging Pretraining and Adaptation",
                    "desc": "This paper presents an approach that combines off-policy Soft Actor-Critic (SAC) with large-batch updates to enhance the pretraining of humanoid locomotion policies. By leveraging high Update-To-Data (UTD) ratios, the method achieves efficient zero-shot deployment of these policies on real robots. For adapting to new environments, the authors utilize model-based techniques that allow for safe data collection through deterministic policies while maintaining stochastic exploration in a physics-informed world model. This strategy effectively bridges the gap between large-scale pretraining and efficient fine-tuning, ensuring both safety and performance in humanoid control tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆç±»äººæ­¥æ€ç­–ç•¥é¢„è®­ç»ƒä¸å®‰å…¨é€‚åº”",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¦»çº¿ç­–ç•¥çš„è½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰æ–¹æ³•ï¼Œç»“åˆå¤§æ‰¹é‡æ›´æ–°ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œç±»äººæ­¥æ€ç­–ç•¥çš„é¢„è®­ç»ƒã€‚é€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ–¹æ³•ï¼Œå®‰å…¨é€‚åº”æ–°ç¯å¢ƒï¼Œé‡‡ç”¨ç¡®å®šæ€§æ•°æ®æ”¶é›†å’Œç‰©ç†çŸ¥è¯†é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œéšæœºæ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡é¢„è®­ç»ƒçš„SACç­–ç•¥å¯ä»¥åœ¨æ–°ç¯å¢ƒå’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿é€‚åº”è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚æ•´ä½“æ–¹æ³•å°†å¤§è§„æ¨¡ä»¿çœŸé¢„è®­ç»ƒçš„æ—¶é—´æ•ˆç‡ä¸æ¨¡å‹é©±åŠ¨å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06694",
            "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
            "url": "https://huggingface.co/papers/2602.06694",
            "abstract": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
            "score": 2,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "164cf8bc03ab7d1b",
            "authors": [
                "Hyochan Chong",
                "Dongkyu Kim",
                "Changdong Kim",
                "Minseop Choi"
            ],
            "affiliations": [
                "Samsung Research, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06694.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "NanoQuant â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ (1-Ğ±Ğ¸Ñ‚) Ğ¸ ÑÑƒĞ±Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ° (ADMM). ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NanoQuant: Revolutionizing Model Compression for Consumer Hardware",
                    "desc": "NanoQuant is a novel method for post-training quantization of large language models (LLMs) that allows them to be compressed to binary and sub-1-bit levels. It uses low-rank binary factorization to convert full-precision weights into low-rank binary matrices, significantly reducing memory usage. The method employs an efficient alternating direction method of multipliers (ADMM) for precise initialization and tuning of these binary matrices. As a result, NanoQuant achieves state-of-the-art accuracy while making it possible to deploy large models on consumer hardware, such as compressing Llama2-70B by 25.8 times."
                },
                "zh": {
                    "title": "NanoQuantï¼šé«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–°æ–¹æ³•",
                    "desc": "NanoQuantæ˜¯ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©åˆ°äºŒè¿›åˆ¶å’Œä½äº1ä½çš„æ°´å¹³ã€‚å®ƒé€šè¿‡ä½ç§©äºŒè¿›åˆ¶åˆ†è§£å’Œäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨å‡å°‘å†…å­˜éœ€æ±‚çš„åŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚NanoQuantå°†é‡åŒ–é—®é¢˜å½¢å¼åŒ–ä¸ºä½ç§©äºŒè¿›åˆ¶åˆ†è§£ï¼Œèƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°å¤§è§„æ¨¡éƒ¨ç½²ã€‚è¯¥æ–¹æ³•åœ¨ä»…ç”¨13å°æ—¶å†…å°†Llama2-70Bæ¨¡å‹å‹ç¼©äº†25.8å€ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨8GBçš„æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08236",
            "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
            "url": "https://huggingface.co/papers/2602.08236",
            "abstract": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
            "score": 1,
            "issue_id": 980,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "93306b7f7aeb8f16",
            "authors": [
                "Shoubin Yu",
                "Yue Zhang",
                "Zun Wang",
                "Jaehong Yoon",
                "Huaxiu Yao",
                "Mingyu Ding",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Department of Computer Science, University of North Carolina, Chapel Hill",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08236.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ĞµÑ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AVIC â€” Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ world models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½ÑƒĞ¶ĞµĞ½ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº world models."
                },
                "en": {
                    "title": "Selective Imagination for Smarter Spatial Reasoning",
                    "desc": "This paper introduces an adaptive framework called AVIC that enhances spatial reasoning by selectively using visual imagination based on the sufficiency of current visual evidence. It addresses the challenges of indiscriminate imagination, which can lead to increased computation and degraded performance. The study analyzes when visual imagination is beneficial and when it can be harmful, providing insights into optimizing its use. The results demonstrate that controlled imagination can improve efficiency and accuracy in spatial reasoning tasks compared to fixed strategies."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è§†è§‰æƒ³è±¡æå‡ç©ºé—´æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æµ‹è¯•æ—¶æ¡†æ¶ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹æ¥é€‰æ‹©æ€§åœ°è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œä»¥æé«˜ç©ºé—´æ¨ç†çš„æ•ˆç‡å’Œå¯é æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé™æ€è§†è§‰è¯æ®å·²ç»è¶³å¤Ÿï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæƒ³è±¡å¯ä»¥æ”¹å–„æ¨ç†æ•ˆæœï¼Œä½†è¿‡åº¦çš„æƒ³è±¡å¯èƒ½ä¼šå¯¼è‡´å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸‹é™ã€‚é€šè¿‡å¼•å…¥AVICæ¡†æ¶ï¼Œè®ºæ–‡åˆ†æäº†ä½•æ—¶éœ€è¦æƒ³è±¡ä»¥åŠæƒ³è±¡çš„é€‚åº¦ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‰æ‹©æ€§æ§åˆ¶çš„æƒ³è±¡ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå›ºå®šçš„æƒ³è±¡ç­–ç•¥ï¼Œå‡å°‘äº†ä¸–ç•Œæ¨¡å‹çš„è°ƒç”¨å’Œè¯­è¨€æ ‡è®°çš„ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06445",
            "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
            "url": "https://huggingface.co/papers/2602.06445",
            "abstract": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
            "score": 0,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a859fde68e6d19ae",
            "authors": [
                "Weidong Huang",
                "Jingwen Zhang",
                "Jiongye Li",
                "Shibowen Zhang",
                "Jiayang Wu",
                "Jiayi Wang",
                "Hangxin Liu",
                "Yaodong Yang",
                "Yao Su"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Department of Automation, University of Science and Technology of China",
                "Department of Computer Science, Harbin Institute of Technology",
                "Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06445.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ECO (Energy-Constrained Optimization) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ-Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶ĞµĞ² Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ BRUCE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ECO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ…Ğ¾Ğ´ÑŒĞ±Ğµ."
                },
                "en": {
                    "title": "ECO: Revolutionizing Energy-Efficient Humanoid Robot Locomotion",
                    "desc": "The paper presents a new framework called Energy-Constrained Optimization (ECO) for improving the locomotion of humanoid robots. It separates energy metrics from rewards using a Lagrangian method, allowing for clearer constraints on energy consumption. This approach reduces the need for extensive hyperparameter tuning, leading to more efficient and stable walking patterns. Experimental results show that ECO significantly lowers energy use while maintaining robust performance compared to existing methods."
                },
                "zh": {
                    "title": "èƒ½é‡çº¦æŸä¼˜åŒ–ï¼šæå‡ç±»äººæœºå™¨äººè¡Œèµ°æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½é‡çº¦æŸä¼˜åŒ–æ¡†æ¶ï¼ˆECOï¼‰ï¼Œæ—¨åœ¨æé«˜ç±»äººæœºå™¨äººåœ¨è¡Œèµ°æ—¶çš„èƒ½é‡æ•ˆç‡å’Œç¨³å®šæ€§ã€‚é€šè¿‡å°†èƒ½é‡ç›¸å…³æŒ‡æ ‡ä¸å¥–åŠ±åˆ†ç¦»ï¼Œå¹¶å°†å…¶é‡æ–°è¡¨è¿°ä¸ºæ˜ç¡®çš„ä¸ç­‰å¼çº¦æŸï¼ŒECOç®€åŒ–äº†è¶…å‚æ•°è°ƒä¼˜çš„è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥æ–¹æ³•å¼•å…¥ä¸“é—¨çš„èƒ½é‡æ¶ˆè€—å’Œå‚è€ƒè¿åŠ¨çº¦æŸï¼Œä»è€Œå®ç°å¯¹ç±»äººæœºå™¨äººç¨³å®šã€å¯¹ç§°ä¸”é«˜æ•ˆçš„è¡Œèµ°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECOåœ¨èƒ½é‡æ¶ˆè€—ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è¡Œèµ°æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-09.html",
    "link_next": "2026-02-11.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "09.02",
        "en": "02/09",
        "zh": "2æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    }
}