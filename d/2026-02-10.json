{
    "date": {
        "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 10",
        "zh": "2æœˆ10æ—¥"
    },
    "time_utc": "2026-02-10 07:56",
    "weekday": 1,
    "issue_id": 982,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.07026",
            "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2602.07026",
            "abstract": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
            "score": 72,
            "issue_id": 980,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "56c55d551e82f8ed",
            "authors": [
                "Xiaomin Yu",
                "Yi Xin",
                "Wenjie Zhang",
                "Chonghan Liu",
                "Hanzhen Zhao",
                "Xiaoxing Hu",
                "Xinlei Yu",
                "Ziyue Qiao",
                "Hao Tang",
                "Xue Yang",
                "Xiaobin Hu",
                "Chengwei Qin",
                "Hui Xiong",
                "Yu Qiao",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "GBU",
                "HKUST(GZ)",
                "NUS",
                "PKU",
                "SII",
                "SJTU",
                "UCLA",
                "sh AILab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07026.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReAlign, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReAlign Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ReVision Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Modality Gap for Efficient Multimodal Learning",
                    "desc": "This paper tackles the challenge of the Modality Gap in multimodal learning, which occurs when different types of data (like images and text) that mean the same thing are not aligned properly in their representation spaces. The authors introduce a new theory called Fixed-frame Modality Gap Theory, which helps to understand and decompose the geometric misalignment between these modalities. They propose a method called ReAlign that aligns text and image representations without needing extensive training, using unpaired data instead. Finally, they present ReVision, a scalable approach for training Multimodal Large Language Models that leverages this alignment to improve learning efficiency without relying on large sets of paired data."
                },
                "zh": {
                    "title": "è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å›ºå®šæ¡†æ¶ç†è®ºå’Œæ— è®­ç»ƒå¯¹é½æ–¹æ³•ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·é—®é¢˜ã€‚æ¨¡æ€å·®è·æ˜¯æŒ‡ä¸åŒæ¨¡æ€çš„åµŒå…¥åœ¨å‡ ä½•ä¸Šå­˜åœ¨ç³»ç»Ÿæ€§åç§»ï¼Œå¯¼è‡´ç›¸åŒè¯­ä¹‰çš„è¡¨ç¤ºä¸åœ¨åŒä¸€åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡ç²¾ç¡®æè¿°æ¨¡æ€å·®è·çš„å‡ ä½•å½¢çŠ¶ï¼Œæå‡ºäº†ReAlignæ–¹æ³•ï¼Œåˆ©ç”¨å¤§é‡æœªé…å¯¹æ•°æ®è¿›è¡Œæ¨¡æ€å¯¹é½ã€‚åŸºäºReAlignï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ReVisionè®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§è§„æ¨¡é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ‰©å±•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08794",
            "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
            "url": "https://huggingface.co/papers/2602.08794",
            "abstract": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
            "score": 61,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "4f8806b98c1c0d7e",
            "authors": [
                "SII-OpenMOSS Team",
                ":",
                "Donghua Yu",
                "Mingshu Chen",
                "Qi Chen",
                "Qi Luo",
                "Qianyi Wu",
                "Qinyuan Cheng",
                "Ruixiao Li",
                "Tianyi Liang",
                "Wenbo Zhang",
                "Wenming Tu",
                "Xiangyu Peng",
                "Yang Gao",
                "Yanru Huo",
                "Ying Zhu",
                "Yinze Luo",
                "Yiyang Zhang",
                "Yuerong Song",
                "Zhe Xu",
                "Zhiyu Zhang",
                "Chenchen Yang",
                "Cheng Chang",
                "Chushu Zhou",
                "Hanfu Chen",
                "Hongnan Ma",
                "Jiaxi Li",
                "Jingqi Tong",
                "Junxi Liu",
                "Ke Chen",
                "Shimin Li",
                "Songlin Wang",
                "Wei Jiang",
                "Zhaoye Fei",
                "Zhiyuan Ning",
                "Chunguo Li",
                "Chenhui Li",
                "Ziwei He",
                "Zengfeng Huang",
                "Xie Chen",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "SII-OpenMOSS Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08794.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#audio",
                    "#dataset",
                    "#architecture",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ",
                    "desc": "MOVA â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. MOVA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Image-Text to Video-Audio generation Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡ÑŒÑ, Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· LoRA Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°."
                },
                "en": {
                    "title": "MOVA: Revolutionizing Synchronized Audio-Visual Generation",
                    "desc": "MOVA is an innovative open-source model designed to generate synchronized audio-visual content by utilizing a Mixture-of-Experts (MoE) architecture with 32 billion parameters. This model addresses the common oversight of audio in existing video generation systems, which often rely on inefficient cascaded pipelines that can lead to quality degradation. By enabling simultaneous generation of audio and video, MOVA enhances the realism of outputs, including lip-synced speech and environment-aware sound effects. The release of MOVA's model weights and code aims to promote collaboration and progress in the field of audio-visual content creation."
                },
                "zh": {
                    "title": "MOVAï¼šå¼€æºéŸ³é¢‘-è§†è§‰ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "MOVAæ˜¯ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„ç”ŸæˆåŒæ­¥çš„éŸ³é¢‘-è§†è§‰å†…å®¹ï¼Œå…·æœ‰320äº¿ä¸ªå‚æ•°ï¼Œæ”¯æŒå›¾åƒ-æ–‡æœ¬åˆ°è§†é¢‘-éŸ³é¢‘çš„ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹å¿½è§†éŸ³é¢‘ç»„ä»¶çš„é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŒæ­¥éŸ³é¢‘-è§†è§‰å†…å®¹ï¼ŒåŒ…æ‹¬é€¼çœŸçš„å£å‹åŒæ­¥è¯­éŸ³å’Œç¯å¢ƒæ„ŸçŸ¥éŸ³æ•ˆã€‚MOVAçš„è®¾è®¡æ—¨åœ¨å…‹æœçº§è”ç®¡é“å¸¦æ¥çš„æˆæœ¬å’Œé”™è¯¯ç´¯ç§¯é—®é¢˜ï¼ŒåŒæ—¶æä¾›é«˜æ•ˆçš„æ¨ç†å’Œå¾®è°ƒæ”¯æŒã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡å’Œä»£ç ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¿ƒè¿›åˆ›ä½œè€…ç¤¾åŒºçš„æ´»è·ƒå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06422",
            "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
            "url": "https://huggingface.co/papers/2602.06422",
            "abstract": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
            "score": 32,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "92898f90dd66cb60",
            "authors": [
                "Yunze Tong",
                "Mushui Liu",
                "Canyu Zhao",
                "Wanggui He",
                "Shiyi Zhang",
                "Hongwei Zhang",
                "Peng Zhang",
                "Jinlong Liu",
                "Ju Huang",
                "Jiamang Wang",
                "Hao Jiang",
                "Pipei Huang"
            ],
            "affiliations": [
                "Alibaba Group, Hangzhou, China",
                "Tsinghua University, Beijing, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06422.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "TP-GRPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ â€” Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ³Ğ´Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ´ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ â€” Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¸Ğ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TP-GRPO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Reward Signals for Better Denoising in Flow Matching",
                    "desc": "TP-GRPO is a novel framework designed to improve reward mechanisms in flow matching models, particularly for text-to-image generation tasks. It introduces step-level incremental rewards that provide a more detailed learning signal, allowing for better isolation of the effects of individual denoising actions. Additionally, TP-GRPO identifies turning points in the reward trend, which are critical steps that influence future rewards, thus capturing long-term dependencies in the denoising process. This approach enhances the efficiency of reward utilization and leads to consistent improvements in generation quality."
                },
                "zh": {
                    "title": "TP-GRPOï¼šæå‡å»å™ªæ¨¡å‹çš„å¥–åŠ±åˆ©ç”¨æ•ˆç‡",
                    "desc": "TP-GRPOæ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æµåŒ¹é…æ¨¡å‹ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥é€æ­¥å¢é‡å¥–åŠ±å’Œè¯†åˆ«è½¬æŠ˜ç‚¹æ¥æ•æ‰å»å™ªè½¨è¿¹ä¸­çš„é•¿æœŸå½±å“ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTP-GRPOä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤æä¾›äº†æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«é‚£äº›å½±å“åç»­å¥–åŠ±çš„å…³é”®æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTP-GRPOåœ¨ç”Ÿæˆä»»åŠ¡ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07085",
            "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
            "url": "https://huggingface.co/papers/2602.07085",
            "abstract": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
            "score": 30,
            "issue_id": 981,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "10b950e0be093e2a",
            "authors": [
                "Jun Han",
                "Shuo Zhang",
                "Wei Li",
                "Zhi Yang",
                "Yifan Dong",
                "Tu Hu",
                "Jialuo Yuan",
                "Xiaomin Yu",
                "Yumo Zhu",
                "Fangqi Lou",
                "Xin Guo",
                "Zhaowei Liu",
                "Tianyi Jiang",
                "Ruichuan An",
                "Jingping Liu",
                "Biao Wu",
                "Rongze Chen",
                "Kunyi Wang",
                "Yifan Wang",
                "Sen Hu",
                "Xinbing Kong",
                "Liwen Zhang",
                "Ronghao Chen",
                "Huacan Wang"
            ],
            "affiliations": [
                "PKU",
                "QuantaAlpha",
                "SEU",
                "SUFE",
                "SYSU",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07085.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "QuantaAlpha Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°Ğ»ÑŒÑ„Ğ°-Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ñ‹Ğ½ĞºĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ° ĞºĞ°Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑĞ¾Ğ²ĞµÑ€Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒÑ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¾Ğ¹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿ĞµÑ€ĞµĞ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğº ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ñ‹Ğ½ĞºĞ°, Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 0.1501 Ğ¸ Ğ³Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒÑ 27.75% Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ°Ğ´ĞºĞµ 7.98% Ğ½Ğ° Ğ¸Ğ½Ğ´ĞµĞºÑĞµ CSI 300."
                },
                "en": {
                    "title": "QuantaAlpha: Evolving Alpha Mining for Robust Financial Insights",
                    "desc": "QuantaAlpha is an innovative framework designed for alpha mining in financial markets, which are often unpredictable and changeable. It enhances the process by treating each mining attempt as a trajectory, allowing for targeted improvements through mutation and crossover techniques. This method not only revises suboptimal steps but also combines successful segments to optimize the mining process. The framework ensures that the generated factors maintain semantic consistency and reduces complexity, leading to significant performance improvements over existing models, as demonstrated by its strong results on various market indices."
                },
                "zh": {
                    "title": "QuantaAlphaï¼šæ™ºèƒ½åŒ–çš„é˜¿å°”æ³•æŒ–æ˜æ¡†æ¶",
                    "desc": "é‡‘èå¸‚åœºå™ªå£°å¤§ä¸”éå¹³ç¨³ï¼Œä½¿å¾—é˜¿å°”æ³•æŒ–æ˜å¯¹å›æµ‹ç»“æœä¸­çš„å™ªå£°å’Œå¸‚åœºçŠ¶æ€çªå˜éå¸¸æ•æ„Ÿã€‚æˆ‘ä»¬æå‡ºäº†QuantaAlphaï¼Œè¿™æ˜¯ä¸€ç§è¿›åŒ–çš„é˜¿å°”æ³•æŒ–æ˜æ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹çº§çš„å˜å¼‚å’Œäº¤å‰æ“ä½œæ¥æ”¹è¿›æŒ–æ˜è¿‡ç¨‹ã€‚QuantaAlphaèƒ½å¤Ÿå®šä½æ¯ä¸ªè½¨è¿¹ä¸­çš„æ¬¡ä¼˜æ­¥éª¤è¿›è¡Œé’ˆå¯¹æ€§ä¿®æ­£ï¼Œå¹¶é‡æ–°ç»„åˆé«˜æ”¶ç›Šçš„äº’è¡¥ç‰‡æ®µï¼Œä»¥ä¾¿åœ¨æŒ–æ˜è¿­ä»£ä¸­å®ç°ç»“æ„åŒ–æ¢ç´¢å’Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuantaAlphaåœ¨ä¸­å›½è¯åˆ¸æŒ‡æ•°300ï¼ˆCSI 300ï¼‰ä¸Šè¡¨ç°ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹å’Œä¹‹å‰çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¸‚åœºåˆ†å¸ƒå˜åŒ–ä¸‹çš„å¼ºå¤§é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07845",
            "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
            "url": "https://huggingface.co/papers/2602.07845",
            "abstract": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
            "score": 26,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "22a085d4fed206a9",
            "authors": [
                "Yalcin Tur",
                "Jalal Naghiyev",
                "Haoquan Fang",
                "Wei-Chuan Tsai",
                "Jiafei Duan",
                "Dieter Fox",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Stanford University",
                "Technical University of Munich",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07845.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#robotics",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹: ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ robotics",
                    "desc": "RD-VLA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language-action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ-ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ truncated backpropagation through time Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğµ Ñ€ĞµÑˆĞ°Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 90% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 80 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Adaptive Depth for Efficient Vision-Language-Action Models",
                    "desc": "The RD-VLA paper presents a new architecture for vision-language-action models that allows for flexible computational depth through a process called latent iterative refinement. Unlike traditional models that use a fixed amount of computation, RD-VLA adapts its resource usage based on the complexity of the task, maintaining a constant memory footprint. This is achieved by employing a recurrent action head and using truncated backpropagation through time for training, which enhances the model's ability to handle complex tasks efficiently. Experimental results demonstrate that RD-VLA significantly improves task success rates, especially in challenging scenarios, by dynamically adjusting its inference depth."
                },
                "zh": {
                    "title": "é€’å½’æ·±åº¦ï¼Œæ™ºèƒ½å†³ç­–çš„æ–°è·¯å¾„",
                    "desc": "RD-VLAæå‡ºäº†ä¸€ç§é€’å½’æ¶æ„ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡æ½œåœ¨çš„è¿­ä»£ä¼˜åŒ–æ¥é€‚åº”è®¡ç®—æ·±åº¦ï¼Œä»è€Œå®ç°æ’å®šçš„å†…å­˜ä½¿ç”¨å’Œæé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šè®¡ç®—æ·±åº¦æ¨¡å‹ä¸åŒï¼ŒRD-VLAèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºï¼Œé¿å…äº†åœ¨ç®€å•å’Œå¤æ‚ä»»åŠ¡ä¸Šéƒ½æ¶ˆè€—ç›¸åŒè®¡ç®—é‡çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†é€’å½’çš„ã€æƒé‡ç»‘å®šçš„åŠ¨ä½œå¤´ï¼Œæ”¯æŒä»»æ„æ¨ç†æ·±åº¦ï¼ŒåŒæ—¶ä¿æŒæ’å®šçš„å†…å­˜å ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œé€’å½’æ·±åº¦å¯¹ä»»åŠ¡æˆåŠŸç‡è‡³å…³é‡è¦ï¼ŒRD-VLAåœ¨æœºå™¨äººé¢†åŸŸæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æµ‹è¯•æ—¶è®¡ç®—è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06025",
            "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
            "url": "https://huggingface.co/papers/2602.06025",
            "abstract": "BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
            "score": 23,
            "issue_id": 982,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "a764ee2002f6dbd6",
            "authors": [
                "Haozhen Zhang",
                "Haodong Yue",
                "Tao Feng",
                "Quanyu Long",
                "Jianzhu Bao",
                "Bowen Jin",
                "Weizhi Zhang",
                "Xiao Li",
                "Jiaxuan You",
                "Chengwei Qin",
                "Wenya Wang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Tsinghua University",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06025.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#long_context",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "BudgetMem â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LoCoMo, LongMemEval Ğ¸ HotpotQA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BudgetMem Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Memory Management in LLMs with BudgetMem",
                    "desc": "BudgetMem is a novel framework designed to enhance memory management in Large Language Model (LLM) agents by providing a structured approach to runtime memory usage. It introduces three budget tiersâ€”Low, Mid, and Highâ€”allowing for flexible performance-cost trade-offs based on the task requirements. A neural policy router intelligently directs memory requests to the appropriate budget tier, optimizing both efficiency and effectiveness. Through extensive testing, BudgetMem demonstrates superior performance in high-budget scenarios and improved accuracy under constrained budgets, offering valuable insights into memory management strategies."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "BudgetMem æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è¿è¡Œæ—¶å†…å­˜æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å†…å­˜ä½¿ç”¨ä¸­çš„æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–ç»„ä»¶ï¼Œæä¾›ä¸‰ç§é¢„ç®—å±‚çº§ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰ï¼Œå¹¶é€šè¿‡ç¥ç»ç­–ç•¥è·¯ç”±å™¨è¿›è¡Œå†…å­˜å¤„ç†ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿å†…å­˜æ„å»ºæ–¹æ³•ç›¸æ¯”ï¼ŒBudgetMem å…è®¸æ›´çµæ´»çš„æŸ¥è¯¢æ„ŸçŸ¥æ§åˆ¶ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„ä¿¡æ¯ä¸¢å¤±ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒBudgetMem æ˜¾ç¤ºäº†åœ¨ä¸åŒé¢„ç®—æ¡ä»¶ä¸‹å®ç°æœ€ä½³æ€§èƒ½çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08676",
            "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
            "url": "https://huggingface.co/papers/2602.08676",
            "abstract": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
            "score": 22,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "fcabdb36b8d8199a",
            "authors": [
                "Tiwei Bie",
                "Maosong Cao",
                "Xiang Cao",
                "Bingsen Chen",
                "Fuyuan Chen",
                "Kun Chen",
                "Lun Du",
                "Daozhuo Feng",
                "Haibo Feng",
                "Mingliang Gong",
                "Zhuocheng Gong",
                "Yanmei Gu",
                "Jian Guan",
                "Kaiyuan Guan",
                "Hongliang He",
                "Zenan Huang",
                "Juyong Jiang",
                "Zhonghui Jiang",
                "Zhenzhong Lan",
                "Chengxi Li",
                "Jianguo Li",
                "Zehuan Li",
                "Huabin Liu",
                "Lin Liu",
                "Guoshan Lu",
                "Yuan Lu",
                "Yuxin Ma",
                "Xingyu Mou",
                "Zhenxuan Pan",
                "Kaida Qiu",
                "Yuji Ren",
                "Jianfeng Tan",
                "Yiding Tian",
                "Zian Wang",
                "Lanning Wei",
                "Tao Wu",
                "Yipeng Xing",
                "Wentao Ye",
                "Liangyu Zha",
                "Tianze Zhang",
                "Xiaolu Zhang",
                "Junbo Zhao",
                "Da Zheng",
                "Hao Zhong",
                "Wanli Zhong",
                "Jun Zhou",
                "Junlin Zhou",
                "Liwang Zhu",
                "Muzhi Zhu",
                "Yihong Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Southern University of Science and Technology",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08676.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#benchmark",
                    "#architecture",
                    "#open_source",
                    "#reasoning",
                    "#diffusion",
                    "#plp"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· T2T Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "LLaDA2.1 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Mask-to-Token Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Token-to-Token Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaDA2.1-Mini (16B) Ğ¸ LLaDA2.1-Flash (100B) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 33 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ±Ğ¾Ğ»ĞµĞµ 800 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Speed and Quality in Language Models with LLaDA2.1",
                    "desc": "LLaDA2.1 presents a new method for editing tokens in large language models, balancing speed and quality through innovative techniques. It introduces a Token-to-Token (T2T) editing approach alongside the existing Mask-to-Token (M2T) method, allowing for configurable decoding modes. The Speedy Mode prioritizes fast output generation, while the Quality Mode focuses on achieving high performance with a slight efficiency trade-off. Additionally, it employs a large-scale Reinforcement Learning framework to enhance reasoning and instruction adherence, resulting in impressive performance across various benchmarks."
                },
                "zh": {
                    "title": "çªç ´é€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡ï¼ŒLLaDA2.1å¼•é¢†æ–°æ½®æµ",
                    "desc": "LLaDA2.1æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰Œåˆ°ä»¤ç‰Œç¼–è¾‘æ–¹æ³•ï¼Œç»“åˆäº†é€Ÿåº¦å’Œè´¨é‡æ¨¡å¼ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡äº†å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹çš„æ¨ç†å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¼ ç»Ÿçš„æ©ç åˆ°ä»¤ç‰Œæ–¹æ¡ˆä¸­æ— ç¼èå…¥äº†ä»¤ç‰Œåˆ°ä»¤ç‰Œç¼–è¾‘ï¼Œå½¢æˆäº†ä¸€ä¸ªå¯é…ç½®çš„é˜ˆå€¼è§£ç æ–¹æ¡ˆã€‚LLaDA2.1å…·æœ‰ä¸¤ç§æ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å¼é€šè¿‡é™ä½é˜ˆå€¼æ¥æé«˜é€Ÿåº¦ï¼Œè€Œè´¨é‡æ¨¡å¼åˆ™é€šè¿‡ä¿å®ˆçš„é˜ˆå€¼ç¡®ä¿æ›´é«˜çš„æ€§èƒ½ã€‚é€šè¿‡å¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒLLaDA2.1åœ¨æ¨ç†ç²¾åº¦å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08439",
            "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
            "url": "https://huggingface.co/papers/2602.08439",
            "abstract": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
            "score": 21,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "faa968b3b5bd841c",
            "authors": [
                "Yuhao Dong",
                "Shulin Tian",
                "Shuai Liu",
                "Shuangrui Ding",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Yuhang Cao",
                "Jiaqi Wang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "CUHK-MMLab",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08439.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#rlhf"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Demo-ICL-Bench ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 1200 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ YouTube Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹: Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Demo-ICL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Learning from Few Examples in Video Understanding",
                    "desc": "This paper introduces a new task called Demo-driven Video In-Context Learning, which focuses on how models can learn from a few examples in dynamic video contexts. The authors present a benchmark, Demo-ICL-Bench, that evaluates this learning ability using 1200 instructional YouTube videos and associated questions. They propose a specialized Multimodal Large Language Model (MLLM) called Demo-ICL, which is trained in two stages: first through video supervision and then through preference optimization. The results show that Demo-ICL effectively addresses the challenges of the new benchmark, highlighting the potential for future advancements in video understanding."
                },
                "zh": {
                    "title": "ç¤ºä¾‹é©±åŠ¨çš„è§†é¢‘ç†è§£æ–°æŒ‘æˆ˜",
                    "desc": "ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€é¡¹æ–°çš„è§†é¢‘ç†è§£ä»»åŠ¡å’ŒåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚è¯¥ä»»åŠ¡ç§°ä¸ºç¤ºä¾‹é©±åŠ¨çš„è§†é¢‘ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹å›ç­”å…³äºç›®æ ‡è§†é¢‘çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†Demo-ICLï¼Œä¸€ä¸ªé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆè§†é¢‘ç›‘ç£å’Œåå¥½ä¼˜åŒ–ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†Demo-ICL-Benchçš„æŒ‘æˆ˜æ€§ï¼Œå¹¶å±•ç¤ºäº†Demo-ICLçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08222",
            "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
            "url": "https://huggingface.co/papers/2602.08222",
            "abstract": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
            "score": 19,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5de7bf3e511c0cff",
            "authors": [
                "Zehao Chen",
                "Gongxun Li",
                "Tianxiang Ai",
                "Yifei Li",
                "Zixuan Huang",
                "Wang Zhou",
                "Fuzhen Zhuang",
                "Xianglong Liu",
                "Jianxin Li",
                "Deqing Wang",
                "Yikun Ban"
            ],
            "affiliations": [
                "Beihang University",
                "China Teleco"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08222.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ WMSS â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Hidden Potential in Language Models with WMSS",
                    "desc": "WMSS is a new method for improving large language models after their initial training. It focuses on using weaker versions of the model, called weak checkpoints, to find areas where the model can still learn and grow. By analyzing the model's past performance and identifying gaps in its knowledge, WMSS helps the model to continue improving even when it seems to have reached its peak. Tests show that models using WMSS perform better in tasks like math reasoning and code generation without increasing the cost of using the model."
                },
                "zh": {
                    "title": "å¼±æ¨¡å‹åŠ©åŠ›å¼ºæ¨¡å‹æ›´å¼º",
                    "desc": "WMSSæ˜¯ä¸€ç§åè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å¼±æ¨¡å‹æ£€æŸ¥ç‚¹æ¥è¯†åˆ«å’Œå¡«è¡¥å­¦ä¹ ç©ºç™½ï¼Œä»è€Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼ ç»Ÿé¥±å’Œç‚¹ä¹‹å¤–ç»§ç»­æ”¹è¿›ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒä¸­å˜å¾—é«˜åº¦è‡ªä¿¡ï¼Œä½†è¿›ä¸€æ­¥çš„è®­ç»ƒæ•ˆæœé€’å‡ã€‚WMSSé€šè¿‡è¯†åˆ«å¯æ¢å¤çš„å­¦ä¹ å·®è·ï¼Œå¹¶é€šè¿‡è¡¥å¿å­¦ä¹ æ¥å¼ºåŒ–è¿™äº›å·®è·ï¼Œä½¿å¼ºæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„åè®­ç»ƒé¥±å’Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰æ•ˆçš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ²¡æœ‰å¢åŠ é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07962",
            "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
            "url": "https://huggingface.co/papers/2602.07962",
            "abstract": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
            "score": 19,
            "issue_id": 980,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "483ad401f8702de9",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07962.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#long_context",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LOCA-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ€Ğ¾ÑÑ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ (ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Â«ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ğ´Ğ°Â»). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, LOCA-bench Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Language Agents in Dynamic Long-Context Scenarios",
                    "desc": "LOCA-bench is a new benchmark designed to evaluate language agents in complex, long-context scenarios where they must manage their environment effectively. It addresses the issue of 'context rot', where the performance of large language models declines as the context length increases. Unlike existing benchmarks that focus on single-step tasks, LOCA-bench allows for dynamic context management, enabling agents to handle tasks that require exploration and decision-making over extended interactions. By using advanced context management strategies, LOCA-bench aims to improve the reliability and success rates of language agents in real-world applications."
                },
                "zh": {
                    "title": "LOCA-benchï¼šé•¿ä¸Šä¸‹æ–‡ä»£ç†çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "LOCA-benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†åœ¨é•¿ä¸Šä¸‹æ–‡å’Œä»£ç†åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„ç¯å¢ƒçŠ¶æ€æ§åˆ¶ï¼Œè°ƒèŠ‚ä»£ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥åº”å¯¹å¤æ‚çš„ä»»åŠ¡ã€‚å°½ç®¡ç¯å¢ƒçŠ¶æ€çš„å¤æ‚æ€§ä¼šå¯¼è‡´ä»£ç†æ€§èƒ½ä¸‹é™ï¼Œä½†å…ˆè¿›çš„ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æˆåŠŸç‡ã€‚LOCA-benchä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾çš„å¹³å°ï¼Œä»¥è¯„ä¼°åœ¨é•¿ä¸Šä¸‹æ–‡ä»£ç†åœºæ™¯ä¸­çš„æ¨¡å‹å’Œæ”¯æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09007",
            "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
            "url": "https://huggingface.co/papers/2602.09007",
            "abstract": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
            "score": 14,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5b641f3312e9b596",
            "authors": [
                "Haodong Li",
                "Jingwei Wu",
                "Quan Sun",
                "Guopeng Li",
                "Juanxi Tian",
                "Huanyu Zhang",
                "Yanlin Lai",
                "Ruichuan An",
                "Hongbo Peng",
                "Yuhong Dai",
                "Chenxi Li",
                "Chunmei Qing",
                "Jia Wang",
                "Ziyang Meng",
                "Zheng Ge",
                "Xiangyu Zhang",
                "Daxin Jiang"
            ],
            "affiliations": [
                "Peking University",
                "South China University of Technology",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09007.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GEBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 700 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ GE-Score Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸: Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸, Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ĞºĞ¾Ğ½Ğ¾Ğº, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ GUI."
                },
                "en": {
                    "title": "GEBench: Elevating GUI Generation with Temporal Coherence Metrics",
                    "desc": "This paper introduces GEBench, a new benchmark designed to evaluate the temporal coherence and dynamic interaction capabilities of GUI generation models. It highlights the limitations of existing benchmarks that focus mainly on visual fidelity, neglecting the assessment of state transitions in GUI contexts. The authors propose a novel metric called GE-Score, which evaluates five dimensions: Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. The findings reveal that while current models excel in single-step interactions, they face significant challenges in maintaining coherence and accuracy over longer sequences of user interactions."
                },
                "zh": {
                    "title": "æå‡GUIç”Ÿæˆæ¨¡å‹çš„æ—¶é—´ä¸€è‡´æ€§ä¸åŠ¨æ€äº¤äº’è¯„ä¼°",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç”Ÿæˆæ¨¡å‹ä¸­çš„æ—¶é—´ä¸€è‡´æ€§å’ŒåŠ¨æ€äº¤äº’ã€‚ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨è§†è§‰è´¨é‡ï¼Œè€Œå¯¹çŠ¶æ€è½¬æ¢å’Œæ—¶é—´ä¸€è‡´æ€§çš„è¯„ä¼°åˆ™ç›¸å¯¹ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GEBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ¶µç›–äº†700ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠå•æ­¥å’Œå¤šæ­¥äº¤äº’çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GE-Scoreï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„äº”ç»´æŒ‡æ ‡ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°ç”Ÿæˆçš„GUIçš„å„ä¸ªæ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07075",
            "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
            "url": "https://huggingface.co/papers/2602.07075",
            "abstract": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
            "score": 14,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "8c5ebd4aa9539a12",
            "authors": [
                "Xinwu Ye",
                "Yicheng Mao",
                "Jia Zhang",
                "Yimeng Liu",
                "Li Hao",
                "Fang Wu",
                "Zhiwei Li",
                "Yuxuan Liao",
                "Zehong Wang",
                "Zhiyuan Liu",
                "Zhenfei Yin",
                "Li Yuan",
                "Philip Torr",
                "Huan Sun",
                "Xiangxiang Zeng",
                "Mengdi Wang",
                "Le Cong",
                "Shenghua Gao",
                "Xiangru Tang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.07075.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning"
                ],
                "emoji": "âš—ï¸",
                "ru": {
                    "title": "Ğ¥Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸",
                    "desc": "LatentChem Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 59.88% Ğ¿Ğ¾Ğ±ĞµĞ´Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 10.84-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Chemical Reasoning with Continuous Latent Dynamics",
                    "desc": "LatentChem is a novel approach that enhances chemical reasoning by utilizing continuous latent space computations instead of relying on discrete textual tokens. Traditional methods, which use Chain-of-Thought (CoT) reasoning, often struggle with the inherent continuous nature of chemical data, leading to inefficiencies. By allowing models to perform multi-step reasoning directly in latent space, LatentChem improves both performance and speed, achieving a significant win rate over existing CoT-based methods. This research demonstrates that chemical reasoning benefits from a continuous representation, resulting in faster and more effective computations."
                },
                "zh": {
                    "title": "åŒ–å­¦æ¨ç†çš„æ–°æ–¹æ³•ï¼šæ½œåœ¨ç©ºé—´è®¡ç®—",
                    "desc": "LatentChem æ˜¯ä¸€ç§æ–°çš„åŒ–å­¦æ¨ç†æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿ç»­çš„æ½œåœ¨ç©ºé—´è®¡ç®—æ¥å®ç°ï¼Œè€Œä¸æ˜¯ä¾èµ–äºç¦»æ•£çš„æ–‡æœ¬ç¬¦å·ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿé“¾å¼æ€ç»´åœ¨åŒ–å­¦æ¨ç†ä¸­å­˜åœ¨çš„è¡¨ç¤ºä¸åŒ¹é…é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹ä»…ä¼˜åŒ–ä»»åŠ¡æˆåŠŸæ—¶ï¼Œå®ƒä»¬ä¼šè‡ªå‘åœ°å†…åŒ–æ¨ç†ï¼Œé€æ¸æ”¾å¼ƒå†—é•¿çš„æ–‡æœ¬æ¨å¯¼ï¼Œè½¬è€Œä½¿ç”¨éšå¼çš„æ½œåœ¨è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLatentChem åœ¨åŒ–å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09022",
            "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
            "url": "https://huggingface.co/papers/2602.09022",
            "abstract": "WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
            "score": 12,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "977be04170016363",
            "authors": [
                "Zehan Wang",
                "Tengfei Wang",
                "Haiyu Zhang",
                "Xuhui Zuo",
                "Junta Wu",
                "Haoyuan Wang",
                "Wenqiang Sun",
                "Zhenwei Wang",
                "Chenjie Cao",
                "Hengshuang Zhao",
                "Chunchao Guo",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Tencent Hunyuan",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09022.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#rl"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "WorldCompass Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¼Ğ¸Ñ€Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Video World Models with Efficient Reinforcement Learning",
                    "desc": "WorldCompass is a new framework that improves long-horizon video-based world models using reinforcement learning (RL) after their initial training. It introduces a clip-level rollout strategy that allows the model to generate and assess multiple video samples at once, enhancing efficiency and providing detailed reward feedback. Additionally, it incorporates complementary reward functions that focus on both the accuracy of interactions and the quality of visuals, helping to prevent the model from exploiting loopholes in the reward system. Finally, the framework uses an efficient RL algorithm that optimizes the model's performance, leading to better interaction accuracy and visual quality in various scenarios."
                },
                "zh": {
                    "title": "WorldCompassï¼šæå‡è§†é¢‘ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›",
                    "desc": "WorldCompass æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºè§†é¢‘çš„é•¿æ—¶é—´äº¤äº’ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å‰ªè¾‘çº§å›æ”¾ç­–ç•¥ã€äº’è¡¥å¥–åŠ±å‡½æ•°å’Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„æ¢ç´¢è¿‡ç¨‹ã€‚å‰ªè¾‘çº§å›æ”¾ç­–ç•¥å¯ä»¥åœ¨å•ä¸ªç›®æ ‡å‰ªè¾‘ä¸­ç”Ÿæˆå’Œè¯„ä¼°å¤šä¸ªæ ·æœ¬ï¼Œä»è€Œæé«˜å›æ”¾æ•ˆç‡ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒWorldCompass æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„äº¤äº’å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07055",
            "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
            "url": "https://huggingface.co/papers/2602.07055",
            "abstract": "Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
            "score": 12,
            "issue_id": 982,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "e9848ab21f78fd95",
            "authors": [
                "Pingyue Zhang",
                "Zihan Huang",
                "Yue Wang",
                "Jieyu Zhang",
                "Letian Xue",
                "Zihan Wang",
                "Qineng Wang",
                "Keshigeyan Chandrasegaran",
                "Ruohan Zhang",
                "Yejin Choi",
                "Ranjay Krishna",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Manling Li"
            ],
            "affiliations": [
                "Cornell University",
                "Northwestern University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07055.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² foundation models",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ foundation models Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Theory of Space' â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ 'spatial belief probing' Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¸Ğ½ĞµÑ€Ñ†Ğ¸Ñ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ current foundation models Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Enhancing Spatial Intelligence in Active Exploration",
                    "desc": "This paper addresses the limitations of current multimodal foundation models in maintaining coherent spatial beliefs during active exploration. It introduces the Theory of Space, which emphasizes the importance of self-directed exploration for agents to gather information and update their spatial knowledge. The study reveals significant performance drops when agents transition from passive perception to active information gathering, highlighting an Active-Passive Gap. Additionally, it identifies issues like inefficiency in exploration strategies and Belief Inertia, where agents struggle to revise outdated spatial beliefs, particularly in vision-based models."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ¢ç´¢ä¸­çš„ç©ºé—´ä¿¡å¿µæŒ‘æˆ˜",
                    "desc": "å½“å‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ä¸»åŠ¨æ¢ç´¢è¿‡ç¨‹ä¸­å­˜åœ¨ä¿æŒä¸€è‡´ç©ºé—´ä¿¡å¿µçš„å±€é™æ€§ï¼Œè¡¨ç°å‡ºä¸»åŠ¨ä¸è¢«åŠ¨æ€§èƒ½ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ç©ºé—´ç†è®ºï¼Œå®šä¹‰ä¸ºä»£ç†é€šè¿‡è‡ªæˆ‘å¯¼å‘çš„ä¸»åŠ¨æ¢ç´¢æ¥ä¸»åŠ¨è·å–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä»åºåˆ—çš„éƒ¨åˆ†è§‚å¯Ÿä¸­æ„å»ºã€ä¿®è®¢å’Œåˆ©ç”¨ç©ºé—´ä¿¡å¿µã€‚é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨è‡ªä¸»æ”¶é›†ä¿¡æ¯æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¹¶ä¸”æ¢ç´¢ç­–ç•¥æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„åŸºç¡€æ¨¡å‹åœ¨ä¸»åŠ¨æ¢ç´¢ä¸­éš¾ä»¥ç»´æŒä¸€è‡´å’Œå¯ä¿®è®¢çš„ç©ºé—´ä¿¡å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06454",
            "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2602.06454",
            "abstract": "RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
            "score": 10,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "5323131100597ece",
            "authors": [
                "Jiwon Song",
                "Yoongon Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06454.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "RelayGen â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° (Ğ´Ğ¾ 2.2 Ñ€Ğ°Ğ·) Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. RelayGen Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Dynamic Model Switching for Efficient Reasoning",
                    "desc": "RelayGen is a novel framework designed to enhance the efficiency of large reasoning models (LRMs) by dynamically switching between large and small models based on the difficulty of reasoning tasks. It identifies transitions in difficulty at the segment level, allowing for faster inference without significant loss in accuracy. Unlike traditional methods that either overlook difficulty variations or require complex supervised routing, RelayGen operates without additional training, making it simpler and more effective. By leveraging token probability margins to analyze generation uncertainty, RelayGen achieves substantial speed improvements while maintaining high performance across various reasoning benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆæ¨ç†",
                    "desc": "RelayGen æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ‡æ¢å¤§æ¨¡å‹å’Œå°æ¨¡å‹ï¼Œæ¥è¯†åˆ«æ®µçº§åˆ«çš„éš¾åº¦å˜åŒ–ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ä¸”å‡ ä¹ä¸æŸå¤±å‡†ç¡®æ€§ã€‚å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†æ—¶çš„æ‰©å±•ä¼šå¸¦æ¥é«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ã€‚RelayGen é€šè¿‡ç¦»çº¿åˆ†æç”Ÿæˆä¸ç¡®å®šæ€§ï¼Œåˆ©ç”¨ç²—ç²’åº¦çš„æ®µçº§æ§åˆ¶æ¥æ•æ‰æ¨ç†è½¨è¿¹ä¸­çš„éš¾åº¦å˜åŒ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹ç‰¹å®šçš„åˆ‡æ¢ä¿¡å·ï¼Œå°†ä½éš¾åº¦æ®µçš„æ¨ç†äº¤ç»™å°æ¨¡å‹å¤„ç†ï¼ŒåŒæ—¶ä¿æŒé«˜éš¾åº¦æ¨ç†åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06540",
            "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
            "url": "https://huggingface.co/papers/2602.06540",
            "abstract": "AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
            "score": 9,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "99c9091242c48388",
            "authors": [
                "Yishan Li",
                "Wentong Chen",
                "Yukun Yan",
                "Mingwei Li",
                "Sen Mei",
                "Xiaorong Wang",
                "Kunpeng Liu",
                "Xin Cong",
                "Shuo Wang",
                "Zhong Zhang",
                "Yaxi Lu",
                "Zhenghao Liu",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06540.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "AgentCPM-Report Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Writing As Reasoning Policy Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ²Ğ¸Ğ·Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RL, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Small Models for Insightful Research Reports",
                    "desc": "AgentCPM-Report introduces a novel approach for generating deep research reports using a lightweight local model that enhances reasoning and outline development. It employs a Writing As Reasoning Policy (WARP) that allows the model to iteratively refine its outline while drafting the report. This method addresses the limitations of traditional plan-then-write strategies by integrating evidence-based drafting with reasoning-driven enhancements. The Multi-Stage Agentic Training strategy further empowers smaller models to achieve performance levels comparable to larger, closed-source systems, while ensuring user data privacy and safety."
                },
                "zh": {
                    "title": "è½»é‡çº§æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæå‡æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›",
                    "desc": "AgentCPM-Report æ˜¯ä¸€ç§è½»é‡çº§çš„æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å†™ä½œä½œä¸ºæ¨ç†æ”¿ç­–æ¡†æ¶å’Œå¤šé˜¶æ®µä»£ç†è®­ç»ƒæ¥ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šæ—¶é¢ä¸´çš„ä¿¡æ¯è·å–å’Œåˆ†æåˆæˆçš„æŒ‘æˆ˜ã€‚é€šè¿‡åŠ¨æ€ä¿®è®¢å¤§çº²ï¼ŒAgentCPM-Report æé«˜äº†å°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¤§çº²æ¼”å˜èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„é—­æºç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æ´å¯ŸåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08990",
            "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
            "url": "https://huggingface.co/papers/2602.08990",
            "abstract": "InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
            "score": 8,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "838f93e075c62ada",
            "authors": [
                "Shiyang Feng",
                "Runmin Ma",
                "Xiangchao Yan",
                "Yue Fan",
                "Yusong Hu",
                "Songtao Huang",
                "Shuaiyu Zhang",
                "Zongsheng Cao",
                "Tianshuo Peng",
                "Jiakang Yuan",
                "Zijie Guo",
                "Zhijie Zhong",
                "Shangheng Du",
                "Weida Wang",
                "Jinxin Shi",
                "Yuhao Zhou",
                "Xiaohan He",
                "Zhiyin Yu",
                "Fangchen Yu",
                "Qihao Zheng",
                "Jiamin Wu",
                "Mianxin Liu",
                "Chi Zhang",
                "Shaowei Hou",
                "Shuya Li",
                "Yankai Jiang",
                "Wenjie Lou",
                "Lilong Wang",
                "Zifu Wang",
                "Jiong Wang",
                "Wanghan Xu",
                "Yue Deng",
                "Dongrui Liu",
                "Yiheng Wang",
                "Wenlong Zhang",
                "Fenghua Ling",
                "Shufei Zhang",
                "Xiaosong Wang",
                "Shuangjia Zheng",
                "Xun Huang",
                "Siqi Sun",
                "Shuyue Hu",
                "Peng Ye",
                "Chunfeng Song",
                "Bin Wang",
                "Conghui He",
                "Yihao Liu",
                "Xin Li",
                "Qibin Hou",
                "Tao Chen",
                "Xiangyu Yue",
                "Bin Wang",
                "Liang He",
                "Dahua Lin",
                "Bowen Zhou",
                "Bo Zhang",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08990.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "InternAgent-1.5 â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ. InternAgent-1.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (GAIA, HLE, GPQA, FrontierScience) Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¾Ñ‚ Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°ÑƒĞº."
                },
                "en": {
                    "title": "Empowering Autonomous Scientific Discovery with InternAgent-1.5",
                    "desc": "InternAgent-1.5 is a comprehensive system that facilitates autonomous scientific discovery by integrating computational modeling with experimental research. It consists of three main subsystems: generation, verification, and evolution, which work together to enhance the discovery process. The system is capable of performing both algorithm discovery and empirical experiments, producing significant scientific insights across various domains. Evaluated on multiple benchmarks, InternAgent-1.5 demonstrates superior performance and offers a scalable framework for ongoing scientific exploration."
                },
                "zh": {
                    "title": "è‡ªä¸»ç§‘å­¦å‘ç°çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "InternAgent-1.5 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åè°ƒçš„å­ç³»ç»Ÿå®ç°è‡ªä¸»ç§‘å­¦å‘ç°ï¼Œæ•´åˆè®¡ç®—å»ºæ¨¡å’Œå®éªŒç ”ç©¶ã€‚è¯¥ç³»ç»Ÿç”±ä¸‰ä¸ªåè°ƒçš„å­ç³»ç»Ÿç»„æˆï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆã€éªŒè¯å’Œæ¼”åŒ–ï¼Œæ”¯æŒæ·±åº¦ç ”ç©¶ã€è§£å†³æ–¹æ¡ˆä¼˜åŒ–å’Œé•¿æœŸè®°å¿†ç­‰åŸºç¡€èƒ½åŠ›ã€‚InternAgent-1.5 èƒ½å¤Ÿåœ¨å»¶ç»­çš„å‘ç°å‘¨æœŸä¸­æŒç»­è¿è¡Œï¼ŒåŒæ—¶ä¿æŒä¸€è‡´æ€§å’Œæ”¹è¿›è¡Œä¸ºã€‚é€šè¿‡åœ¨ç§‘å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥ç³»ç»Ÿåœ¨ç®—æ³•å‘ç°å’Œå®è¯å‘ç°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„åŸºç¡€èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06694",
            "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
            "url": "https://huggingface.co/papers/2602.06694",
            "abstract": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
            "score": 5,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "164cf8bc03ab7d1b",
            "authors": [
                "Hyochan Chong",
                "Dongkyu Kim",
                "Changdong Kim",
                "Minseop Choi"
            ],
            "affiliations": [
                "Samsung Research, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06694.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "NanoQuant â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ (1-Ğ±Ğ¸Ñ‚) Ğ¸ ÑÑƒĞ±Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ° (ADMM). ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NanoQuant: Revolutionizing Model Compression for Consumer Hardware",
                    "desc": "NanoQuant is a novel method for post-training quantization of large language models (LLMs) that allows them to be compressed to binary and sub-1-bit levels. It uses low-rank binary factorization to convert full-precision weights into low-rank binary matrices, significantly reducing memory usage. The method employs an efficient alternating direction method of multipliers (ADMM) for precise initialization and tuning of these binary matrices. As a result, NanoQuant achieves state-of-the-art accuracy while making it possible to deploy large models on consumer hardware, such as compressing Llama2-70B by 25.8 times."
                },
                "zh": {
                    "title": "NanoQuantï¼šé«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–°æ–¹æ³•",
                    "desc": "NanoQuantæ˜¯ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©åˆ°äºŒè¿›åˆ¶å’Œä½äº1ä½çš„æ°´å¹³ã€‚å®ƒé€šè¿‡ä½ç§©äºŒè¿›åˆ¶åˆ†è§£å’Œäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨å‡å°‘å†…å­˜éœ€æ±‚çš„åŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚NanoQuantå°†é‡åŒ–é—®é¢˜å½¢å¼åŒ–ä¸ºä½ç§©äºŒè¿›åˆ¶åˆ†è§£ï¼Œèƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°å¤§è§„æ¨¡éƒ¨ç½²ã€‚è¯¥æ–¹æ³•åœ¨ä»…ç”¨13å°æ—¶å†…å°†Llama2-70Bæ¨¡å‹å‹ç¼©äº†25.8å€ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨8GBçš„æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08236",
            "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
            "url": "https://huggingface.co/papers/2602.08236",
            "abstract": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
            "score": 4,
            "issue_id": 980,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "93306b7f7aeb8f16",
            "authors": [
                "Shoubin Yu",
                "Yue Zhang",
                "Zun Wang",
                "Jaehong Yoon",
                "Huaxiu Yao",
                "Mingyu Ding",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Department of Computer Science, University of North Carolina, Chapel Hill",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08236.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ĞµÑ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AVIC â€” Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ world models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½ÑƒĞ¶ĞµĞ½ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº world models."
                },
                "en": {
                    "title": "Selective Imagination for Smarter Spatial Reasoning",
                    "desc": "This paper introduces an adaptive framework called AVIC that enhances spatial reasoning by selectively using visual imagination based on the sufficiency of current visual evidence. It addresses the challenges of indiscriminate imagination, which can lead to increased computation and degraded performance. The study analyzes when visual imagination is beneficial and when it can be harmful, providing insights into optimizing its use. The results demonstrate that controlled imagination can improve efficiency and accuracy in spatial reasoning tasks compared to fixed strategies."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è§†è§‰æƒ³è±¡æå‡ç©ºé—´æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æµ‹è¯•æ—¶æ¡†æ¶ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹æ¥é€‰æ‹©æ€§åœ°è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œä»¥æé«˜ç©ºé—´æ¨ç†çš„æ•ˆç‡å’Œå¯é æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé™æ€è§†è§‰è¯æ®å·²ç»è¶³å¤Ÿï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæƒ³è±¡å¯ä»¥æ”¹å–„æ¨ç†æ•ˆæœï¼Œä½†è¿‡åº¦çš„æƒ³è±¡å¯èƒ½ä¼šå¯¼è‡´å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸‹é™ã€‚é€šè¿‡å¼•å…¥AVICæ¡†æ¶ï¼Œè®ºæ–‡åˆ†æäº†ä½•æ—¶éœ€è¦æƒ³è±¡ä»¥åŠæƒ³è±¡çš„é€‚åº¦ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‰æ‹©æ€§æ§åˆ¶çš„æƒ³è±¡ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå›ºå®šçš„æƒ³è±¡ç­–ç•¥ï¼Œå‡å°‘äº†ä¸–ç•Œæ¨¡å‹çš„è°ƒç”¨å’Œè¯­è¨€æ ‡è®°çš„ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08145",
            "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2602.08145",
            "abstract": "Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
            "score": 3,
            "issue_id": 981,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "bc07f9e6a92f2e07",
            "authors": [
                "Xinyu Yang",
                "Junlin Han",
                "Rishi Bommasani",
                "Jinqi Luo",
                "Wenjie Qu",
                "Wangchunshu Zhou",
                "Adel Bibi",
                "Xiyao Wang",
                "Jaehong Yoon",
                "Elias Stengel-Eskin",
                "Shengbang Tong",
                "Lingfeng Shen",
                "Rafael Rafailov",
                "Runjia Li",
                "Zhaoyang Wang",
                "Yiyang Zhou",
                "Chenhang Cui",
                "Yu Wang",
                "Wenhao Zheng",
                "Huichi Zhou",
                "Jindong Gu",
                "Zhaorun Chen",
                "Peng Xia",
                "Tony Lee",
                "Thomas Zollo",
                "Vikash Sehwag",
                "Jixuan Leng",
                "Jiuhai Chen",
                "Yuxin Wen",
                "Huan Zhang",
                "Zhun Deng",
                "Linjun Zhang",
                "Pavel Izmailov",
                "Pang Wei Koh",
                "Yulia Tsvetkov",
                "Andrew Wilson",
                "Jiaheng Zhang",
                "James Zou",
                "Cihang Xie",
                "Hao Wang",
                "Philip Torr",
                "Julian McAuley",
                "David Alvarez-Melis",
                "Florian TramÃ¨r",
                "Kaidi Xu",
                "Suman Jana",
                "Chris Callison-Burch",
                "Rene Vidal",
                "Filippos Kokkinos",
                "Mohit Bansal",
                "Beidi Chen",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Columbia University",
                "Drexel University",
                "ETH Zurich",
                "Harvard University",
                "Imperial College London",
                "Johns Hopkins University",
                "Mila",
                "National University of Singapore",
                "New York University",
                "Princeton University",
                "Rutgers University",
                "Stanford University",
                "University College London",
                "University of California, San Diego",
                "University of California, Santa Cruz",
                "University of Chicago",
                "University of Maryland",
                "University of Montreal",
                "University of North Carolina at Chapel Hill",
                "University of Oxford",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08145.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#alignment",
                    "#ethics",
                    "#hallucinations",
                    "#survey",
                    "#interpretability"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LLM Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹: Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ…, ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Building Trustworthy Foundation Models for a Responsible AI Future",
                    "desc": "This paper discusses the development of foundation models, which include Large Language Models (LLMs) and generative models, emphasizing the need for their responsible and reliable use. It highlights critical issues such as bias, security, explainability, and the challenges of deploying these models in real-world applications. The authors review current limitations of these models, including hallucinations, and propose methods for improving alignment and detecting AI-generated content. The goal is to encourage the creation of powerful yet ethical models that can be trusted across various sectors."
                },
                "zh": {
                    "title": "æ¨åŠ¨åŸºç¡€æ¨¡å‹çš„å¯é ä¸è´£ä»»å‘å±•",
                    "desc": "åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼Œå·²æˆä¸ºå„ä¸ªé¢†åŸŸçš„é‡è¦å·¥å…·ã€‚éšç€è¿™äº›æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ—¥ç›Šå¢åŠ ï¼Œç¡®ä¿å…¶å¯é æ€§å’Œè´£ä»»æ€§å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†åè§ä¸å…¬å¹³æ€§ã€å®‰å…¨ä¸éšç§ã€ä¸ç¡®å®šæ€§ã€å¯è§£é‡Šæ€§å’Œåˆ†å¸ƒå˜åŒ–ç­‰å…³é”®é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›æ¨åŠ¨åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œä½¿å…¶ä¸ä»…å¼ºå¤§ï¼Œè€Œä¸”ç¬¦åˆä¼¦ç†ã€å€¼å¾—ä¿¡èµ–ã€å¯é å’Œå…·æœ‰ç¤¾ä¼šè´£ä»»æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21363",
            "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
            "url": "https://huggingface.co/papers/2601.21363",
            "abstract": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "2f5a5c620860610c",
            "authors": [
                "Weidong Huang",
                "Zhehan Li",
                "Hangxin Liu",
                "Biao Hou",
                "Yao Su",
                "Jingwen Zhang"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Xidian University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Soft Actor-Critic Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (UTD) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ğ° ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞºĞ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Humanoid Control: Bridging Pretraining and Adaptation",
                    "desc": "This paper presents an approach that combines off-policy Soft Actor-Critic (SAC) with large-batch updates to enhance the pretraining of humanoid locomotion policies. By leveraging high Update-To-Data (UTD) ratios, the method achieves efficient zero-shot deployment of these policies on real robots. For adapting to new environments, the authors utilize model-based techniques that allow for safe data collection through deterministic policies while maintaining stochastic exploration in a physics-informed world model. This strategy effectively bridges the gap between large-scale pretraining and efficient fine-tuning, ensuring both safety and performance in humanoid control tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆç±»äººæ­¥æ€ç­–ç•¥é¢„è®­ç»ƒä¸å®‰å…¨é€‚åº”",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¦»çº¿ç­–ç•¥çš„è½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰æ–¹æ³•ï¼Œç»“åˆå¤§æ‰¹é‡æ›´æ–°ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œç±»äººæ­¥æ€ç­–ç•¥çš„é¢„è®­ç»ƒã€‚é€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ–¹æ³•ï¼Œå®‰å…¨é€‚åº”æ–°ç¯å¢ƒï¼Œé‡‡ç”¨ç¡®å®šæ€§æ•°æ®æ”¶é›†å’Œç‰©ç†çŸ¥è¯†é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œéšæœºæ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡é¢„è®­ç»ƒçš„SACç­–ç•¥å¯ä»¥åœ¨æ–°ç¯å¢ƒå’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿é€‚åº”è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚æ•´ä½“æ–¹æ³•å°†å¤§è§„æ¨¡ä»¿çœŸé¢„è®­ç»ƒçš„æ—¶é—´æ•ˆç‡ä¸æ¨¡å‹é©±åŠ¨å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08961",
            "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
            "url": "https://huggingface.co/papers/2602.08961",
            "abstract": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
            "score": 2,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "a03c379e4c6f9864",
            "authors": [
                "Ruijie Zhu",
                "Jiahao Lu",
                "Wenbo Hu",
                "Xiaoguang Han",
                "Jianfei Cai",
                "Ying Shan",
                "Chuanxia Zheng"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "CUHK(SZ)",
                "HKUST",
                "Monash University",
                "NTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08961.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#architecture",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 4D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "MotionCrafter â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° â€” Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ 3D Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ÑÑ†ĞµĞ½Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ 4D VAE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ RGB VAE Ğ½ĞµĞ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VAE Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² â€” Ğ½Ğ° 38.64% Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 25.0% Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing 4D Geometry and Motion Estimation with MotionCrafter",
                    "desc": "MotionCrafter is a cutting-edge framework that utilizes video diffusion techniques to reconstruct 4D geometry and estimate dense motion from single-camera video inputs. It introduces a unique joint representation that combines dense 3D point maps with 3D scene flows, allowing for a more cohesive understanding of motion and structure in a scene. The framework employs a novel 4D Variational Autoencoder (VAE) that enhances the learning of this representation without the need for strict alignment with RGB VAE latents, which can hinder performance. Through innovative data normalization and training strategies, MotionCrafter significantly improves the quality of both geometry reconstruction and motion estimation, achieving remarkable performance gains over previous methods."
                },
                "zh": {
                    "title": "MotionCrafterï¼šè§†é¢‘é‡å»ºä¸è¿åŠ¨ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "MotionCrafter æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘æ‰©æ•£çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶é‡å»ºå››ç»´å‡ ä½•ç»“æ„å¹¶ä¼°è®¡å¯†é›†è¿åŠ¨ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è”åˆè¡¨ç¤ºï¼Œå°†å¯†é›†çš„ä¸‰ç»´ç‚¹å›¾å’Œä¸‰ç»´åœºæ™¯æµåœ¨å…±äº«åæ ‡ç³»ç»Ÿä¸­ç»“åˆã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å†å¼ºåˆ¶ä¸‰ç»´å€¼ä¸ RGB VAE æ½œå˜é‡ä¸¥æ ¼å¯¹é½ï¼Œè€Œæ˜¯é€šè¿‡æ–°çš„æ•°æ®å½’ä¸€åŒ–å’Œ VAE è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotionCrafter åœ¨å‡ ä½•é‡å»ºå’Œå¯†é›†åœºæ™¯æµä¼°è®¡æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº† 38.64% å’Œ 25.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08808",
            "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
            "url": "https://huggingface.co/papers/2602.08808",
            "abstract": "A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
            "score": 2,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "7cb6296b3849734d",
            "authors": [
                "Yapei Chang",
                "Kyle Lo",
                "Mohit Iyyer",
                "Luca Soldaini"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Maryland",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08808.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° How2Everything - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 351 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (How2Mine), ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 7 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² (How2Bench) Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ LLM-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ (How2Score). Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 10 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ."
                },
                "en": {
                    "title": "Enhancing Instruction Quality with Scalable Evaluation Frameworks",
                    "desc": "This paper presents How2Everything, a framework designed to evaluate and enhance the generation of goal-oriented procedures using large-scale web data. It introduces How2Mine, which collects a vast number of procedures from the internet, and How2Bench, a balanced evaluation set for assessing the quality of these procedures. The framework employs How2Score, an evaluation method that utilizes a language model to identify critical failures in generated instructions. Additionally, reinforcement learning is applied to improve the performance of models based on the scoring system, demonstrating significant advancements in procedural generation without compromising on standard benchmarks."
                },
                "zh": {
                    "title": "æå‡ç›®æ ‡å¯¼å‘ç¨‹åºç”Ÿæˆçš„å¯æ‰©å±•æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºHow2Everythingçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›åŸºäºç›®æ ‡çš„ç¨‹åºç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ç½‘ç»œä¸ŠæŒ–æ˜351Kä¸ªç¨‹åºï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«7000ä¸ªç¤ºä¾‹çš„è¯„ä¼°é›†How2Benchï¼Œä»¥ä¾¿äºå¤§è§„æ¨¡è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†How2Scoreè¯„ä¼°åè®®ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ¤æ–­ç”Ÿæˆå†…å®¹æ˜¯å¦å­˜åœ¨å…³é”®æ€§é”™è¯¯ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨How2Scoreä½œä¸ºå¥–åŠ±ï¼Œæ¨¡å‹åœ¨How2Benchä¸Šçš„è¡¨ç°æé«˜äº†è¶…è¿‡10åˆ†ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç½‘ç»œæ•°æ®è¿›è¡Œèƒ½åŠ›è¯„ä¼°å’Œæ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08829",
            "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
            "url": "https://huggingface.co/papers/2602.08829",
            "abstract": "WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "d398683bec2a3790",
            "authors": [
                "Hao Peng",
                "Yunjia Qi",
                "Xiaozhi Wang",
                "Zijun Yao",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08829.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#rlhf",
                    "#data"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WildReward â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸Ğ· 186 Ñ‚Ñ‹ÑÑÑ‡ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ DPO Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Harnessing User Diversity for Superior Reward Models",
                    "desc": "WildReward introduces a novel approach to training reward models using ordinal regression based on real user interactions, rather than relying on traditional human-annotated preference pairs. This method leverages the implicit feedback from diverse users, resulting in a rich dataset of 186,000 high-quality instances for training. The experiments show that WildReward not only matches but can outperform conventional reward models in terms of performance, calibration, and consistency across samples. Additionally, the model benefits from user diversity, indicating that a broader user base leads to stronger reward model performance."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”¨æˆ·äº¤äº’æå‡å¥–åŠ±æ¨¡å‹çš„å¤šæ ·æ€§ä¸æ€§èƒ½",
                    "desc": "WildRewardå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç”¨æˆ·çš„è‡ªç„¶äº¤äº’æ¥æœ‰æ•ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½¿ç”¨äº†åºæ•°å›å½’çš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒWildRewardåœ¨æ€§èƒ½ä¸Šç›¸å½“ï¼ŒåŒæ—¶ä¹Ÿåˆ©ç”¨äº†ç”¨æˆ·çš„å¤šæ ·æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›´æ¥ä»ç”¨æˆ·åé¦ˆä¸­æå–çš„é«˜è´¨é‡å®ä¾‹å¯ä»¥æ˜¾è‘—æå‡å¥–åŠ±æ¨¡å‹çš„æ•ˆæœã€‚æœ€ç»ˆï¼ŒWildRewardåœ¨åœ¨çº¿DPOè®­ç»ƒä¸­åº”ç”¨ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08543",
            "title": "GISA: A Benchmark for General Information-Seeking Assistant",
            "url": "https://huggingface.co/papers/2602.08543",
            "abstract": "A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "bba14fa5a052b07d",
            "authors": [
                "Yutao Zhu",
                "Xingshuo Zhang",
                "Maosen Zhang",
                "Jiajie Jin",
                "Liancheng Zhang",
                "Xiaoshuai Song",
                "Kangzhi Zhao",
                "Wencong Zeng",
                "Ruiming Tang",
                "Han Li",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08543.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "GISA: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GISA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 373 ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ğ½Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°."
                },
                "en": {
                    "title": "GISA: A New Benchmark for Real-World Information-Seeking Evaluation",
                    "desc": "The paper introduces GISA, a new benchmark designed to evaluate information-seeking assistants, particularly those powered by large language models. It features 373 human-crafted queries that mimic real-world information-seeking scenarios and includes structured answer formats like lists and tables for clear evaluation. GISA also incorporates live updates to answers, which helps prevent models from simply memorizing responses. The results show that even top-performing models struggle with complex tasks, indicating significant opportunities for enhancement in this area."
                },
                "zh": {
                    "title": "GISAï¼šè¯„ä¼°ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹çš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•GISAï¼Œç”¨äºè¯„ä¼°ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ã€‚GISAåŒ…å«373ä¸ªç”±äººç±»è®¾è®¡çš„æŸ¥è¯¢ï¼Œåæ˜ çœŸå®çš„ä¿¡æ¯æ£€ç´¢åœºæ™¯ï¼Œå¹¶æä¾›å››ç§ç»“æ„åŒ–ç­”æ¡ˆæ ¼å¼ã€‚è¯¥åŸºå‡†æµ‹è¯•ç»“åˆäº†æ·±åº¦æ¨ç†å’Œå¹¿æ³›çš„ä¿¡æ¯èšåˆï¼Œä¸”åŒ…å«å®æ—¶æ›´æ–°çš„ç­”æ¡ˆï¼Œä»¥é˜²æ­¢è®°å¿†åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶å‡†ç¡®åŒ¹é…ç‡ä¹Ÿä»…ä¸º19.30%ï¼Œè¡¨æ˜åœ¨å¤æ‚è§„åˆ’å’Œä¿¡æ¯æ”¶é›†ä»»åŠ¡ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07970",
            "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
            "url": "https://huggingface.co/papers/2602.07970",
            "abstract": "Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
            "score": 1,
            "issue_id": 982,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "11607ec65d9805f1",
            "authors": [
                "Zheyuan Hu",
                "Weitao Chen",
                "Cengiz Ã–ztireli",
                "Chenliang Zhou",
                "Fangcheng Zhong"
            ],
            "affiliations": [
                "University of Cambridge, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07970.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… (Ğ£Ğ§ĞŸ) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CNF Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ£Ğ§ĞŸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Harnessing Neural Networks for Advanced PDE Solutions",
                    "desc": "This paper investigates the use of neural network frameworks for solving Partial Differential Equations (PDEs) in scientific simulations. It addresses challenges such as high computational costs and the curse of dimensionality that traditional numerical methods face. The research extends the recent Continuous Normalizing Flow (CNF) framework to handle multi-variable and non-linear PDEs, focusing on applications like forward solutions, inverse problems, and equation discovery. The findings include the implementation of various methods, self-tuning techniques, and a thorough evaluation of neural PDE solvers in practical scenarios."
                },
                "zh": {
                    "title": "æ¢ç´¢åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨çš„æœªæ¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰æ±‚è§£å™¨ï¼ŒåŒ…æ‹¬ç”¨äºç§‘å­¦æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒPDEæ±‚è§£å™¨çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å°†å…¶åº”ç”¨äºç‰¹å®šçš„ç§‘å­¦æ¨¡æ‹Ÿé—®é¢˜ï¼Œå¦‚æ­£å‘è§£ã€é€†é—®é¢˜å’Œæ–¹ç¨‹å‘ç°ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æ‰©å±•äº†æœ€è¿‘çš„CNFæ¡†æ¶æ±‚è§£å™¨ï¼Œä»¥é€‚åº”å¤šå˜é‡å’Œéçº¿æ€§è®¾ç½®ï¼Œå¹¶è¿›è¡Œä¸‹æ¸¸åº”ç”¨ã€‚ç ”ç©¶ç»“æœåŒ…æ‹¬æ‰€é€‰æ–¹æ³•çš„å®ç°ã€è‡ªè°ƒæŠ€æœ¯ã€åŸºå‡†é—®é¢˜çš„è¯„ä¼°ä»¥åŠå¯¹ç¥ç»PDEæ±‚è§£å™¨å’Œç§‘å­¦æ¨¡æ‹Ÿåº”ç”¨çš„å…¨é¢è°ƒæŸ¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07803",
            "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
            "url": "https://huggingface.co/papers/2602.07803",
            "abstract": "A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "89a41e234847ede2",
            "authors": [
                "Jiale Qian",
                "Hao Meng",
                "Tian Zheng",
                "Pengcheng Zhu",
                "Haopeng Lin",
                "Yuhang Dai",
                "Hanke Xie",
                "Wenxiao Cao",
                "Ruixuan Shang",
                "Jun Wu",
                "Hongmei Liu",
                "Hanlin Wen",
                "Jian Zhao",
                "Zhonglin Jiang",
                "Yong Chen",
                "Shunshun Yin",
                "Ming Tao",
                "Jianguo Wei",
                "Lei Xie",
                "Xinsheng Wang"
            ],
            "affiliations": [
                "AI Center, Geely Automobile Research Institute (Ningbo) Co., Ltd., Ningbo, China",
                "Audio, Speech and Language Processing Group (ASLP@NPU), Northwestern Polytechnical University, Xian, China",
                "Audio-Visual Cognitive Computing Team, Tianjin University, Tianjin, China",
                "Soul AI Lab, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07803.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#audio",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SoulX-Singer â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ² (Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¸Ğ¹, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ ĞºĞ°Ğ½Ñ‚Ğ¾Ğ½ÑĞºĞ¸Ğ¹) Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MIDI-Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 42 000 Ñ‡Ğ°ÑĞ°Ñ… Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SoulX-Singer-Eval Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (zero-shot) Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°."
                },
                "en": {
                    "title": "SoulX-Singer: Revolutionizing Singing Voice Synthesis with Flexibility and Quality",
                    "desc": "This paper presents SoulX-Singer, an advanced open-source singing voice synthesis (SVS) system that enhances the quality and flexibility of AI-generated singing. It allows for controllable singing generation based on musical scores or melodies, making it suitable for various production needs. The system is trained on a vast dataset of over 42,000 hours of vocal recordings, supporting multiple languages including Mandarin, English, and Cantonese, while achieving high synthesis quality. Additionally, the authors introduce SoulX-Singer-Eval, a benchmark designed to evaluate the system's zero-shot performance, ensuring reliable assessments in real-world applications."
                },
                "zh": {
                    "title": "é«˜è´¨é‡å¼€æºå”±æ­Œåˆæˆç³»ç»ŸSoulX-Singer",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜è´¨é‡çš„å¼€æºå”±æ­Œå£°éŸ³åˆæˆç³»ç»ŸSoulX-Singerï¼Œæ”¯æŒå¤šç§è¯­è¨€å’Œå¯æ§ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ç¬¦å·éŸ³ä¹ä¹è°±ï¼ˆMIDIï¼‰æˆ–æ—‹å¾‹è¡¨ç¤ºè¿›è¡Œçµæ´»çš„å”±æ­Œç”Ÿæˆï¼Œé€‚ç”¨äºå®é™…ç”Ÿäº§å·¥ä½œæµç¨‹ã€‚SoulX-Singeråœ¨è¶…è¿‡42,000å°æ—¶çš„å£°ä¹æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæ™®é€šè¯ã€è‹±è¯­å’Œç²¤è¯­ï¼Œå¹¶åœ¨ä¸åŒéŸ³ä¹æ¡ä»¶ä¸‹å§‹ç»ˆå®ç°æœ€å…ˆè¿›çš„åˆæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†SoulX-Singer-EvalåŸºå‡†ï¼Œä»¥ä¾¿åœ¨å®é™…åœºæ™¯ä¸­å¯é åœ°è¯„ä¼°é›¶-shotåˆæˆæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07796",
            "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
            "url": "https://huggingface.co/papers/2602.07796",
            "abstract": "Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  \t\t\t\t\tAI-generated summary \t\t\t\t Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
            "score": 1,
            "issue_id": 982,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "43b8400b761765b6",
            "authors": [
                "Jiatong Li",
                "Changdae Oh",
                "Hyeong Kyu Choi",
                "Jindong Wang",
                "Sharon Li"
            ],
            "affiliations": [
                "University of Wisconsin-Madison",
                "William & Mary"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07796.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾? ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Transparency Over Thinking: Enhancing LLM Performance in User Engagement",
                    "desc": "This paper investigates how explicit reasoning in large language models (LLMs) affects their performance in user-engaged scenarios. The authors find that requiring LLMs to think explicitly can actually harm their ability to communicate effectively with users, leading to shorter and less informative responses. Through extensive experiments, they show that this 'introverted' behavior results in degraded performance on various tasks. The study highlights the importance of transparency in agent interactions, suggesting that encouraging information disclosure can enhance LLM performance in real-world applications."
                },
                "zh": {
                    "title": "ä¿¡æ¯é€æ˜åº¦æ˜¯æå‡ä»£ç†æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨ç”¨æˆ·äº¤äº’åœºæ™¯ä¸­ï¼Œæ˜¾å¼æ¨ç†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¼ºåˆ¶æ¨ç†å¾€å¾€å¯¼è‡´ä»£ç†çš„è¡¨ç°ä¸‹é™ï¼Œå› ä¸ºå®ƒä½¿å¾—ä»£ç†çš„å›ç­”å˜å¾—æ›´ç®€çŸ­ï¼Œå‡å°‘äº†ä¸ç”¨æˆ·çš„ä¿¡æ¯äº¤æµã€‚é€šè¿‡å¯¹ä¸ƒä¸ªæ¨¡å‹å’Œå¤šä¸ªåŸºå‡†çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸»åŠ¨çš„ä¿¡æ¯é€æ˜åº¦å¯ä»¥æ˜¾è‘—æå‡ä»£ç†çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œä¿¡æ¯é€æ˜åº¦æ„è¯†æ˜¯æœªæ¥æ¨ç†ä»£ç†è®¾è®¡ä¸­ä¸€ä¸ªé‡è¦ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07090",
            "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
            "url": "https://huggingface.co/papers/2602.07090",
            "abstract": "SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "988a2e9f31f78d97",
            "authors": [
                "Yu-Che Tsai",
                "Hsiang Hsiao",
                "Kuan-Yu Chen",
                "Shou-De Lin"
            ],
            "affiliations": [
                "Department of Computer Science and Information Engineering, National Taiwan University",
                "National Taiwan University AI Center of Research Excellence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07090.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#leakage"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²: Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ĞµĞ¿Ğ¾Ğ³Ğ¾",
                    "desc": "SPARSE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¾Ñ‚ ÑƒÑ‚ĞµÑ‡ĞµĞº Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ, Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞœĞ°Ñ…Ğ°Ğ»Ğ°Ğ½Ğ¾Ğ±Ğ¸ÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ»Ğ»Ğ¸Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, SPARSE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞº Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "SPARSE: Tailored Privacy for Text Embeddings",
                    "desc": "SPARSE is a framework designed to enhance the privacy of text embeddings by focusing on user-defined sensitive dimensions. It uses differentiable masking to identify which dimensions of the embeddings are sensitive and need protection. Additionally, it employs Mahalanobis noise calibration to apply targeted noise to these sensitive dimensions, rather than uniformly across all dimensions. This approach minimizes privacy leakage while maintaining the utility of the embeddings for various natural language processing tasks."
                },
                "zh": {
                    "title": "SPARSEï¼šä¿æŠ¤æ–‡æœ¬åµŒå…¥éšç§çš„æ–°æ–¹æ³•",
                    "desc": "SPARSEæ˜¯ä¸€ä¸ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§åœ°æ‰°åŠ¨æ•æ„Ÿç»´åº¦æ¥ä¿æŠ¤æ–‡æœ¬åµŒå…¥çš„éšç§ã€‚å®ƒç»“åˆäº†å¯å¾®åˆ†æ©ç å­¦ä¹ å’Œé©¬å“ˆæ‹‰è¯ºæ¯”æ–¯å™ªå£°æ ¡å‡†ï¼Œèƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·å®šä¹‰æ¦‚å¿µçš„éšç§æ•æ„Ÿç»´åº¦ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€æ•æ„Ÿæ€§å‡è®¾ä¸åŒï¼ŒSPARSEåœ¨ä¿æŒéæ•æ„Ÿè¯­ä¹‰çš„åŒæ—¶ï¼Œä¸“æ³¨äºæ‰°åŠ¨éšç§æ•æ„Ÿçš„ç»´åº¦ã€‚ç»è¿‡åœ¨å…­ä¸ªæ•°æ®é›†å’Œä¸‰ç§åµŒå…¥æ¨¡å‹çš„è¯„ä¼°ï¼ŒSPARSEåœ¨å‡å°‘éšç§æ³„éœ²çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰å·®åˆ†éšç§æ–¹æ³•çš„ä¸‹æ¸¸æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06445",
            "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
            "url": "https://huggingface.co/papers/2602.06445",
            "abstract": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
            "score": 1,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a859fde68e6d19ae",
            "authors": [
                "Weidong Huang",
                "Jingwen Zhang",
                "Jiongye Li",
                "Shibowen Zhang",
                "Jiayang Wu",
                "Jiayi Wang",
                "Hangxin Liu",
                "Yaodong Yang",
                "Yao Su"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Department of Automation, University of Science and Technology of China",
                "Department of Computer Science, Harbin Institute of Technology",
                "Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06445.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ECO (Energy-Constrained Optimization) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ-Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶ĞµĞ² Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ BRUCE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ECO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ…Ğ¾Ğ´ÑŒĞ±Ğµ."
                },
                "en": {
                    "title": "ECO: Revolutionizing Energy-Efficient Humanoid Robot Locomotion",
                    "desc": "The paper presents a new framework called Energy-Constrained Optimization (ECO) for improving the locomotion of humanoid robots. It separates energy metrics from rewards using a Lagrangian method, allowing for clearer constraints on energy consumption. This approach reduces the need for extensive hyperparameter tuning, leading to more efficient and stable walking patterns. Experimental results show that ECO significantly lowers energy use while maintaining robust performance compared to existing methods."
                },
                "zh": {
                    "title": "èƒ½é‡çº¦æŸä¼˜åŒ–ï¼šæå‡ç±»äººæœºå™¨äººè¡Œèµ°æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½é‡çº¦æŸä¼˜åŒ–æ¡†æ¶ï¼ˆECOï¼‰ï¼Œæ—¨åœ¨æé«˜ç±»äººæœºå™¨äººåœ¨è¡Œèµ°æ—¶çš„èƒ½é‡æ•ˆç‡å’Œç¨³å®šæ€§ã€‚é€šè¿‡å°†èƒ½é‡ç›¸å…³æŒ‡æ ‡ä¸å¥–åŠ±åˆ†ç¦»ï¼Œå¹¶å°†å…¶é‡æ–°è¡¨è¿°ä¸ºæ˜ç¡®çš„ä¸ç­‰å¼çº¦æŸï¼ŒECOç®€åŒ–äº†è¶…å‚æ•°è°ƒä¼˜çš„è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥æ–¹æ³•å¼•å…¥ä¸“é—¨çš„èƒ½é‡æ¶ˆè€—å’Œå‚è€ƒè¿åŠ¨çº¦æŸï¼Œä»è€Œå®ç°å¯¹ç±»äººæœºå™¨äººç¨³å®šã€å¯¹ç§°ä¸”é«˜æ•ˆçš„è¡Œèµ°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECOåœ¨èƒ½é‡æ¶ˆè€—ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è¡Œèµ°æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07054",
            "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
            "url": "https://huggingface.co/papers/2602.07054",
            "abstract": "A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.",
            "score": 1,
            "issue_id": 982,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "9f87bc5a2487213a",
            "authors": [
                "Ashutosh Chaubey",
                "Jiacheng Pang",
                "Maksim Siniukov",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "Institute for Creative Technologies, University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07054.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#video",
                    "#hallucinations",
                    "#audio",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ˜Š",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EmoReAlM Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ AVEm-DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ preference optimization Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 6-19% Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… DFEW, RAVDESS Ğ¸ EMER."
                },
                "en": {
                    "title": "Enhancing Emotion Understanding in AI with Robust Benchmarking and Optimization",
                    "desc": "This paper introduces EmoReAlM, a benchmark designed to evaluate multimodal large language models (MLLMs) on their ability to understand emotions through audiovisual cues. It addresses two main challenges: spurious associations between emotions and irrelevant cues, and hallucinations where the model generates incorrect audiovisual information based on text. The authors propose AVEm-DPO, an optimization technique that aligns model outputs with both audiovisual inputs and emotion-focused queries, while penalizing over-reliance on text priors. Experimental results show significant performance improvements in emotion understanding tasks, demonstrating the effectiveness of the proposed methods."
                },
                "zh": {
                    "title": "æå‡æƒ…æ„Ÿç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºå‡†å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶ä¸­è¯†åˆ«äº†æƒ…æ„Ÿä¸æ— å…³è§†å¬çº¿ç´¢ä¹‹é—´çš„è™šå‡å…³è”å’Œç”±æ–‡æœ¬å…ˆéªŒå¼•èµ·çš„è§†å¬çº¿ç´¢å¹»è§‰è¿™ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EmoReAlMåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿçº¿ç´¢å…³è”ã€å¹»è§‰å’Œæ¨¡æ€ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æå‡ºAVEm-DPOä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½æ¨¡å‹å“åº”ä¸è§†å¬è¾“å…¥å’Œæƒ…æ„Ÿä¸­å¿ƒæŸ¥è¯¢ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07775",
            "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
            "url": "https://huggingface.co/papers/2602.07775",
            "abstract": "Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
            "score": 0,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "b20640068525b746",
            "authors": [
                "Haodong Li",
                "Shaoteng Liu",
                "Zhe Lin",
                "Manmohan Chandraker"
            ],
            "affiliations": [
                "Adobe Research",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07775.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘ĞµÑĞ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…, Ğ½Ğ¾ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rolling Sink, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞµĞ¼ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° (Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Rolling Sink Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Ultra-Long Video Synthesis with Rolling Sink",
                    "desc": "This paper introduces Rolling Sink, a training-free method designed to improve the performance of autoregressive video diffusion models when generating long videos. Traditional models face a train-test gap, where the quality of generated videos degrades significantly when tested beyond their training duration. Rolling Sink addresses this issue by maintaining an autoregressive cache, allowing for the synthesis of ultra-long videos without the need for extensive training. The method demonstrates superior visual fidelity and temporal consistency, enabling the generation of coherent and stable videos lasting from 5 to 30 minutes."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„è¶…é•¿è§†é¢‘åˆæˆæ–°æ–¹æ³•",
                    "desc": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶å­˜åœ¨è®­ç»ƒä¸æµ‹è¯•ä¹‹é—´çš„å·®è·ï¼Œå¯¼è‡´è§†è§‰è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºRolling Sinkï¼Œå®ƒé€šè¿‡ç»´æŠ¤è‡ªå›å½’ç¼“å­˜æ¥å®ç°è¶…é•¿è§†é¢‘åˆæˆã€‚è¯¥æ–¹æ³•åŸºäºSelf Forcingçš„ç ”ç©¶ï¼Œä¸“æ³¨äºè®­ç»ƒæ—¶é™ä¸æµ‹è¯•æ—¶é™ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRolling Sinkåœ¨é•¿æ—¶é—´è§†é¢‘åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¸€è‡´çš„ä¸»é¢˜ã€ç¨³å®šçš„è‰²å½©å’Œæµç•…çš„è¿åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07040",
            "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
            "url": "https://huggingface.co/papers/2602.07040",
            "abstract": "Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.   We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.   Aster is accessible via a web interface and API at asterlab.ai.",
            "score": 0,
            "issue_id": 981,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "c0356800a9ac2db2",
            "authors": [
                "Emmett Bicker"
            ],
            "affiliations": [
                "Aster AI Labs Inc., San Francisco, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07040.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#agents",
                    "#plp"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "Aster â€” ÑÑ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Aster Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU-ÑĞ´ĞµÑ€, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ—Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Aster: Revolutionizing Scientific Discovery with Speed and Efficiency",
                    "desc": "Aster is an advanced AI agent designed to enhance scientific discovery by iteratively refining existing programs. It operates over 20 times faster than traditional frameworks, allowing for quicker improvements and achieving state-of-the-art results in various fields such as mathematics and biology. By significantly reducing the number of iterations needed for program enhancement, Aster can tackle complex tasks that require long evaluation times, like extensive machine learning training. Its effectiveness is demonstrated across multiple applications, achieving top results while using far less computational power than conventional methods."
                },
                "zh": {
                    "title": "Asterï¼šåŠ é€Ÿç§‘å­¦å‘ç°çš„æ™ºèƒ½ä»£ç†",
                    "desc": "Asteræ˜¯ä¸€ä¸ªè‡ªä¸»ç§‘å­¦å‘ç°çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿä»¥è¶…è¿‡ç°æœ‰æ¡†æ¶20å€çš„é€Ÿåº¦è¿è¡Œã€‚å®ƒé€šè¿‡è¿­ä»£æ”¹è¿›ç¨‹åºï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€ç”Ÿç‰©å­¦å’Œæœºå™¨å­¦ä¹ ï¼‰ä¸­å®ç°æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—éœ€æ±‚ã€‚Asterçš„è¿­ä»£æ¬¡æ•°å¤§å¹…å‡å°‘ï¼Œä½¿å¾—å¤„ç†é•¿æ—¶é—´è¯„ä¼°çš„ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä¾‹å¦‚å¤šå°æ—¶çš„æœºå™¨å­¦ä¹ è®­ç»ƒã€‚Asteråœ¨å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-09.html",
    "link_next": "2026-02-11.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "09.02",
        "en": "02/09",
        "zh": "2æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 13,
        "#agents": 8,
        "#cv": 2,
        "#rl": 7,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 2,
        "#inference": 5,
        "#3d": 1,
        "#audio": 3,
        "#video": 6,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 16,
        "#robotics": 4,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 9,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 10,
        "#survey": 2,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 10,
        "#small_models": 2,
        "#science": 3,
        "#low_resource": 1
    }
}