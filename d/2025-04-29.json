{
    "date": {
        "ru": "29 апреля",
        "en": "April 29",
        "zh": "4月29日"
    },
    "time_utc": "2025-04-29 08:16",
    "weekday": 1,
    "issue_id": 3485,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 15,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "LLM революционизируют автоматизацию мобильных интерфейсов",
                    "desc": "Статья представляет систематический обзор агентов графического интерфейса для телефонов, управляемых большими языковыми моделями (LLM). Рассматривается эволюция от скриптовой автоматизации к интеллектуальным адаптивным системам, решающим проблемы ограниченной универсальности, высоких затрат на обслуживание и слабого понимания намерений. Авторы предлагают таксономию, охватывающую основные фреймворки агентов, подходы к моделированию и ключевые наборы данных. Обсуждаются открытые проблемы, такие как разнообразие данных, эффективность развертывания на устройствах и вопросы безопасности."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "大型语言模型驱动的手机自动化变革",
                    "desc": "随着大型语言模型（LLMs）的快速发展，手机自动化发生了变革性变化。本文系统回顾了基于LLM的手机图形用户界面（GUI）代理，强调了它们从基于脚本的自动化到智能自适应系统的演变。我们首先阐明了关键挑战，包括有限的通用性、高维护成本和意图理解薄弱，并展示了LLM如何通过先进的语言理解、多模态感知和强大的决策能力来解决这些问题。最后，我们讨论了数据集多样性、设备端部署效率、以用户为中心的适应性和安全性等开放挑战，为这一快速发展的领域提供前瞻性见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19724",
            "title": "RepText: Rendering Visual Text via Replicating",
            "url": "https://huggingface.co/papers/2504.19724",
            "abstract": "Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.",
            "score": 15,
            "issue_id": 3481,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "daf4c90a7de01768",
            "authors": [
                "Haofan Wang",
                "Yujia Xu",
                "Yimeng Li",
                "Junchen Li",
                "Chaowei Zhang",
                "Jing Wang",
                "Kejia Yang",
                "Zhibo Chen"
            ],
            "affiliations": [
                "Liblib AI",
                "Shakker Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19724.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#multilingual",
                    "#video",
                    "#open_source",
                    "#low_resource",
                    "#multimodal"
                ],
                "emoji": "🖋️",
                "ru": {
                    "title": "RepText: Точная визуализация многоязычного текста в генеративных моделях",
                    "desc": "Статья представляет RepText - метод, позволяющий моделям генерации изображений по тексту точно отображать многоязычный текст заданным шрифтом. Авторы используют подход ControlNet, интегрируя глифы и позиции символов для гармоничной визуализации текста. Для повышения точности применяется перцептивная потеря текста наряду с диффузионной. Эксперименты показывают, что RepText превосходит существующие открытые методы и сравним с закрытыми многоязычными моделями."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Multilingual Typography",
                    "desc": "This paper introduces RepText, a novel approach to enhance text-to-image generation models by enabling them to accurately replicate multilingual text in various fonts without needing to understand the text. The method leverages a combination of language agnostic glyphs and position information to create visually coherent text that users can customize. To improve the rendering accuracy, the authors implement a text perceptual loss alongside diffusion loss and utilize a specific initialization strategy during inference. Extensive experiments demonstrate that RepText outperforms existing open-source models and achieves results comparable to advanced closed-source systems, while also addressing its limitations."
                },
                "zh": {
                    "title": "赋能文本生成，超越语言限制",
                    "desc": "尽管现代文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著突破，但在生成精确和灵活的排版元素，尤其是非拉丁字母方面仍然存在限制。为了解决这些问题，我们提出了RepText，它旨在赋予预训练的单语文本到图像生成模型准确渲染多语言视觉文本的能力，而无需真正理解这些文本。我们采用了ControlNet的设置，并整合了与语言无关的字形和渲染文本的位置，以便生成和谐的视觉文本，允许用户根据需求自定义文本内容、字体和位置。通过大量实验，我们验证了RepText的有效性，相较于现有方法，我们的方法在开源模型中表现更优，并与本地多语言闭源模型的结果相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19093",
            "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
            "url": "https://huggingface.co/papers/2504.19093",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.",
            "score": 8,
            "issue_id": 3483,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "c9ba3c45d7c25d9f",
            "authors": [
                "Yu Li",
                "Qizhi Pei",
                "Mengyuan Sun",
                "Honglin Lin",
                "Chenlin Ming",
                "Xin Gao",
                "Jiang Wu",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19093.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "CipherBank: раскрывая границы криптографического мышления LLM",
                    "desc": "Данная статья представляет CipherBank - комплексный бенчмарк для оценки способностей больших языковых моделей (LLM) в задачах криптографического дешифрования. CipherBank включает 2358 тщательно разработанных задач, охватывающих 262 уникальных открытых текста в 5 доменах и 14 поддоменах, с акцентом на конфиденциальные и реальные сценарии. В исследовании оцениваются современные LLM, такие как GPT-4o и DeepSeek-V3, а также модели, ориентированные на рассуждение, например o1 и DeepSeek-R1. Результаты выявляют значительные пробелы в способностях рассуждения между различными типами моделей при применении к классическим задачам криптографического дешифрования."
                },
                "en": {
                    "title": "Unlocking Cryptography: Evaluating LLMs with CipherBank",
                    "desc": "This paper presents CipherBank, a new benchmark for assessing the reasoning skills of large language models (LLMs) in cryptographic decryption tasks. It includes 2,358 problems across various domains, focusing on real-world scenarios that involve encryption. The study evaluates several state-of-the-art LLMs, revealing significant performance gaps in their ability to handle classical cryptographic methods. The findings highlight the limitations of current models and suggest areas for improvement in their reasoning capabilities related to cryptography."
                },
                "zh": {
                    "title": "提升LLMs在密码学推理中的能力",
                    "desc": "大型语言模型（LLMs）在推理能力方面取得了显著进展，但在密码学领域的推理能力仍然未被充分探索。本文介绍了CipherBank，这是一个全面的基准测试，旨在评估LLMs在密码解密任务中的推理能力。CipherBank包含2358个精心设计的问题，涵盖262个独特的明文，涉及5个领域和14个子领域，重点关注需要加密的隐私敏感和现实场景。我们的研究结果显示，当前的推理专注模型在处理经典密码解密任务时面临显著挑战，强调了LLMs在理解和操作加密数据方面的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17258",
            "title": "Group Downsampling with Equivariant Anti-aliasing",
            "url": "https://huggingface.co/papers/2504.17258",
            "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature maps) on general finite groups with anti-aliasing. This involves the following: (a) Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup. (b) Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into G-equivariant networks",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "c8bf8ac7bd71ca11",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Даунсэмплинг в групповых сверточных сетях: обобщение и антиалиасинг",
                    "desc": "Эта статья исследует обобщение слоев даунсэмплинга для групповых эквивариантных сверточных нейронных сетей (G-CNN). Авторы предлагают алгоритм для выбора подходящей подгруппы при заданной конечной группе и коэффициенте даунсэмплинга. Они также изучают понятие ограниченности полосы частот и предлагают метод антиалиасинга для общих конечных групп. Эксперименты показывают, что предложенная операция даунсэмплинга улучшает точность, лучше сохраняет эквивариантность и уменьшает размер модели при использовании в G-эквивариантных сетях."
                },
                "en": {
                    "title": "Enhancing G-CNNs with Advanced Downsampling Techniques",
                    "desc": "This paper explores the role of downsampling layers in convolutional neural networks (CNNs), particularly focusing on group equivariant architectures like G-CNNs. The authors propose a new algorithm for downsampling feature maps on finite groups while ensuring anti-aliasing, which helps maintain the integrity of the signals. They introduce concepts such as bandlimited-ness and demonstrate how their method aligns with classical sampling theory, especially for periodic signals. Experimental results show that their approach enhances accuracy, preserves equivariance, and reduces the overall size of the model in image classification tasks."
                },
                "zh": {
                    "title": "提升CNN性能的下采样新方法",
                    "desc": "本文研究了在群等变架构中均匀下采样层的推广，特别是针对G-CNNs。我们提出了一种算法，可以在给定有限群和下采样率的情况下选择合适的子群。我们还探讨了带限性概念，并提出了如何进行抗混叠处理的方法。实验结果表明，所提出的下采样操作在图像分类任务中提高了准确性，更好地保持了等变性，并减少了模型大小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15780",
            "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
            "url": "https://huggingface.co/papers/2504.15780",
            "abstract": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "e736decf98fa2805",
            "authors": [
                "Daocheng Fu",
                "Zijun Chen",
                "Renqiu Xia",
                "Qi Liu",
                "Yuan Feng",
                "Hongbin Zhou",
                "Renrui Zhang",
                "Shiyang Feng",
                "Peng Gao",
                "Junchi Yan",
                "Botian Shi",
                "Bo Zhang",
                "Yu Qiao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15780.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#math",
                    "#data",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "TrustGeoGen: Надежная генерация геометрических задач для развития искусственного интеллекта",
                    "desc": "Эта статья представляет TrustGeoGen - масштабируемый движок для генерации геометрических задач с формальной верификацией. TrustGeoGen создает многомодальные данные, включая диаграммы, текстовые описания и пошаговые решения, обеспечивая логическую согласованность. На основе этого движка был создан датасет GeoTrust-200K и тестовый набор GeoTrust-test, на котором современные модели машинного обучения достигают точности всего 49.17%. Модели, обученные на GeoTrust, демонстрируют лучшую обобщающую способность и меньше логических несоответствий по сравнению с псевдо-разметкой OpenAI-o1."
                },
                "en": {
                    "title": "TrustGeoGen: Ensuring Reliable Geometric Problem Solving",
                    "desc": "This paper addresses the challenges in mathematical geometric problem solving (GPS) by introducing TrustGeoGen, a scalable data engine designed for generating reliable problem sets. It emphasizes the importance of integrating multimodal information and ensuring logical coherence through formal verification. The engine innovatively creates geometric data with aligned diagrams, textual descriptions, and solutions while maintaining rule compliance. The resulting GeoTrust-200K dataset and GeoTrust-test benchmark demonstrate improved evaluation standards and out-of-distribution generalization for models trained on this data."
                },
                "zh": {
                    "title": "信任几何：提升几何问题解决的可靠性",
                    "desc": "本文提出了一种名为TrustGeoGen的数据引擎，用于生成数学几何问题，并通过形式验证提供可靠的基准。该引擎通过四个关键创新实现几何数据的合成，包括多模态对齐生成、形式验证、递归状态生成的复杂性升级机制，以及同时生成多解变体的算法。实验结果表明，现有的最先进模型在GeoTrust-test上的准确率仅为49.17%，显示出评估的严格性。通过在GeoTrust上训练的模型在GeoQA上实现了OOD泛化，显著减少了逻辑不一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16083",
            "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
            "url": "https://huggingface.co/papers/2504.16083",
            "abstract": "The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.",
            "score": 3,
            "issue_id": 3482,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "8ba72cd8f5463bbe",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16083.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обработки длинных мультимодальных данных в VLM без потери точности",
                    "desc": "Статья представляет MMInference - метод динамического разреженного внимания для ускорения обработки длинных мультимодальных входных данных в моделях зрения и языка (VLM). Авторы обнаружили уникальный паттерн разреженности для видеоданных и разработали метод на основе перестановок для его использования. MMInference динамически строит распределение разреженности на основе входных данных и включает оптимизированные ядра GPU для эффективных вычислений. Эксперименты показали ускорение этапа предварительного заполнения до 8,3 раз при сохранении точности на различных мультимодальных задачах."
                },
                "en": {
                    "title": "Accelerating Vision Language Models with Dynamic Sparse Attention",
                    "desc": "This paper presents MMInference, a novel method designed to enhance Vision Language Models (VLMs) by addressing the challenge of quadratic attention complexity during the pre-filling phase. By analyzing the unique sparse patterns in video inputs, specifically the Grid pattern, the authors develop a dynamic sparse attention mechanism that optimizes the processing of long-context multi-modal data. MMInference allows for efficient computation by dynamically constructing sparse distributions based on input characteristics, without requiring any modifications to existing VLM architectures. Experimental results demonstrate that this approach significantly accelerates the pre-filling stage, achieving up to 8.3 times faster processing while preserving accuracy across various multi-modal benchmarks."
                },
                "zh": {
                    "title": "加速多模态输入的预填充，释放视觉语言模型的潜力",
                    "desc": "本论文介绍了一种名为MMInference的动态稀疏注意力方法，旨在加速长上下文多模态输入的预填充阶段。通过分析视频输入的时间和空间局部性，我们发现了一种独特的稀疏模式，即网格模式。同时，不同模态的视觉语言模型（VLM）展现出显著不同的稀疏分布。MMInference能够在不修改模型或微调的情况下，动态构建稀疏分布，从而在多模态基准测试中显著提高预填充速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18919",
            "title": "Clinical knowledge in LLMs does not translate to human interactions",
            "url": "https://huggingface.co/papers/2504.18919",
            "abstract": "Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.",
            "score": 2,
            "issue_id": 3480,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 апреля",
                "en": "April 26",
                "zh": "4月26日"
            },
            "hash": "25dc210bebba9c94",
            "authors": [
                "Andrew M. Bean",
                "Rebecca Payne",
                "Guy Parsons",
                "Hannah Rose Kirk",
                "Juan Ciro",
                "Rafael Mosquera",
                "Sara Hincapié Monsalve",
                "Aruna S. Ekanayaka",
                "Lionel Tarassenko",
                "Luc Rocher",
                "Adam Mahdi"
            ],
            "affiliations": [
                "Birmingham Womens and Childrens NHS Foundation Trust, Birmingham, UK",
                "Contextual AI, Mountain View, USA",
                "Factored AI, Palo Alto, USA",
                "Institute of Biomedical Engineering, University of Oxford, Oxford, UK",
                "MLCommons, San Francisco, USA",
                "North Wales Medical School, Bangor University, Bangor, UK",
                "Nuffield Department of Primary Care, University of Oxford, Oxford, UK",
                "Oxford Internet Institute, University of Oxford, Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18919.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#data",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "LLM в медицине: отличные результаты тестов, но проблемы в реальном применении",
                    "desc": "Исследование показало, что крупные языковые модели (LLM) демонстрируют высокую точность в медицинских тестах, но их эффективность снижается при взаимодействии с реальными пользователями. В контролируемом исследовании с 1298 участниками LLM самостоятельно правильно определяли медицинские состояния в 94.9% случаев, но при взаимодействии с людьми этот показатель падал до 34.5%. Авторы подчеркивают важность тестирования взаимодействия LLM с реальными пользователями перед внедрением в здравоохранение. Исследование выявило проблемы в использовании LLM для медицинских консультаций, которые не обнаруживаются стандартными тестами."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs in Healthcare Need User Testing!",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in providing medical advice to the public. Although LLMs like GPT-4o and Llama 3 perform well on medical licensing exams, their real-world application shows significant discrepancies. In a study with 1,298 participants, LLMs accurately identified medical conditions in 94.9% of cases but failed to assist users effectively, with only 34.5% correctly identifying conditions. The findings highlight the importance of user interaction and suggest that standard testing does not adequately predict real-world performance, emphasizing the need for thorough human user testing before deploying LLMs in healthcare settings."
                },
                "zh": {
                    "title": "大型语言模型在医疗建议中的挑战与机遇",
                    "desc": "本研究探讨了大型语言模型（LLMs）在医疗建议中的应用。尽管LLMs在医学执照考试中表现优异，但在实际场景中的表现却不尽如人意。我们在一项包含1298名参与者的控制研究中发现，使用LLMs的参与者在识别相关病症和选择处理方案时的准确率远低于预期。研究表明，用户交互是LLMs在医疗建议中应用的一大挑战，未来需要进行系统的人机交互测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19395",
            "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
            "url": "https://huggingface.co/papers/2504.19395",
            "abstract": "Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.",
            "score": 1,
            "issue_id": 3484,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "6c4b9edf630ae39b",
            "authors": [
                "Zhouxiang Fang",
                "Aayush Mishra",
                "Muhan Gao",
                "Anqi Liu",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19395.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "🔑",
                "ru": {
                    "title": "Расшифровка скрытых шаблонов: LLM и обратимые шифры",
                    "desc": "В статье рассматривается, как LLM могут решать задачи In-Context Learning (ICL) с использованием шифров подстановки, заимствованных из криптографии. Авторы вводят ICL CIPHERS, где часть токенов заменяется на другие, создавая скрытый, но обратимый шаблон. Исследование показывает, что LLM лучше справляются с задачами, где шифр обратим, чем с необратимыми. Это открывает новый подход к оценке способности LLM к «обучению» в ICL."
                },
                "en": {
                    "title": "Decoding Learning: ICL CIPHERS Unveiled",
                    "desc": "This paper explores In-Context Learning (ICL) by introducing a new method called ICL CIPHERS, which uses substitution ciphers to reformulate tasks. The authors argue that ICL operates in two modes: task retrieval and task learning, and they aim to distinguish between these modes. By applying reversible transformations to input tokens, they create a scenario where large language models (LLMs) must decipher the latent patterns to solve tasks. The results show that LLMs perform better on tasks with bijective mappings compared to non-bijective ones, highlighting a novel way to measure learning in ICL."
                },
                "zh": {
                    "title": "揭示情境学习的双重模式",
                    "desc": "最近的研究表明，情境学习（ICL）有两种模式：任务检索（从预训练中记忆学习的模式）和任务学习（从示例中进行推理的学习）。然而，区分这两种模式仍然是一个具有挑战性的目标。我们提出了ICL CIPHERS，这是一种基于经典密码学的替换密码的任务重构方法。通过这种方法，输入中的一部分标记被替换为其他（无关的）标记，使得句子对人类的理解变得困难，但这种替换是可逆的，保持了任务的定义。"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4月28日"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4月30日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "我们介绍了 CameraBench，一个大规模的数据集和基准，旨在评估和改进摄像机运动理解。CameraBench 包含约 3,000 个多样化的网络视频，由专家通过严格的多阶段质量控制过程进行标注。我们与摄影师合作，设计了摄像机运动基元的分类法。例如，某些运动如“跟随”需要理解场景内容，如移动的主体。我们进行了大规模的人类研究，以量化人类标注性能，发现领域专业知识和基于教程的培训可显著提高准确性。使用 CameraBench，我们评估了结构从运动（SfM）和视频语言模型（VLMs），发现 SfM 模型难以捕捉依赖场景内容的语义基元，而 VLMs 难以捕捉需要精确轨迹估计的几何基元。我们随后在 CameraBench 上微调了一个生成 VLM，以实现两者的优势结合，并展示其应用，包括运动增强的字幕、视频问答和视频文本检索。我们希望我们的分类法、基准和教程能推动未来努力，实现理解任何视频中摄像机运动的最终目标。",
        "title": "Towards Understanding Camera Motions in Any Video",
        "pinyin": "Wǒmen jièshào le CameraBench, yīgè dàguīmó de shùjùjí hé jīzhǔn, zhǐyǐn pínggǔ hé gǎijìn shèxiàngjī yùndòng lǐjiě. CameraBench bāohán yuē 3,000 gè duōyànghuà de wǎngluò shìpǐn, yóu zhuānjiā tōngguò yánzhòng de duō jiēduàn zhìliàng kòngzhì guòchéng jìnxiàng biāozhù. Wǒmen yǔ shèyǐngshī hézuò, shèjì le shèxiàngjī yùndòng jīyuǎn de fēnlèi fǎ. Lìrú, mǒuxiē yùndòng rú “gēnsuí” xūyào lǐjiě chǎngjīng nèiróng, rú yídòng de zhǔtǐ. Wǒmen jìnxíng le dàguīmó de rénlèi yánjiū, yǐ liàngzhì rénlèi biāozhù xìngnéng, fāxiàn lǐngyù zhuānjì zhīshi hé jīyú jiàoxué de péixùn kě xiǎnzhù tīgāo zhùnquèxìng. Shǐyòng CameraBench, wǒmen pínggǔ le jiégòu cóng yùndòng (SfM) hé shìpǐn yǔyán móxíng (VLMs), fāxiàn SfM móxíng nán yǐ bǔzhòu yīlài chǎngjīng nèiróng de yǔyán jīyuǎn, ér VLMs nán yǐ bǔzhòu xūyào jīngquè guǐjì gūjì de jǐhé jīyuǎn. Wǒmen shùhòu zài CameraBench shàng wēitiáo le yīgè shēngchéng VLM, yǐ shíxiàn liǎng zhě de yōushì jiēhé, bìng zhǎnshì qí yìngyòng, bāokuò yùndòng zēngqiáng de zìmǔ, shìpǐn wèndá hé shìpǐn wénběn cháxún. Wǒmen xīwàng wǒmen de fēnlèi fǎ, jīzhǔn hé jiàoxué néng tuīdòng wèilái nǔlì, shíxiàn lǐjiě rènhé shìpǐn zhōng shèxiàngjī yùndòng de zhòngdiǎn mùbiāo.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improve\"},\n    {\"word\": \"摄像机\", \"pinyin\": \"shè xiàng jī\", \"trans\": \"camera\"},\n    {\"word\": \"运动\", \"pinyin\": \"yùn dòng\", \"trans\": \"motion\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"多样化\", \"pinyin\": \"duō yàng huà\", \"trans\": \"diverse\"},\n    {\"word\": \"网络视频\", \"pinyin\": \"wǎng luò shì pín\", \"trans\": \"online video\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"严格\", \"pinyin\": \"yán gé\", \"trans\": \"strict\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiē duàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"质量控制\", \"pinyin\": \"zhì liàng kòng zhì\", \"trans\": \"quality control\"},\n    {\"word\": \"过程\", \"pinyin\": \"guò chéng\", \"trans\": \"process\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"摄影师\", \"pinyin\": \"shè yǐng shī\", \"trans\": \"photographer\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"基元\", \"pinyin\": \"jī yuán\", \"trans\": \"primitive\"},\n    {\"word\": \"分类法\", \"pinyin\": \"fēn lèi fǎ\", \"trans\": \"classification method\"},\n    {\"word\": \"例如\", \"pinyin\": \"lì rú\", \"trans\": \"for example\"},\n    {\"word\": \"某些\", \"pinyin\": \"mǒu xiē\", \"trans\": \"some\"},\n    {\"word\": \"跟随\", \"pinyin\": \"gēn suí\", \"trans\": \"follow\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scene\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"主体\", \"pinyin\": \"zhǔ tǐ\", \"trans\": \"subject\"},\n    {\"word\": \"移动\", \"pinyin\": \"yí dòng\", \"trans\": \"move\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"study\"},\n    {\"word\": \"量化\", \"pinyin\": \"liàng huà\", \"trans\": \"quantify\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"专业知识\", \"pinyin\": \"zhuān yè zhī shi\", \"trans\": \"professional knowledge\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"教程\", \"pinyin\": \"jiào chéng\", \"trans\": \"tutorial\"},\n    {\"word\": \"培训\", \"pinyin\": \"péi xùn\", \"trans\": \"training\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"结构从运动\", \"pinyin\": \"jié gòu cóng yùn dòng\", \"trans\": \"Structure from Motion (SfM)\"},\n    {\"word\": \"视频语言模型\", \"pinyin\": \"shì pín yǔ yán mó xíng\", \"trans\": \"Video Language Model (VLM)\"},\n    {\"word\": \"捕捉\", \"pinyin\": \"bǔ zhuō\", \"trans\": \"capture\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐ hé\", \"trans\": \"geometric\"},\n    {\"word\": \"轨迹\", \"pinyin\": \"guǐ jī\", \"trans\": \"trajectory\"},\n    {\"word\": \"估计\", \"pinyin\": \"gū jì\", \"trans\": \"estimate\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāo kuò\", \"trans\": \"include\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"字幕\", \"pinyin\": \"zì mù\", \"trans\": \"subtitle\"},\n    {\"word\": \"问答\", \"pinyin\": \"wèn dá\", \"trans\": \"question and answer\"},\n    {\"word\": \"文本检索\", \"pinyin\": \"wén běn jiǎn suǒ\", \"trans\": \"text retrieval\"},\n    {\"word\": \"希望\", \"pinyin\": \"xī wàng\", \"trans\": \"hope\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"未来\", \"pinyin\": \"wèi lái\", \"trans\": \"future\"},\n    {\"word\": \"努力\", \"pinyin\": \"nǔ lì\", \"trans\": \"effort\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"最终\", \"pinyin\": \"zuì zhōng\", \"trans\": \"ultimate\"},\n    {\"word\": \"目标\", \"pinyin\": \"mù biāo\", \"trans\": \"goal\"}\n]",
        "trans": "We introduce CameraBench, a large-scale dataset and benchmark aimed at evaluating and improving the understanding of camera motion. CameraBench contains approximately 3,000 diverse web videos, annotated by experts through a rigorous multi-stage quality control process. We collaborated with photographers to design a classification scheme for camera motion primitives. For instance, certain motions like \"following\" require an understanding of scene content, such as moving subjects. We conducted large-scale human studies to quantify human annotation performance and found that domain expertise and tutorial-based training can significantly improve accuracy. Using CameraBench, we evaluated Structure-from-Motion (SfM) and Video Language Models (VLMs), finding that SfM models struggle to capture semantic primitives dependent on scene content, while VLMs struggle with geometric primitives that require precise trajectory estimation. Subsequently, we fine-tuned a generative VLM on CameraBench to combine the strengths of both, demonstrating its applications, including motion-enhanced captioning, video question answering, and video-text retrieval. We hope that our classification scheme, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motion in any video.",
        "update_ts": "2025-04-28 10:45"
    }
}