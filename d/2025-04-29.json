{
    "date": {
        "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 29",
        "zh": "4æœˆ29æ—¥"
    },
    "time_utc": "2025-04-29 15:12",
    "weekday": 1,
    "issue_id": 3492,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19724",
            "title": "RepText: Rendering Visual Text via Replicating",
            "url": "https://huggingface.co/papers/2504.19724",
            "abstract": "Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.",
            "score": 19,
            "issue_id": 3481,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "daf4c90a7de01768",
            "authors": [
                "Haofan Wang",
                "Yujia Xu",
                "Yimeng Li",
                "Junchen Li",
                "Chaowei Zhang",
                "Jing Wang",
                "Kejia Yang",
                "Zhibo Chen"
            ],
            "affiliations": [
                "Liblib AI",
                "Shakker Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19724.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#multilingual",
                    "#video",
                    "#open_source",
                    "#low_resource",
                    "#multimodal"
                ],
                "emoji": "ğŸ–‹ï¸",
                "ru": {
                    "title": "RepText: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RepText - Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆÑ€Ğ¸Ñ„Ñ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ControlNet, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»Ğ¸Ñ„Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RepText Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Multilingual Typography",
                    "desc": "This paper introduces RepText, a novel approach to enhance text-to-image generation models by enabling them to accurately replicate multilingual text in various fonts without needing to understand the text. The method leverages a combination of language agnostic glyphs and position information to create visually coherent text that users can customize. To improve the rendering accuracy, the authors implement a text perceptual loss alongside diffusion loss and utilize a specific initialization strategy during inference. Extensive experiments demonstrate that RepText outperforms existing open-source models and achieves results comparable to advanced closed-source systems, while also addressing its limitations."
                },
                "zh": {
                    "title": "èµ‹èƒ½æ–‡æœ¬ç”Ÿæˆï¼Œè¶…è¶Šè¯­è¨€é™åˆ¶",
                    "desc": "å°½ç®¡ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ ï¼Œå°¤å…¶æ˜¯éæ‹‰ä¸å­—æ¯æ–¹é¢ä»ç„¶å­˜åœ¨é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RepTextï¼Œå®ƒæ—¨åœ¨èµ‹äºˆé¢„è®­ç»ƒçš„å•è¯­æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶æ•´åˆäº†ä¸è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä»¥ä¾¿ç”Ÿæˆå’Œè°çš„è§†è§‰æ–‡æœ¬ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†RepTextçš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æºæ¨¡å‹ä¸­è¡¨ç°æ›´ä¼˜ï¼Œå¹¶ä¸æœ¬åœ°å¤šè¯­è¨€é—­æºæ¨¡å‹çš„ç»“æœç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 16,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "LLM Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‰‹æœºè‡ªåŠ¨åŒ–å˜é©",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ‰‹æœºè‡ªåŠ¨åŒ–å‘ç”Ÿäº†å˜é©æ€§å˜åŒ–ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†åŸºäºLLMçš„æ‰‹æœºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œå¼ºè°ƒäº†å®ƒä»¬ä»åŸºäºè„šæœ¬çš„è‡ªåŠ¨åŒ–åˆ°æ™ºèƒ½è‡ªé€‚åº”ç³»ç»Ÿçš„æ¼”å˜ã€‚æˆ‘ä»¬é¦–å…ˆé˜æ˜äº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„é€šç”¨æ€§ã€é«˜ç»´æŠ¤æˆæœ¬å’Œæ„å›¾ç†è§£è–„å¼±ï¼Œå¹¶å±•ç¤ºäº†LLMå¦‚ä½•é€šè¿‡å…ˆè¿›çš„è¯­è¨€ç†è§£ã€å¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¼ºå¤§çš„å†³ç­–èƒ½åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡ç«¯éƒ¨ç½²æ•ˆç‡ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ç­‰å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸæä¾›å‰ç»æ€§è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18919",
            "title": "Clinical knowledge in LLMs does not translate to human interactions",
            "url": "https://huggingface.co/papers/2504.18919",
            "abstract": "Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.",
            "score": 15,
            "issue_id": 3480,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 26",
                "zh": "4æœˆ26æ—¥"
            },
            "hash": "25dc210bebba9c94",
            "authors": [
                "Andrew M. Bean",
                "Rebecca Payne",
                "Guy Parsons",
                "Hannah Rose Kirk",
                "Juan Ciro",
                "Rafael Mosquera",
                "Sara HincapiÃ© Monsalve",
                "Aruna S. Ekanayaka",
                "Lionel Tarassenko",
                "Luc Rocher",
                "Adam Mahdi"
            ],
            "affiliations": [
                "Birmingham Womens and Childrens NHS Foundation Trust, Birmingham, UK",
                "Contextual AI, Mountain View, USA",
                "Factored AI, Palo Alto, USA",
                "Institute of Biomedical Engineering, University of Oxford, Oxford, UK",
                "MLCommons, San Francisco, USA",
                "North Wales Medical School, Bangor University, Bangor, UK",
                "Nuffield Department of Primary Care, University of Oxford, Oxford, UK",
                "Oxford Internet Institute, University of Oxford, Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18919.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#data",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "LLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ: Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½Ğ¾ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ’ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ 1298 ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ LLM ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞ»Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² 94.9% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ°Ğ´Ğ°Ğ» Ğ´Ğ¾ 34.5%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ LLM Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs in Healthcare Need User Testing!",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in providing medical advice to the public. Although LLMs like GPT-4o and Llama 3 perform well on medical licensing exams, their real-world application shows significant discrepancies. In a study with 1,298 participants, LLMs accurately identified medical conditions in 94.9% of cases but failed to assist users effectively, with only 34.5% correctly identifying conditions. The findings highlight the importance of user interaction and suggest that standard testing does not adequately predict real-world performance, emphasizing the need for thorough human user testing before deploying LLMs in healthcare settings."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å»ºè®®ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—å»ºè®®ä¸­çš„åº”ç”¨ã€‚å°½ç®¡LLMsåœ¨åŒ»å­¦æ‰§ç…§è€ƒè¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ã€‚æˆ‘ä»¬åœ¨ä¸€é¡¹åŒ…å«1298åå‚ä¸è€…çš„æ§åˆ¶ç ”ç©¶ä¸­å‘ç°ï¼Œä½¿ç”¨LLMsçš„å‚ä¸è€…åœ¨è¯†åˆ«ç›¸å…³ç—…ç—‡å’Œé€‰æ‹©å¤„ç†æ–¹æ¡ˆæ—¶çš„å‡†ç¡®ç‡è¿œä½äºé¢„æœŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”¨æˆ·äº¤äº’æ˜¯LLMsåœ¨åŒ»ç–—å»ºè®®ä¸­åº”ç”¨çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œæœªæ¥éœ€è¦è¿›è¡Œç³»ç»Ÿçš„äººæœºäº¤äº’æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19093",
            "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
            "url": "https://huggingface.co/papers/2504.19093",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.",
            "score": 12,
            "issue_id": 3483,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 27",
                "zh": "4æœˆ27æ—¥"
            },
            "hash": "c9ba3c45d7c25d9f",
            "authors": [
                "Yu Li",
                "Qizhi Pei",
                "Mengyuan Sun",
                "Honglin Lin",
                "Chenlin Ming",
                "Xin Gao",
                "Jiang Wu",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19093.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "CipherBank: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CipherBank - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´ĞµÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. CipherBank Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2358 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 262 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 5 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¸ 14 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o Ğ¸ DeepSeek-V3, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ o1 Ğ¸ DeepSeek-R1. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´ĞµÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Cryptography: Evaluating LLMs with CipherBank",
                    "desc": "This paper presents CipherBank, a new benchmark for assessing the reasoning skills of large language models (LLMs) in cryptographic decryption tasks. It includes 2,358 problems across various domains, focusing on real-world scenarios that involve encryption. The study evaluates several state-of-the-art LLMs, revealing significant performance gaps in their ability to handle classical cryptographic methods. The findings highlight the limitations of current models and suggest areas for improvement in their reasoning capabilities related to cryptography."
                },
                "zh": {
                    "title": "æå‡LLMsåœ¨å¯†ç å­¦æ¨ç†ä¸­çš„èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¯†ç å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†CipherBankï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¯†ç è§£å¯†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚CipherBankåŒ…å«2358ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¶µç›–262ä¸ªç‹¬ç‰¹çš„æ˜æ–‡ï¼Œæ¶‰åŠ5ä¸ªé¢†åŸŸå’Œ14ä¸ªå­é¢†åŸŸï¼Œé‡ç‚¹å…³æ³¨éœ€è¦åŠ å¯†çš„éšç§æ•æ„Ÿå’Œç°å®åœºæ™¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„æ¨ç†ä¸“æ³¨æ¨¡å‹åœ¨å¤„ç†ç»å…¸å¯†ç è§£å¯†ä»»åŠ¡æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†LLMsåœ¨ç†è§£å’Œæ“ä½œåŠ å¯†æ•°æ®æ–¹é¢çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16083",
            "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
            "url": "https://huggingface.co/papers/2504.16083",
            "abstract": "The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.",
            "score": 8,
            "issue_id": 3482,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "8ba72cd8f5463bbe",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16083.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² VLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMInference - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ğº Ğ´Ğ»Ñ ĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. MMInference Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° GPU Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 8,3 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Accelerating Vision Language Models with Dynamic Sparse Attention",
                    "desc": "This paper presents MMInference, a novel method designed to enhance Vision Language Models (VLMs) by addressing the challenge of quadratic attention complexity during the pre-filling phase. By analyzing the unique sparse patterns in video inputs, specifically the Grid pattern, the authors develop a dynamic sparse attention mechanism that optimizes the processing of long-context multi-modal data. MMInference allows for efficient computation by dynamically constructing sparse distributions based on input characteristics, without requiring any modifications to existing VLM architectures. Experimental results demonstrate that this approach significantly accelerates the pre-filling stage, achieving up to 8.3 times faster processing while preserving accuracy across various multi-modal benchmarks."
                },
                "zh": {
                    "title": "åŠ é€Ÿå¤šæ¨¡æ€è¾“å…¥çš„é¢„å¡«å……ï¼Œé‡Šæ”¾è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMMInferenceçš„åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€è¾“å…¥çš„é¢„å¡«å……é˜¶æ®µã€‚é€šè¿‡åˆ†æè§†é¢‘è¾“å…¥çš„æ—¶é—´å’Œç©ºé—´å±€éƒ¨æ€§ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§ç‹¬ç‰¹çš„ç¨€ç–æ¨¡å¼ï¼Œå³ç½‘æ ¼æ¨¡å¼ã€‚åŒæ—¶ï¼Œä¸åŒæ¨¡æ€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±•ç°å‡ºæ˜¾è‘—ä¸åŒçš„ç¨€ç–åˆ†å¸ƒã€‚MMInferenceèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€æ„å»ºç¨€ç–åˆ†å¸ƒï¼Œä»è€Œåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜é¢„å¡«å……é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19162",
            "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.19162",
            "abstract": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a \"sneaky generator\" that deliberately produces erroneous steps designed to be difficult to detect, and a \"critic\" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.",
            "score": 5,
            "issue_id": 3486,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 27",
                "zh": "4æœˆ27æ—¥"
            },
            "hash": "2571d5875df9e077",
            "authors": [
                "Jiaqi Chen",
                "Bang Zhang",
                "Ruotian Ma",
                "Peisong Wang",
                "Xiaodan Liang",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Kwan-Yee K. Wong"
            ],
            "affiliations": [
                "MBZUAI",
                "Tencent",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19162.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#math"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñƒ Ğ² 'ĞºĞ¾ÑˆĞºĞ¸-Ğ¼Ñ‹ÑˆĞºĞ¸'",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Self-Play Critic (SPC) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SPC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ³Ñ€Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ¾Ğ»Ğ¸ 'Ñ…Ğ¸Ñ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°' Ğ¸ 'ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½ÑƒÑÑ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ² adversarial Ğ¸Ğ³Ñ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ³Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SPC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Self-Play Critic: Evolving Error Detection in LLMs through Adversarial Learning",
                    "desc": "This paper presents Self-Play Critic (SPC), a method for improving the evaluation of reasoning steps in large language models (LLMs) without needing manual annotations. SPC uses two models in an adversarial setup: a 'sneaky generator' that creates challenging erroneous reasoning steps and a 'critic' that assesses their correctness. Through reinforcement learning, these models iteratively enhance their performance by rewarding successful detections and penalizing failures. Experiments show that SPC significantly boosts error detection accuracy and improves mathematical reasoning in LLMs, outperforming existing models."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¯¹å¼ˆè¯„è®ºå®¶ï¼šæå‡æ¨ç†å¯é æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè‡ªæˆ‘å¯¹å¼ˆè¯„è®ºå®¶ï¼ˆSPCï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„é€æ­¥å¯é æ€§ã€‚SPCé€šè¿‡å¯¹æŠ—æ€§è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆï¼Œæ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨é€æ­¥æ³¨é‡Šçš„éœ€æ±‚ï¼Œä¸¤ä¸ªæ¨¡å‹åˆ†åˆ«æ‰®æ¼”â€œç‹¡çŒ¾ç”Ÿæˆå™¨â€å’Œâ€œè¯„è®ºå®¶â€çš„è§’è‰²ã€‚ç”Ÿæˆå™¨æ•…æ„äº§ç”Ÿéš¾ä»¥æ£€æµ‹çš„é”™è¯¯æ­¥éª¤ï¼Œè€Œè¯„è®ºå®¶åˆ™åˆ†æè¿™äº›æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPCåœ¨é”™è¯¯æ£€æµ‹èƒ½åŠ›ä¸Šé€æ­¥æå‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œæ˜¾è‘—æ”¹å–„äº†æ•°å­¦æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18589",
            "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual\n  Dependency",
            "url": "https://huggingface.co/papers/2504.18589",
            "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.",
            "score": 5,
            "issue_id": 3486,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "ce75d00af948d575",
            "authors": [
                "Zhikai Wang",
                "Jiashuo Sun",
                "Wenqi Zhang",
                "Zhiqiang Hu",
                "Xin Li",
                "Fan Wang",
                "Deli Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Singapore University of Technology and Design",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18589.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#multimodal"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "VCBENCH: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCBENCH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. VCBENCH Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1720 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 3.9 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 26 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ñ‚ÑŒ 50% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging Visual and Mathematical Reasoning in AI",
                    "desc": "This paper discusses the limitations of current Large Vision-Language Models (LVLMs) in handling elementary-level math problems that require visual reasoning. It introduces VCBENCH, a new benchmark designed to evaluate multimodal mathematical reasoning by incorporating explicit visual dependencies across multiple images. The benchmark consists of 1,720 problems and 6,697 images, allowing for a comprehensive assessment of LVLMs' capabilities. The evaluation of 26 state-of-the-art models on VCBENCH shows that even the best-performing models struggle to achieve over 50% accuracy, indicating significant challenges in integrating visual and mathematical reasoning."
                },
                "zh": {
                    "title": "å¡«è¡¥è§†è§‰æ•°å­¦æ¨ç†çš„ç©ºç™½",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†å®ƒä»¬æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæ¥è¿‘äººç±»åœ¨ç‰©ä½“è¯†åˆ«ã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¯„ä¼°æ ‡å‡†é€šå¸¸ä¾§é‡äºçŸ¥è¯†ä¸­å¿ƒçš„è¯„ä¼°ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨åŸºæœ¬æ•°å­¦å…ƒç´ å’Œè§†è§‰æ¦‚å¿µæ¨ç†æ–¹é¢çš„æ ¸å¿ƒèƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°è¯„ä¼°åŸºç¡€æ•°å­¦é—®é¢˜çš„ç©ºç™½ï¼Œè¿™äº›é—®é¢˜ä¾èµ–äºæ˜ç¡®çš„è§†è§‰ä¾èµ–å…³ç³»ï¼Œè¦æ±‚æ¨¡å‹åœ¨æ•´åˆå¸¸è¯†çŸ¥è¯†çš„åŒæ—¶ï¼Œè·¨å¤šä¸ªå›¾åƒè¿›è¡Œæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VCBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ï¼ŒåŒ…å«1720ä¸ªé—®é¢˜å’Œ6697å¼ å›¾åƒï¼Œä»¥ç¡®ä¿å¤šå›¾åƒæ¨ç†çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17258",
            "title": "Group Downsampling with Equivariant Anti-aliasing",
            "url": "https://huggingface.co/papers/2504.17258",
            "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature maps) on general finite groups with anti-aliasing. This involves the following: (a) Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup. (b) Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into G-equivariant networks",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "c8bf8ac7bd71ca11",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…: Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ½Ñ‚Ğ¸Ğ°Ğ»Ğ¸Ğ°ÑĞ¸Ğ½Ğ³",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (G-CNN). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğµ Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ñ‚Ğ¸Ğ°Ğ»Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² G-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing G-CNNs with Advanced Downsampling Techniques",
                    "desc": "This paper explores the role of downsampling layers in convolutional neural networks (CNNs), particularly focusing on group equivariant architectures like G-CNNs. The authors propose a new algorithm for downsampling feature maps on finite groups while ensuring anti-aliasing, which helps maintain the integrity of the signals. They introduce concepts such as bandlimited-ness and demonstrate how their method aligns with classical sampling theory, especially for periodic signals. Experimental results show that their approach enhances accuracy, preserves equivariance, and reduces the overall size of the model in image classification tasks."
                },
                "zh": {
                    "title": "æå‡CNNæ€§èƒ½çš„ä¸‹é‡‡æ ·æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨ç¾¤ç­‰å˜æ¶æ„ä¸­å‡åŒ€ä¸‹é‡‡æ ·å±‚çš„æ¨å¹¿ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹G-CNNsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šæœ‰é™ç¾¤å’Œä¸‹é‡‡æ ·ç‡çš„æƒ…å†µä¸‹é€‰æ‹©åˆé€‚çš„å­ç¾¤ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å¸¦é™æ€§æ¦‚å¿µï¼Œå¹¶æå‡ºäº†å¦‚ä½•è¿›è¡ŒæŠ—æ··å å¤„ç†çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ä¸‹é‡‡æ ·æ“ä½œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œæ›´å¥½åœ°ä¿æŒäº†ç­‰å˜æ€§ï¼Œå¹¶å‡å°‘äº†æ¨¡å‹å¤§å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15780",
            "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
            "url": "https://huggingface.co/papers/2504.15780",
            "abstract": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "e736decf98fa2805",
            "authors": [
                "Daocheng Fu",
                "Zijun Chen",
                "Renqiu Xia",
                "Qi Liu",
                "Yuan Feng",
                "Hongbin Zhou",
                "Renrui Zhang",
                "Shiyang Feng",
                "Peng Gao",
                "Junchi Yan",
                "Botian Shi",
                "Bo Zhang",
                "Yu Qiao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15780.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#math",
                    "#data",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "TrustGeoGen: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TrustGeoGen - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. TrustGeoGen ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GeoTrust-200K Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ GeoTrust-test, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 49.17%. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° GeoTrust, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ OpenAI-o1."
                },
                "en": {
                    "title": "TrustGeoGen: Ensuring Reliable Geometric Problem Solving",
                    "desc": "This paper addresses the challenges in mathematical geometric problem solving (GPS) by introducing TrustGeoGen, a scalable data engine designed for generating reliable problem sets. It emphasizes the importance of integrating multimodal information and ensuring logical coherence through formal verification. The engine innovatively creates geometric data with aligned diagrams, textual descriptions, and solutions while maintaining rule compliance. The resulting GeoTrust-200K dataset and GeoTrust-test benchmark demonstrate improved evaluation standards and out-of-distribution generalization for models trained on this data."
                },
                "zh": {
                    "title": "ä¿¡ä»»å‡ ä½•ï¼šæå‡å‡ ä½•é—®é¢˜è§£å†³çš„å¯é æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTrustGeoGençš„æ•°æ®å¼•æ“ï¼Œç”¨äºç”Ÿæˆæ•°å­¦å‡ ä½•é—®é¢˜ï¼Œå¹¶é€šè¿‡å½¢å¼éªŒè¯æä¾›å¯é çš„åŸºå‡†ã€‚è¯¥å¼•æ“é€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°å‡ ä½•æ•°æ®çš„åˆæˆï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¯¹é½ç”Ÿæˆã€å½¢å¼éªŒè¯ã€é€’å½’çŠ¶æ€ç”Ÿæˆçš„å¤æ‚æ€§å‡çº§æœºåˆ¶ï¼Œä»¥åŠåŒæ—¶ç”Ÿæˆå¤šè§£å˜ä½“çš„ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨GeoTrust-testä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º49.17%ï¼Œæ˜¾ç¤ºå‡ºè¯„ä¼°çš„ä¸¥æ ¼æ€§ã€‚é€šè¿‡åœ¨GeoTrustä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨GeoQAä¸Šå®ç°äº†OODæ³›åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†é€»è¾‘ä¸ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19144",
            "title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile\n  Hardware Development",
            "url": "https://huggingface.co/papers/2504.19144",
            "abstract": "The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM",
            "score": 3,
            "issue_id": 3489,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 27",
                "zh": "4æœˆ27æ—¥"
            },
            "hash": "834b43a7a91c5ac8",
            "authors": [
                "Bowei Wang",
                "Jiaran Gao",
                "Yelai Feng",
                "Renzhi Chen",
                "Shanshan Li",
                "Lei Wang"
            ],
            "affiliations": [
                "College of Computer Science and Technology National University of Defense Technology Changsha, China",
                "Computer Department National University of Defense Technology Changsha, China",
                "Defense Innovation Institute Academy of Military Science Beijing, China",
                "Intelligent Microelectronics Center Qiyuan Lab Beijing, China",
                "School of Computer & Communication Engineering University of Science and Technology Beijing Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19144.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#dataset",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ChiseLLM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Chisel Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ChiseLLM - ÑÑ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Chisel Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ChiseLLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚ÑƒÑ€Ñ‹ (HCL)."
                },
                "en": {
                    "title": "ChiseLLM: Enhancing Chisel Code Generation with Domain Adaptation",
                    "desc": "This paper introduces ChiseLLM, a novel approach to improve code generation for Hardware Construction Language (HCL) like Chisel, which is essential for Agile Hardware Development Methodology (AHDM). The authors highlight the limitations of existing Large Language Models (LLMs) in generating correct Chisel syntax and accommodating design variability. By employing domain adaptation techniques and structured prompt-guided reasoning, ChiseLLM significantly enhances the performance of code generation tasks. The results show substantial improvements in syntax correctness and design variability, making ChiseLLM a valuable resource for hardware developers and researchers."
                },
                "zh": {
                    "title": "ChiseLLMï¼šæå‡Chiselä»£ç ç”Ÿæˆçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "éšç€å¯¹ç‰¹å®šé¢†åŸŸæ¶æ„ï¼ˆDSAï¼‰çš„éœ€æ±‚å¢åŠ ï¼Œæ•æ·ç¡¬ä»¶å¼€å‘æ–¹æ³•ï¼ˆAHDMï¼‰å¾—åˆ°äº†å‘å±•ã€‚ç¡¬ä»¶æ„é€ è¯­è¨€ï¼ˆHCLï¼‰å¦‚Chiselæä¾›äº†é«˜å±‚æ¬¡çš„æŠ½è±¡ç‰¹æ€§ï¼Œéå¸¸é€‚åˆç”¨äºåŸºäºHCLçš„AHDMã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨Chiselç”Ÿæˆæ–¹é¢ä»é¢ä¸´è¯­æ³•æ­£ç¡®æ€§å’Œè®¾è®¡å˜å¼‚æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ChiseLLMï¼Œé€šè¿‡æ•°æ®å¤„ç†ä¸è½¬æ¢ã€æç¤ºå¼•å¯¼çš„æ¨ç†è½¨è¿¹åˆæˆå’Œé¢†åŸŸé€‚åº”æ¨¡å‹è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†Chiselä»£ç ç”Ÿæˆçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19395",
            "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
            "url": "https://huggingface.co/papers/2504.19395",
            "abstract": "Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.",
            "score": 2,
            "issue_id": 3484,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "6c4b9edf630ae39b",
            "authors": [
                "Zhouxiang Fang",
                "Aayush Mishra",
                "Muhan Gao",
                "Anqi Liu",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19395.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "ğŸ”‘",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ²: LLM Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğµ ÑˆĞ¸Ñ„Ñ€Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ, ĞºĞ°Ğº LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ In-Context Learning (ICL) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑˆĞ¸Ñ„Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸, Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ICL CIPHERS, Ğ³Ğ´Ğµ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹, Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğ¹ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ ÑˆĞ¸Ñ„Ñ€ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼, Ñ‡ĞµĞ¼ Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Â«Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ÑÂ» Ğ² ICL."
                },
                "en": {
                    "title": "Decoding Learning: ICL CIPHERS Unveiled",
                    "desc": "This paper explores In-Context Learning (ICL) by introducing a new method called ICL CIPHERS, which uses substitution ciphers to reformulate tasks. The authors argue that ICL operates in two modes: task retrieval and task learning, and they aim to distinguish between these modes. By applying reversible transformations to input tokens, they create a scenario where large language models (LLMs) must decipher the latent patterns to solve tasks. The results show that LLMs perform better on tasks with bijective mappings compared to non-bijective ones, highlighting a novel way to measure learning in ICL."
                },
                "zh": {
                    "title": "æ­ç¤ºæƒ…å¢ƒå­¦ä¹ çš„åŒé‡æ¨¡å¼",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæƒ…å¢ƒå­¦ä¹ ï¼ˆICLï¼‰æœ‰ä¸¤ç§æ¨¡å¼ï¼šä»»åŠ¡æ£€ç´¢ï¼ˆä»é¢„è®­ç»ƒä¸­è®°å¿†å­¦ä¹ çš„æ¨¡å¼ï¼‰å’Œä»»åŠ¡å­¦ä¹ ï¼ˆä»ç¤ºä¾‹ä¸­è¿›è¡Œæ¨ç†çš„å­¦ä¹ ï¼‰ã€‚ç„¶è€Œï¼ŒåŒºåˆ†è¿™ä¸¤ç§æ¨¡å¼ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†ICL CIPHERSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç»å…¸å¯†ç å­¦çš„æ›¿æ¢å¯†ç çš„ä»»åŠ¡é‡æ„æ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè¾“å…¥ä¸­çš„ä¸€éƒ¨åˆ†æ ‡è®°è¢«æ›¿æ¢ä¸ºå…¶ä»–ï¼ˆæ— å…³çš„ï¼‰æ ‡è®°ï¼Œä½¿å¾—å¥å­å¯¹äººç±»çš„ç†è§£å˜å¾—å›°éš¾ï¼Œä½†è¿™ç§æ›¿æ¢æ˜¯å¯é€†çš„ï¼Œä¿æŒäº†ä»»åŠ¡çš„å®šä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19062",
            "title": "Versatile Framework for Song Generation with Prompt-based Control",
            "url": "https://huggingface.co/papers/2504.19062",
            "abstract": "Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.",
            "score": 1,
            "issue_id": 3491,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 27",
                "zh": "4æœˆ27æ—¥"
            },
            "hash": "fd59eefcb2d985bf",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#story_generation",
                    "#synthetic",
                    "#multimodal",
                    "#audio"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "VersBand: ĞœĞ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑĞµĞ½",
                    "desc": "VersBand - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ»Ğ° Ğ¸ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VocalBand Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ»Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° flow-matching, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AccompBand Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LyricBand Ğ¸ MelodyBand Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VersBand Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑĞµĞ½."
                },
                "en": {
                    "title": "VersBand: Mastering Multi-Task Song Generation with Control",
                    "desc": "This paper presents VersBand, a novel framework for generating high-quality songs with precise control based on various prompts. It addresses the limitations of existing methods in producing aligned vocals and accompaniments by introducing specialized models like VocalBand and AccompBand. VocalBand uses flow-matching to create diverse singing styles and pitches, while AccompBand employs a flow-based transformer with expert selection for better quality and alignment. Additionally, LyricBand and MelodyBand enhance the system's capabilities, allowing for comprehensive control over lyrics and melodies, resulting in superior performance compared to baseline models."
                },
                "zh": {
                    "title": "VersBandï¼šå¯æ§é«˜è´¨é‡æ­Œæ›²ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVersBandçš„å¤šä»»åŠ¡æ­Œæ›²ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®ä¸åŒçš„æç¤ºç”Ÿæˆé«˜è´¨é‡ä¸”å¯æ§çš„æ­Œæ›²ã€‚VersBandåŒ…å«å¤šä¸ªä¸»è¦æ¨¡å‹ï¼ŒåŒ…æ‹¬VocalBandå’ŒAccompBandï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆäººå£°å’Œä¼´å¥ï¼Œç¡®ä¿å®ƒä»¬ä¹‹é—´çš„è‰¯å¥½å¯¹é½ã€‚VocalBandåˆ©ç”¨æµåŒ¹é…æ–¹æ³•å¿«é€Ÿç”Ÿæˆå…·æœ‰é£æ ¼æ§åˆ¶çš„é«˜è´¨é‡äººå£°ï¼Œè€ŒAccompBandåˆ™é€šè¿‡é€‰æ‹©åˆé€‚çš„ä¸“å®¶æ¥æå‡ä¼´å¥çš„è´¨é‡å’Œæ§åˆ¶èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVersBandåœ¨å¤šä¸ªæ­Œæ›²ç”Ÿæˆä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨æ­Œè¯å’Œæ—‹å¾‹ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆéæ‹‰ä¸å­—æ¯çš„ç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RepTextï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„æ¨¡å‹ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚RepText é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶é¢å¤–æ•´åˆäº†è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€è¦è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡ä½¿ç”¨æ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œæ‰©æ•£æŸå¤±ï¼Œä»¥åŠåœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨å™ªå£°å­—å½¢æ½œåœ¨åˆå§‹åŒ–å’ŒåŒºåŸŸæ©ç ï¼ŒRepText æ˜¾è‘—æé«˜äº†æ¸²æŸ“çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepText åœ¨å¼€æºæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä¸å°é—­æºçš„å¤šè¯­è¨€æ¨¡å‹ç›¸åª²ç¾ã€‚æ–‡ç« æœ€åè¿˜è®¨è®ºäº†RepTextçš„å±€é™æ€§ã€‚",
        "title": "RepText: Rendering Visual Text via Replicating",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆéæ‹‰ä¸å­—æ¯çš„ç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RepTextï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„æ¨¡å‹ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚RepText é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶é¢å¤–æ•´åˆäº†è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€è¦è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡ä½¿ç”¨æ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œæ‰©æ•£æŸå¤±ï¼Œä»¥åŠåœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨å™ªå£°å­—å½¢æ½œåœ¨åˆå§‹åŒ–å’ŒåŒºåŸŸæ©ç ï¼ŒRepText æ˜¾è‘—æé«˜äº†æ¸²æŸ“çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepText åœ¨å¼€æºæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä¸å°é—­æºçš„å¤šè¯­è¨€æ¨¡å‹ç›¸åª²ç¾ã€‚æ–‡ç« æœ€åè¿˜è®¨è®ºäº†RepTextçš„å±€é™æ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÄng qiÃ¡n wÃ©n bÄ›n dÃ o tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng de jÃº xiÃ n xÃ¬ng, tÃ¨ biÃ© shÃ¬ zÃ i shÄ“ng chÃ©ng fÄ“i lÄ dÄ«ng zÃ¬ mÇ” de jÄ«ng quÃ¨ hÃ© lÃ­ng huÃ³ de pÃ¡i bÇn yuÃ¡n sÇ” fÄng miÃ n. wÃ¨i le jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le RepText, zhÃ¨ shÃ¬ yÄ« zhÇ’ng nÃ©ng gÃ²u zhÇ”n quÃ¨ xuÃ n rÃ¡n duÅ yÇ” yÃ¡n shÃ¬ jÃ¹ wÃ©n bÄ›n de mÃ³ xÃ­ng, Ã©r wÃº xÅ« zhÄ“n zhÃ¨ng lÇ jiÄ› zhÃ¨ xiÄ“ wÃ©n bÄ›n. RepText cuÅ yÃ²ng le ControlNet de shÃ¨ zhÃ¬, bÃ¬ng Ã© wÃ i zhÄ›ng hÃ© le yÇ” yÃ¡n wÃº guÄn de zÃ¬ xÃ­ng hÃ© xuÃ n rÃ¡n wÃ©n bÄ›n de wÃ¨i zhÃ¬, shÇ yÃ²ng hÃ¹ nÃ©ng gÃ²u yÄ«n yÃ²ng zhÃ¨n dÄ«ng wÃ©n bÄ›n nÃ¨i rÃ³ng, zÃ¬ tÇ hÃ© wÃ¨i zhÃ¬. tÅng guÃ² shÇ yÃ²ng wÃ©n bÄ›n gÇn juÃ© sÇ”n shÄ« hÃ© kuÃ² sÃ n sÇ”n shÄ«, yÇ jiÄ shÇ yÃ²ng tuÄ« lÇ jiÄ“ duÃ n cuÃ² yÃ²ng zÃ o shÄ“ng zÃ¬ xÃ­ng qiÃ¡n zÃ i chÅ« shÇ huÃ  hÃ© qÅ« yÃ¹ mÃ³ zhÃ o, RepText xiÇn zhÃ¹ tÃ­ gÄo le xuÃ n rÃ¡n de zhÇ”n quÃ¨ xÃ¬ng hÃ© wÄ›n dÃ¬ng xÃ¬ng. shÃ­ yÃ n jiÃ© guÇ’ biÇo mÃ­ng, RepText zÃ i kÄi yuÃ¡n fÄng fÇ zhÅng biÇo xiÃ n chÅ« sÃ¨, bÃ¬ng qiÄ› yÇ” fÄ“ng bÃ¬ yuÃ¡n de duÅ yÇ” yÃ¡n mÃ³ xÃ­ng xiÄng jÃ¬ mÇ. wÃ©n zhÄng zuÃ¬ hÃ²u hÃ¡i tÇo lÃ¹n le RepText de jÃº xiÃ n xÃ¬ng.\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÄng qiÃ¡n wÃ©n bÄ›n dÃ o tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng de jÃº xiÃ n xÃ¬ng, tÃ¨ biÃ© shÃ¬ zÃ i shÄ“ng chÃ©ng fÄ“i lÄ dÄ«ng zÃ¬ mÇ” de jÄ«ng quÃ¨ hÃ© lÃ­ng huÃ³ de pÃ¡i bÇn yuÃ¡n sÇ” fÄng miÃ n. wÃ¨i le jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le RepText, zhÃ¨ shÃ¬ yÄ« zhÇ’ng nÃ©ng gÃ²u zhÇ”n quÃ¨ xuÃ n rÃ¡n duÅ yÇ” yÃ¡n shÃ¬ jÃ¹ wÃ©n bÄ›n de mÃ³ xÃ­ng, Ã©r wÃº xÅ« zhÄ“n zhÃ¨ng lÇ jiÄ› zhÃ¨ xiÄ“ wÃ©n bÄ›n. RepText cuÅ yÃ²ng le ControlNet de shÃ¨ zhÃ¬, bÃ¬ng Ã© wÃ i zhÄ›ng hÃ© le yÇ” yÃ¡n wÃº guÄn de zÃ¬ xÃ­ng hÃ© xuÃ n rÃ¡n wÃ©n bÄ›n de wÃ¨i zhÃ¬, shÇ yÃ²ng hÃ¹ nÃ©ng gÃ²u yÄ«n yÃ²ng zhÃ¨n dÄ«ng wÃ©n bÄ›n nÃ¨i rÃ³ng, zÃ¬ tÇ hÃ© wÃ¨i zhÃ¬. tÅng guÃ² shÇ yÃ²ng wÃ©n bÄ›n gÇn juÃ© sÇ”n shÄ« hÃ© kuÃ² sÃ n sÇ”n shÄ«, yÇ jiÄ shÇ yÃ²ng tuÄ« lÇ jiÄ“ duÃ n cuÃ² yÃ²ng zÃ o shÄ“ng zÃ¬ xÃ­ng qiÃ¡n zÃ i chÅ« shÇ huÃ  hÃ© qÅ« yÃ¹ mÃ³ zhÃ o, RepText xiÇn zhÃ¹ tÃ­ gÄo le xuÃ n rÃ¡n de zhÇ”n quÃ¨ xÃ¬ng hÃ© wÄ›n dÃ¬ng xÃ¬ng. shÃ­ yÃ n jiÃ© guÇ’ biÇo mÃ­ng, RepText zÃ i kÄi yuÃ¡n fÄng fÇ zhÅng biÇo xiÃ n chÅ« sÃ¨, bÃ¬ng qiÄ› yÇ” fÄ“ng bÃ¬ yuÃ¡n de duÅ yÇ” yÃ¡n mÃ³ xÃ­ng xiÄng jÃ¬ mÇ. wÃ©n zhÄng zuÃ¬ hÃ²u hÃ¡i tÇo lÃ¹n le RepText de jÃº xiÃ n xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å½“å‰\", \"pinyin\": \"dÄng qiÃ¡n\", \"trans\": \"current\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å±€é™æ€§\", \"pinyin\": \"jÃº xiÃ n xÃ¬ng\", \"trans\": \"limitations\"},\n    {\"word\": \"ç‰¹åˆ«\", \"pinyin\": \"tÃ¨ biÃ©\", \"trans\": \"especially\"},\n    {\"word\": \"éæ‹‰ä¸å­—æ¯\", \"pinyin\": \"fÄ“i lÄ dÄ«ng zÃ¬ mÇ”\", \"trans\": \"non-Latin characters\"},\n    {\"word\": \"ç²¾ç¡®\", \"pinyin\": \"jÄ«ng quÃ¨\", \"trans\": \"precise\"},\n    {\"word\": \"çµæ´»\", \"pinyin\": \"lÃ­ng huÃ³\", \"trans\": \"flexible\"},\n    {\"word\": \"æ’ç‰ˆ\", \"pinyin\": \"pÃ¡i bÇn\", \"trans\": \"typesetting\"},\n    {\"word\": \"å…ƒç´ \", \"pinyin\": \"yuÃ¡n sÃ¹\", \"trans\": \"elements\"},\n    {\"word\": \"æ–¹é¢\", \"pinyin\": \"fÄng miÃ n\", \"trans\": \"aspect\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"RepText\", \"pinyin\": \"RepText\", \"trans\": \"RepText\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"æ¸²æŸ“\", \"pinyin\": \"xuÃ n rÃ¡n\", \"trans\": \"render\"},\n    {\"word\": \"å¤šè¯­è¨€\", \"pinyin\": \"duÅ yÇ” yÃ¡n\", \"trans\": \"multilingual\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"æ— éœ€\", \"pinyin\": \"wÃº xÅ«\", \"trans\": \"without needing\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understand\"},\n    {\"word\": \"é‡‡ç”¨\", \"pinyin\": \"cÇi yÃ²ng\", \"trans\": \"adopt\"},\n    {\"word\": \"ControlNet\", \"pinyin\": \"ControlNet\", \"trans\": \"ControlNet\"},\n    {\"word\": \"è®¾ç½®\", \"pinyin\": \"shÃ¨ zhÃ¬\", \"trans\": \"setting\"},\n    {\"word\": \"é¢å¤–\", \"pinyin\": \"Ã© wÃ i\", \"trans\": \"additional\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"è¯­è¨€æ— å…³\", \"pinyin\": \"yÇ” yÃ¡n wÃº guÄn\", \"trans\": \"language-agnostic\"},\n    {\"word\": \"å­—å½¢\", \"pinyin\": \"zÃ¬ xÃ­ng\", \"trans\": \"glyph\"},\n    {\"word\": \"ä½ç½®\", \"pinyin\": \"wÃ¨i zhÃ¬\", \"trans\": \"position\"},\n    {\"word\": \"ä½¿ç”¨æˆ·\", \"pinyin\": \"shÇ yÃ²ng hÃ¹\", \"trans\": \"enable users\"},\n    {\"word\": \"æ ¹æ®\", \"pinyin\": \"gÄ“n jÃ¹\", \"trans\": \"according to\"},\n    {\"word\": \"éœ€è¦\", \"pinyin\": \"xÅ« yÃ o\", \"trans\": \"need\"},\n    {\"word\": \"è‡ªå®šä¹‰\", \"pinyin\": \"zÃ¬ dÃ¬ng yÃ¬\", \"trans\": \"customize\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"å­—ä½“\", \"pinyin\": \"zÃ¬ tÇ\", \"trans\": \"font\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"æ–‡æœ¬æ„ŸçŸ¥\", \"pinyin\": \"wÃ©n bÄ›n gÇn zhÄ«\", \"trans\": \"text-aware\"},\n    {\"word\": \"æŸå¤±\", \"pinyin\": \"sÇ”n shÄ«\", \"trans\": \"loss\"},\n    {\"word\": \"æ‰©æ•£\", \"pinyin\": \"kuÃ² sÃ n\", \"trans\": \"diffusion\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"é˜¶æ®µ\", \"pinyin\": \"jiÄ“ duÃ n\", \"trans\": \"stage\"},\n    {\"word\": \"å™ªå£°\", \"pinyin\": \"zÃ o shÄ“ng\", \"trans\": \"noise\"},\n    {\"word\": \"æ½œåœ¨\", \"pinyin\": \"qiÃ¡n zÃ i\", \"trans\": \"latent\"},\n    {\"word\": \"åˆå§‹åŒ–\", \"pinyin\": \"chÅ« shÇ huÃ \", \"trans\": \"initialization\"},\n    {\"word\": \"åŒºåŸŸ\", \"pinyin\": \"qÅ« yÃ¹\", \"trans\": \"region\"},\n    {\"word\": \"æ©ç \", \"pinyin\": \"yÇn mÇ\", \"trans\": \"mask\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"ç¨³å®šæ€§\", \"pinyin\": \"wÄ›n dÃ¬ng xÃ¬ng\", \"trans\": \"stability\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"å°é—­æº\", \"pinyin\": \"fÄ“ng bÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"},\n    {\"word\": \"ç›¸åª²ç¾\", \"pinyin\": \"xiÄng pÃ¬ mÄ›i\", \"trans\": \"compare favorably\"},\n    {\"word\": \"æ–‡ç« \", \"pinyin\": \"wÃ©n zhÄng\", \"trans\": \"article\"},\n    {\"word\": \"æœ€å\", \"pinyin\": \"zuÃ¬ hÃ²u\", \"trans\": \"finally\"},\n    {\"word\": \"è¿˜\", \"pinyin\": \"hÃ¡i\", \"trans\": \"still\"}\n]",
        "trans": "This article discusses the limitations of current text-to-image generation models, particularly in generating precise and flexible typographic elements for non-Latin scripts. To address these issues, the authors propose RepText, a model capable of accurately rendering multilingual visual text without truly understanding the text. RepText adopts the setup of ControlNet and additionally integrates language-agnostic glyphs and the positioning of rendered text, allowing users to customize text content, fonts, and positions as needed. By utilizing text-aware loss and diffusion loss, along with noisy glyph latent initialization and regional masking during the inference stage, RepText significantly improves the accuracy and stability of rendering. Experimental results demonstrate that RepText performs excellently among open-source methods and is comparable to closed-source multilingual models. The article concludes with a discussion of RepText's limitations.",
        "update_ts": "2025-04-29 09:12"
    }
}