{
    "date": {
        "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 29",
        "zh": "4æœˆ29æ—¥"
    },
    "time_utc": "2025-04-29 02:26",
    "weekday": 1,
    "issue_id": 3479,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 6,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "LLM Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‰‹æœºè‡ªåŠ¨åŒ–å˜é©",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ‰‹æœºè‡ªåŠ¨åŒ–å‘ç”Ÿäº†å˜é©æ€§å˜åŒ–ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†åŸºäºLLMçš„æ‰‹æœºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œå¼ºè°ƒäº†å®ƒä»¬ä»åŸºäºè„šæœ¬çš„è‡ªåŠ¨åŒ–åˆ°æ™ºèƒ½è‡ªé€‚åº”ç³»ç»Ÿçš„æ¼”å˜ã€‚æˆ‘ä»¬é¦–å…ˆé˜æ˜äº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„é€šç”¨æ€§ã€é«˜ç»´æŠ¤æˆæœ¬å’Œæ„å›¾ç†è§£è–„å¼±ï¼Œå¹¶å±•ç¤ºäº†LLMå¦‚ä½•é€šè¿‡å…ˆè¿›çš„è¯­è¨€ç†è§£ã€å¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¼ºå¤§çš„å†³ç­–èƒ½åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡ç«¯éƒ¨ç½²æ•ˆç‡ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ç­‰å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸæä¾›å‰ç»æ€§è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† CameraBenchï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›æ‘„åƒæœºè¿åŠ¨ç†è§£ã€‚CameraBench åŒ…å«çº¦ 3,000 ä¸ªå¤šæ ·åŒ–çš„ç½‘ç»œè§†é¢‘ï¼Œç”±ä¸“å®¶é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿‡ç¨‹è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬ä¸æ‘„å½±å¸ˆåˆä½œï¼Œè®¾è®¡äº†æ‘„åƒæœºè¿åŠ¨åŸºå…ƒçš„åˆ†ç±»æ³•ã€‚ä¾‹å¦‚ï¼ŒæŸäº›è¿åŠ¨å¦‚â€œè·Ÿéšâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹ï¼Œå¦‚ç§»åŠ¨çš„ä¸»ä½“ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œä»¥é‡åŒ–äººç±»æ ‡æ³¨æ€§èƒ½ï¼Œå‘ç°é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’ŒåŸºäºæ•™ç¨‹çš„åŸ¹è®­å¯æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚ä½¿ç”¨ CameraBenchï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å’Œè§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå‘ç° SfM æ¨¡å‹éš¾ä»¥æ•æ‰ä¾èµ–åœºæ™¯å†…å®¹çš„è¯­ä¹‰åŸºå…ƒï¼Œè€Œ VLMs éš¾ä»¥æ•æ‰éœ€è¦ç²¾ç¡®è½¨è¿¹ä¼°è®¡çš„å‡ ä½•åŸºå…ƒã€‚æˆ‘ä»¬éšååœ¨ CameraBench ä¸Šå¾®è°ƒäº†ä¸€ä¸ªç”Ÿæˆ VLMï¼Œä»¥å®ç°ä¸¤è€…çš„ä¼˜åŠ¿ç»“åˆï¼Œå¹¶å±•ç¤ºå…¶åº”ç”¨ï¼ŒåŒ…æ‹¬è¿åŠ¨å¢å¼ºçš„å­—å¹•ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘æ–‡æœ¬æ£€ç´¢ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†ç±»æ³•ã€åŸºå‡†å’Œæ•™ç¨‹èƒ½æ¨åŠ¨æœªæ¥åŠªåŠ›ï¼Œå®ç°ç†è§£ä»»ä½•è§†é¢‘ä¸­æ‘„åƒæœºè¿åŠ¨çš„æœ€ç»ˆç›®æ ‡ã€‚",
        "title": "Towards Understanding Camera Motions in Any Video",
        "pinyin": "WÇ’men jiÃ¨shÃ o le CameraBench, yÄ«gÃ¨ dÃ guÄ«mÃ³ de shÃ¹jÃ¹jÃ­ hÃ© jÄ«zhÇ”n, zhÇyÇn pÃ­nggÇ” hÃ© gÇijÃ¬n shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng lÇjiÄ›. CameraBench bÄohÃ¡n yuÄ“ 3,000 gÃ¨ duÅyÃ nghuÃ  de wÇngluÃ² shÃ¬pÇn, yÃ³u zhuÄnjiÄ tÅngguÃ² yÃ¡nzhÃ²ng de duÅ jiÄ“duÃ n zhÃ¬liÃ ng kÃ²ngzhÃ¬ guÃ²chÃ©ng jÃ¬nxiÃ ng biÄozhÃ¹. WÇ’men yÇ” shÃ¨yÇngshÄ« hÃ©zuÃ², shÃ¨jÃ¬ le shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng jÄ«yuÇn de fÄ“nlÃ¨i fÇ. LÃ¬rÃº, mÇ’uxiÄ“ yÃ¹ndÃ²ng rÃº â€œgÄ“nsuÃ­â€ xÅ«yÃ o lÇjiÄ› chÇngjÄ«ng nÃ¨irÃ³ng, rÃº yÃ­dÃ²ng de zhÇ”tÇ. WÇ’men jÃ¬nxÃ­ng le dÃ guÄ«mÃ³ de rÃ©nlÃ¨i yÃ¡njiÅ«, yÇ liÃ ngzhÃ¬ rÃ©nlÃ¨i biÄozhÃ¹ xÃ¬ngnÃ©ng, fÄxiÃ n lÇngyÃ¹ zhuÄnjÃ¬ zhÄ«shi hÃ© jÄ«yÃº jiÃ oxuÃ© de pÃ©ixÃ¹n kÄ› xiÇnzhÃ¹ tÄ«gÄo zhÃ¹nquÃ¨xÃ¬ng. ShÇyÃ²ng CameraBench, wÇ’men pÃ­nggÇ” le jiÃ©gÃ²u cÃ³ng yÃ¹ndÃ²ng (SfM) hÃ© shÃ¬pÇn yÇ”yÃ¡n mÃ³xÃ­ng (VLMs), fÄxiÃ n SfM mÃ³xÃ­ng nÃ¡n yÇ bÇ”zhÃ²u yÄ«lÃ i chÇngjÄ«ng nÃ¨irÃ³ng de yÇ”yÃ¡n jÄ«yuÇn, Ã©r VLMs nÃ¡n yÇ bÇ”zhÃ²u xÅ«yÃ o jÄ«ngquÃ¨ guÇjÃ¬ gÅ«jÃ¬ de jÇhÃ© jÄ«yuÇn. WÇ’men shÃ¹hÃ²u zÃ i CameraBench shÃ ng wÄ“itiÃ¡o le yÄ«gÃ¨ shÄ“ngchÃ©ng VLM, yÇ shÃ­xiÃ n liÇng zhÄ› de yÅushÃ¬ jiÄ“hÃ©, bÃ¬ng zhÇnshÃ¬ qÃ­ yÃ¬ngyÃ²ng, bÄokuÃ² yÃ¹ndÃ²ng zÄ“ngqiÃ¡ng de zÃ¬mÇ”, shÃ¬pÇn wÃ¨ndÃ¡ hÃ© shÃ¬pÇn wÃ©nbÄ›n chÃ¡xÃºn. WÇ’men xÄ«wÃ ng wÇ’men de fÄ“nlÃ¨i fÇ, jÄ«zhÇ”n hÃ© jiÃ oxuÃ© nÃ©ng tuÄ«dÃ²ng wÃ¨ilÃ¡i nÇ”lÃ¬, shÃ­xiÃ n lÇjiÄ› rÃ¨nhÃ© shÃ¬pÇn zhÅng shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng de zhÃ²ngdiÇn mÃ¹biÄo.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"æ‘„åƒæœº\", \"pinyin\": \"shÃ¨ xiÃ ng jÄ«\", \"trans\": \"camera\"},\n    {\"word\": \"è¿åŠ¨\", \"pinyin\": \"yÃ¹n dÃ²ng\", \"trans\": \"motion\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understand\"},\n    {\"word\": \"å¤šæ ·åŒ–\", \"pinyin\": \"duÅ yÃ ng huÃ \", \"trans\": \"diverse\"},\n    {\"word\": \"ç½‘ç»œè§†é¢‘\", \"pinyin\": \"wÇng luÃ² shÃ¬ pÃ­n\", \"trans\": \"online video\"},\n    {\"word\": \"ä¸“å®¶\", \"pinyin\": \"zhuÄn jiÄ\", \"trans\": \"expert\"},\n    {\"word\": \"ä¸¥æ ¼\", \"pinyin\": \"yÃ¡n gÃ©\", \"trans\": \"strict\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“ duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"è´¨é‡æ§åˆ¶\", \"pinyin\": \"zhÃ¬ liÃ ng kÃ²ng zhÃ¬\", \"trans\": \"quality control\"},\n    {\"word\": \"è¿‡ç¨‹\", \"pinyin\": \"guÃ² chÃ©ng\", \"trans\": \"process\"},\n    {\"word\": \"æ ‡æ³¨\", \"pinyin\": \"biÄo zhÃ¹\", \"trans\": \"annotate\"},\n    {\"word\": \"æ‘„å½±å¸ˆ\", \"pinyin\": \"shÃ¨ yÇng shÄ«\", \"trans\": \"photographer\"},\n    {\"word\": \"è®¾è®¡\", \"pinyin\": \"shÃ¨ jÃ¬\", \"trans\": \"design\"},\n    {\"word\": \"åŸºå…ƒ\", \"pinyin\": \"jÄ« yuÃ¡n\", \"trans\": \"primitive\"},\n    {\"word\": \"åˆ†ç±»æ³•\", \"pinyin\": \"fÄ“n lÃ¨i fÇ\", \"trans\": \"classification method\"},\n    {\"word\": \"ä¾‹å¦‚\", \"pinyin\": \"lÃ¬ rÃº\", \"trans\": \"for example\"},\n    {\"word\": \"æŸäº›\", \"pinyin\": \"mÇ’u xiÄ“\", \"trans\": \"some\"},\n    {\"word\": \"è·Ÿéš\", \"pinyin\": \"gÄ“n suÃ­\", \"trans\": \"follow\"},\n    {\"word\": \"éœ€è¦\", \"pinyin\": \"xÅ« yÃ o\", \"trans\": \"need\"},\n    {\"word\": \"åœºæ™¯\", \"pinyin\": \"chÇng jÇng\", \"trans\": \"scene\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"ä¸»ä½“\", \"pinyin\": \"zhÇ” tÇ\", \"trans\": \"subject\"},\n    {\"word\": \"ç§»åŠ¨\", \"pinyin\": \"yÃ­ dÃ²ng\", \"trans\": \"move\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"study\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ ng huÃ \", \"trans\": \"quantify\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"ä¸“ä¸šçŸ¥è¯†\", \"pinyin\": \"zhuÄn yÃ¨ zhÄ« shi\", \"trans\": \"professional knowledge\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ•™ç¨‹\", \"pinyin\": \"jiÃ o chÃ©ng\", \"trans\": \"tutorial\"},\n    {\"word\": \"åŸ¹è®­\", \"pinyin\": \"pÃ©i xÃ¹n\", \"trans\": \"training\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”n quÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"ç»“æ„ä»è¿åŠ¨\", \"pinyin\": \"jiÃ© gÃ²u cÃ³ng yÃ¹n dÃ²ng\", \"trans\": \"Structure from Motion (SfM)\"},\n    {\"word\": \"è§†é¢‘è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ pÃ­n yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"Video Language Model (VLM)\"},\n    {\"word\": \"æ•æ‰\", \"pinyin\": \"bÇ” zhuÅ\", \"trans\": \"capture\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ” yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇ hÃ©\", \"trans\": \"geometric\"},\n    {\"word\": \"è½¨è¿¹\", \"pinyin\": \"guÇ jÄ«\", \"trans\": \"trajectory\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ« jÃ¬\", \"trans\": \"estimate\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅu shÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"åŒ…æ‹¬\", \"pinyin\": \"bÄo kuÃ²\", \"trans\": \"include\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"å­—å¹•\", \"pinyin\": \"zÃ¬ mÃ¹\", \"trans\": \"subtitle\"},\n    {\"word\": \"é—®ç­”\", \"pinyin\": \"wÃ¨n dÃ¡\", \"trans\": \"question and answer\"},\n    {\"word\": \"æ–‡æœ¬æ£€ç´¢\", \"pinyin\": \"wÃ©n bÄ›n jiÇn suÇ’\", \"trans\": \"text retrieval\"},\n    {\"word\": \"å¸Œæœ›\", \"pinyin\": \"xÄ« wÃ ng\", \"trans\": \"hope\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ« dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"åŠªåŠ›\", \"pinyin\": \"nÇ” lÃ¬\", \"trans\": \"effort\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€ç»ˆ\", \"pinyin\": \"zuÃ¬ zhÅng\", \"trans\": \"ultimate\"},\n    {\"word\": \"ç›®æ ‡\", \"pinyin\": \"mÃ¹ biÄo\", \"trans\": \"goal\"}\n]",
        "trans": "We introduce CameraBench, a large-scale dataset and benchmark aimed at evaluating and improving the understanding of camera motion. CameraBench contains approximately 3,000 diverse web videos, annotated by experts through a rigorous multi-stage quality control process. We collaborated with photographers to design a classification scheme for camera motion primitives. For instance, certain motions like \"following\" require an understanding of scene content, such as moving subjects. We conducted large-scale human studies to quantify human annotation performance and found that domain expertise and tutorial-based training can significantly improve accuracy. Using CameraBench, we evaluated Structure-from-Motion (SfM) and Video Language Models (VLMs), finding that SfM models struggle to capture semantic primitives dependent on scene content, while VLMs struggle with geometric primitives that require precise trajectory estimation. Subsequently, we fine-tuned a generative VLM on CameraBench to combine the strengths of both, demonstrating its applications, including motion-enhanced captioning, video question answering, and video-text retrieval. We hope that our classification scheme, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motion in any video.",
        "update_ts": "2025-04-28 10:45"
    }
}