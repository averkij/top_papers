{
    "date": {
        "ru": "29 апреля",
        "en": "April 29",
        "zh": "4月29日"
    },
    "time_utc": "2025-04-29 15:12",
    "weekday": 1,
    "issue_id": 3492,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19724",
            "title": "RepText: Rendering Visual Text via Replicating",
            "url": "https://huggingface.co/papers/2504.19724",
            "abstract": "Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.",
            "score": 19,
            "issue_id": 3481,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "daf4c90a7de01768",
            "authors": [
                "Haofan Wang",
                "Yujia Xu",
                "Yimeng Li",
                "Junchen Li",
                "Chaowei Zhang",
                "Jing Wang",
                "Kejia Yang",
                "Zhibo Chen"
            ],
            "affiliations": [
                "Liblib AI",
                "Shakker Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19724.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#multilingual",
                    "#video",
                    "#open_source",
                    "#low_resource",
                    "#multimodal"
                ],
                "emoji": "🖋️",
                "ru": {
                    "title": "RepText: Точная визуализация многоязычного текста в генеративных моделях",
                    "desc": "Статья представляет RepText - метод, позволяющий моделям генерации изображений по тексту точно отображать многоязычный текст заданным шрифтом. Авторы используют подход ControlNet, интегрируя глифы и позиции символов для гармоничной визуализации текста. Для повышения точности применяется перцептивная потеря текста наряду с диффузионной. Эксперименты показывают, что RepText превосходит существующие открытые методы и сравним с закрытыми многоязычными моделями."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Multilingual Typography",
                    "desc": "This paper introduces RepText, a novel approach to enhance text-to-image generation models by enabling them to accurately replicate multilingual text in various fonts without needing to understand the text. The method leverages a combination of language agnostic glyphs and position information to create visually coherent text that users can customize. To improve the rendering accuracy, the authors implement a text perceptual loss alongside diffusion loss and utilize a specific initialization strategy during inference. Extensive experiments demonstrate that RepText outperforms existing open-source models and achieves results comparable to advanced closed-source systems, while also addressing its limitations."
                },
                "zh": {
                    "title": "赋能文本生成，超越语言限制",
                    "desc": "尽管现代文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著突破，但在生成精确和灵活的排版元素，尤其是非拉丁字母方面仍然存在限制。为了解决这些问题，我们提出了RepText，它旨在赋予预训练的单语文本到图像生成模型准确渲染多语言视觉文本的能力，而无需真正理解这些文本。我们采用了ControlNet的设置，并整合了与语言无关的字形和渲染文本的位置，以便生成和谐的视觉文本，允许用户根据需求自定义文本内容、字体和位置。通过大量实验，我们验证了RepText的有效性，相较于现有方法，我们的方法在开源模型中表现更优，并与本地多语言闭源模型的结果相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 16,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "LLM революционизируют автоматизацию мобильных интерфейсов",
                    "desc": "Статья представляет систематический обзор агентов графического интерфейса для телефонов, управляемых большими языковыми моделями (LLM). Рассматривается эволюция от скриптовой автоматизации к интеллектуальным адаптивным системам, решающим проблемы ограниченной универсальности, высоких затрат на обслуживание и слабого понимания намерений. Авторы предлагают таксономию, охватывающую основные фреймворки агентов, подходы к моделированию и ключевые наборы данных. Обсуждаются открытые проблемы, такие как разнообразие данных, эффективность развертывания на устройствах и вопросы безопасности."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "大型语言模型驱动的手机自动化变革",
                    "desc": "随着大型语言模型（LLMs）的快速发展，手机自动化发生了变革性变化。本文系统回顾了基于LLM的手机图形用户界面（GUI）代理，强调了它们从基于脚本的自动化到智能自适应系统的演变。我们首先阐明了关键挑战，包括有限的通用性、高维护成本和意图理解薄弱，并展示了LLM如何通过先进的语言理解、多模态感知和强大的决策能力来解决这些问题。最后，我们讨论了数据集多样性、设备端部署效率、以用户为中心的适应性和安全性等开放挑战，为这一快速发展的领域提供前瞻性见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18919",
            "title": "Clinical knowledge in LLMs does not translate to human interactions",
            "url": "https://huggingface.co/papers/2504.18919",
            "abstract": "Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.",
            "score": 15,
            "issue_id": 3480,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 апреля",
                "en": "April 26",
                "zh": "4月26日"
            },
            "hash": "25dc210bebba9c94",
            "authors": [
                "Andrew M. Bean",
                "Rebecca Payne",
                "Guy Parsons",
                "Hannah Rose Kirk",
                "Juan Ciro",
                "Rafael Mosquera",
                "Sara Hincapié Monsalve",
                "Aruna S. Ekanayaka",
                "Lionel Tarassenko",
                "Luc Rocher",
                "Adam Mahdi"
            ],
            "affiliations": [
                "Birmingham Womens and Childrens NHS Foundation Trust, Birmingham, UK",
                "Contextual AI, Mountain View, USA",
                "Factored AI, Palo Alto, USA",
                "Institute of Biomedical Engineering, University of Oxford, Oxford, UK",
                "MLCommons, San Francisco, USA",
                "North Wales Medical School, Bangor University, Bangor, UK",
                "Nuffield Department of Primary Care, University of Oxford, Oxford, UK",
                "Oxford Internet Institute, University of Oxford, Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18919.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#data",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "LLM в медицине: отличные результаты тестов, но проблемы в реальном применении",
                    "desc": "Исследование показало, что крупные языковые модели (LLM) демонстрируют высокую точность в медицинских тестах, но их эффективность снижается при взаимодействии с реальными пользователями. В контролируемом исследовании с 1298 участниками LLM самостоятельно правильно определяли медицинские состояния в 94.9% случаев, но при взаимодействии с людьми этот показатель падал до 34.5%. Авторы подчеркивают важность тестирования взаимодействия LLM с реальными пользователями перед внедрением в здравоохранение. Исследование выявило проблемы в использовании LLM для медицинских консультаций, которые не обнаруживаются стандартными тестами."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs in Healthcare Need User Testing!",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in providing medical advice to the public. Although LLMs like GPT-4o and Llama 3 perform well on medical licensing exams, their real-world application shows significant discrepancies. In a study with 1,298 participants, LLMs accurately identified medical conditions in 94.9% of cases but failed to assist users effectively, with only 34.5% correctly identifying conditions. The findings highlight the importance of user interaction and suggest that standard testing does not adequately predict real-world performance, emphasizing the need for thorough human user testing before deploying LLMs in healthcare settings."
                },
                "zh": {
                    "title": "大型语言模型在医疗建议中的挑战与机遇",
                    "desc": "本研究探讨了大型语言模型（LLMs）在医疗建议中的应用。尽管LLMs在医学执照考试中表现优异，但在实际场景中的表现却不尽如人意。我们在一项包含1298名参与者的控制研究中发现，使用LLMs的参与者在识别相关病症和选择处理方案时的准确率远低于预期。研究表明，用户交互是LLMs在医疗建议中应用的一大挑战，未来需要进行系统的人机交互测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19093",
            "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
            "url": "https://huggingface.co/papers/2504.19093",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.",
            "score": 12,
            "issue_id": 3483,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "c9ba3c45d7c25d9f",
            "authors": [
                "Yu Li",
                "Qizhi Pei",
                "Mengyuan Sun",
                "Honglin Lin",
                "Chenlin Ming",
                "Xin Gao",
                "Jiang Wu",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19093.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "CipherBank: раскрывая границы криптографического мышления LLM",
                    "desc": "Данная статья представляет CipherBank - комплексный бенчмарк для оценки способностей больших языковых моделей (LLM) в задачах криптографического дешифрования. CipherBank включает 2358 тщательно разработанных задач, охватывающих 262 уникальных открытых текста в 5 доменах и 14 поддоменах, с акцентом на конфиденциальные и реальные сценарии. В исследовании оцениваются современные LLM, такие как GPT-4o и DeepSeek-V3, а также модели, ориентированные на рассуждение, например o1 и DeepSeek-R1. Результаты выявляют значительные пробелы в способностях рассуждения между различными типами моделей при применении к классическим задачам криптографического дешифрования."
                },
                "en": {
                    "title": "Unlocking Cryptography: Evaluating LLMs with CipherBank",
                    "desc": "This paper presents CipherBank, a new benchmark for assessing the reasoning skills of large language models (LLMs) in cryptographic decryption tasks. It includes 2,358 problems across various domains, focusing on real-world scenarios that involve encryption. The study evaluates several state-of-the-art LLMs, revealing significant performance gaps in their ability to handle classical cryptographic methods. The findings highlight the limitations of current models and suggest areas for improvement in their reasoning capabilities related to cryptography."
                },
                "zh": {
                    "title": "提升LLMs在密码学推理中的能力",
                    "desc": "大型语言模型（LLMs）在推理能力方面取得了显著进展，但在密码学领域的推理能力仍然未被充分探索。本文介绍了CipherBank，这是一个全面的基准测试，旨在评估LLMs在密码解密任务中的推理能力。CipherBank包含2358个精心设计的问题，涵盖262个独特的明文，涉及5个领域和14个子领域，重点关注需要加密的隐私敏感和现实场景。我们的研究结果显示，当前的推理专注模型在处理经典密码解密任务时面临显著挑战，强调了LLMs在理解和操作加密数据方面的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16083",
            "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
            "url": "https://huggingface.co/papers/2504.16083",
            "abstract": "The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.",
            "score": 8,
            "issue_id": 3482,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "8ba72cd8f5463bbe",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16083.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обработки длинных мультимодальных данных в VLM без потери точности",
                    "desc": "Статья представляет MMInference - метод динамического разреженного внимания для ускорения обработки длинных мультимодальных входных данных в моделях зрения и языка (VLM). Авторы обнаружили уникальный паттерн разреженности для видеоданных и разработали метод на основе перестановок для его использования. MMInference динамически строит распределение разреженности на основе входных данных и включает оптимизированные ядра GPU для эффективных вычислений. Эксперименты показали ускорение этапа предварительного заполнения до 8,3 раз при сохранении точности на различных мультимодальных задачах."
                },
                "en": {
                    "title": "Accelerating Vision Language Models with Dynamic Sparse Attention",
                    "desc": "This paper presents MMInference, a novel method designed to enhance Vision Language Models (VLMs) by addressing the challenge of quadratic attention complexity during the pre-filling phase. By analyzing the unique sparse patterns in video inputs, specifically the Grid pattern, the authors develop a dynamic sparse attention mechanism that optimizes the processing of long-context multi-modal data. MMInference allows for efficient computation by dynamically constructing sparse distributions based on input characteristics, without requiring any modifications to existing VLM architectures. Experimental results demonstrate that this approach significantly accelerates the pre-filling stage, achieving up to 8.3 times faster processing while preserving accuracy across various multi-modal benchmarks."
                },
                "zh": {
                    "title": "加速多模态输入的预填充，释放视觉语言模型的潜力",
                    "desc": "本论文介绍了一种名为MMInference的动态稀疏注意力方法，旨在加速长上下文多模态输入的预填充阶段。通过分析视频输入的时间和空间局部性，我们发现了一种独特的稀疏模式，即网格模式。同时，不同模态的视觉语言模型（VLM）展现出显著不同的稀疏分布。MMInference能够在不修改模型或微调的情况下，动态构建稀疏分布，从而在多模态基准测试中显著提高预填充速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19162",
            "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.19162",
            "abstract": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a \"sneaky generator\" that deliberately produces erroneous steps designed to be difficult to detect, and a \"critic\" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.",
            "score": 5,
            "issue_id": 3486,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "2571d5875df9e077",
            "authors": [
                "Jiaqi Chen",
                "Bang Zhang",
                "Ruotian Ma",
                "Peisong Wang",
                "Xiaodan Liang",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Kwan-Yee K. Wong"
            ],
            "affiliations": [
                "MBZUAI",
                "Tencent",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19162.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#math"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самоулучшение ИИ через игру в 'кошки-мышки'",
                    "desc": "Статья представляет новый подход под названием Self-Play Critic (SPC) для оценки надежности рассуждений больших языковых моделей. SPC использует две копии базовой модели, играющие роли 'хитрого генератора' и 'критика', которые соревнуются друг с другом в adversarial игре. Модели улучшаются с помощью обучения с подкреплением на основе результатов игры. Эксперименты показывают, что SPC превосходит сильные базовые модели и улучшает математические рассуждения различных языковых моделей."
                },
                "en": {
                    "title": "Self-Play Critic: Evolving Error Detection in LLMs through Adversarial Learning",
                    "desc": "This paper presents Self-Play Critic (SPC), a method for improving the evaluation of reasoning steps in large language models (LLMs) without needing manual annotations. SPC uses two models in an adversarial setup: a 'sneaky generator' that creates challenging erroneous reasoning steps and a 'critic' that assesses their correctness. Through reinforcement learning, these models iteratively enhance their performance by rewarding successful detections and penalizing failures. Experiments show that SPC significantly boosts error detection accuracy and improves mathematical reasoning in LLMs, outperforming existing models."
                },
                "zh": {
                    "title": "自我对弈评论家：提升推理可靠性的创新方法",
                    "desc": "本文提出了一种新方法，称为自我对弈评论家（SPC），用于评估大型语言模型（LLM）推理的逐步可靠性。SPC通过对抗性自我对弈游戏，消除了对手动逐步注释的需求，两个模型分别扮演“狡猾生成器”和“评论家”的角色。生成器故意产生难以检测的错误步骤，而评论家则分析这些推理步骤的正确性。实验结果表明，SPC在错误检测能力上逐步提升，并在多个基准测试中超越了强基线，显著改善了数学推理性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18589",
            "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual\n  Dependency",
            "url": "https://huggingface.co/papers/2504.18589",
            "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.",
            "score": 5,
            "issue_id": 3486,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "ce75d00af948d575",
            "authors": [
                "Zhikai Wang",
                "Jiashuo Sun",
                "Wenqi Zhang",
                "Zhiqiang Hu",
                "Xin Li",
                "Fan Wang",
                "Deli Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Singapore University of Technology and Design",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18589.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#multimodal"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "VCBENCH: новый стандарт для оценки мультимодального математического мышления ИИ",
                    "desc": "В статье представлен новый бенчмарк VCBENCH для оценки способностей крупных визуально-языковых моделей (LVLM) решать элементарные математические задачи с использованием визуальной информации. VCBENCH включает 1720 задач из шести когнитивных областей, содержащих в среднем 3.9 изображения на вопрос. Авторы протестировали 26 современных LVLM на этом бенчмарке, выявив значительные различия в производительности моделей. Результаты показывают, что даже лучшие модели не смогли превысить 50% точности, что указывает на существующие проблемы в интеграции визуальной и математической информации."
                },
                "en": {
                    "title": "Bridging Visual and Mathematical Reasoning in AI",
                    "desc": "This paper discusses the limitations of current Large Vision-Language Models (LVLMs) in handling elementary-level math problems that require visual reasoning. It introduces VCBENCH, a new benchmark designed to evaluate multimodal mathematical reasoning by incorporating explicit visual dependencies across multiple images. The benchmark consists of 1,720 problems and 6,697 images, allowing for a comprehensive assessment of LVLMs' capabilities. The evaluation of 26 state-of-the-art models on VCBENCH shows that even the best-performing models struggle to achieve over 50% accuracy, indicating significant challenges in integrating visual and mathematical reasoning."
                },
                "zh": {
                    "title": "填补视觉数学推理的空白",
                    "desc": "最近，大型视觉语言模型（LVLMs）的进展显著提升了它们整合视觉和语言信息的能力，接近人类在物体识别、图像描述和视觉问答等任务中的表现。然而，目前的评估标准通常侧重于知识中心的评估，忽视了模型在基本数学元素和视觉概念推理方面的核心能力。我们发现评估基础数学问题的空白，这些问题依赖于明确的视觉依赖关系，要求模型在整合常识知识的同时，跨多个图像进行推理。为了解决这一问题，我们引入了VCBENCH，这是一个全面的多模态数学推理基准，包含1720个问题和6697张图像，以确保多图像推理的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17258",
            "title": "Group Downsampling with Equivariant Anti-aliasing",
            "url": "https://huggingface.co/papers/2504.17258",
            "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature maps) on general finite groups with anti-aliasing. This involves the following: (a) Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup. (b) Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into G-equivariant networks",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "c8bf8ac7bd71ca11",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Даунсэмплинг в групповых сверточных сетях: обобщение и антиалиасинг",
                    "desc": "Эта статья исследует обобщение слоев даунсэмплинга для групповых эквивариантных сверточных нейронных сетей (G-CNN). Авторы предлагают алгоритм для выбора подходящей подгруппы при заданной конечной группе и коэффициенте даунсэмплинга. Они также изучают понятие ограниченности полосы частот и предлагают метод антиалиасинга для общих конечных групп. Эксперименты показывают, что предложенная операция даунсэмплинга улучшает точность, лучше сохраняет эквивариантность и уменьшает размер модели при использовании в G-эквивариантных сетях."
                },
                "en": {
                    "title": "Enhancing G-CNNs with Advanced Downsampling Techniques",
                    "desc": "This paper explores the role of downsampling layers in convolutional neural networks (CNNs), particularly focusing on group equivariant architectures like G-CNNs. The authors propose a new algorithm for downsampling feature maps on finite groups while ensuring anti-aliasing, which helps maintain the integrity of the signals. They introduce concepts such as bandlimited-ness and demonstrate how their method aligns with classical sampling theory, especially for periodic signals. Experimental results show that their approach enhances accuracy, preserves equivariance, and reduces the overall size of the model in image classification tasks."
                },
                "zh": {
                    "title": "提升CNN性能的下采样新方法",
                    "desc": "本文研究了在群等变架构中均匀下采样层的推广，特别是针对G-CNNs。我们提出了一种算法，可以在给定有限群和下采样率的情况下选择合适的子群。我们还探讨了带限性概念，并提出了如何进行抗混叠处理的方法。实验结果表明，所提出的下采样操作在图像分类任务中提高了准确性，更好地保持了等变性，并减少了模型大小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15780",
            "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
            "url": "https://huggingface.co/papers/2504.15780",
            "abstract": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen",
            "score": 4,
            "issue_id": 3481,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "e736decf98fa2805",
            "authors": [
                "Daocheng Fu",
                "Zijun Chen",
                "Renqiu Xia",
                "Qi Liu",
                "Yuan Feng",
                "Hongbin Zhou",
                "Renrui Zhang",
                "Shiyang Feng",
                "Peng Gao",
                "Junchi Yan",
                "Botian Shi",
                "Bo Zhang",
                "Yu Qiao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15780.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#math",
                    "#data",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "TrustGeoGen: Надежная генерация геометрических задач для развития искусственного интеллекта",
                    "desc": "Эта статья представляет TrustGeoGen - масштабируемый движок для генерации геометрических задач с формальной верификацией. TrustGeoGen создает многомодальные данные, включая диаграммы, текстовые описания и пошаговые решения, обеспечивая логическую согласованность. На основе этого движка был создан датасет GeoTrust-200K и тестовый набор GeoTrust-test, на котором современные модели машинного обучения достигают точности всего 49.17%. Модели, обученные на GeoTrust, демонстрируют лучшую обобщающую способность и меньше логических несоответствий по сравнению с псевдо-разметкой OpenAI-o1."
                },
                "en": {
                    "title": "TrustGeoGen: Ensuring Reliable Geometric Problem Solving",
                    "desc": "This paper addresses the challenges in mathematical geometric problem solving (GPS) by introducing TrustGeoGen, a scalable data engine designed for generating reliable problem sets. It emphasizes the importance of integrating multimodal information and ensuring logical coherence through formal verification. The engine innovatively creates geometric data with aligned diagrams, textual descriptions, and solutions while maintaining rule compliance. The resulting GeoTrust-200K dataset and GeoTrust-test benchmark demonstrate improved evaluation standards and out-of-distribution generalization for models trained on this data."
                },
                "zh": {
                    "title": "信任几何：提升几何问题解决的可靠性",
                    "desc": "本文提出了一种名为TrustGeoGen的数据引擎，用于生成数学几何问题，并通过形式验证提供可靠的基准。该引擎通过四个关键创新实现几何数据的合成，包括多模态对齐生成、形式验证、递归状态生成的复杂性升级机制，以及同时生成多解变体的算法。实验结果表明，现有的最先进模型在GeoTrust-test上的准确率仅为49.17%，显示出评估的严格性。通过在GeoTrust上训练的模型在GeoQA上实现了OOD泛化，显著减少了逻辑不一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19144",
            "title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile\n  Hardware Development",
            "url": "https://huggingface.co/papers/2504.19144",
            "abstract": "The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM",
            "score": 3,
            "issue_id": 3489,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "834b43a7a91c5ac8",
            "authors": [
                "Bowei Wang",
                "Jiaran Gao",
                "Yelai Feng",
                "Renzhi Chen",
                "Shanshan Li",
                "Lei Wang"
            ],
            "affiliations": [
                "College of Computer Science and Technology National University of Defense Technology Changsha, China",
                "Computer Department National University of Defense Technology Changsha, China",
                "Defense Innovation Institute Academy of Military Science Beijing, China",
                "Intelligent Microelectronics Center Qiyuan Lab Beijing, China",
                "School of Computer & Communication Engineering University of Science and Technology Beijing Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19144.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#dataset",
                    "#data",
                    "#training"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "ChiseLLM: Улучшение генерации кода Chisel с помощью адаптированных языковых моделей",
                    "desc": "ChiseLLM - это решение для улучшения генерации кода на языке Chisel с использованием больших языковых моделей (LLM). Авторы разработали методику, включающую обработку данных, синтез рассуждений с помощью промптов и адаптацию модели к предметной области. Эксперименты показали значительное улучшение синтаксической корректности и способности к вариативному проектированию по сравнению с базовыми моделями. ChiseLLM предлагает эффективный подход для разработки аппаратного обеспечения на основе языка описания аппаратуры (HCL)."
                },
                "en": {
                    "title": "ChiseLLM: Enhancing Chisel Code Generation with Domain Adaptation",
                    "desc": "This paper introduces ChiseLLM, a novel approach to improve code generation for Hardware Construction Language (HCL) like Chisel, which is essential for Agile Hardware Development Methodology (AHDM). The authors highlight the limitations of existing Large Language Models (LLMs) in generating correct Chisel syntax and accommodating design variability. By employing domain adaptation techniques and structured prompt-guided reasoning, ChiseLLM significantly enhances the performance of code generation tasks. The results show substantial improvements in syntax correctness and design variability, making ChiseLLM a valuable resource for hardware developers and researchers."
                },
                "zh": {
                    "title": "ChiseLLM：提升Chisel代码生成的智能解决方案",
                    "desc": "随着对特定领域架构（DSA）的需求增加，敏捷硬件开发方法（AHDM）得到了发展。硬件构造语言（HCL）如Chisel提供了高层次的抽象特性，非常适合用于基于HCL的AHDM。尽管大型语言模型（LLMs）在代码生成任务中表现出色，但在Chisel生成方面仍面临语法正确性和设计变异性的挑战。本文提出了ChiseLLM，通过数据处理与转换、提示引导的推理轨迹合成和领域适应模型训练，显著提高了Chisel代码生成的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19395",
            "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
            "url": "https://huggingface.co/papers/2504.19395",
            "abstract": "Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.",
            "score": 2,
            "issue_id": 3484,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "6c4b9edf630ae39b",
            "authors": [
                "Zhouxiang Fang",
                "Aayush Mishra",
                "Muhan Gao",
                "Anqi Liu",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19395.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "🔑",
                "ru": {
                    "title": "Расшифровка скрытых шаблонов: LLM и обратимые шифры",
                    "desc": "В статье рассматривается, как LLM могут решать задачи In-Context Learning (ICL) с использованием шифров подстановки, заимствованных из криптографии. Авторы вводят ICL CIPHERS, где часть токенов заменяется на другие, создавая скрытый, но обратимый шаблон. Исследование показывает, что LLM лучше справляются с задачами, где шифр обратим, чем с необратимыми. Это открывает новый подход к оценке способности LLM к «обучению» в ICL."
                },
                "en": {
                    "title": "Decoding Learning: ICL CIPHERS Unveiled",
                    "desc": "This paper explores In-Context Learning (ICL) by introducing a new method called ICL CIPHERS, which uses substitution ciphers to reformulate tasks. The authors argue that ICL operates in two modes: task retrieval and task learning, and they aim to distinguish between these modes. By applying reversible transformations to input tokens, they create a scenario where large language models (LLMs) must decipher the latent patterns to solve tasks. The results show that LLMs perform better on tasks with bijective mappings compared to non-bijective ones, highlighting a novel way to measure learning in ICL."
                },
                "zh": {
                    "title": "揭示情境学习的双重模式",
                    "desc": "最近的研究表明，情境学习（ICL）有两种模式：任务检索（从预训练中记忆学习的模式）和任务学习（从示例中进行推理的学习）。然而，区分这两种模式仍然是一个具有挑战性的目标。我们提出了ICL CIPHERS，这是一种基于经典密码学的替换密码的任务重构方法。通过这种方法，输入中的一部分标记被替换为其他（无关的）标记，使得句子对人类的理解变得困难，但这种替换是可逆的，保持了任务的定义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19062",
            "title": "Versatile Framework for Song Generation with Prompt-based Control",
            "url": "https://huggingface.co/papers/2504.19062",
            "abstract": "Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.",
            "score": 1,
            "issue_id": 3491,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "fd59eefcb2d985bf",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#story_generation",
                    "#synthetic",
                    "#multimodal",
                    "#audio"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "VersBand: Многозадачная система для контролируемой генерации высококачественных песен",
                    "desc": "VersBand - это многозадачная система генерации песен, которая решает проблемы существующих методов в создании вокала и аккомпанемента с контролем на основе промптов и правильным выравниванием. Система включает в себя модель VocalBand для генерации вокала с использованием метода flow-matching, модель AccompBand на основе трансформера для создания аккомпанемента, а также модели LyricBand и MelodyBand для генерации текстов и мелодий соответственно. Экспериментальные результаты показывают, что VersBand превосходит базовые модели по объективным и субъективным метрикам в различных задачах генерации песен."
                },
                "en": {
                    "title": "VersBand: Mastering Multi-Task Song Generation with Control",
                    "desc": "This paper presents VersBand, a novel framework for generating high-quality songs with precise control based on various prompts. It addresses the limitations of existing methods in producing aligned vocals and accompaniments by introducing specialized models like VocalBand and AccompBand. VocalBand uses flow-matching to create diverse singing styles and pitches, while AccompBand employs a flow-based transformer with expert selection for better quality and alignment. Additionally, LyricBand and MelodyBand enhance the system's capabilities, allowing for comprehensive control over lyrics and melodies, resulting in superior performance compared to baseline models."
                },
                "zh": {
                    "title": "VersBand：可控高质量歌曲生成的新框架",
                    "desc": "这篇论文介绍了一种名为VersBand的多任务歌曲生成框架，旨在根据不同的提示生成高质量且可控的歌曲。VersBand包含多个主要模型，包括VocalBand和AccompBand，分别用于生成人声和伴奏，确保它们之间的良好对齐。VocalBand利用流匹配方法快速生成具有风格控制的高质量人声，而AccompBand则通过选择合适的专家来提升伴奏的质量和控制能力。实验结果表明，VersBand在多个歌曲生成任务中优于基线模型，展示了其在歌词和旋律生成方面的强大能力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4月28日"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4月30日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了当前文本到图像生成模型的局限性，特别是在生成非拉丁字母的精确和灵活的排版元素方面。为了解决这些问题，作者提出了RepText，这是一种能够准确渲染多语言视觉文本的模型，而无需真正理解这些文本。RepText 采用了ControlNet的设置，并额外整合了语言无关的字形和渲染文本的位置，使用户能够根据需要自定义文本内容、字体和位置。通过使用文本感知损失和扩散损失，以及在推理阶段采用噪声字形潜在初始化和区域掩码，RepText 显著提高了渲染的准确性和稳定性。实验结果表明，RepText 在开源方法中表现出色，并且与封闭源的多语言模型相媲美。文章最后还讨论了RepText的局限性。",
        "title": "RepText: Rendering Visual Text via Replicating",
        "pinyin": "这篇文章讨论了当前文本到图像生成模型的局限性，特别是在生成非拉丁字母的精确和灵活的排版元素方面。为了解决这些问题，作者提出了RepText，这是一种能够准确渲染多语言视觉文本的模型，而无需真正理解这些文本。RepText 采用了ControlNet的设置，并额外整合了语言无关的字形和渲染文本的位置，使用户能够根据需要自定义文本内容、字体和位置。通过使用文本感知损失和扩散损失，以及在推理阶段采用噪声字形潜在初始化和区域掩码，RepText 显著提高了渲染的准确性和稳定性。实验结果表明，RepText 在开源方法中表现出色，并且与封闭源的多语言模型相媲美。文章最后还讨论了RepText的局限性。\n\nzhè piān wén zhāng tǎo lùn le dāng qián wén běn dào tú xiàng shēng chéng mó xíng de jú xiàn xìng, tè bié shì zài shēng chéng fēi lā dīng zì mǔ de jīng què hé líng huó de pái bǎn yuán sǔ fāng miàn. wèi le jiě jué zhè xiē wèn tí, zuò zhě tí chū le RepText, zhè shì yī zhǒng néng gòu zhǔn què xuàn rán duō yǔ yán shì jù wén běn de mó xíng, ér wú xū zhēn zhèng lǐ jiě zhè xiē wén běn. RepText cuō yòng le ControlNet de shè zhì, bìng é wài zhěng hé le yǔ yán wú guān de zì xíng hé xuàn rán wén běn de wèi zhì, shǐ yòng hù néng gòu yīn yòng zhèn dīng wén běn nèi róng, zì tǐ hé wèi zhì. tōng guò shǐ yòng wén běn gǎn jué sǔn shī hé kuò sàn sǔn shī, yǐ jiā shǐ yòng tuī lǐ jiē duàn cuò yòng zào shēng zì xíng qián zài chū shǐ huà hé qū yù mó zhào, RepText xiǎn zhù tí gāo le xuàn rán de zhǔn què xìng hé wěn dìng xìng. shí yàn jié guǒ biǎo míng, RepText zài kāi yuán fāng fǎ zhōng biǎo xiàn chū sè, bìng qiě yǔ fēng bì yuán de duō yǔ yán mó xíng xiāng jì mǐ. wén zhāng zuì hòu hái tǎo lùn le RepText de jú xiàn xìng.\n\nzhè piān wén zhāng tǎo lùn le dāng qián wén běn dào tú xiàng shēng chéng mó xíng de jú xiàn xìng, tè bié shì zài shēng chéng fēi lā dīng zì mǔ de jīng què hé líng huó de pái bǎn yuán sǔ fāng miàn. wèi le jiě jué zhè xiē wèn tí, zuò zhě tí chū le RepText, zhè shì yī zhǒng néng gòu zhǔn què xuàn rán duō yǔ yán shì jù wén běn de mó xíng, ér wú xū zhēn zhèng lǐ jiě zhè xiē wén běn. RepText cuō yòng le ControlNet de shè zhì, bìng é wài zhěng hé le yǔ yán wú guān de zì xíng hé xuàn rán wén běn de wèi zhì, shǐ yòng hù néng gòu yīn yòng zhèn dīng wén běn nèi róng, zì tǐ hé wèi zhì. tōng guò shǐ yòng wén běn gǎn jué sǔn shī hé kuò sàn sǔn shī, yǐ jiā shǐ yòng tuī lǐ jiē duàn cuò yòng zào shēng zì xíng qián zài chū shǐ huà hé qū yù mó zhào, RepText xiǎn zhù tí gāo le xuàn rán de zhǔn què xìng hé wěn dìng xìng. shí yàn jié guǒ biǎo míng, RepText zài kāi yuán fāng fǎ zhōng biǎo xiàn chū sè, bìng qiě yǔ fēng bì yuán de duō yǔ yán mó xíng xiāng jì mǐ. wén zhāng zuì hòu hái tǎo lùn le RepText de jú xiàn xìng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"当前\", \"pinyin\": \"dāng qián\", \"trans\": \"current\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"局限性\", \"pinyin\": \"jú xiàn xìng\", \"trans\": \"limitations\"},\n    {\"word\": \"特别\", \"pinyin\": \"tè bié\", \"trans\": \"especially\"},\n    {\"word\": \"非拉丁字母\", \"pinyin\": \"fēi lā dīng zì mǔ\", \"trans\": \"non-Latin characters\"},\n    {\"word\": \"精确\", \"pinyin\": \"jīng què\", \"trans\": \"precise\"},\n    {\"word\": \"灵活\", \"pinyin\": \"líng huó\", \"trans\": \"flexible\"},\n    {\"word\": \"排版\", \"pinyin\": \"pái bǎn\", \"trans\": \"typesetting\"},\n    {\"word\": \"元素\", \"pinyin\": \"yuán sù\", \"trans\": \"elements\"},\n    {\"word\": \"方面\", \"pinyin\": \"fāng miàn\", \"trans\": \"aspect\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"RepText\", \"pinyin\": \"RepText\", \"trans\": \"RepText\"},\n    {\"word\": \"准确\", \"pinyin\": \"zhǔn què\", \"trans\": \"accurate\"},\n    {\"word\": \"渲染\", \"pinyin\": \"xuàn rán\", \"trans\": \"render\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duō yǔ yán\", \"trans\": \"multilingual\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"without needing\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"采用\", \"pinyin\": \"cǎi yòng\", \"trans\": \"adopt\"},\n    {\"word\": \"ControlNet\", \"pinyin\": \"ControlNet\", \"trans\": \"ControlNet\"},\n    {\"word\": \"设置\", \"pinyin\": \"shè zhì\", \"trans\": \"setting\"},\n    {\"word\": \"额外\", \"pinyin\": \"é wài\", \"trans\": \"additional\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"语言无关\", \"pinyin\": \"yǔ yán wú guān\", \"trans\": \"language-agnostic\"},\n    {\"word\": \"字形\", \"pinyin\": \"zì xíng\", \"trans\": \"glyph\"},\n    {\"word\": \"位置\", \"pinyin\": \"wèi zhì\", \"trans\": \"position\"},\n    {\"word\": \"使用户\", \"pinyin\": \"shǐ yòng hù\", \"trans\": \"enable users\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēn jù\", \"trans\": \"according to\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"自定义\", \"pinyin\": \"zì dìng yì\", \"trans\": \"customize\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"字体\", \"pinyin\": \"zì tǐ\", \"trans\": \"font\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"文本感知\", \"pinyin\": \"wén běn gǎn zhī\", \"trans\": \"text-aware\"},\n    {\"word\": \"损失\", \"pinyin\": \"sǔn shī\", \"trans\": \"loss\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"阶段\", \"pinyin\": \"jiē duàn\", \"trans\": \"stage\"},\n    {\"word\": \"噪声\", \"pinyin\": \"zào shēng\", \"trans\": \"noise\"},\n    {\"word\": \"潜在\", \"pinyin\": \"qián zài\", \"trans\": \"latent\"},\n    {\"word\": \"初始化\", \"pinyin\": \"chū shǐ huà\", \"trans\": \"initialization\"},\n    {\"word\": \"区域\", \"pinyin\": \"qū yù\", \"trans\": \"region\"},\n    {\"word\": \"掩码\", \"pinyin\": \"yǎn mǎ\", \"trans\": \"mask\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"稳定性\", \"pinyin\": \"wěn dìng xìng\", \"trans\": \"stability\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēng bì yuán\", \"trans\": \"closed-source\"},\n    {\"word\": \"相媲美\", \"pinyin\": \"xiāng pì měi\", \"trans\": \"compare favorably\"},\n    {\"word\": \"文章\", \"pinyin\": \"wén zhāng\", \"trans\": \"article\"},\n    {\"word\": \"最后\", \"pinyin\": \"zuì hòu\", \"trans\": \"finally\"},\n    {\"word\": \"还\", \"pinyin\": \"hái\", \"trans\": \"still\"}\n]",
        "trans": "This article discusses the limitations of current text-to-image generation models, particularly in generating precise and flexible typographic elements for non-Latin scripts. To address these issues, the authors propose RepText, a model capable of accurately rendering multilingual visual text without truly understanding the text. RepText adopts the setup of ControlNet and additionally integrates language-agnostic glyphs and the positioning of rendered text, allowing users to customize text content, fonts, and positions as needed. By utilizing text-aware loss and diffusion loss, along with noisy glyph latent initialization and regional masking during the inference stage, RepText significantly improves the accuracy and stability of rendering. Experimental results demonstrate that RepText performs excellently among open-source methods and is comparable to closed-source multilingual models. The article concludes with a discussion of RepText's limitations.",
        "update_ts": "2025-04-29 09:12"
    }
}