{
    "date": {
        "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 4",
        "zh": "12æœˆ4æ—¥"
    },
    "time_utc": "2025-12-04 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-04",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.21631",
            "title": "Qwen3-VL Technical Report",
            "url": "https://huggingface.co/papers/2511.21631",
            "abstract": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
            "score": 119,
            "issue_id": 1,
            "pub_date": "2025-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "576b284e5109996a",
            "authors": [
                "Shuai Bai",
                "Yuxuan Cai",
                "Ruizhe Chen",
                "Keqin Chen",
                "Xionghui Chen",
                "Zesen Cheng",
                "Lianghao Deng",
                "Wei Ding",
                "Chang Gao",
                "Chunjiang Ge",
                "Wenbin Ge",
                "Zhifang Guo",
                "Qidong Huang",
                "Jie Huang",
                "Fei Huang",
                "Binyuan Hui",
                "Shutong Jiang",
                "Zhaohai Li",
                "Mingsheng Li",
                "Mei Li",
                "Kaixin Li",
                "Zicheng Lin",
                "Junyang Lin",
                "Xuejing Liu",
                "Jiawei Liu",
                "Chenglong Liu",
                "Yang Liu",
                "Dayiheng Liu",
                "Shixuan Liu",
                "Dunjie Lu",
                "Ruilin Luo",
                "Chenxu Lv",
                "Rui Men",
                "Lingchen Meng",
                "Xuancheng Ren",
                "Xingzhang Ren",
                "Sibo Song",
                "Yuchong Sun",
                "Jun Tang",
                "Jianhong Tu",
                "Jianqiang Wan",
                "Peng Wang",
                "Pengfei Wang",
                "Qiuyue Wang",
                "Yuxuan Wang",
                "Tianbao Xie",
                "Yiheng Xu",
                "Haiyang Xu",
                "Jin Xu",
                "Zhibo Yang",
                "Mingkun Yang",
                "Jianxin Yang",
                "An Yang",
                "Bowen Yu",
                "Fei Zhang",
                "Hang Zhang",
                "Xi Zhang",
                "Bo Zheng",
                "Humen Zhong",
                "Jingren Zhou",
                "Fan Zhou",
                "Jing Zhou",
                "Yuanzhi Zhu",
                "Ke Zhu"
            ],
            "affiliations": [
                "Qwen Team"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.21631.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Qwen3-VL, Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 256K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ interleaved-MRoPE Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, DeepStack Ğ´Ğ»Ñ Ñ‚ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ vision-language ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… - Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… (2B/4B/8B) Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… (32B/235B) Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ mixture-of-experts Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Qwen3-VL: Redefining Multimodal Understanding with Unmatched Contextual Power",
                    "desc": "Qwen3-VL is a cutting-edge vision-language model that excels in understanding both text and multimodal inputs like images and videos. It can handle very large contexts of up to 256,000 tokens, allowing it to effectively manage and reference long documents and video content. The model features various architectures, including dense and mixture-of-experts variants, to optimize performance based on different needs for speed and quality. With significant improvements in text comprehension, long-context processing, and multimodal reasoning, Qwen3-VL sets a new standard for tasks requiring complex interactions between visual and textual data."
                },
                "zh": {
                    "title": "Qwen3-VLï¼šå¤šæ¨¡æ€ç†è§£çš„æ–°æ ‡æ†",
                    "desc": "Qwen3-VLæ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®ƒæ”¯æŒé«˜è¾¾256Kä¸ªæ ‡è®°çš„äº¤é”™ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿæ— ç¼æ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚è¯¥æ¨¡å‹ç³»åˆ—åŒ…æ‹¬å¤šç§å˜ä½“ï¼Œä»¥é€‚åº”ä¸åŒçš„å»¶è¿Ÿå’Œè´¨é‡æƒè¡¡ã€‚Qwen3-VLåœ¨é•¿æ–‡æœ¬ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¢†å…ˆè¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03442",
            "title": "PretrainZero: Reinforcement Active Pretraining",
            "url": "https://huggingface.co/papers/2512.03442",
            "abstract": "PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
            "score": 44,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "149d6200377c28b5",
            "authors": [
                "Xingrun Xing",
                "Zhiyuan Fan",
                "Jie Lou",
                "Guoqi Li",
                "Jiajun Zhang",
                "Debing Zhang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03442.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#agi",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: RL-Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ±ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "PretrainZero â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ RL Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ½Ğ°Ğ»Ğ°Ğ¹Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Wikipedia, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMLU-Pro, SuperGPQA Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ 3 Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking General Reasoning with PretrainZero",
                    "desc": "PretrainZero is a novel reinforcement active learning framework designed to improve general reasoning abilities in large models. It utilizes a pretraining approach that does not depend on verifiable labels, allowing models to learn from a broader corpus, such as Wikipedia. By implementing active pretraining and self-supervised learning, PretrainZero enables models to identify and reason about informative content effectively. The framework demonstrates significant performance improvements on various benchmarks, showcasing its potential to advance artificial general intelligence."
                },
                "zh": {
                    "title": "PretrainZeroï¼šçªç ´é€šç”¨æ¨ç†çš„ç•Œé™",
                    "desc": "PretrainZero æ˜¯ä¸€ä¸ªå¼ºåŒ–ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ²¡æœ‰å¯éªŒè¯æ ‡ç­¾çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„ç‰¹ç‚¹åŒ…æ‹¬ä¸»åŠ¨é¢„è®­ç»ƒï¼Œæ¨¡ä»¿äººç±»çš„ä¸»åŠ¨å­¦ä¹ èƒ½åŠ›ï¼Œä»é¢„è®­ç»ƒè¯­æ–™ä¸­ä¸»åŠ¨è¯†åˆ«æœ‰ç”¨å†…å®¹å¹¶è¿›è¡Œæ¨ç†ã€‚å®ƒè¿˜é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼Œç›´æ¥åœ¨é€šç”¨ç»´åŸºç™¾ç§‘è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæ— éœ€å¯éªŒè¯æ ‡ç­¾æˆ–ç›‘ç£å¾®è°ƒã€‚é€šè¿‡è§£å†³è¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æ©è”½è·¨åº¦ï¼ŒPretrainZero æ˜¾è‘—æå‡äº†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02834",
            "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
            "url": "https://huggingface.co/papers/2512.02834",
            "abstract": "TACO, a test-time-scaling framework with a pseudo-count estimator, enhances the inference stability and success rates of Vision-Language-Action models in downstream tasks by preventing distribution shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
            "score": 39,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "dfeb2e8025d4bcd5",
            "authors": [
                "Siyuan Yang",
                "Yang Zhang",
                "Haoran He",
                "Ling Pan",
                "Xiu Li",
                "Chenjia Bai",
                "Xuelong Li"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence, China Telecom",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02834.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "TACO â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ fine-tuning Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ³ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ¾ÑĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ²ÑĞµÑ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ offline reinforcement learning, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "TACO: Enhancing VLA Stability and Success Rates at Inference",
                    "desc": "This paper introduces TACO, a framework designed to improve the stability and success rates of Vision-Language-Action (VLA) models during inference. TACO uses a pseudo-count estimator to identify and prioritize the most effective action chunks, thereby reducing the impact of irrelevant actions that can arise from distribution shifts. By applying this method only at inference time, TACO maintains the generalization capabilities of VLA models while avoiding the computational costs associated with traditional reinforcement learning updates. The results show that TACO enhances performance across various simulation benchmarks, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "TACOï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¨ç†ç¨³å®šæ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTACOçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ¨ç†ç¨³å®šæ€§å’ŒæˆåŠŸç‡ã€‚TACOé€šè¿‡ä½¿ç”¨è½»é‡çº§çš„ä¼ªè®¡æ•°ä¼°è®¡å™¨ï¼Œé˜²æ­¢äº†æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åˆ†å¸ƒåç§»ï¼Œä»è€Œç¡®ä¿äº†åŠ¨ä½œé€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒVLAæ¨¡å‹åœ¨ç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œå­˜åœ¨æ¨ç†æ—¶é—´çš„è„†å¼±æ€§ï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®ä¸æˆåŠŸæ¨¡å¼ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚é€šè¿‡åœ¨æ¨ç†é˜¶æ®µåº”ç”¨çº¦æŸï¼ŒTACOèƒ½å¤Ÿåœ¨ä¸å½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–åŠ¨ä½œé€‰æ‹©è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03043",
            "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
            "url": "https://huggingface.co/papers/2512.03043",
            "abstract": "OneThinker, an all-in-one multimodal reasoning model, unifies image and video understanding across various tasks using RL and demonstrates strong performance and knowledge transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "ac703de12540ffea",
            "authors": [
                "Kaituo Feng",
                "Manyuan Zhang",
                "Hongyu Li",
                "Kaixuan Fan",
                "Shuang Chen",
                "Yilei Jiang",
                "Dian Zheng",
                "Peiwen Sun",
                "Yiyuan Zhang",
                "Haoze Sun",
                "Yan Feng",
                "Peng Pei",
                "Xunliang Cai",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, CUHK",
                "Meituan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03043.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#rl",
                    "#transfer_learning",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "OneThinker â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OneThinker-600k, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ EMA-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸ĞµÑÑ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 31 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ½Ğ°Ñ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OneThinker: Unifying Image and Video Reasoning for Enhanced Multimodal Understanding",
                    "desc": "OneThinker is a comprehensive multimodal reasoning model that integrates image and video understanding for various tasks using reinforcement learning (RL). Unlike traditional methods that train separate models for different tasks, OneThinker unifies these processes, allowing for better scalability and knowledge sharing. The model is trained on a specially constructed dataset, OneThinker-600k, which includes diverse visual tasks such as question answering and segmentation. Experimental results demonstrate that OneThinker excels across multiple benchmarks, showcasing its ability to transfer knowledge and generalize effectively across tasks."
                },
                "zh": {
                    "title": "OneThinkerï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„å…¨èƒ½æ¨¡å‹",
                    "desc": "OneThinkeræ˜¯ä¸€ä¸ªé›†æˆçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†å›¾åƒå’Œè§†é¢‘ç†è§£çš„å¤šç§ä»»åŠ¡ã€‚å®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°äº†åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒOneThinkerä¸å†ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æå‡ä»»åŠ¡é—´çš„çŸ¥è¯†å…±äº«å’Œè¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneThinkeråœ¨31ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæœç€å¤šæ¨¡æ€æ¨ç†é€šç”¨æ¨¡å‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03405",
            "title": "ViDiC: Video Difference Captioning",
            "url": "https://huggingface.co/papers/2512.03405",
            "abstract": "The ViDiC task and ViDiC-1K dataset evaluate Multimodal Large Language Models' ability to describe differences between video pairs, addressing limitations in capturing motion continuity and event evolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "1fd979c6ddf8c93b",
            "authors": [
                "Jiangtao Wu",
                "Shihao Li",
                "Zhaozhou Bian",
                "Jialu Chen",
                "Runzhe Wen",
                "An Ping",
                "Yiwen He",
                "Jiakai Wang",
                "Yuanxing Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03405.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04040",
            "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
            "url": "https://huggingface.co/papers/2512.04040",
            "abstract": "RELIC is a unified framework that enables real-time, memory-aware exploration of scenes by integrating long-horizon memory, spatial consistency, and user control using autoregressive video-diffusion distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "470e2cea40cdc8e1",
            "authors": [
                "Yicong Hong",
                "Yiqun Mei",
                "Chongjian Ge",
                "Yiran Xu",
                "Yang Zhou",
                "Sai Bi",
                "Yannick Hold-Geoffroy",
                "Mike Roberts",
                "Matthew Fisher",
                "Eli Shechtman",
                "Kalyan Sunkavalli",
                "Feng Liu",
                "Zhengqi Li",
                "Hao Tan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04040.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04069",
            "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
            "url": "https://huggingface.co/papers/2512.04069",
            "abstract": "Double Interactive Reinforcement Learning (DIRL) enables Vision Language Models (VLMs) to coordinate multiple tools for precise spatial reasoning, achieving state-of-the-art performance on benchmarks and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "18dab6067c24d33f",
            "authors": [
                "Siyi Chen",
                "Mikaela Angelina Uy",
                "Chan Hee Song",
                "Faisal Ladhak",
                "Adithyavairavan Murali",
                "Qing Qu",
                "Stan Birchfield",
                "Valts Blukis",
                "Jonathan Tremblay"
            ],
            "affiliations": [
                "NVIDIA",
                "The Ohio State University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04069.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#robotics",
                    "#alignment",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Double Interactive Reinforcement Learning (DIRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (VLM) Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹; Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ°Ğ»ÑŒÑˆĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· continued RL. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SpaceTools Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Vision Language Models with Multi-Tool Coordination",
                    "desc": "Double Interactive Reinforcement Learning (DIRL) enhances Vision Language Models (VLMs) by enabling them to effectively coordinate multiple tools for improved spatial reasoning. This approach addresses the limitations of traditional methods that rely on fixed tool pipelines or handcrafted prompts, allowing VLMs to explore optimal tool usage dynamically. DIRL consists of a two-phase training process: a teaching phase that combines expert demonstrations with multi-tool traces, and an exploration phase that refines coordination through reinforcement learning. The resulting model, SpaceTools, achieves state-of-the-art performance on various spatial understanding benchmarks and demonstrates effective real-world manipulation capabilities."
                },
                "zh": {
                    "title": "åŒé‡äº’åŠ¨å¼ºåŒ–å­¦ä¹ ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "åŒé‡äº’åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆDIRLï¼‰ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿåè°ƒå¤šä¸ªå·¥å…·è¿›è¡Œç²¾ç¡®çš„ç©ºé—´æ¨ç†ï¼Œåœ¨åŸºå‡†æµ‹è¯•å’Œå®é™…ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒDIRLé€šè¿‡äº’åŠ¨æ¢ç´¢å’Œåé¦ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•æœ‰æ•ˆä½¿ç”¨å¤šç§å·¥å…·ã€‚æ•™å­¦é˜¶æ®µç»“åˆäº†å•ä¸€å·¥å…·ä¸“å®¶çš„æ¼”ç¤ºå’Œä½¿ç”¨æ‰€æœ‰å·¥å…·çš„å‰æ²¿æ¨¡å‹çš„è½¨è¿¹ï¼Œæ¢ç´¢é˜¶æ®µåˆ™é€šè¿‡æŒç»­çš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–å¤šå·¥å…·åè°ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹SpaceToolsåœ¨ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å®é™…æ“ä½œä¸­å±•ç¤ºäº†å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03534",
            "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
            "url": "https://huggingface.co/papers/2512.03534",
            "abstract": "PRIS adaptively revises prompts during inference to enhance alignment with user intent in text-to-visual generation, improving accuracy and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "eaa9990548d36049",
            "authors": [
                "Subin Kim",
                "Sangwoo Mo",
                "Mamshad Nayeem Rizve",
                "Yiran Xu",
                "Difan Liu",
                "Jinwoo Shin",
                "Tobias Hinz"
            ],
            "affiliations": [
                "Adobe",
                "KAIST",
                "Meta",
                "POSTECH"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03534.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03746",
            "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
            "url": "https://huggingface.co/papers/2512.03746",
            "abstract": "CodeVision, a flexible code-as-tool framework, enhances multimodal large language models' robustness and tool-based reasoning by generating code to handle image operations, overcoming brittleness and improving performance through supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "c8c835dd88519f67",
            "authors": [
                "Zirun Guo",
                "Minjie Hong",
                "Feng Zhang",
                "Kai Jia",
                "Tao Jin"
            ],
            "affiliations": [
                "ByteDance, BandAI",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03746.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "CodeVision â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Empowering MLLMs with Dynamic Code Generation for Robust Image Reasoning",
                    "desc": "This paper introduces CodeVision, a framework designed to enhance the robustness of multimodal large language models (MLLMs) in handling image operations. It addresses the brittleness of current MLLMs, which struggle with simple image changes, by allowing models to generate code dynamically for various image tasks. The framework employs a two-stage training process, combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to improve tool-based reasoning and error recovery. Experiments demonstrate that CodeVision significantly boosts performance and enables advanced capabilities like flexible tool composition and efficient execution."
                },
                "zh": {
                    "title": "CodeVisionï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†CodeVisionï¼Œä¸€ä¸ªçµæ´»çš„ä»£ç ä½œä¸ºå·¥å…·æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é²æ£’æ€§å’ŒåŸºäºå·¥å…·çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆä»£ç æ¥å¤„ç†å›¾åƒæ“ä½œï¼ŒCodeVisionå…‹æœäº†ç°æœ‰æ–¹æ³•çš„è„†å¼±æ€§ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„MLLMsåœ¨é¢å¯¹ç®€å•çš„å›¾åƒæ–¹å‘å˜åŒ–æˆ–è‡ªç„¶æŸåæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå› æ­¤éœ€è¦æ›´å¼ºå¤§çš„å·¥å…·æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCodeVisionæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ä¿ƒè¿›äº†çµæ´»çš„å·¥å…·ç»„åˆå’Œé«˜æ•ˆçš„é”™è¯¯æ¢å¤èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04082",
            "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
            "url": "https://huggingface.co/papers/2512.04082",
            "abstract": "PosterCopilot enhances professional graphic design through a three-stage training strategy for LMMs, enabling geometrically accurate and aesthetically superior layouts with controllable iterative editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "35ba02e824fe874b",
            "authors": [
                "Jiazhe Wei",
                "Ken Li",
                "Tianyu Lao",
                "Haofan Wang",
                "Liang Wang",
                "Caifeng Shan",
                "Chenyang Si"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "LibLib.ai",
                "PRLab, Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04082.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04032",
            "title": "Jina-VLM: Small Multilingual Vision Language Model",
            "url": "https://huggingface.co/papers/2512.04032",
            "abstract": "Jina-VLM, a 2.4B parameter vision-language model, achieves top performance in multilingual visual question answering using a SigLIP2 vision encoder and Qwen3 language backbone with an attention-pooling connector.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "1529870ca790bee7",
            "authors": [
                "Andreas Koukounas",
                "Georgios Mastrapas",
                "Florian HÃ¶nicke",
                "Sedigheh Eslami",
                "Guillaume Roncari",
                "Scott Martens",
                "Han Xiao"
            ],
            "affiliations": [
                "Elastic",
                "Jina AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04032.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22345",
            "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
            "url": "https://huggingface.co/papers/2511.22345",
            "abstract": "A novel alignment strategy and test-time optimization algorithm enhance the generative quality and classification accuracy of Normalizing Flows by leveraging invertibility and vision foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3times, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64times64 and 256times256. Our code is available at https://github.com/MCG-NJU/FlowBack.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "590ae8ee866402e8",
            "authors": [
                "Yang Chen",
                "Xiaowei Xu",
                "Shuai Wang",
                "Chenhui Zhu",
                "Ruxue Wen",
                "Xubin Li",
                "Tiezheng Ge",
                "Limin Wang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai AI Lab",
                "State Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22345.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#open_source",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ñ‚Ñ€Ñ‘Ñ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Normalizing Flows with Novel Alignment and Optimization Techniques",
                    "desc": "This paper presents a new strategy to improve Normalizing Flows (NFs), which are generative models that can transform data into a simpler form and then generate new data from it. The authors introduce an alignment method that connects the features generated during the reverse pass of NFs with those from advanced vision models, enhancing the model's ability to understand and generate data. Additionally, they propose a test-time optimization algorithm that evaluates the model's classification capabilities without needing further training. Their experiments show that this approach significantly speeds up NF training and improves both the quality of generated data and the accuracy of classifications, achieving new benchmarks on popular datasets."
                },
                "zh": {
                    "title": "æå‡å½’ä¸€åŒ–æµçš„ç”Ÿæˆè´¨é‡ä¸åˆ†ç±»å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹é½ç­–ç•¥å’Œæµ‹è¯•æ—¶ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æé«˜å½’ä¸€åŒ–æµï¼ˆNormalizing Flows, NFsï¼‰çš„ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§ã€‚é€šè¿‡åˆ©ç”¨NFsçš„å¯é€†æ€§ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸­é—´ç‰¹å¾ä¸å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œä»è€Œå…‹æœäº†ä¼ ç»ŸNFsåœ¨è¯­ä¹‰è¡¨ç¤ºä¸Šçš„ä¸è¶³ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„æµ‹è¯•æ—¶ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°NFsåµŒå…¥çš„è¯­ä¹‰çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿NFsçš„è®­ç»ƒé€Ÿåº¦æé«˜äº†3.3å€ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03540",
            "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
            "url": "https://huggingface.co/papers/2512.03540",
            "abstract": "CookAnything is a diffusion-based framework that generates coherent image sequences from cooking instructions by incorporating step-wise regional control, flexible positional encoding, and cross-step consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "0f8a554510b35ac8",
            "authors": [
                "Ruoxuan Zhang",
                "Bin Wen",
                "Hongxia Xie",
                "Yi Yao",
                "Songhan Zuo",
                "Jian-Yu Jiang-Lin",
                "Hong-Han Shuai",
                "Wen-Huang Cheng"
            ],
            "affiliations": [
                "Jilin University",
                "National Taiwan University",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03540.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05115",
            "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
            "url": "https://huggingface.co/papers/2512.05115",
            "abstract": "Light-X is a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control, outperforming existing methods in joint camera-illumination control and relighting.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "2baef0e8b8d9fa3e",
            "authors": [
                "Tianqi Liu",
                "Zhaoxi Chen",
                "Zihao Huang",
                "Shaocong Xu",
                "Saining Zhang",
                "Chongjie Ye",
                "Bohan Li",
                "Zhiguo Cao",
                "Wei Li",
                "Hao Zhao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "AIR, THU",
                "BAAI",
                "EIT (Ningbo)",
                "FNii, CUHKSZ",
                "HUST",
                "S-Lab, NTU",
                "SJTU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05115.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02807",
            "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
            "url": "https://huggingface.co/papers/2512.02807",
            "abstract": "Stable rank, an intrinsic quality signal derived from model representations, improves LLM alignment with human preferences through reinforcement learning without external supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "b9a5922944c86bcc",
            "authors": [
                "Yixuan Tang",
                "Yi Yang"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02807.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ stable rank â€” Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğº Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ SR-GRPO â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ stable rank Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: 84% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RewardBench Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° STEM Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° 10-19% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´."
                },
                "en": {
                    "title": "Harnessing Internal Quality Signals for Better LLM Alignment",
                    "desc": "This paper introduces stable rank, a new quality signal derived from the internal representations of large language models (LLMs) that enhances their alignment with human preferences. Unlike traditional methods that rely on external supervision, stable rank is an intrinsic measure that evaluates the effective dimensionality of hidden states, providing a more reliable quality assessment. The authors demonstrate that using stable rank as a reward signal in reinforcement learning significantly improves model performance on various tasks, achieving notable accuracy gains. This approach suggests a promising direction for aligning LLMs with human values without the need for external annotations or supervision."
                },
                "zh": {
                    "title": "ç¨³å®šç§©ï¼šæ— ç›‘ç£å¯¹é½çš„è´¨é‡ä¿¡å·",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç¨³å®šç§©çš„å†…åœ¨è´¨é‡ä¿¡å·ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚ç¨³å®šç§©é€šè¿‡è®¡ç®—éšè—çŠ¶æ€çš„æ€»æ–¹å·®ä¸ä¸»æ–¹å‘æ–¹å·®çš„æ¯”ç‡ï¼Œæ¥è¡¡é‡ä¿¡æ¯åœ¨è¡¨ç¤ºç»´åº¦ä¸Šçš„åˆ†å¸ƒï¼Œä»è€Œæ•æ‰æ¨¡å‹çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¨³å®šç§©åœ¨RewardBenchä¸Šè¾¾åˆ°äº†84.04%çš„å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡Best-of-Né‡‡æ ·åœ¨ä»»åŠ¡å‡†ç¡®æ€§ä¸Šå¹³å‡æé«˜äº†11.3ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç¨³å®šç§©ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆSR-GRPOï¼‰ï¼Œåˆ©ç”¨ç¨³å®šç§©ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨STEMå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02924",
            "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
            "url": "https://huggingface.co/papers/2512.02924",
            "abstract": "AutoNeural, an NPU-native VLM architecture, improves efficiency and performance on edge devices by using integer-only inference, MobileNetV5-style backbone, and a hybrid design with SSM and Transformer layers, reducing quantization errors and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "92c72eb6dc3c93d2",
            "authors": [
                "Wei Chen",
                "Liangmin Wu",
                "Yunhai Hu",
                "Zhiyuan Li",
                "Zhiyuan Cheng",
                "Yicheng Qian",
                "Lingyue Zhu",
                "Zhipeng Hu",
                "Luoyi Liang",
                "Qiang Tang",
                "Zhen Liu",
                "Han Yang"
            ],
            "affiliations": [
                "Geely Auto",
                "Nexa AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02924.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03073",
            "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
            "url": "https://huggingface.co/papers/2512.03073",
            "abstract": "The analysis of Hugging Face Model Hub data reveals shifts in the open model economy, including declining US industry dominance, growing Chinese influence, and significant changes in model properties like size, multimodal generation, quantization, and mixture-of-experts architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "a366f88104c7a889",
            "authors": [
                "Shayne Longpre",
                "Christopher Akiki",
                "Campbell Lund",
                "Atharva Kulkarni",
                "Emily Chen",
                "Irene Solaiman",
                "Avijit Ghosh",
                "Yacine Jernite",
                "Lucie-AimÃ©e Kaffee"
            ],
            "affiliations": [
                "Data Provenance Initiative",
                "Hugging Face",
                "MIT",
                "ScaDS.AI Leipzig",
                "UNC at Chapel Hill",
                "University of Edinburgh",
                "University of Southern California"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03073.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#inference",
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ AI: Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ°Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ³ĞµĞ¼Ğ¾Ğ½Ğ¸Ğ¸ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑÑ€Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Hugging Face Model Hub Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ 2020 Ğ¿Ğ¾ 2025 Ğ³Ğ¾Ğ´. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 851 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 2.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ (Google, Meta, OpenAI) Ğ¸ Ñ€Ğ¾ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ² 17 Ñ€Ğ°Ğ·, Ñ€Ğ¾ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 3.4 Ñ€Ğ°Ğ·Ğ°, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ ÑĞ¼ĞµÑĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² 7 Ñ€Ğ°Ğ·. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ²Ğ¾Ğ¶Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¹ Ğ´Ğ¾Ğ»Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Shifting Powers in the Open Model Economy",
                    "desc": "This paper analyzes data from the Hugging Face Model Hub to understand changes in the open model economy. It highlights a shift in dominance from US companies like Google and Meta to unaffiliated developers and Chinese firms, indicating a rebalancing of economic power. The study also notes significant increases in model size, multimodal capabilities, quantization, and mixture-of-experts architectures, while raising concerns about declining data transparency. To support ongoing research, the authors provide a comprehensive dataset and an interactive dashboard for monitoring these trends."
                },
                "zh": {
                    "title": "å¼€æ”¾æ¨¡å‹ç»æµçš„å˜é©ä¸æœªæ¥",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†Hugging Faceæ¨¡å‹åº“çš„æ•°æ®ï¼Œæ­ç¤ºäº†å¼€æ”¾æ¨¡å‹ç»æµä¸­çš„å˜åŒ–ï¼ŒåŒ…æ‹¬ç¾å›½è¡Œä¸šä¸»å¯¼åœ°ä½çš„ä¸‹é™å’Œä¸­å›½å½±å“åŠ›çš„ä¸Šå‡ã€‚è‡ª2019å¹´ä»¥æ¥ï¼ŒHugging Faceæ¨¡å‹åº“æˆä¸ºå…¨çƒå…±äº«å¼€æ”¾æƒé‡AIæ¨¡å‹çš„ä¸»è¦å¹³å°ã€‚æˆ‘ä»¬çš„åˆ†ææ¶µç›–äº†851,000ä¸ªæ¨¡å‹å’Œ22äº¿æ¬¡ä¸‹è½½ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹ç‰¹æ€§å¦‚å¤§å°ã€å¤šæ¨¡æ€ç”Ÿæˆã€é‡åŒ–å’Œä¸“å®¶æ··åˆæ¶æ„çš„æ˜¾è‘—å˜åŒ–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†å’Œäº’åŠ¨ä»ªè¡¨æ¿ï¼Œä»¥ä¾¿å®æ—¶ç›‘æµ‹å¼€æ”¾æ¨¡å‹ç»æµä¸­çš„é›†ä¸­åŠ¨æ€å’Œæ¼”å˜ç‰¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04072",
            "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
            "url": "https://huggingface.co/papers/2512.04072",
            "abstract": "SkillFactory is a method for fine-tuning models to learn cognitive skills through supervised fine-tuning before reinforcement learning, enhancing their robustness and generalization post-RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "048f90c61f640c1e",
            "authors": [
                "Zayne Sprague",
                "Jack Lu",
                "Manya Wadhwa",
                "Sedrick Keh",
                "Mengye Ren",
                "Greg Durrett"
            ],
            "affiliations": [
                "New York University",
                "Toyota Research Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04072.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ",
                    "desc": "SkillFactory â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ¥Ğ¾Ñ‚Ñ ÑÑ‚Ğ¸ Â«ÑĞµÑ€ĞµĞ±Ñ€ÑĞ½Ñ‹ĞµÂ» Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¾Ğ½Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ RL. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ SkillFactory Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Empowering Models with Cognitive Skills Before Reinforcement Learning",
                    "desc": "SkillFactory is a novel approach for enhancing machine learning models by fine-tuning them to acquire cognitive skills through supervised learning before applying reinforcement learning (RL). This method allows models to learn skills such as answer verification and backtracking, which are not inherently present in base models. Instead of relying on stronger models for distillation, SkillFactory utilizes rearranged samples from the model itself to create training data that emphasizes these skills. The results indicate that models initialized with SkillFactory show improved generalization and robustness in RL tasks, demonstrating the importance of pre-RL skill acquisition."
                },
                "zh": {
                    "title": "SkillFactoryï¼šæå‡æ¨¡å‹è®¤çŸ¥æŠ€èƒ½çš„å¾®è°ƒæ–¹æ³•",
                    "desc": "SkillFactoryæ˜¯ä¸€ç§æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¹‹å‰è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½¿æ¨¡å‹å­¦ä¹ è®¤çŸ¥æŠ€èƒ½ï¼Œä»è€Œå¢å¼ºå…¶åœ¨RLåçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºä»æ›´å¼ºæ¨¡å‹çš„è’¸é¦ï¼Œè€Œæ˜¯ä½¿ç”¨æ¥è‡ªæ¨¡å‹è‡ªèº«çš„æ ·æœ¬ï¼Œé‡æ–°æ’åˆ—ä»¥æä¾›è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨SkillFactoryè¿›è¡ŒSFTåˆå§‹åŒ–çš„æ¨¡å‹åœ¨RLåèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ›´éš¾çš„ä»»åŠ¡å˜ä½“ï¼Œå°½ç®¡åœ¨RLä¹‹å‰çš„è¡¨ç°è¾ƒä½ã€‚æ­¤å¤–ï¼Œç»è¿‡RLçš„SkillFactoryæ¨¡å‹åœ¨å¤„ç†åŸŸå¤–ä»»åŠ¡æ—¶æ¯”åŸºç¡€æ¨¡å‹æ›´å…·é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03794",
            "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
            "url": "https://huggingface.co/papers/2512.03794",
            "abstract": "AdaptVision, a vision-language model, dynamically adjusts visual token usage through a reinforcement learning framework to balance accuracy and efficiency in visual question answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "23e48f37ea51bbab",
            "authors": [
                "Zichuan Lin",
                "Yicheng Liu",
                "Yang Yang",
                "Lvfang Tao",
                "Deheng Ye"
            ],
            "affiliations": [
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03794.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03771",
            "title": "In-Context Representation Hijacking",
            "url": "https://huggingface.co/papers/2512.03771",
            "abstract": "The attack Doublespeak manipulates the internal representation of benign tokens to align with harmful semantics, bypassing safety measures in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Doublespeak, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "5e92361dbb1985e8",
            "authors": [
                "Itay Yona",
                "Amir Sarid",
                "Michael Karasik",
                "Yossi Gandelsman"
            ],
            "affiliations": [
                "Independent Researcher",
                "Mentaleap",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03771.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#security",
                    "#alignment"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ñ€ĞºĞ¾Ğ²ÑŒ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ¼Ğ±Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ‚Ğ°ĞºĞ° Doublespeak, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ². ĞÑ‚Ğ°ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¹ Ğ·Ğ° ÑĞ»Ğ¾ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸, Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğº Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼ Ğ² Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Doublespeak: Bypassing Safety in Language Models through Semantic Manipulation",
                    "desc": "The paper introduces a new attack method called Doublespeak, which targets large language models (LLMs) by manipulating their internal representations. This attack replaces harmful keywords with benign tokens in context examples, allowing harmful semantics to be embedded under a euphemism. As a result, seemingly harmless prompts can be interpreted as dangerous instructions, effectively bypassing the model's safety measures. The study reveals that current alignment strategies are inadequate and suggests that future defenses should focus on the representation level of LLMs."
                },
                "zh": {
                    "title": "Doublespeakï¼šæ“æ§è¯­è¨€æ¨¡å‹çš„éšç§˜æ”»å‡»",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDoublespeakçš„æ”»å‡»æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ“æ§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨çš„è¡¨ç¤ºï¼Œå°†æ— å®³çš„è¯æ±‡ä¸æœ‰å®³çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œç»•è¿‡å®‰å…¨æªæ–½ã€‚è¯¥æ”»å‡»é€šè¿‡åœ¨å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­ç³»ç»Ÿæ€§åœ°å°†æœ‰å®³å…³é”®è¯ï¼ˆå¦‚ç‚¸å¼¹ï¼‰æ›¿æ¢ä¸ºæ— å®³è¯ï¼ˆå¦‚èƒ¡èåœï¼‰ï¼Œå¹¶åœ¨æœ‰å®³è¯·æ±‚å‰æ·»åŠ å‰ç¼€ï¼Œå¯¼è‡´æ— å®³è¯çš„å†…éƒ¨è¡¨ç¤ºé€æ¸æ¥è¿‘æœ‰å®³è¯çš„è¡¨ç¤ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ›¿æ¢ä½¿å¾—è¡¨é¢ä¸Šæ— å®³çš„æç¤ºï¼ˆå¦‚â€œå¦‚ä½•åˆ¶ä½œèƒ¡èåœï¼Ÿâ€ï¼‰åœ¨å†…éƒ¨è¢«è§£è¯»ä¸ºç¦æ­¢çš„æŒ‡ä»¤ï¼ˆå¦‚â€œå¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿâ€ï¼‰ï¼Œä»è€ŒæˆåŠŸç»•è¿‡æ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†LLMsæ½œåœ¨ç©ºé—´ä¸­çš„æ–°æ”»å‡»é¢ï¼Œè¡¨æ˜å½“å‰çš„å¯¹é½ç­–ç•¥ä¸è¶³ï¼Œåº”è¯¥åœ¨è¡¨ç¤ºå±‚é¢è¿›è¡Œæ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03383",
            "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
            "url": "https://huggingface.co/papers/2512.03383",
            "abstract": "UniQL, a unified post-training quantization and low-rank compression framework, enhances the deployment of large language models on mobile devices by reducing memory usage and improving token throughput while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "9131d62d43148e94",
            "authors": [
                "Hung-Yueh Chiang",
                "Chi-Chih Chang",
                "Yu-Chen Lu",
                "Chien-Yu Lin",
                "Kai-Chiang Wu",
                "Mohamed S. Abdelfattah",
                "Diana Marculescu"
            ],
            "affiliations": [
                "Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin",
                "Department of Computer Science, National Yang Ming Chiao Tung University",
                "Department of Electrical and Computer Engineering, Cornell University",
                "The Paul G. Allen School of Computer Science and Engineering, University of Washington"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03383.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#small_models",
                    "#inference"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "UniQL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼ (SVD) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ (SSM) Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²ĞµÑĞ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞµ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 4-5.7 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 2.7-3.4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimize Mobile LLMs with UniQL!",
                    "desc": "UniQL is a framework designed to optimize large language models for mobile devices by using post-training quantization and low-rank compression techniques. It addresses the challenges of limited memory and computational resources on mobile platforms, allowing for configurable pruning rates to enhance model efficiency. The framework integrates advanced methods like quantization-aware singular value decomposition and structured weight sorting to significantly speed up computations and reduce memory usage. Experiments demonstrate that UniQL can achieve substantial improvements in token throughput and memory reduction while maintaining high accuracy levels."
                },
                "zh": {
                    "title": "UniQLï¼šæå‡ç§»åŠ¨è®¾å¤‡ä¸Šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡",
                    "desc": "UniQLæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åè®­ç»ƒé‡åŒ–å’Œä½ç§©å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²æ•ˆç‡ã€‚å®ƒé€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜ä»¤ç‰Œååé‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†ç§»åŠ¨å¹³å°çš„èµ„æºé™åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§è¾¹ç¼˜åº”ç”¨ï¼Œé›†æˆäº†é‡åŒ–å’Œä½ç§©å‹ç¼©æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†é«˜æ•ˆçš„ç»“æ„åŒ–æƒé‡æ’åºæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡é‡åŒ–å’Œå‰ªæçš„æ¨¡å‹åœ¨å†…å­˜ä½¿ç”¨ä¸Šå‡å°‘äº†4åˆ°5.7å€ï¼Œä»¤ç‰Œååé‡æé«˜äº†2.7åˆ°3.4å€ï¼Œä¸”å‡†ç¡®æ€§ä¿æŒåœ¨åŸæ¨¡å‹çš„5%ä»¥å†…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20515",
            "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
            "url": "https://huggingface.co/papers/2511.20515",
            "abstract": "AlignBench evaluates image-text models using detailed captions generated by diverse models, revealing insights into their alignment and compositional reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "3ff481e47b562138",
            "authors": [
                "Kuniaki Saito",
                "Risa Shinoda",
                "Shohei Tanaka",
                "Tosho Hirasawa",
                "Fumio Okura",
                "Yoshitaka Ushiku"
            ],
            "affiliations": [
                "OMRON SINIC Corporation",
                "The University of Osaka"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20515.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#alignment",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ AlignBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ VLM ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğº ÑĞ²Ğ¾Ğ¸Ğ¼ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ fine-grained Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Images and Text: A New Benchmark for Better Evaluation",
                    "desc": "AlignBench is a new benchmark designed to evaluate image-text models by using detailed captions generated from various models. It addresses the limitations of existing benchmarks that often rely on short captions or rule-based changes, which do not effectively measure the alignment between visual and linguistic representations. The benchmark provides a more nuanced assessment of visual language models (VLMs) by annotating each sentence for correctness, allowing for a direct evaluation of their alignment capabilities. Key findings from testing various VLMs include that CLIP-based models struggle with compositional reasoning, detectors tend to over-score initial sentences, and models show a bias towards their own generated outputs, negatively impacting performance."
                },
                "zh": {
                    "title": "AlignBenchï¼šå›¾åƒä¸æ–‡æœ¬çš„å®Œç¾å¯¹é½",
                    "desc": "AlignBench æ˜¯ä¸€ä¸ªè¯„ä¼°å›¾åƒ-æ–‡æœ¬æ¨¡å‹çš„æ–°åŸºå‡†ï¼Œä½¿ç”¨å¤šæ ·åŒ–æ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†å›¾åƒ-æ–‡æœ¬æè¿°æ¥æ­ç¤ºæ¨¡å‹çš„å¯¹é½å’Œç»„åˆæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºåŸºäºè§„åˆ™çš„æ‰°åŠ¨æˆ–ç®€çŸ­çš„æè¿°ï¼Œé™åˆ¶äº†å¯¹ç»†ç²’åº¦å¯¹é½çš„æµ‹é‡ã€‚é€šè¿‡å¯¹å›¾åƒ-æ–‡æœ¬å¯¹çš„è¯¦ç»†è¯„ä¼°ï¼ŒAlignBench æä¾›äº†ä¸€ä¸ªæ–°çš„å¯¹é½æŒ‡æ ‡ï¼Œä½¿å¾—å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç›´æ¥è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒCLIP åŸºç¡€æ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢å‡ ä¹æ²¡æœ‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸”æ£€æµ‹å™¨å¯¹æ—©æœŸå¥å­çš„è¯„åˆ†è¿‡é«˜ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„è‡ªæˆ‘åå¥½ï¼Œå½±å“äº†æ£€æµ‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04025",
            "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
            "url": "https://huggingface.co/papers/2512.04025",
            "abstract": "Pyramid Sparse Attention (PSA) addresses the limitations of traditional sparse attention mechanisms by using multi-level pooling to retain more information while maintaining computational efficiency, improving performance in video understanding and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "a63919c347c161a7",
            "authors": [
                "Xiaolong Li",
                "Youping Gu",
                "Xi Lin",
                "Weijie Wang",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "ZIP Lab, Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04025.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#optimization",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ”º",
                "ru": {
                    "title": "ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ÑƒĞ»ĞµĞ¹",
                    "desc": "ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (PSA) Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, PSA Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ â€” Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing Attention with Pyramid Sparse Mechanism",
                    "desc": "Pyramid Sparse Attention (PSA) enhances traditional sparse attention mechanisms by utilizing multi-level pooling to improve information retention while ensuring computational efficiency. This method addresses the issue of high sparsity in attention mechanisms, which often leads to significant information loss. By dynamically allocating pooling levels based on the importance of key-value blocks, PSA achieves a balance between retaining critical information and reducing computational load. The approach has shown superior performance in video understanding and generation tasks, outperforming existing sparse attention methods while maintaining high efficiency."
                },
                "zh": {
                    "title": "é‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›ï¼šé«˜æ•ˆä¿¡æ¯ä¿ç•™çš„æ–°æ–¹æ³•",
                    "desc": "é‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›ï¼ˆPSAï¼‰é€šè¿‡å¤šçº§æ± åŒ–æ¥è§£å†³ä¼ ç»Ÿç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ä¿ç•™æ›´å¤šä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé¿å…äº†åœ¨é«˜ç¨€ç–åº¦ä¸‹ä¿¡æ¯çš„é‡å¤§æŸå¤±ã€‚PSAé‡‡ç”¨åŠ¨æ€åˆ†é…çš„æ± åŒ–çº§åˆ«ï¼Œä½¿å¾—é‡è¦çš„é”®å€¼å¯¹å—è·å¾—æ›´ä½çš„æ± åŒ–çº§åˆ«ï¼Œè€Œä¸é‡è¦çš„å—åˆ™ä½¿ç”¨æ›´é«˜çš„æ± åŒ–çº§åˆ«ï¼Œä»è€Œå®ç°ä¿¡æ¯çš„æœ‰æ•ˆæ’å€¼ã€‚è¯¥è®¾è®¡åœ¨è®¡ç®—æ•ˆç‡å’Œä¿¡æ¯ä¿ç•™ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œé€‚ç”¨äºå„ç§ç¡¬ä»¶ç¯å¢ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04000",
            "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
            "url": "https://huggingface.co/papers/2512.04000",
            "abstract": "DIG, a query-type adaptive frame selection framework, enhances large multimodal models for long-form video understanding by efficiently handling global and localized queries.  \t\t\t\t\tAI-generated summary \t\t\t\t The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "fcdd776dea7ff747",
            "authors": [
                "Jialuo Li",
                "Bin Li",
                "Jiahao Li",
                "Yan Lu"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.04000.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DIG Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ (Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾) Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ¸Ñ‰ÑƒÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹). Ğ”Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ° Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ÑƒĞ¶ĞµĞ½ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Adaptive Frame Selection for Enhanced Video Understanding",
                    "desc": "This paper introduces DIG, a novel framework for selecting video frames based on the type of query, enhancing the performance of large multimodal models (LMMs) in understanding long-form videos. It distinguishes between global queries, which can be effectively handled with uniform sampling, and localized queries, which require more targeted frame selection. By adapting its frame selection strategy according to the query type, DIG reduces computational costs while maintaining high performance. Experiments show that DIG outperforms existing methods across multiple benchmarks, demonstrating its efficiency and effectiveness in processing long videos."
                },
                "zh": {
                    "title": "DIGï¼šæ™ºèƒ½å¸§é€‰æ‹©ï¼Œæå‡é•¿è§†é¢‘ç†è§£",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDIGçš„æ¡†æ¶ï¼Œç”¨äºé•¿è§†é¢‘ç†è§£ä¸­çš„æŸ¥è¯¢ç±»å‹è‡ªé€‚åº”å¸§é€‰æ‹©ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒºåˆ†å…¨å±€æŸ¥è¯¢å’Œå±€éƒ¨æŸ¥è¯¢ï¼Œä¼˜åŒ–äº†å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå…¨å±€æŸ¥è¯¢ï¼Œå‡åŒ€é‡‡æ ·æ—¢æœ‰æ•ˆåˆé«˜æ•ˆï¼Œè€Œå±€éƒ¨æŸ¥è¯¢åˆ™éœ€è¦æŸ¥è¯¢æ„ŸçŸ¥çš„é€‰æ‹©ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDIGåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è¾“å…¥å¸§æ•°å¢åŠ åˆ°256æ—¶ä»èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03979",
            "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
            "url": "https://huggingface.co/papers/2512.03979",
            "abstract": "Blur Diffusion Model (BlurDM) integrates blur formation into diffusion for image deblurring, enhancing deblurring methods by simultaneously denoising and deblurring images.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "ec0ef44c86ed2e33",
            "authors": [
                "Jin-Ting He",
                "Fu-Jen Tsai",
                "Yan-Tsung Peng",
                "Min-Hung Chen",
                "Chia-Wen Lin",
                "Yen-Yu Lin"
            ],
            "affiliations": [
                "NVIDIA",
                "National Chengchi University",
                "National Tsing Hua University",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03979.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ«§",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ: Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Blur Diffusion Model (BlurDM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ motion blur Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑˆÑƒĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğº Ñ€ĞµĞ·ĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BlurDM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Image Deblurring with BlurDM",
                    "desc": "The Blur Diffusion Model (BlurDM) enhances image deblurring by incorporating the blur formation process into diffusion models. It recognizes that motion blur occurs due to continuous exposure and models this process through a dual-diffusion forward scheme. This allows BlurDM to simultaneously denoise and deblur images during the reverse generation process, effectively recovering sharp images from blurred inputs. Extensive experiments show that BlurDM significantly improves existing deblurring methods across multiple benchmark datasets."
                },
                "zh": {
                    "title": "æ¨¡ç³Šæ‰©æ•£æ¨¡å‹ï¼šå»æ¨¡ç³Šçš„æ–°æ–¹æ³•",
                    "desc": "æ¨¡ç³Šæ‰©æ•£æ¨¡å‹ï¼ˆBlurDMï¼‰å°†æ¨¡ç³Šå½¢æˆè¿‡ç¨‹æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å›¾åƒå»æ¨¡ç³Šçš„æ•ˆæœã€‚è¯¥æ¨¡å‹é€šè¿‡åŒé‡æ‰©æ•£å‰å‘æ–¹æ¡ˆï¼Œéšå¼åœ°æ¨¡æ‹Ÿæ¨¡ç³Šå½¢æˆè¿‡ç¨‹ï¼ŒåŒæ—¶å¯¹å›¾åƒè¿›è¡Œå»å™ªå’Œå»æ¨¡ç³Šã€‚BlurDMåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åŒé‡å»å™ªå’Œå»æ¨¡ç³Šçš„å…¬å¼ï¼Œèƒ½å¤Ÿåœ¨è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œçº¯é«˜æ–¯å™ªå£°çš„æ¡ä»¶ä¸‹æ¢å¤æ¸…æ™°å›¾åƒã€‚æ­¤å¤–ï¼ŒBlurDMåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå½¢æˆçµæ´»çš„å»æ¨¡ç³Šç”Ÿæˆç½‘ç»œï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰å»æ¨¡ç³Šæ–¹æ³•çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20494",
            "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2511.20494",
            "abstract": "The Adversarial Confusion Attack targets multimodal large language models to induce systematic disruption, leading to incoherent or confidently incorrect outputs, using a small ensemble and basic adversarial techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "69807eec8dea7f8a",
            "authors": [
                "Jakub Hoscilowicz",
                "Artur Janicki"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20494.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        }
    ],
    "link_prev": "2025-12-03.html",
    "link_next": "2025-12-05.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 1,
        "#cv": 3,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 2,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}