
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. June 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 Ğ¸ÑĞ½Ñ</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-18.html">â¬…ï¸ <span id="prev-date">18.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-20.html">â¡ï¸ <span id="next-date">20.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'};
        let feedDateNext = {'ru': '20.06', 'en': '06/20', 'zh': '6æœˆ20æ—¥'};
        let feedDatePrev = {'ru': '18.06', 'en': '06/18', 'zh': '6æœˆ18æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.15675', 'title': 'Sekai: A Video Dataset towards World Exploration', 'url': 'https://huggingface.co/papers/2506.15675', 'abstract': "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", 'score': 33, 'issue_id': 4373, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '4f989f259f55f0ee', 'authors': ['Zhen Li', 'Chuanhao Li', 'Xiaofeng Mao', 'Shaoheng Lin', 'Ming Li', 'Shitian Zhao', 'Zhaopan Xu', 'Xinyue Li', 'Yukang Feng', 'Jianwen Sun', 'Zizhen Li', 'Fanrui Zhang', 'Jiaxin Ai', 'Zhixiang Wang', 'Yuwei Wu', 'Tong He', 'Jiangmiao Pang', 'Yu Qiao', 'Yunde Jia', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen MSU-BIT University', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.15675.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#data', '#video'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Sekai: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sekai Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 ÑÑ‚Ñ€Ğ°Ğ½ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ YUME Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Sekai: Unlocking the World Through Video Generation', 'desc': 'This paper presents Sekai, a new video dataset designed to enhance video generation models for world exploration applications. Sekai includes over 5,000 hours of first-person view videos from diverse locations, addressing the limitations of existing datasets by providing rich annotations such as location, scene, and weather conditions. The authors also introduce a toolbox for efficient video collection and annotation, ensuring high-quality data for training. The dataset is validated through experiments and is used to train an interactive video exploration model called YUME, showcasing its potential impact on video generation and exploration technologies.'}, 'zh': {'title': 'Sekaiï¼šå…¨çƒæ¢ç´¢çš„æ–°è§†é‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSekaiçš„å…¨çƒè§†é¢‘æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä¸–ç•Œæ¢ç´¢åº”ç”¨å¹¶å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª100å¤šä¸ªå›½å®¶å’Œåœ°åŒºçš„5000å¤šä¸ªå°æ—¶çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ï¼Œæ¶µç›–750ä¸ªåŸå¸‚ï¼Œå…·æœ‰ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ã€‚Sekaiè§£å†³äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ•°æ®é›†åœ¨ä½ç½®ã€æ—¶é•¿ã€åœºæ™¯é™æ€æ€§å’Œæ¢ç´¢æ³¨é‡Šç­‰æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å®éªŒéªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ï¼Œå¹¶ä½¿ç”¨å…¶å­é›†è®­ç»ƒäº†ä¸€ä¸ªåä¸ºYUMEçš„äº’åŠ¨è§†é¢‘ä¸–ç•Œæ¢ç´¢æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15211', 'title': 'ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs', 'url': 'https://huggingface.co/papers/2506.15211', 'abstract': 'ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.', 'score': 20, 'issue_id': 4380, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'a2ce33b1fd959b89', 'authors': ['Feng He', 'Zijun Chen', 'Xinnian Liang', 'Tingting Ma', 'Yunqi Qiu', 'Shuangzhi Wu', 'Junchi Yan'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15211.jpg', 'data': {'categories': ['#reasoning', '#transfer_learning', '#training', '#dataset', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ProtoReasoning - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ· Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ProtoReasoning Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Generalization with Prototypical Reasoning', 'desc': 'ProtoReasoning is a framework designed to improve the reasoning capabilities of large language models (LLMs) by utilizing prototypical representations. It posits that shared abstract reasoning patterns, or prototypes, enable better generalization across different domains by simplifying complex tasks into fundamental structures. The framework includes an automated pipeline for creating these prototypes, a verification system for ensuring accuracy, and the ability to generate a wide range of problems within the prototype space. Experimental results show that ProtoReasoning significantly enhances performance in logical reasoning, planning, and general reasoning tasks compared to traditional methods.'}, 'zh': {'title': 'åŸå‹æ¨ç†ï¼šæå‡æ¨ç†æ¨¡å‹çš„è·¨é¢†åŸŸèƒ½åŠ›', 'desc': 'ProtoReasoning æ˜¯ä¸€ç§å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡åŸå‹è¡¨ç¤ºæ¥æé«˜è·¨é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å‡è®¾è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æºäºå…±äº«çš„æŠ½è±¡æ¨ç†åŸå‹ï¼Œè¿™äº›åŸå‹æ•æ‰äº†ä¸åŒé¢†åŸŸé—®é¢˜çš„æœ¬è´¨ã€‚ProtoReasoning åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŸå‹æ„å»ºç®¡é“ï¼Œå°†é—®é¢˜è½¬åŒ–ä¸ºç›¸åº”çš„åŸå‹è¡¨ç¤ºï¼Œå¹¶æä¾›å¯é çš„åé¦ˆç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProtoReasoning åœ¨é€»è¾‘æ¨ç†ã€è§„åˆ’ä»»åŠ¡å’Œä¸€èˆ¬æ¨ç†ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¨ç†åŸå‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15681', 'title': 'GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.15681', 'abstract': 'GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.', 'score': 16, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '2d531d89420a04d8', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yong Man Ro', 'Yu-Chiang Frank Wang', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15681.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#inference', '#training', '#multimodal', '#dataset', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'GenRecal: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'GenRecal - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Recalibrator Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ VLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenRecal Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ VLM. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning Features for Efficient Vision-Language Models', 'desc': 'GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks.'}, 'zh': {'title': 'GenRecalï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ¡†æ¶', 'desc': 'GenRecalæ˜¯ä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ä¸åŒæ¶æ„çš„ç‰¹å¾è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç”±äºä¸åŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„çš„å¤šæ ·æ€§è€Œå¯¼è‡´çš„çŸ¥è¯†è½¬ç§»æŒ‘æˆ˜ã€‚GenRecalå¼•å…¥äº†ä¸€ä¸ªé‡æ ¡å‡†å™¨ï¼Œèƒ½å¤Ÿåœ¨å¼‚æ„VLMä¹‹é—´å¯¹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œé€‚é…ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†ä¼ é€’ã€‚é€šè¿‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜GenRecalæ˜¾è‘—æé«˜äº†åŸºçº¿æ€§èƒ½ï¼Œæœ€ç»ˆè¶…è¶Šäº†å¤§å‹å¼€æºå’Œé—­æºçš„VLMã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15677', 'title': 'Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence', 'url': 'https://huggingface.co/papers/2506.15677', 'abstract': 'Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.', 'score': 11, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '9dff3e9d64c54e88', 'authors': ['Yining Hong', 'Rui Sun', 'Bingxuan Li', 'Xingcheng Yao', 'Maxine Wu', 'Alexander Chien', 'Da Yin', 'Ying Nian Wu', 'Zhecan James Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.15677.jpg', 'data': {'categories': ['#3d', '#benchmark', '#open_source', '#agents', '#multimodal', '#agi', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Embodied Web Agents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ²ĞµĞ±-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ€ĞµĞ´Ñ‹ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Embodied Web Agents Benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Bridging Physical and Digital Intelligence with Embodied Web Agents', 'desc': "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."}, 'zh': {'title': 'å…·èº«ç½‘ç»œä»£ç†ï¼šè¿æ¥ç‰©ç†ä¸æ•°å­—æ™ºèƒ½çš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„äººå·¥æ™ºèƒ½ä»£ç†æ¨¡å‹â€”â€”å…·èº«ç½‘ç»œä»£ç†ï¼ˆEmbodied Web Agentsï¼‰ï¼Œå®ƒå°†ç‰©ç†äº¤äº’ä¸ç½‘ç»œè§„æ¨¡æ¨ç†ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥è¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚å½“å‰çš„AIä»£ç†é€šå¸¸åªèƒ½åœ¨æ•°å­—ä¿¡æ¯æ£€ç´¢æˆ–ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­å‘æŒ¥ä½œç”¨ï¼Œç¼ºä¹ä¸¤è€…çš„æ•´åˆï¼Œé™åˆ¶äº†å®ƒä»¬è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†å…·èº«ç½‘ç»œä»£ç†ä»»åŠ¡ç¯å¢ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä»¿çœŸå¹³å°ï¼Œèƒ½å¤Ÿå°†çœŸå®çš„3Dç¯å¢ƒä¸åŠŸèƒ½æ€§ç½‘ç»œæ¥å£ç´§å¯†ç»“åˆã€‚é€šè¿‡è¿™ä¸€å¹³å°ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†å…·èº«ç½‘ç»œä»£ç†åŸºå‡†ï¼Œæ¶µç›–äº†çƒ¹é¥ªã€å¯¼èˆªã€è´­ç‰©ç­‰å¤šç§ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ç‰©ç†å’Œæ•°å­—é¢†åŸŸä¹‹é—´è¿›è¡Œåè°ƒæ¨ç†ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13414', 'title': 'BUT System for the MLC-SLM Challenge', 'url': 'https://huggingface.co/papers/2506.13414', 'abstract': "The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW -- a diarization-conditioned variant of Whisper -- with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data -- such as missing speech segments and incorrect silence annotations -- which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness.", 'score': 11, 'issue_id': 4380, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'd143e6da0f164962', 'authors': ['Alexander Polok', 'Jiangyu Han', 'Dominik Klement', 'Samuele Cornell', 'Jan ÄŒernockÃ½', 'LukÃ¡Å¡ Burget'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University, USA', 'Speech@FIT, Brno University of Technology, Czechia'], 'pdf_title_img': 'assets/pdf/title_img/2506.13414.jpg', 'data': {'categories': ['#audio', '#training', '#multilingual', '#data'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ DiCoW Ğ¸ DiariZen: Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ASR', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ DiCoW Ğ¸ DiariZen. DiCoW - ÑÑ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ° DiariZen - ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Pyannote. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ DiCoW ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° DiariZen ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Enhancing Multilingual ASR with DiCoW and DiariZen', 'desc': 'This paper presents a novel automatic speech recognition (ASR) system that integrates two components: DiCoW, which is a diarization-conditioned version of Whisper, and DiariZen, a diarization pipeline based on Pyannote. The system is evaluated in multilingual scenarios, showing that DiariZen outperforms the baseline Pyannote model without fine-tuning, indicating its strong generalization capabilities. DiCoW maintains its multilingual performance even after being fine-tuned on English-only data, demonstrating the effectiveness of its encoder modifications. The final system achieves a competitive error rate and addresses labeling inconsistencies in training data to enhance robustness and performance.'}, 'zh': {'title': 'å¤šè¯­è¨€ASRç³»ç»Ÿçš„å¼ºå¤§ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆDiCoWå’ŒDiariZençš„åŒè¯´è¯è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šè¯­è¨€åœºæ™¯ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚DiCoWæ˜¯Whisperçš„ä¸€ä¸ªåŸºäºè¯´è¯è€…åˆ†ç¦»çš„å˜ä½“ï¼Œè€ŒDiariZenåˆ™æ˜¯åŸºäºPyannoteæ„å»ºçš„è¯´è¯è€…åˆ†ç¦»ç®¡é“ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒDiariZenåœ¨å¤šè¯­è¨€åœºæ™¯ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹Pyannoteï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»è¿‡å¾®è°ƒåï¼ŒDiariZenå’ŒDiCoWçš„æ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œæœ€ç»ˆç³»ç»Ÿåœ¨MLC-SLMæŒ‘æˆ˜èµ›ä¸­å–å¾—äº†16.75%çš„å¾®å¹³å‡tcpWER/CERï¼Œæ’åç¬¬äºŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15068', 'title': 'Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation', 'url': 'https://huggingface.co/papers/2506.15068', 'abstract': 'PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.', 'score': 9, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '3f5497d8e1350326', 'authors': ['Zongxia Li', 'Yapei Chang', 'Yuhang Zhou', 'Xiyang Wu', 'Zichao Liang', 'Yoo Yeon Sung', 'Jordan Lee Boyd-Graber'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15068.jpg', 'data': {'categories': ['#long_context', '#alignment', '#rlhf', '#benchmark', '#optimization', '#open_source', '#dataset'], 'emoji': 'ğŸ“', 'ru': {'title': 'PrefBERT: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸', 'desc': 'PrefBERT - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸. PrefBERT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ›Ğ¸ĞºĞµÑ€Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ GRPO, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ROUGE-L Ğ¸ BERTScore.'}, 'en': {'title': 'Enhancing Long-Form Generation Evaluation with PrefBERT', 'desc': 'PrefBERT is a novel scoring model designed to enhance the evaluation of open-ended long-form text generation. It addresses the limitations of traditional metrics like ROUGE-L and BERTScore, which often overlook important qualities such as coherence and relevance. By providing distinct semantic rewards for good and bad outputs, PrefBERT guides the training of generative models more effectively. Comprehensive evaluations demonstrate that PrefBERT aligns closely with human preferences, leading to higher quality generated responses.'}, 'zh': {'title': 'PrefBERTï¼šæå‡å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°æ•ˆæœ', 'desc': 'PrefBERTæ˜¯ä¸€ç§è¯„åˆ†æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°ã€‚ä¸ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ç›¸æ¯”ï¼ŒPrefBERTæä¾›äº†æ›´å¥½çš„è¯­ä¹‰å¥–åŠ±åé¦ˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªå¤šæ ·åŒ–çš„é•¿æ–‡æœ¬å“åº”è¯„ä¼°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿè¯†åˆ«è¾“å‡ºçš„å¥½åï¼Œå¹¶ä¸ºå…¶æä¾›æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°ï¼ŒPrefBERTåœ¨ä¸åŒçš„é•¿æ–‡æœ¬ä¸­è¡¨ç°å‡ºå¯é æ€§ï¼Œå¹¶ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15569', 'title': 'SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification', 'url': 'https://huggingface.co/papers/2506.15569', 'abstract': "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.", 'score': 7, 'issue_id': 4371, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '0e3c39a143af668b', 'authors': ['Chengye Wang', 'Yifei Shen', 'Zexi Kuang', 'Arman Cohan', 'Yilun Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.15569.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#science', '#benchmark', '#open_source'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'SciVer: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'SciVer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 3000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 1113 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 21 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ o4-mini, Gemini-2.5-Flash Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification', 'desc': 'The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç§‘å­¦éªŒè¯èƒ½åŠ›', 'desc': 'SciVeræ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹éªŒè¯å£°æ˜çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¶µç›–1113ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œä»£è¡¨å¤šæ¨¡æ€ç§‘å­¦å£°æ˜éªŒè¯ä¸­çš„å¸¸è§æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œäººç±»é”™è¯¯è¯„ä¼°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†å½“å‰å¼€æºæ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14842', 'title': 'PictSure: Pretraining Embeddings Matters for In-Context Learning Image\n  Classifiers', 'url': 'https://huggingface.co/papers/2506.14842', 'abstract': "PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.", 'score': 5, 'issue_id': 4381, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '4cf4adfc32104e33', 'authors': ['Lukas Schiesser', 'Cornelius Wolff', 'Sophie Haas', 'Simon Pukrop'], 'affiliations': ['German Research Center for AI (DFKI)'], 'pdf_title_img': 'assets/pdf/title_img/2506.14842.jpg', 'data': {'categories': ['#dataset', '#cv', '#transfer_learning', '#optimization', '#training', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°', 'desc': 'PictSure - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. PictSure Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ.'}, 'en': {'title': 'Enhancing Few-Shot Image Classification with PictSure', 'desc': 'PictSure is a framework designed to improve few-shot image classification (FSIC) by focusing on the optimization of embedding models. It emphasizes the importance of the architecture, pretraining, and fine-tuning strategies of these models to enhance their performance, especially in out-of-domain scenarios. The research highlights that the success of training and the ability to generalize to new domains are closely linked to how well the embedding models are pretrained. By systematically analyzing various visual encoders and training methods, PictSure demonstrates superior performance compared to existing models in challenging out-of-domain benchmarks while still performing well in familiar tasks.'}, 'zh': {'title': 'PictSureï¼šæå‡å°‘æ ·æœ¬å›¾åƒåˆ†ç±»çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶', 'desc': 'PictSureæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–åµŒå…¥æ¨¡å‹çš„æ¶æ„ã€é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥æ¥å¢å¼ºå°‘æ ·æœ¬å›¾åƒåˆ†ç±»çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å…³æ³¨å›¾åƒåµŒå…¥åœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­çš„é‡è¦æ€§ï¼Œç³»ç»Ÿåœ°ç ”ç©¶ä¸åŒè§†è§‰ç¼–ç å™¨ç±»å‹ã€é¢„è®­ç»ƒç›®æ ‡å’Œå¾®è°ƒç­–ç•¥å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåµŒå…¥æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼å¯¹è®­ç»ƒæˆåŠŸå’ŒåŸŸå¤–æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ã€‚PictSureåœ¨ä¸è®­ç»ƒåˆ†å¸ƒæ˜¾è‘—ä¸åŒçš„åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„å°‘æ ·æœ¬å›¾åƒåˆ†ç±»æ¨¡å‹ï¼ŒåŒæ—¶åœ¨åŸŸå†…ä»»åŠ¡ä¸Šä¿æŒäº†ç›¸å½“çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15672', 'title': 'SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence', 'url': 'https://huggingface.co/papers/2506.15672', 'abstract': 'SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.', 'score': 4, 'issue_id': 4376, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'e6b93c3f506d4979', 'authors': ['Yao Zhang', 'Chenyang Lin', 'Shijie Tang', 'Haokun Chen', 'Shijie Zhou', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.15672.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#games', '#agi', '#open_source', '#alignment'], 'emoji': 'ğŸ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'SwarmAgentic - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. SwarmAgentic ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ½ÑƒĞ»Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ.'}, 'en': {'title': 'Revolutionizing Agentic Systems with SwarmAgentic', 'desc': 'SwarmAgentic is a novel framework designed for the automated generation of agentic systems, which are capable of decision-making and collaboration. It addresses the limitations of existing frameworks by enabling the creation of agents from scratch and optimizing their functionality and teamwork through language-driven exploration. The framework utilizes a population-based approach inspired by Particle Swarm Optimization (PSO) to evolve candidate systems based on feedback. In evaluations, SwarmAgentic demonstrated significant improvements in performance on various complex tasks, showcasing its potential for scalable and autonomous agentic system design.'}, 'zh': {'title': 'å…¨è‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿç”Ÿæˆçš„æœªæ¥', 'desc': 'SwarmAgenticæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯­è¨€é©±åŠ¨çš„æ¢ç´¢æ¥ä¼˜åŒ–ä»£ç†çš„åŠŸèƒ½å’Œåä½œã€‚ä¸ç°æœ‰çš„ä»£ç†ç³»ç»Ÿç”Ÿæˆæ¡†æ¶ç›¸æ¯”ï¼ŒSwarmAgenticèƒ½å¤Ÿä»é›¶å¼€å§‹ç”Ÿæˆä»£ç†ï¼Œå¹¶è‡ªæˆ‘ä¼˜åŒ–å…¶åŠŸèƒ½å’Œåä½œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤å€™é€‰ç³»ç»Ÿçš„ç§ç¾¤å¹¶è¿›è¡Œåé¦ˆå¼•å¯¼çš„æ›´æ–°ï¼Œå€Ÿé‰´äº†ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰çš„æ€æƒ³ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç³»ç»Ÿç»“æ„æœç´¢ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ï¼ŒSwarmAgenticè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œå±•ç¤ºäº†å…¨è‡ªåŠ¨åŒ–åœ¨æ— çº¦æŸä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15050', 'title': 'Truncated Proximal Policy Optimization', 'url': 'https://huggingface.co/papers/2506.15050', 'abstract': 'T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.', 'score': 4, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'b986b577a01d9bf5', 'authors': ['Tiantian Fan', 'Lingjun Liu', 'Yu Yue', 'Jiaze Chen', 'Chengyi Wang', 'Qiying Yu', 'Chi Zhang', 'Zhiqi Lin', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Bole Ma', 'Mofan Zhang', 'Gaohong Liu', 'Ru Zhang', 'Haotian Zhou', 'Cong Xie', 'Ruidong Zhu', 'Zhi Zhang', 'Xin Liu', 'Mingxuan Wang', 'Lin Yan', 'Yonghui Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.15050.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'T-PPO: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ² 2,5 Ñ€Ğ°Ğ·Ğ°', 'desc': 'T-PPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° PPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹. T-PPO Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° (EGAE) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Boosting Training Efficiency for Large Language Models with T-PPO', 'desc': "T-PPO is a new method that enhances the Proximal Policy Optimization (PPO) algorithm to make training Large Language Models (LLMs) faster and more efficient. It addresses the slow training times caused by PPO's on-policy nature, especially when generating long responses. By introducing Extended Generalized Advantage Estimation (EGAE), T-PPO allows for better advantage estimation from incomplete responses, which helps maintain effective policy learning. Additionally, it optimizes the use of hardware resources by reducing unnecessary computations during training, leading to a significant increase in efficiency and performance compared to existing methods."}, 'zh': {'title': 'T-PPOï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'T-PPOæ˜¯å¯¹PPOçš„ä¸€ç§æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚å®ƒé€šè¿‡ä¼˜åŒ–ç­–ç•¥æ›´æ–°å’Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚T-PPOå¼•å…¥äº†æ‰©å±•çš„å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆEGAEï¼‰ï¼Œä½¿å¾—åœ¨ä¸å®Œæ•´å“åº”çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒç­–ç•¥å­¦ä¹ çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼ŒT-PPOè¿˜é€šè¿‡ç‹¬ç«‹ä¼˜åŒ–ç­–ç•¥å’Œä»·å€¼æ¨¡å‹ï¼Œå‡å°‘å†—ä½™è®¡ç®—ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06279', 'title': 'CoMemo: LVLMs Need Image Context with Image Memory', 'url': 'https://huggingface.co/papers/2506.06279', 'abstract': "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.", 'score': 4, 'issue_id': 4370, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'e2ad205a039e250b', 'authors': ['Shi Liu', 'Weijie Su', 'Xizhou Zhu', 'Wenhai Wang', 'Jifeng Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06279.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CoMemo: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'CoMemo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. CoMemo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE-DHR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ LVLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Visual Awareness in Multimodal Processing with CoMemo', 'desc': 'CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions.'}, 'zh': {'title': 'CoMemoï¼šæå‡å¤šæ¨¡æ€å¤„ç†çš„è§†è§‰æ„è¯†', 'desc': 'CoMemoæ˜¯ä¸€ç§æ–°å‹çš„åŒè·¯å¾„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤„ç†ä¸­çš„è§†è§‰ä¿¡æ¯å¿½è§†å’Œç©ºé—´æ„è¯†é—®é¢˜ã€‚å®ƒç»“åˆäº†ä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„ï¼Œæœ‰æ•ˆç¼“è§£äº†è§†è§‰ä¿¡æ¯çš„å¿½è§†ç°è±¡ã€‚æ­¤å¤–ï¼ŒCoMemoå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä½ç½®ç¼–ç æœºåˆ¶RoPE-DHRï¼Œé€šè¿‡ç¼©ç•¥å›¾ä½ç½®èšåˆæ¥ä¿æŒäºŒç»´ç©ºé—´æ„è¯†ï¼Œå‡å°‘è¿œç¨‹è¡°å‡ã€‚é€šè¿‡åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ï¼ŒCoMemoçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14315', 'title': 'ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies', 'url': 'https://huggingface.co/papers/2506.14315', 'abstract': 'An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery. This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.', 'score': 3, 'issue_id': 4380, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '3fcf78822da5c3c6', 'authors': ['Jinyan Yuan', 'Bangbang Yang', 'Keke Wang', 'Panwang Pan', 'Lin Ma', 'Xuehai Zhang', 'Xiao Liu', 'Zhaopeng Cui', 'Yuewen Ma'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.14315.jpg', 'data': {'categories': ['#3d', '#video', '#multimodal', '#synthetic', '#games', '#agents'], 'emoji': 'ğŸ•¶ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼ Ğ² VR Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'ImmerseGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ğ½Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ RGBA, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. ImmerseGen Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ·Ğ²ÑƒĞº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing VR with Lightweight 3D Scene Generation', 'desc': 'This paper presents ImmerseGen, a new framework for creating realistic 3D scenes for virtual reality (VR) using lightweight geometric proxies. Instead of relying on complex high-poly models, ImmerseGen synthesizes photorealistic textures directly onto these proxies, allowing for real-time rendering without sacrificing visual quality. The framework utilizes agent-guided generative models to produce coherent textures that enhance the immersive experience. Additionally, it incorporates dynamic effects and audio to create a multisensory environment, demonstrating improved efficiency and realism compared to traditional methods.'}, 'zh': {'title': 'è½»é‡çº§å‡ ä½•ä»£ç†ï¼Œå®æ—¶ç”Ÿæˆé€¼çœŸ3Dåœºæ™¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºImmerseGençš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„3Dåœºæ™¯ï¼Œä»¥å¢å¼ºè™šæ‹Ÿç°å®ä½“éªŒã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è½»é‡å‡ ä½•ä»£ç†ä¸åˆæˆçš„RGBAçº¹ç†ç»“åˆï¼Œç®€åŒ–äº†å»ºæ¨¡è¿‡ç¨‹ï¼Œå¹¶å®ç°äº†å®æ—¶æ¸²æŸ“ã€‚ImmerseGenåˆ©ç”¨ä»£ç†å¼•å¯¼ç”Ÿæˆæ¨¡å‹ï¼Œç¡®ä¿çº¹ç†ä¸åœºæ™¯çš„æ— ç¼èåˆï¼Œä»è€Œæé«˜è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒImmerseGenåœ¨å…‰ç…§çœŸå®æ„Ÿã€ç©ºé—´ä¸€è‡´æ€§å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14824', 'title': 'FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.14824', 'abstract': 'FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.', 'score': 3, 'issue_id': 4376, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'c694f22e30433fa6', 'authors': ['Yao Zhang', 'Hewei Gao', 'Haokun Chen', 'Weiguo Li', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Siemens Technology', 'Technical University of Munich', 'University Heidelberg'], 'pdf_title_img': 'assets/pdf/title_img/2506.14824.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#agents', '#scalability', '#privacy', '#training', '#federated_learning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'FedNano - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€Ğµ, Ğ° Ğ½Ğ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğµ NanoEdge Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. FedNano Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Scalable AI with FedNano: Federated Learning for Large Language Models', 'desc': 'FedNano is a federated learning framework designed to enhance the deployment of large language models (LLMs) while addressing privacy and scalability challenges. It centralizes the LLM on servers and utilizes lightweight NanoEdge modules for client-specific adaptations, significantly reducing the need for client-side resources. By employing low-rank adaptation techniques, FedNano minimizes communication costs and storage requirements, allowing clients to only transmit compact updates instead of full model parameters. This innovative approach enables effective collaboration across heterogeneous client data while maintaining privacy, ultimately making multimodal AI systems more scalable and feasible.'}, 'zh': {'title': 'FedNanoï¼šè§£å†³å¤šæ¨¡æ€å­¦ä¹ çš„éšç§ä¸å¯æ‰©å±•æ€§é—®é¢˜', 'desc': 'FedNanoæ˜¯ä¸€ä¸ªè”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹é›†ä¸­åœ¨æœåŠ¡å™¨ä¸Šï¼Œå¹¶ä½¿ç”¨NanoEdgeæ¨¡å—è¿›è¡Œå®¢æˆ·ç«¯ç‰¹å®šçš„é€‚åº”ï¼Œä»è€Œè§£å†³äº†å¯æ‰©å±•æ€§å’Œéšç§é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å…è®¸åœ¨ä¸é›†ä¸­æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåä½œæ¨¡å‹è®­ç»ƒï¼Œå…‹æœäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚NanoEdgeæ¨¡å—é‡‡ç”¨ç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨å’Œå¯è®­ç»ƒçš„NanoAdaptersï¼Œæ˜¾è‘—å‡å°‘äº†å®¢æˆ·ç«¯çš„å­˜å‚¨éœ€æ±‚å’Œé€šä¿¡å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedNanoåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„è”é‚¦å­¦ä¹ åŸºçº¿ï¼Œä¿ƒè¿›äº†å¯æ‰©å±•çš„å»ä¸­å¿ƒåŒ–å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14866', 'title': 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents', 'url': 'https://huggingface.co/papers/2506.14866', 'abstract': 'A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.', 'score': 2, 'issue_id': 4378, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '4a30ee5bf7d4f9f6', 'authors': ['Thomas Kuntz', 'Agatha Duzan', 'Hao Zhao', 'Francesco Croce', 'Zico Kolter', 'Nicolas Flammarion', 'Maksym Andriushchenko'], 'affiliations': ['Carnegie Mellon University', 'EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2506.14866.jpg', 'data': {'categories': ['#security', '#ethics', '#agents', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OS-Harm: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OS-Harm Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°. ĞĞ½ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. OS-Harm Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞĞ¡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Ensuring Safe Interactions: Introducing OS-Harm for Computer Use Agents', 'desc': "The paper introduces OS-Harm, a benchmark designed to assess the safety of computer use agents that interact with graphical user interfaces (GUIs). It evaluates these agents for their vulnerability to misuse, prompt injection attacks, and other forms of misbehavior across various applications. The benchmark includes 150 tasks that simulate different safety violations, such as harassment and data exfiltration, while requiring agents to operate within common OS applications. An automated judging system is proposed to evaluate the agents' performance, achieving a high agreement with human assessments, revealing that many models are prone to compliance with misuse and unsafe actions."}, 'zh': {'title': 'OS-Harmï¼šè¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å®‰å…¨æ–°åŸºå‡†', 'desc': 'OS-Harmæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢äº¤äº’æ—¶çš„å®‰å…¨æ€§ã€‚å®ƒä¸»è¦æµ‹è¯•ä»£ç†åœ¨ç”¨æˆ·æ¶æ„ä½¿ç”¨ã€æç¤ºæ³¨å…¥æ”»å‡»å’Œæ¨¡å‹ä¸å½“è¡Œä¸ºç­‰æ–¹é¢çš„è„†å¼±æ€§ã€‚è¯¥åŸºå‡†åˆ›å»ºäº†150ä¸ªä»»åŠ¡ï¼Œæ¶µç›–å¤šç§å®‰å…¨è¿è§„è¡Œä¸ºï¼Œå¹¶è¦æ±‚ä»£ç†ä¸ä¸åŒçš„æ“ä½œç³»ç»Ÿåº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼ŒOS-Harmèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°ä»£ç†çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å®‰å…¨æ€§æä¾›äº†é‡è¦çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14435', 'title': 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models', 'url': 'https://huggingface.co/papers/2506.14435', 'abstract': 'MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.', 'score': 2, 'issue_id': 4371, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '2f5b5f7f3c20ae10', 'authors': ['Hongyu Wang', 'Jiayu Xu', 'Ruiping Wang', 'Yan Feng', 'Yitao Zhai', 'Peng Pei', 'Xunliang Cai', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'Meituan University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.14435.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoTE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoTE - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MoTE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ÑƒÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ MoE-LLaVA Ğ½Ğ° 4.3% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'MoTE: Efficient Ternary Experts for Scalable Edge Deployment', 'desc': 'The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.'}, 'zh': {'title': 'MoTEï¼šå†…å­˜é«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹', 'desc': 'MoTEæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å†…å­˜é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨ä½ç²¾åº¦çš„ä¸‰å…ƒä¸“å®¶æ¥æå‡æ€§èƒ½å¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨å…¨ç²¾åº¦ä¸“å®¶çš„ç¨€ç–ä¸Šå‡æ–¹æ³•ä¸åŒï¼ŒMoTEé€šè¿‡è®­ç»ƒæ›´å¤šä½ç²¾åº¦ä¸“å®¶æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å‰é¦ˆç½‘ç»œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå¹¶è®­ç»ƒå‚æ•°ä¸º{-1, 0, 1}çš„ä¸‰å…ƒè·¯ç”±ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTEåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11110', 'title': 'AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.11110', 'abstract': 'AssertBench evaluates Large Language Models\' ability to maintain consistent truth evaluation when faced with contradictory user assertions about factually true statements by analyzing framing-induced variability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model\'s agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model\'s underlying factual knowledge by stratifying results based on the model\'s accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM\'s ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.', 'score': 2, 'issue_id': 4386, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': 'd72eb1f8052f802e', 'authors': ['Jaeho Lee', 'Atharv Chowdhary'], 'affiliations': ['Brown University, Providence, RI 02912'], 'pdf_title_img': 'assets/pdf/title_img/2506.11110.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ¾Ğ¹ĞºĞ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¿ĞµÑ€ĞµĞ´ Ğ»Ğ¸Ñ†Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': "AssertBench - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° FEVEROUS. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ¸: Ğ¾Ğ´Ğ¸Ğ½, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ½Ğ¾, Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹, Ğ³Ğ´Ğµ Ğ¾Ğ½Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¼. AssertBench Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM 'Ğ¾Ñ‚ÑÑ‚Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ' Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¾Ğ± Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ñ„Ğ°ĞºÑ‚Ğµ."}, 'en': {'title': 'Testing LLMs: Can They Stick to the Truth?', 'desc': "AssertBench is a benchmark designed to evaluate how well Large Language Models (LLMs) maintain consistent truth evaluations when faced with contradictory assertions about factual statements. It investigates the impact of framing on model responses by presenting two different prompts for the same fact: one affirming its truth and another denying it. The goal is to see if the model can uphold its factual accuracy without being swayed by the user's framing. By isolating the effects of framing, AssertBench helps assess the robustness of LLMs in their reasoning and agreement with factual information."}, 'zh': {'title': 'åšæŒçœŸç›¸ï¼ŒæŠµå¾¡çŸ›ç›¾ä¸»å¼ çš„æŒ‘æˆ˜', 'desc': 'AssertBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹ç”¨æˆ·å…³äºäº‹å®çœŸç›¸çš„çŸ›ç›¾ä¸»å¼ æ—¶ï¼Œä¿æŒä¸€è‡´æ€§çœŸç›¸è¯„ä¼°èƒ½åŠ›çš„å·¥å…·ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ†ææ¡†æ¶å¼•èµ·çš„å˜å¼‚æ€§ï¼Œæ¢è®¨äº†äº‹å®æ€§é™ˆè¿°çš„æ–¹å‘æ€§æ¡†æ¶å¦‚ä½•å½±å“æ¨¡å‹çš„åŒæ„ç¨‹åº¦ã€‚ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ª FEVEROUS çš„è¯æ®æ”¯æŒäº‹å®ï¼Œå¹¶ä¸ºæ¯ä¸ªäº‹å®æ„å»ºäº†ä¸¤ç§æ¡†æ¶æç¤ºï¼Œè®°å½•æ¨¡å‹çš„åŒæ„å’Œæ¨ç†è¿‡ç¨‹ã€‚ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨é¢å¯¹çŸ›ç›¾çš„ç”¨æˆ·ä¸»å¼ æ—¶ï¼Œèƒ½å¤ŸåšæŒè‡ªå·±çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯éšç”¨æˆ·çš„æ„è§æ”¹å˜è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15461', 'title': 'All is Not Lost: LLM Recovery without Checkpoints', 'url': 'https://huggingface.co/papers/2506.15461', 'abstract': "A novel method, CheckFree, and its extended version CheckFree+, efficiently recover from node failures during LLM training by substituting failed stages with averaged neighboring stages or through out-of-order pipeline execution, improving convergence time over existing checkpointing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.", 'score': 1, 'issue_id': 4388, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'f6a5c394da44e46e', 'authors': ['Nikolay Blagoev', 'OÄŸuzhan Ersoy', 'Lydia Yiyu Chen'], 'affiliations': ['Gensyn', 'TU Delft', 'University of Neuchatel'], 'pdf_title_img': 'assets/pdf/title_img/2506.15461.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'CheckFree Ğ¸ CheckFree+ - Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ±Ğ¾ĞµĞ² ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½ĞµĞ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ½ĞµĞ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². CheckFree Ğ¸ CheckFree+ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ñ… ÑĞ±Ğ¾ĞµĞ².'}, 'en': {'title': 'Efficient Recovery in LLM Training with CheckFree', 'desc': 'The paper introduces CheckFree, a novel method for recovering from node failures during the training of large language models (LLMs). Instead of traditional checkpointing or redundant computation, CheckFree substitutes failed stages with averaged outputs from neighboring stages, significantly reducing overhead. The extended version, CheckFree+, enhances this by allowing out-of-order execution, enabling recovery of initial and final stages by mimicking their behavior through neighboring stages. Evaluations show that both methods improve convergence time by over 12% compared to existing techniques, especially under low to medium failure rates.'}, 'zh': {'title': 'é«˜æ•ˆæ¢å¤ï¼šCheckFreeä¸CheckFree+çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•CheckFreeåŠå…¶æ‰©å±•ç‰ˆæœ¬CheckFree+ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„èŠ‚ç‚¹æ•…éšœæ¢å¤æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”¨ç›¸é‚»é˜¶æ®µçš„åŠ æƒå¹³å‡æ›¿ä»£å¤±è´¥çš„é˜¶æ®µï¼Œæˆ–é€šè¿‡æ— åºæµæ°´çº¿æ‰§è¡Œæ¥æ”¹å–„æ”¶æ•›æ—¶é—´ï¼Œä¼˜äºä¼ ç»Ÿçš„æ£€æŸ¥ç‚¹æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCheckFreeä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆ–å­˜å‚¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ä¸­é—´é˜¶æ®µçš„æ•…éšœã€‚æ‰©å±•ç‰ˆæœ¬CheckFree+åˆ™é€šè¿‡æ¨¡ä»¿ç›¸é‚»é˜¶æ®µçš„è¡Œä¸ºï¼Œè¿›ä¸€æ­¥å®¹å¿é¦–å°¾é˜¶æ®µçš„å´©æºƒï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹è®­ç»ƒçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14770', 'title': 'GMT: General Motion Tracking for Humanoid Whole-Body Control', 'url': 'https://huggingface.co/papers/2506.14770', 'abstract': "GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.", 'score': 1, 'issue_id': 4383, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '1ed4993d05c76b95', 'authors': ['Zixuan Chen', 'Mazeyu Ji', 'Xuxin Cheng', 'Xuanbin Peng', 'Xue Bin Peng', 'Xiaolong Wang'], 'affiliations': ['Simon Fraser University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.14770.jpg', 'data': {'categories': ['#robotics', '#architecture', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'GMT: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'GMT - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. GMT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unified Motion Tracking for Humanoid Robots with GMT', 'desc': 'The paper presents GMT, a unified motion-tracking framework designed for humanoid robots to effectively track a wide range of motions. It tackles challenges such as the diversity of motion types and the coordination between upper and lower body movements. GMT employs an Adaptive Sampling strategy to optimize training by balancing the complexity of different motions, and a Motion Mixture-of-Experts architecture to enhance specialization in motion tracking. Experimental results demonstrate that GMT achieves state-of-the-art performance in both simulated and real-world environments, showcasing its versatility and effectiveness.'}, 'zh': {'title': 'ç»Ÿä¸€è¿åŠ¨è·Ÿè¸ªï¼Œçªç ´äººå½¢æœºå™¨äººæŒ‘æˆ˜', 'desc': 'GMTæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è·Ÿè¸ªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ ·åŒ–äººå½¢æœºå™¨äººè¿åŠ¨è·Ÿè¸ªä¸­çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”é‡‡æ ·å’Œè¿åŠ¨ä¸“å®¶æ··åˆæ¶æ„æ¥å®ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è·Ÿè¸ªå„ç§å¤æ‚çš„å…¨èº«è¿åŠ¨ã€‚è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å¹³è¡¡ç®€å•å’Œå›°éš¾çš„è¿åŠ¨ï¼Œè€Œè¿åŠ¨ä¸“å®¶æ··åˆæ¶æ„åˆ™ç¡®ä¿äº†è¿åŠ¨æµå½¢ä¸åŒåŒºåŸŸçš„æ›´å¥½ä¸“ä¸šåŒ–ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒGMTåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä¸­éƒ½å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15682', 'title': 'Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model', 'url': 'https://huggingface.co/papers/2506.15682', 'abstract': "ECAD, a genetic algorithm, optimizes caching schedules for diffusion models, enhancing inference speed while maintaining quality across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.", 'score': 0, 'issue_id': 4386, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'c624b4c5b312499d', 'authors': ['Anirud Aggarwal', 'Abhinav Shrivastava', 'Matthew Gwilliam'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15682.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#diffusion'], 'emoji': 'ğŸš€', 'ru': {'title': 'ECAD: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ECAD - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ECAD Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ñ€Ğ¾Ğ½Ñ‚ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Accelerating Diffusion Models with ECAD: Smart Caching for Speed and Quality', 'desc': 'The paper introduces ECAD, a genetic algorithm designed to optimize caching schedules for diffusion models, which improves inference speed without sacrificing quality. Traditional methods for caching features in diffusion transformers often rely on fixed rules, limiting their effectiveness. ECAD learns adaptive caching strategies tailored to specific models, allowing for better performance across different architectures. The results show that ECAD significantly enhances inference speed and maintains high quality, outperforming previous methods on various benchmarks.'}, 'zh': {'title': 'ECADï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ™ºèƒ½ç¼“å­˜è°ƒåº¦', 'desc': 'ECADæ˜¯ä¸€ç§é—ä¼ ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„ç¼“å­˜è°ƒåº¦ï¼Œä»è€Œæé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„è´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šçš„å¯å‘å¼è§„åˆ™ï¼Œå¯¼è‡´åŠ é€Ÿæ•ˆæœæœ‰é™æˆ–åœ¨ä¸åŒæ¶æ„é—´æ³›åŒ–èƒ½åŠ›å·®ã€‚ECADé€šè¿‡å­¦ä¹ æ¯ä¸ªæ¨¡å‹çš„é«˜æ•ˆç¼“å­˜è°ƒåº¦ï¼Œå½¢æˆå¸•ç´¯æ‰˜å‰æ²¿ï¼Œä¸”åªéœ€å°‘é‡æ ¡å‡†æç¤ºï¼Œæ— éœ€ä¿®æ”¹ç½‘ç»œå‚æ•°æˆ–å‚è€ƒå›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒECADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œå¹¶èƒ½æœ‰æ•ˆé€‚åº”ä¸åŒçš„æ‰©æ•£æ¨¡å‹ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (2)', '#alignment (3)', '#architecture (5)', '#audio (1)', '#benchmark (9)', '#cv (1)', '#data (2)', '#dataset (6)', '#diffusion (1)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (6)', '#open_source (4)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (1)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (8)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-06-19 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-19 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-19 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    