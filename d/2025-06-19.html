
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. June 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 июня</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-18.html">⬅️ <span id="prev-date">18.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-20.html">➡️ <span id="next-date">20.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'};
        let feedDateNext = {'ru': '20.06', 'en': '06/20', 'zh': '6月20日'};
        let feedDatePrev = {'ru': '18.06', 'en': '06/18', 'zh': '6月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.15675', 'title': 'Sekai: A Video Dataset towards World Exploration', 'url': 'https://huggingface.co/papers/2506.15675', 'abstract': "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", 'score': 22, 'issue_id': 4373, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '4f989f259f55f0ee', 'authors': ['Zhen Li', 'Chuanhao Li', 'Xiaofeng Mao', 'Shaoheng Lin', 'Ming Li', 'Shitian Zhao', 'Zhaopan Xu', 'Xinyue Li', 'Yukang Feng', 'Jianwen Sun', 'Zizhen Li', 'Fanrui Zhang', 'Jiaxin Ai', 'Zhixiang Wang', 'Yuwei Wu', 'Tong He', 'Jiangmiao Pang', 'Yu Qiao', 'Yunde Jia', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen MSU-BIT University', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.15675.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#data', '#video'], 'emoji': '🌎', 'ru': {'title': 'Sekai: глобальный датасет для обучения ИИ исследовать мир', 'desc': 'Представлен новый набор данных Sekai для обучения моделей генерации видео с целью исследования мира. Он содержит более 5000 часов видео от первого лица из более чем 100 стран с богатыми аннотациями. Разработан инструментарий для сбора, обработки и аннотирования видео. На основе подмножества данных обучена модель YUME для интерактивного исследования мира через видео.'}, 'en': {'title': 'Sekai: Unlocking the World Through Video Generation', 'desc': 'This paper presents Sekai, a new video dataset designed to enhance video generation models for world exploration applications. Sekai includes over 5,000 hours of first-person view videos from diverse locations, addressing the limitations of existing datasets by providing rich annotations such as location, scene, and weather conditions. The authors also introduce a toolbox for efficient video collection and annotation, ensuring high-quality data for training. The dataset is validated through experiments and is used to train an interactive video exploration model called YUME, showcasing its potential impact on video generation and exploration technologies.'}, 'zh': {'title': 'Sekai：全球探索的新视野', 'desc': '本文介绍了一个名为Sekai的全球视频数据集，旨在支持世界探索应用并增强视频生成模型。该数据集包含来自100多个国家和地区的5000多个小时的第一人称视角视频，涵盖750个城市，具有丰富的注释信息。Sekai解决了现有视频生成数据集在位置、时长、场景静态性和探索注释等方面的局限性。通过实验验证了数据集的质量，并使用其子集训练了一个名为YUME的互动视频世界探索模型。'}}}, {'id': 'https://huggingface.co/papers/2506.15681', 'title': 'GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.15681', 'abstract': 'GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.', 'score': 9, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '2d531d89420a04d8', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yong Man Ro', 'Yu-Chiang Frank Wang', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15681.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#inference', '#training', '#multimodal', '#dataset', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'GenRecal: Универсальная дистилляция для эффективных визуально-языковых моделей', 'desc': 'GenRecal - это новая система дистилляции для визуально-языковых моделей (VLM), которая улучшает передачу знаний между разнородными архитектурами. Она использует Recalibrator для выравнивания и адаптации представлений признаков между различными типами VLM. Эксперименты показывают, что GenRecal значительно улучшает базовые показатели и превосходит крупномасштабные открытые и закрытые VLM. Это позволяет создавать более эффективные и компактные модели для применения в реальных сценариях с ограниченными ресурсами.'}, 'en': {'title': 'Aligning Features for Efficient Vision-Language Models', 'desc': 'GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks.'}, 'zh': {'title': 'GenRecal：提升视觉-语言模型性能的新框架', 'desc': 'GenRecal是一种新颖的知识蒸馏框架，旨在通过对不同架构的特征表示进行对齐，提升视觉-语言模型的性能。该框架解决了由于不同视觉-语言模型（VLM）架构的多样性而导致的知识转移挑战。GenRecal引入了一个重校准器，能够在异构VLM之间对特征表示进行适配，从而实现有效的知识传递。通过在多个具有挑战性的基准测试上进行广泛实验，我们证明GenRecal显著提高了基线性能，最终超越了大型开源和闭源的VLM。'}}}, {'id': 'https://huggingface.co/papers/2506.15677', 'title': 'Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence', 'url': 'https://huggingface.co/papers/2506.15677', 'abstract': 'Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.', 'score': 6, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '9dff3e9d64c54e88', 'authors': ['Yining Hong', 'Rui Sun', 'Bingxuan Li', 'Xingcheng Yao', 'Maxine Wu', 'Alexander Chien', 'Da Yin', 'Ying Nian Wu', 'Zhecan James Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.15677.jpg', 'data': {'categories': ['#3d', '#benchmark', '#open_source', '#agents', '#multimodal', '#agi', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Объединение физического и цифрового миров в ИИ-агентах нового поколения', 'desc': 'Статья представляет новую парадигму искусственного интеллекта - Embodied Web Agents, которая объединяет физическое взаимодействие и веб-масштабные рассуждения. Авторы разработали симуляционную платформу, интегрирующую реалистичные 3D-среды с функциональными веб-интерфейсами. На основе этой платформы создан набор тестовых заданий Embodied Web Agents Benchmark для оценки кросс-доменного интеллекта ИИ-систем. Результаты экспериментов показывают значительный разрыв между возможностями современных ИИ-систем и человека в задачах, требующих комбинированного физического и цифрового интеллекта.'}, 'en': {'title': 'Bridging Physical and Digital Intelligence with Embodied Web Agents', 'desc': "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."}, 'zh': {'title': '具身网络代理：连接物理与数字智能的桥梁', 'desc': '本文介绍了一种新的人工智能代理模型——具身网络代理（Embodied Web Agents），它将物理交互与网络规模推理结合在一起，以评估跨领域智能。当前的AI代理通常只能在数字信息检索或物理世界交互中发挥作用，缺乏两者的整合，限制了它们解决复杂任务的能力。我们开发了具身网络代理任务环境，这是一个统一的仿真平台，能够将真实的3D环境与功能性网络接口紧密结合。通过这一平台，我们构建并发布了具身网络代理基准，涵盖了烹饪、导航、购物等多种任务，要求在物理和数字领域之间进行协调推理，以系统评估跨领域智能。'}}}, {'id': 'https://huggingface.co/papers/2506.15068', 'title': 'Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation', 'url': 'https://huggingface.co/papers/2506.15068', 'abstract': 'PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.', 'score': 5, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '3f5497d8e1350326', 'authors': ['Zongxia Li', 'Yapei Chang', 'Yuhang Zhou', 'Xiyang Wu', 'Zichao Liang', 'Yoo Yeon Sung', 'Jordan Lee Boyd-Graber'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15068.jpg', 'data': {'categories': ['#long_context', '#alignment', '#rlhf', '#benchmark', '#optimization', '#open_source', '#dataset'], 'emoji': '📝', 'ru': {'title': 'PrefBERT: Улучшение генерации длинных текстов с помощью семантической оценки', 'desc': 'PrefBERT - это модель оценки, разработанная для улучшения генерации длинных текстов с открытым окончанием. Она обеспечивает более качественную семантическую обратную связь по сравнению с традиционными метриками. PrefBERT обучен на двух наборах данных для оценки ответов с различными длинными стилями и оценками качества по шкале Ликерта. Модель эффективно поддерживает GRPO, предоставляя лучшую семантическую обратную связь, чем традиционные метрики ROUGE-L и BERTScore.'}, 'en': {'title': 'Enhancing Long-Form Generation Evaluation with PrefBERT', 'desc': 'PrefBERT is a novel scoring model designed to enhance the evaluation of open-ended long-form text generation. It addresses the limitations of traditional metrics like ROUGE-L and BERTScore, which often overlook important qualities such as coherence and relevance. By providing distinct semantic rewards for good and bad outputs, PrefBERT guides the training of generative models more effectively. Comprehensive evaluations demonstrate that PrefBERT aligns closely with human preferences, leading to higher quality generated responses.'}, 'zh': {'title': 'PrefBERT：提升开放式长文本生成的评估效果', 'desc': 'PrefBERT是一种评分模型，旨在改善开放式长文本生成的评估。与传统的评估指标相比，PrefBERT提供了更好的语义奖励反馈，能够更有效地指导生成模型的训练。该模型在两个多样化的长文本响应评估数据集上进行训练，能够识别输出的好坏，并为其提供明确的奖励信号。通过综合评估，PrefBERT在不同的长文本中表现出可靠性，并与人类偏好高度一致。'}}}, {'id': 'https://huggingface.co/papers/2506.15569', 'title': 'SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification', 'url': 'https://huggingface.co/papers/2506.15569', 'abstract': "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.", 'score': 4, 'issue_id': 4371, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '0e3c39a143af668b', 'authors': ['Chengye Wang', 'Yifei Shen', 'Zexi Kuang', 'Arman Cohan', 'Yilun Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.15569.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#science', '#benchmark', '#open_source'], 'emoji': '🧪', 'ru': {'title': 'SciVer: проверка научных утверждений мультимодальными ИИ-моделями', 'desc': 'SciVer - это новый бенчмарк для оценки способности мультимодальных фундаментальных моделей верифицировать научные утверждения. Он состоит из 3000 аннотированных экспертами примеров из 1113 научных статей, охватывающих четыре типа рассуждений. Авторы протестировали 21 современную мультимодальную модель, включая o4-mini, Gemini-2.5-Flash и другие. Результаты показали значительный разрыв в производительности между моделями и экспертами-людьми, выявив ключевые ограничения в понимании и рассуждениях моделей в задачах с мультимодальной научной литературой.'}, 'en': {'title': 'Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification', 'desc': 'The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.'}, 'zh': {'title': '提升多模态模型的科学验证能力', 'desc': 'SciVer是一个新的基准，专门用于评估多模态基础模型在科学背景下验证声明的能力。它包含3000个专家注释的示例，涵盖1113篇科学论文，分为四个子集，代表多模态科学声明验证中的常见推理类型。我们评估了21个最先进的多模态基础模型的表现，发现这些模型与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）和人类错误评估的深入分析，我们识别了当前开源模型的关键局限性，为提高模型在多模态科学文献任务中的理解和推理能力提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2506.06279', 'title': 'CoMemo: LVLMs Need Image Context with Image Memory', 'url': 'https://huggingface.co/papers/2506.06279', 'abstract': "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.", 'score': 3, 'issue_id': 4370, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'e2ad205a039e250b', 'authors': ['Shi Liu', 'Weijie Su', 'Xizhou Zhu', 'Wenhai Wang', 'Jifeng Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06279.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'CoMemo: Улучшение визуального восприятия в мультимодальных моделях', 'desc': 'CoMemo - это новая архитектура для мультимодальной обработки, решающая проблемы пренебрежения визуальной информацией и пространственной осведомленности. Она использует двухпутевую архитектуру, сочетающую контекстный путь изображения с путем памяти изображения. CoMemo также вводит новый механизм позиционного кодирования RoPE-DHR, основанный на агрегации миниатюр. Эта модель превосходит традиционные архитектуры LVLM в задачах понимания длинного контекста, рассуждения по нескольким изображениям и визуальных вопросов-ответов.'}, 'en': {'title': 'Enhancing Visual Awareness in Multimodal Processing with CoMemo', 'desc': 'CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions.'}, 'zh': {'title': 'CoMemo：提升多模态处理的视觉意识', 'desc': 'CoMemo是一种新型的双路径架构，旨在解决多模态处理中的视觉信息忽视和空间意识问题。它结合了上下文图像路径和图像记忆路径，有效缓解了视觉信息的忽视现象。此外，CoMemo引入了一种新颖的位置编码机制RoPE-DHR，通过缩略图位置聚合来保持二维空间意识，减少远程衰减。通过在七个基准测试中的评估，CoMemo的表现优于传统的大型视觉语言模型架构。'}}}, {'id': 'https://huggingface.co/papers/2506.15050', 'title': 'Truncated Proximal Policy Optimization', 'url': 'https://huggingface.co/papers/2506.15050', 'abstract': 'T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.', 'score': 2, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'b986b577a01d9bf5', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🚀', 'ru': {'title': 'T-PPO: Ускоряем обучение LLM в 2,5 раза', 'desc': 'T-PPO - это новое расширение алгоритма PPO, которое повышает эффективность обучения больших языковых моделей (LLM). Оно оптимизирует обновление политики и более эффективно использует аппаратные ресурсы. T-PPO вводит расширенную обобщенную оценку преимущества (EGAE) для оценки преимущества на основе неполных ответов. Также предлагается механизм, позволяющий независимо оптимизировать модели политики и ценности, что ускоряет процесс обучения.'}, 'en': {'title': 'Boosting Training Efficiency for Large Language Models with T-PPO', 'desc': "T-PPO is a new method that enhances the Proximal Policy Optimization (PPO) algorithm to make training Large Language Models (LLMs) faster and more efficient. It addresses the slow training times caused by PPO's on-policy nature, especially when generating long responses. By introducing Extended Generalized Advantage Estimation (EGAE), T-PPO allows for better advantage estimation from incomplete responses, which helps maintain effective policy learning. Additionally, it optimizes the use of hardware resources by reducing unnecessary computations during training, leading to a significant increase in efficiency and performance compared to existing methods."}, 'zh': {'title': 'T-PPO：提升大型语言模型训练效率的创新方案', 'desc': 'T-PPO是对PPO的一种扩展，旨在提高大型语言模型的训练效率。它通过优化策略更新和更有效地利用硬件资源来实现这一目标。T-PPO引入了扩展的广义优势估计（EGAE），使得在不完整响应的情况下仍能保持策略学习的完整性。此外，T-PPO还通过独立优化策略和价值模型，减少冗余计算，加速训练过程。'}}}, {'id': 'https://huggingface.co/papers/2506.15672', 'title': 'SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence', 'url': 'https://huggingface.co/papers/2506.15672', 'abstract': 'SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.', 'score': 1, 'issue_id': 4376, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'e6b93c3f506d4979', 'authors': ['Yao Zhang', 'Chenyang Lin', 'Shijie Tang', 'Haokun Chen', 'Shijie Zhou', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.15672.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#games', '#agi', '#open_source', '#alignment'], 'emoji': '🐝', 'ru': {'title': 'Автономная эволюция многоагентных систем', 'desc': 'SwarmAgentic - это фреймворк для полностью автоматизированной генерации агентных систем. Он оптимизирует функциональность агентов и их взаимодействие через языковое исследование, превосходя существующие базовые модели в неограниченных задачах. SwarmAgentic создает агентные системы с нуля и совместно оптимизирует функциональность агентов и их сотрудничество как взаимозависимые компоненты. Фреймворк использует принципы роевого интеллекта для эффективного поиска оптимальных структур системного уровня.'}, 'en': {'title': 'Revolutionizing Agentic Systems with SwarmAgentic', 'desc': 'SwarmAgentic is a novel framework designed for the automated generation of agentic systems, which are capable of decision-making and collaboration. It addresses the limitations of existing frameworks by enabling the creation of agents from scratch and optimizing their functionality and teamwork through language-driven exploration. The framework utilizes a population-based approach inspired by Particle Swarm Optimization (PSO) to evolve candidate systems based on feedback. In evaluations, SwarmAgentic demonstrated significant improvements in performance on various complex tasks, showcasing its potential for scalable and autonomous agentic system design.'}, 'zh': {'title': '全自动化代理系统生成的未来', 'desc': 'SwarmAgentic是一个自动化代理系统生成框架，旨在通过语言驱动的探索来优化代理的功能和协作。与现有的代理系统生成框架相比，SwarmAgentic能够从零开始生成代理，并自我优化其功能和协作能力。该框架通过维护候选系统的种群并进行反馈引导的更新，借鉴了粒子群优化（PSO）的思想，从而实现高效的系统结构搜索。在多个真实世界的任务中，SwarmAgentic表现出色，显著超越了现有基准，展示了全自动化在无约束任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.14435', 'title': 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models', 'url': 'https://huggingface.co/papers/2506.14435', 'abstract': 'MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.', 'score': 1, 'issue_id': 4371, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '2f5b5f7f3c20ae10', 'authors': ['Hongyu Wang', 'Jiayu Xu', 'Ruiping Wang', 'Yan Feng', 'Yitao Zhai', 'Peng Pei', 'Xunliang Cai', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'Meituan University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.14435.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'MoTE: Эффективные Mixture-of-Experts модели для устройств с ограниченной памятью', 'desc': 'Статья представляет MoTE - метод для улучшения моделей Mixture-of-Experts с использованием тернарных экспертов низкой точности. Этот подход позволяет масштабировать размер модели, повышая производительность при сохранении фиксированных активных параметров. MoTE демонстрирует многообещающую тенденцию масштабирования и достигает сопоставимой производительности с полноточными базовыми MoE-моделями при меньшем объеме памяти. Метод особенно эффективен для устройств с ограниченной памятью, превосходя MoE-LLaVA на 4.3% по средней точности при том же объеме памяти экспертов.'}, 'en': {'title': 'MoTE: Efficient Ternary Experts for Scalable Edge Deployment', 'desc': 'The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.'}, 'zh': {'title': 'MoTE：内存高效的混合专家模型', 'desc': 'MoTE是一种可扩展且内存高效的方法，旨在改进混合专家模型，使用低精度的三元专家来提升性能并减少内存占用，适合在边缘设备上部署。与以往主要使用全精度专家的稀疏上升方法不同，MoTE通过训练更多低精度专家来实现更好的性能。具体来说，我们使用预训练的前馈网络作为共享专家，并训练参数为{-1, 0, 1}的三元路由专家。实验结果表明，MoTE在模型规模上具有良好的扩展趋势，并在内存受限的情况下表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.14824', 'title': 'FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.14824', 'abstract': 'FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.', 'score': 0, 'issue_id': 4376, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'c694f22e30433fa6', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#multimodal', '#agents', '#scalability', '#privacy', '#training', '#federated_learning'], 'emoji': '🔬', 'ru': {'title': 'Эффективное федеративное обучение мультимодальных ИИ-систем', 'desc': 'FedNano - это новая система федеративного обучения для мультимодальных больших языковых моделей. Она централизует основную модель на сервере, а на клиентах использует легкие NanoEdge модули для адаптации. Такой подход решает проблемы масштабируемости и конфиденциальности при развертывании крупных мультимодальных моделей. FedNano значительно сокращает требования к ресурсам клиентов и объем передаваемых данных по сравнению с традиционными методами федеративного обучения.'}, 'en': {'title': 'Empowering Scalable AI with FedNano: Federated Learning for Large Language Models', 'desc': 'FedNano is a federated learning framework designed to enhance the deployment of large language models (LLMs) while addressing privacy and scalability challenges. It centralizes the LLM on servers and utilizes lightweight NanoEdge modules for client-specific adaptations, significantly reducing the need for client-side resources. By employing low-rank adaptation techniques, FedNano minimizes communication costs and storage requirements, allowing clients to only transmit compact updates instead of full model parameters. This innovative approach enables effective collaboration across heterogeneous client data while maintaining privacy, ultimately making multimodal AI systems more scalable and feasible.'}, 'zh': {'title': 'FedNano：解决多模态学习的隐私与可扩展性问题', 'desc': 'FedNano是一个联邦学习框架，它将大型语言模型集中在服务器上，并使用NanoEdge模块进行客户端特定的适应，从而解决了可扩展性和隐私问题。该框架通过允许在不集中数据的情况下进行协作模型训练，克服了多模态大语言模型在现实场景中的部署挑战。NanoEdge模块采用特定模态的编码器和可训练的NanoAdapters，显著减少了客户端的存储需求和通信开销。实验结果表明，FedNano在性能上优于现有的联邦学习基线，促进了可扩展的去中心化多模态人工智能系统的发展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (2)', '#alignment (2)', '#architecture (3)', '#audio', '#benchmark (5)', '#cv', '#data (1)', '#dataset (4)', '#diffusion', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (5)', '#open_source (4)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (4)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (4)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-19 08:16',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-19 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-19 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    