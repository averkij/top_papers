{
    "date": {
        "ru": "22 Ğ¼Ğ°Ñ",
        "en": "May 22",
        "zh": "5æœˆ22æ—¥"
    },
    "time_utc": "2025-05-22 05:12",
    "weekday": 3,
    "issue_id": 3894,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.15277",
            "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
            "url": "https://huggingface.co/papers/2505.15277",
            "abstract": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.",
            "score": 49,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "8209bc2f2e5e6119",
            "authors": [
                "Hyungjoo Chae",
                "Sunghwan Kim",
                "Junhee Cho",
                "Seungone Kim",
                "Seungjun Moon",
                "Gyeom Hwangbo",
                "Dongha Lim",
                "Minjin Kim",
                "Yeonjun Hwang",
                "Minju Gwak",
                "Dongwook Choi",
                "Minseok Kang",
                "Gwanhoon Im",
                "ByeongUng Cho",
                "Hyojun Kim",
                "Jun Hee Han",
                "Taeyoon Kwon",
                "Minju Kim",
                "Beong-woo Kwak",
                "Dongjin Kang",
                "Jinyoung Yeo"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15277.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#survey",
                    "#benchmark",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Web-Shepherd: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Web-Shepherd - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… WebPRM Collection Ñ 40 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WebRewardBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ PRM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Web-Shepherd Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4o Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Web-Shepherd: Revolutionizing Web Navigation with Process Reward Models",
                    "desc": "This paper introduces Web-Shepherd, a novel process reward model (PRM) designed specifically for web navigation tasks that require long-term decision making. The authors highlight the limitations of using multimodal large language models (MLLMs) as reward models, which can hinder practical applications due to speed and cost issues. They present the WebPRM Collection, a comprehensive dataset containing 40,000 step-level preference pairs to train the PRM effectively. Experimental results demonstrate that Web-Shepherd significantly outperforms existing models, achieving higher accuracy and lower costs in web navigation tasks."
                },
                "zh": {
                    "title": "ç½‘é¡µå¯¼èˆªçš„æ™ºèƒ½è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œåä¸ºWeb-Shepherdï¼Œæ—¨åœ¨è¯„ä¼°ç½‘é¡µå¯¼èˆªçš„å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºWebPRM Collectionçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«4ä¸‡å¯¹æ­¥éª¤çº§åå¥½å¯¹å’Œæ³¨é‡Šæ¸…å•ï¼Œæ¶µç›–å¤šç§é¢†åŸŸå’Œéš¾åº¦çº§åˆ«ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°Web-Shepherdåœ¨WebRewardBenchä¸Šæ¯”ä½¿ç”¨GPT-4oçš„å‡†ç¡®ç‡æé«˜äº†çº¦30ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”åœ¨WebArena-liteæµ‹è¯•ä¸­ï¼Œä½¿ç”¨Web-Shepherdä½œä¸ºéªŒè¯å™¨æ—¶ï¼Œæ€§èƒ½æå‡äº†10.9ç‚¹ï¼Œä¸”æˆæœ¬é™ä½äº†10ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç å‡å·²å…¬å¼€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14302",
            "title": "Scaling Law for Quantization-Aware Training",
            "url": "https://huggingface.co/papers/2505.14302",
            "abstract": "Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.",
            "score": 36,
            "issue_id": 3891,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "ecea48e2af693a28",
            "authors": [
                "Mengzhao Chen",
                "Chaoyi Zhang",
                "Jing Liu",
                "Yutao Zeng",
                "Zeyue Xue",
                "Zhiheng Liu",
                "Yunshui Li",
                "Jin Ma",
                "Jie Huang",
                "Xun Zhou",
                "Ping Luo"
            ],
            "affiliations": [
                "ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14302.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (QAT) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ QAT, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾Ğµ FC2 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼ Ğ² 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ QAT."
                },
                "en": {
                    "title": "Optimizing Quantization-Aware Training for Large Language Models",
                    "desc": "This paper investigates the challenges of deploying large language models (LLMs) due to their high computational and memory requirements. It introduces quantization-aware training (QAT) as a solution to reduce model precision while preserving performance, particularly focusing on 4-bit precision (W4A4). The authors propose a new scaling law for QAT that considers factors like model size, training data volume, and quantization granularity, revealing how quantization error behaves under these conditions. Their experiments show that while increasing model size reduces quantization error, more training tokens and coarser quantization lead to higher errors, highlighting the need for mixed-precision quantization to optimize performance."
                },
                "zh": {
                    "title": "é‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ç»Ÿä¸€æ‰©å±•æ³•åˆ™",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºï¼Œè¿™ç»™éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰é€šè¿‡é™ä½æ¨¡å‹ç²¾åº¦æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚ç„¶è€Œï¼ŒQATåœ¨4ä½ç²¾åº¦ï¼ˆW4A4ï¼‰ä¸‹çš„æ‰©å±•è¡Œä¸ºå°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„QATæ‰©å±•æ³•åˆ™ï¼Œæ¨¡å‹åŒ–é‡åŒ–è¯¯å·®ä¸æ¨¡å‹å¤§å°ã€è®­ç»ƒæ•°æ®é‡å’Œé‡åŒ–ç»„å¤§å°ä¹‹é—´çš„å…³ç³»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15809",
            "title": "MMaDA: Multimodal Large Diffusion Language Models",
            "url": "https://huggingface.co/papers/2505.15809",
            "abstract": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA",
            "score": 24,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "ff99ceb93709180d",
            "authors": [
                "Ling Yang",
                "Ye Tian",
                "Bowen Li",
                "Xinchen Zhang",
                "Ke Shen",
                "Yunhai Tong",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15809.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#diffusion",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MMaDA: Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "MMaDA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ…, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. MMaDA Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ UniGRPO, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MMaDA-8B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "MMaDA: Unifying Multimodal Learning for Superior Performance",
                    "desc": "MMaDA is a new type of multimodal diffusion model that excels in various tasks like understanding text and images, and generating images from text. It features a unified architecture that processes different data types without needing separate components for each type. The model uses a special training method that aligns reasoning across text and visuals, making it easier to learn complex tasks. Additionally, it includes a unique reinforcement learning algorithm that improves performance in both reasoning and generation tasks, showing strong results compared to other leading models."
                },
                "zh": {
                    "title": "MMaDAï¼šå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "MMaDAæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ‰©æ•£åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨æ–‡æœ¬æ¨ç†ã€å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸå®ç°å“è¶Šæ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„ä¸‰ä¸ªå…³é”®åˆ›æ–°åŒ…æ‹¬ï¼šé¦–å…ˆï¼ŒMMaDAé‡‡ç”¨ç»Ÿä¸€çš„æ‰©æ•£æ¶æ„ï¼Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šæ¨¡æ€ç»„ä»¶çš„éœ€æ±‚ï¼Œä»è€Œå®ç°ä¸åŒæ•°æ®ç±»å‹çš„æ— ç¼é›†æˆå’Œå¤„ç†ã€‚å…¶æ¬¡ï¼Œå®æ–½æ··åˆçš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å¾®è°ƒç­–ç•¥ï¼Œç»Ÿä¸€ä¸åŒæ¨¡æ€çš„æ¨ç†è¿‡ç¨‹ï¼Œå¢å¼ºæ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æœ€åï¼Œæå‡ºäº†UniGRPOï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºæ‰©æ•£åŸºç¡€æ¨¡å‹è®¾è®¡çš„ç»Ÿä¸€ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç¡®ä¿åœ¨æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­å®ç°ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13909",
            "title": "Efficient Agent Training for Computer Use",
            "url": "https://huggingface.co/papers/2505.13909",
            "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.",
            "score": 22,
            "issue_id": 3891,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "6614c6f1cb4338a5",
            "authors": [
                "Yanheng He",
                "Jiahe Jin",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "SII",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13909.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PC Agent-E - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 312 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Claude 3.7 Sonnet, Ğ¸Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 141%. PC Agent-E Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Agents with Minimal Data: The PC Agent-E Revolution",
                    "desc": "The paper presents PC Agent-E, a new framework for training computer use agents that minimizes the need for extensive human demonstrations. By starting with only 312 human-annotated trajectories, the authors enhanced the data quality through the synthesis of diverse action decisions using Claude 3.7 Sonnet. This approach led to a significant performance boost, with the PC Agent-E model achieving a 141% relative improvement over previous models on the WindowsAgentArena-V2 benchmark. Additionally, the model shows strong adaptability across different operating systems, indicating that effective computer use skills can be developed from a limited set of high-quality data."
                },
                "zh": {
                    "title": "å°‘é‡é«˜è´¨é‡æ•°æ®ï¼Œæ¿€å‘å¼ºå¤§è®¡ç®—æœºèƒ½åŠ›",
                    "desc": "PC Agent-E æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ä»£ç†è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¯¹å¤§è§„æ¨¡äººç±»ç¤ºèŒƒçš„ä¾èµ–ã€‚æˆ‘ä»¬ä»ä»…æœ‰çš„312ä¸ªäººå·¥æ ‡æ³¨çš„è®¡ç®—æœºä½¿ç”¨è½¨è¿¹å¼€å§‹ï¼Œé€šè¿‡åˆæˆå¤šæ ·çš„è¡ŒåŠ¨å†³ç­–æ¥æé«˜æ•°æ®è´¨é‡ã€‚ç»è¿‡è¿™äº›ä¸°å¯Œè½¨è¿¹çš„è®­ç»ƒï¼ŒPC Agent-E æ¨¡å‹åœ¨ WindowsAgentArena-V2 åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†141%çš„ç›¸å¯¹æå‡ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ Claude 3.7 Sonnetã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°‘é‡é«˜è´¨é‡çš„è½¨è¿¹æ•°æ®å¯ä»¥æ¿€å‘å‡ºå¼ºå¤§çš„è®¡ç®—æœºä½¿ç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15045",
            "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective",
            "url": "https://huggingface.co/papers/2505.15045",
            "abstract": "Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.",
            "score": 13,
            "issue_id": 3892,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "57c6eb57eddeebcc",
            "authors": [
                "Siyue Zhang",
                "Yilun Zhao",
                "Liyuan Geng",
                "Arman Cohan",
                "Anh Tuan Luu",
                "Chen Zhao"
            ],
            "affiliations": [
                "Alibaba-NTU Singapore Joint Research Institute",
                "Center for Data Science, New York University",
                "NYU Shanghai",
                "Nanyang Technological University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15045.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#long_context",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Harnessing Bidirectional Attention for Superior Text Embeddings",
                    "desc": "This paper discusses the limitations of large language model (LLM) embeddings, particularly their unidirectional attention which does not align well with the needs of text embedding tasks. The authors propose using diffusion language models, which have a bidirectional architecture, to improve text embeddings. Their systematic study shows that these diffusion models outperform LLM-based embeddings in various retrieval tasks, especially in long-document and reasoning-intensive scenarios. The findings highlight the importance of bidirectional attention for capturing the global context in complex texts."
                },
                "zh": {
                    "title": "æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼šåŒå‘åµŒå…¥çš„æœªæ¥",
                    "desc": "åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åµŒå…¥æ¨¡å‹åœ¨æ–‡æ¡£æ£€ç´¢ç­‰é€šç”¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºBERTå’ŒT5æ¨¡å‹ã€‚ç„¶è€Œï¼ŒLLMåµŒå…¥çš„ä¸€ä¸ªåŸºæœ¬é™åˆ¶æ˜¯å…¶åœ¨è‡ªå›å½’é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„å•å‘æ³¨æ„åŠ›ï¼Œè¿™ä¸æ–‡æœ¬åµŒå…¥ä»»åŠ¡çš„åŒå‘ç‰¹æ€§ä¸ç¬¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé‡‡ç”¨æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼Œå› å…¶å›ºæœ‰çš„åŒå‘æ¶æ„åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£è¯­è¨€åµŒå…¥æ¨¡å‹åœ¨é•¿æ–‡æ¡£æ£€ç´¢ç­‰ä»»åŠ¡ä¸­ä¼˜äºLLMåµŒå…¥æ¨¡å‹ï¼ŒéªŒè¯äº†åŒå‘æ³¨æ„åŠ›åœ¨ç¼–ç é•¿æ–‡æœ¬å…¨å±€ä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15210",
            "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
            "url": "https://huggingface.co/papers/2505.15210",
            "abstract": "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.",
            "score": 11,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "acf62275d75af161",
            "authors": [
                "Jie Ma",
                "Ning Qu",
                "Zhitao Gao",
                "Rui Xing",
                "Jun Liu",
                "Hongbin Pei",
                "Jiang Xie",
                "Linyun Song",
                "Pinghui Wang",
                "Jing Tao",
                "Zhou Su"
            ],
            "affiliations": [
                "MOE KLINNS Lab, Xian Jiaotong University",
                "School of Artificial Intelligence, Chongqing University of Post and Telecommunications",
                "School of Computer Science and Technology, Xian Jiaotong University",
                "School of Computer Science, Northwestern Polytechnical University",
                "Shaanxi Province Key Laboratory of Big Data Knowledge Engineering"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15210.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Deliberation over Priors' (DP) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. DP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ…, Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Hit@1 Ğ½Ğ° 13% Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ComplexWebQuestions."
                },
                "en": {
                    "title": "Enhancing LLM Trustworthiness with Knowledge Graphs",
                    "desc": "This paper introduces a new framework called Deliberation over Priors (DP) to enhance the reliability of Large Language Models (LLMs) by leveraging knowledge graphs (KGs). DP utilizes a progressive knowledge distillation strategy that incorporates the structural information and constraints from KGs into LLMs, improving the accuracy of reasoning and response generation. The framework also includes a reasoning-introspection strategy that allows LLMs to verify their reasoning based on extracted constraints, leading to more trustworthy outputs. Experimental results show that DP significantly outperforms existing methods, achieving a notable improvement in performance on benchmark datasets."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¿¡æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶ç§°ä¸ºâ€œå…ˆéªŒæ¨ç†æ·±æ€â€ï¼ˆDeliberation over Priors, DPï¼‰ï¼Œå……åˆ†åˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„ç»“æ„ä¿¡æ¯å’Œçº¦æŸæ¡ä»¶ã€‚DPé€šè¿‡é€æ­¥çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå°†ç»“æ„å…ˆéªŒæ•´åˆåˆ°LLMsä¸­ï¼Œä»è€Œæé«˜å…³ç³»è·¯å¾„ç”Ÿæˆçš„å¯ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜é‡‡ç”¨æ¨ç†è‡ªçœç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆå“åº”çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15146",
            "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
            "url": "https://huggingface.co/papers/2505.15146",
            "abstract": "Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.",
            "score": 10,
            "issue_id": 3892,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "96b324d85a6c0d83",
            "authors": [
                "Lanxiang Hu",
                "Mingjia Huo",
                "Yuxuan Zhang",
                "Haoyang Yu",
                "Eric P. Xing",
                "Ion Stoica",
                "Tajana Rosing",
                "Haojian Jin",
                "Hao Zhang"
            ],
            "affiliations": [
                "MBZUAI",
                "UC Berkeley",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15146.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#games",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ñ‹ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ LLM Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ lmgame-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ³Ñ€ Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ API Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ lmgame-Bench ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating LLMs with lmgame-Bench: A Game-Changer!",
                    "desc": "This paper addresses the challenges of evaluating large language models (LLMs) using video games, highlighting issues like poor visual perception, sensitivity to prompts, and data contamination. The authors propose lmgame-Bench, a framework that standardizes game evaluations through a unified API and incorporates tools for perception and memory. This framework allows for a more reliable assessment of LLMs across various game types, including platformers and puzzles. The results demonstrate that lmgame-Bench effectively distinguishes between models and shows that training on one game can enhance performance on others and on planning tasks."
                },
                "zh": {
                    "title": "æ¸¸æˆè¯„ä¼°ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨è§†é¢‘æ¸¸æˆæ¥è¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œç›´æ¥å°†LLMåº”ç”¨äºæ¸¸æˆä¸­è¿›è¡Œè¯„ä¼°å­˜åœ¨è§†è§‰æ„ŸçŸ¥è„†å¼±ã€æç¤ºæ•æ„Ÿæ€§å’Œæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†lmgame-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç»Ÿä¸€çš„Gymé£æ ¼APIæä¾›çš„å¹³å°æ¸¸æˆã€è§£è°œæ¸¸æˆå’Œå™äº‹æ¸¸æˆçš„è¯„ä¼°å·¥å…·ã€‚é€šè¿‡å¯¹13ä¸ªé¢†å…ˆæ¨¡å‹çš„æµ‹è¯•ï¼Œlmgame-Benchèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨å•ä¸€æ¸¸æˆä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯ä»¥è½¬ç§»åˆ°æœªè§è¿‡çš„æ¸¸æˆå’Œå¤–éƒ¨è§„åˆ’ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15779",
            "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2505.15779",
            "abstract": "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.",
            "score": 9,
            "issue_id": 3893,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "97c03041dc8579a8",
            "authors": [
                "Chuanhao Li",
                "Jianwen Sun",
                "Yukang Feng",
                "Mingliang Zhai",
                "Yifan Chang",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15779.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#dataset",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-augmented T2I: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (T2I) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº IA-T2I, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Img-Ref-T2I Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4."
                },
                "en": {
                    "title": "Enhancing T2I Models with Internet-Augmented Knowledge",
                    "desc": "This paper introduces the Internet-Augmented text-to-image generation (IA-T2I) framework, which enhances traditional text-to-image (T2I) models by addressing uncertainties in text prompts. The framework includes an active retrieval module to assess the need for reference images, a hierarchical image selection module to find the best matching images, and a self-reflection mechanism for continuous improvement of generated images. A new dataset, Img-Ref-T2I, is created to test the framework, featuring prompts with various types of uncertain knowledge. Experimental results show that IA-T2I significantly improves the quality of generated images, outperforming existing models in human evaluations."
                },
                "zh": {
                    "title": "äº’è”ç½‘å¢å¼ºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶",
                    "desc": "å½“å‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æç¤ºä¸­éšå«çš„ä¸ç¡®å®šçŸ¥è¯†æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº’è”ç½‘å¢å¼ºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼ˆIA-T2Iï¼‰ï¼Œé€šè¿‡æä¾›å‚è€ƒå›¾åƒæ¥å¸®åŠ©æ¨¡å‹ç†è§£ä¸ç¡®å®šçš„çŸ¥è¯†ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸»åŠ¨æ£€ç´¢æ¨¡å—ã€åˆ†å±‚å›¾åƒé€‰æ‹©æ¨¡å—å’Œè‡ªæˆ‘åæ€æœºåˆ¶ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å’Œä¸ç¡®å®šçš„æ–‡æœ¬æç¤ºæ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15765",
            "title": "Constructing a 3D Town from a Single Image",
            "url": "https://huggingface.co/papers/2505.15765",
            "abstract": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach.",
            "score": 7,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "c8acf46d639df2bb",
            "authors": [
                "Kaizhi Zheng",
                "Ruijian Zhang",
                "Jing Gu",
                "Jie Yang",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Columbia University",
                "Cybever AI",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15765.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "3DTown - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° ÑĞ²ĞµÑ€Ñ…Ñƒ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ 3D-Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹. 3DTown Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 3DTown Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Transforming Top-Down Images into Stunning 3D Towns!",
                    "desc": "This paper presents 3DTown, a novel framework for generating realistic 3D scenes from a single top-down image without the need for extensive training. The method utilizes region-based generation to enhance the alignment between the 2D image and the 3D output, while also employing spatial-aware inpainting to ensure the overall coherence and quality of the generated geometry. By breaking down the image into overlapping regions and using a pretrained 3D object generator, the framework effectively fills in missing parts of the scene, maintaining structural integrity. The results indicate that 3DTown surpasses existing models in producing high-quality, coherent 3D scenes, demonstrating the potential of training-free approaches in 3D scene synthesis."
                },
                "zh": {
                    "title": "ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º3DTownçš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ ä¿¯è§†å›¾ç”Ÿæˆé€¼çœŸçš„3Dåœºæ™¯ï¼Œè€Œæ— éœ€å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åŸºäºåŒºåŸŸç”Ÿæˆå’Œç©ºé—´æ„ŸçŸ¥çš„3Dä¿®å¤æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„åœºæ™¯åœ¨å‡ ä½•å½¢çŠ¶å’Œå¸ƒå±€ä¸Šä¿æŒä¸€è‡´æ€§ã€‚é€šè¿‡å°†è¾“å…¥å›¾åƒåˆ†è§£ä¸ºé‡å åŒºåŸŸï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„3Dç‰©ä½“ç”Ÿæˆå™¨ç”Ÿæˆæ¯ä¸ªåŒºåŸŸï¼Œæœ€åè¿›è¡Œå‡ ä½•å¡«å……ï¼Œä¿æŒç»“æ„è¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DTownåœ¨å‡ ä½•è´¨é‡ã€ç©ºé—´ä¸€è‡´æ€§å’Œçº¹ç†ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14357",
            "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
            "url": "https://huggingface.co/papers/2505.14357",
            "abstract": "World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.",
            "score": 7,
            "issue_id": 3891,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "de65ad0f3c9cf5c2",
            "authors": [
                "Siqiao Huang",
                "Jialong Wu",
                "Qixing Zhou",
                "Shangchen Miao",
                "Mingsheng Long"
            ],
            "affiliations": [
                "Chongqing University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14357.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#games",
                    "#robotics",
                    "#diffusion",
                    "#transfer_learning",
                    "#rl",
                    "#agents",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Vid2World, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Transforming Video Models into Interactive World Models",
                    "desc": "This paper introduces Vid2World, a novel method that enhances world models by utilizing pre-trained video diffusion models. Traditional world models struggle with low-quality predictions and require extensive training, limiting their use in complex scenarios. Vid2World addresses these issues by adapting the architecture and training objectives of video diffusion models for autoregressive generation. The approach also incorporates a causal action guidance mechanism, improving the controllability of actions in interactive environments, as demonstrated through experiments in robot manipulation and game simulations."
                },
                "zh": {
                    "title": "å°†è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºäº¤äº’å¼ä¸–ç•Œæ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVid2Worldçš„æ–¹æ³•ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºäº¤äº’å¼ä¸–ç•Œæ¨¡å‹ï¼Œä»¥æé«˜æ•°æ®æ•ˆç‡ã€‚ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒï¼Œå¹¶ä¸”é¢„æµ‹ç²¾åº¦è¾ƒä½ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚Vid2Worldé€šè¿‡è°ƒæ•´è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼Œå®ç°äº†è‡ªå›å½’ç”Ÿæˆï¼Œå¹¶å¼•å…¥äº†å› æœåŠ¨ä½œå¼•å¯¼æœºåˆ¶ï¼Œä»¥å¢å¼ºäº¤äº’å¼ä¸–ç•Œæ¨¡å‹ä¸­çš„åŠ¨ä½œå¯æ§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œå’Œæ¸¸æˆæ¨¡æ‹Ÿé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15778",
            "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
            "url": "https://huggingface.co/papers/2505.15778",
            "abstract": "Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like \"soft\" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.",
            "score": 6,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "96715f0b3118eceb",
            "authors": [
                "Zhen Zhang",
                "Xuehai He",
                "Weixiang Yan",
                "Ao Shen",
                "Chenyang Zhao",
                "Shuohang Wang",
                "Yelong Shen",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "LMSYS Org",
                "Microsoft",
                "Purdue University",
                "University of California, Los Angeles",
                "University of California, Santa Barbara",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15778.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Soft Thinking', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, 'Soft Thinking' Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ, Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 2.48 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ 22.4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Chain-of-Thought."
                },
                "en": {
                    "title": "Soft Thinking: Beyond Discrete Boundaries in Reasoning",
                    "desc": "This paper presents Soft Thinking, a novel approach to reasoning that mimics human cognitive processes by utilizing continuous concept spaces instead of discrete token embeddings. Traditional reasoning models are limited by their reliance on fixed linguistic tokens, which restricts their ability to explore diverse reasoning paths. Soft Thinking generates abstract concept tokens through a probability-weighted mixture of existing token embeddings, allowing for smoother transitions and richer representations. Empirical results show that this method improves accuracy and reduces token usage, while maintaining interpretability, thus addressing the limitations of conventional Chain-of-Thought reasoning."
                },
                "zh": {
                    "title": "çªç ´ç¦»æ•£é™åˆ¶ï¼Œæ‹¥æŠ±è½¯æ€ç»´ï¼",
                    "desc": "äººç±»çš„è®¤çŸ¥é€šå¸¸æ¶‰åŠæŠ½è±¡å’ŒæµåŠ¨çš„æ¦‚å¿µï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–äºç¦»æ•£çš„è¯­è¨€ç¬¦å·ã€‚å½“å‰çš„æ¨ç†æ¨¡å‹å—é™äºäººç±»è¯­è¨€çš„è¾¹ç•Œï¼Œåªèƒ½å¤„ç†ä»£è¡¨å›ºå®šè¯­ä¹‰ç‚¹çš„ç¦»æ•£æ ‡è®°åµŒå…¥ã€‚è¿™ç§ç¦»æ•£é™åˆ¶é™ä½äº†æ¨ç†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¯¼è‡´æ¨ç†è·¯å¾„çš„æ¢ç´¢ä¸å¤Ÿå…¨é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè½¯æ€ç»´â€çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¿ç»­æ¦‚å¿µç©ºé—´ä¸­ç”Ÿæˆè½¯çš„æŠ½è±¡æ¦‚å¿µæ ‡è®°ï¼Œæ¨¡æ‹Ÿäººç±»çš„â€œè½¯â€æ¨ç†ï¼Œä»è€Œæé«˜æ¨ç†çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15656",
            "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
            "url": "https://huggingface.co/papers/2505.15656",
            "abstract": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.",
            "score": 6,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "eed97ed4c1582120",
            "authors": [
                "Zhexin Zhang",
                "Yuhao Sun",
                "Junxiao Yang",
                "Shiyao Cui",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "The Conversational AI (CoAI) group, DCST, Tsinghua University",
                "The University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15656.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#security",
                    "#open_source",
                    "#training",
                    "#leakage"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°: ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸ ÑĞµĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ¸ÑĞº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ¼ĞµÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 3 Ğ´Ğ¾ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ - Ğ´Ğ¾ 76.3% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM."
                },
                "en": {
                    "title": "Exposing the Hidden Risks of Fine-Tuning LLMs with Proprietary Data",
                    "desc": "This paper discusses a significant risk associated with fine-tuning open-source Large Language Models (LLMs) using proprietary data. The authors demonstrate that creators of these LLMs can exploit backdoor training techniques to extract sensitive fine-tuning data from the models, even with only black-box access. Their experiments reveal that up to 76.3% of the fine-tuning data can be successfully extracted, with even higher rates in optimal conditions. The study emphasizes the urgent need for further research to address this data security issue in the context of LLM fine-tuning."
                },
                "zh": {
                    "title": "å¾®è°ƒä¸­çš„æ•°æ®æ³„éœ²é£é™©è­¦ç¤º",
                    "desc": "æœ¬è®ºæ–‡æ­ç¤ºäº†åœ¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šè¿›è¡Œå¾®è°ƒæ—¶å¯èƒ½å­˜åœ¨çš„éšç§é£é™©ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼€æºæ¨¡å‹çš„åˆ›å»ºè€…å¯ä»¥é€šè¿‡ç®€å•çš„åé—¨è®­ç»ƒæå–ç§æœ‰çš„å¾®è°ƒæ•°æ®ï¼Œå³ä½¿åªé€šè¿‡é»‘ç®±è®¿é—®å¾®è°ƒåçš„æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å®é™…æƒ…å†µä¸‹ï¼Œæœ€å¤šå¯æå–76.3%çš„å¾®è°ƒæ•°æ®ï¼Œè€Œåœ¨ç†æƒ³æƒ…å†µä¸‹æˆåŠŸç‡å¯è¾¾94.9%ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸€ç§åŸºäºæ£€æµ‹çš„é˜²å¾¡ç­–ç•¥ï¼Œä½†å‘ç°è¯¥ç­–ç•¥å¯ä»¥è¢«æ”¹è¿›çš„æ”»å‡»ç»•è¿‡ï¼Œå› æ­¤å¼ºè°ƒäº†è¿™ä¸€æ–°è¯†åˆ«çš„æ•°æ®æ³„éœ²é£é™©çš„ç´§è¿«æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15612",
            "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
            "url": "https://huggingface.co/papers/2505.15612",
            "abstract": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant \"self-reflections\". Resources are at https://github.com/hkust-nlp/Laser.",
            "score": 6,
            "issue_id": 3892,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "145c2f1bfcaa0c1e",
            "authors": [
                "Wei Liu",
                "Ruochen Zhou",
                "Yiyun Deng",
                "Yuzhen Huang",
                "Junteng Liu",
                "Yuntian Deng",
                "Yizhe Zhang",
                "Junxian He"
            ],
            "affiliations": [
                "Apple",
                "City University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15612.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LASER Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. LASER Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ LASER-D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Reasoning Efficiency with Dynamic Length-Based Rewards",
                    "desc": "This paper explores how Large Reasoning Models (LRMs) can improve their problem-solving efficiency using reinforcement learning (RL). It introduces a new method called Length-bAsed StEp Reward shaping (LASER), which optimizes reasoning outputs by shaping rewards based on the length of reasoning traces. LASER-D, an extension of LASER, adapts the reward system to be dynamic and difficulty-aware, penalizing longer reasoning for easier queries. The results show that this approach significantly enhances reasoning performance while reducing redundancy and token usage in outputs."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ•ˆç‡çš„åŠ¨æ€å¥–åŠ±æœºåˆ¶",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³å¤æ‚é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆé•¿æ¨ç†é“¾æ—¶ã€‚ç„¶è€Œï¼Œè¿™äº›å†—é•¿çš„è¾“å‡ºå¾€å¾€å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ï¼Œé™åˆ¶äº†LRMsçš„æ•ˆç‡ã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºRLçš„æ–¹æ³•ä»¥æé«˜æ¨ç†æ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é•¿åº¦åŸºç¡€æ­¥éª¤å¥–åŠ±å¡‘å½¢æ–¹æ³•ï¼ˆLASERï¼‰ï¼Œé€šè¿‡ç›®æ ‡é•¿åº¦æ§åˆ¶å¥–åŠ±ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„ä¼˜è¶Šå¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŠ¨æ€å’Œéš¾åº¦æ„ŸçŸ¥çš„LASER-Dæ–¹æ³•ï¼Œä»¥é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨ç†è¡Œä¸ºå˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15404",
            "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
            "url": "https://huggingface.co/papers/2505.15404",
            "abstract": "Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important research question: how can we enhance the safety of LRMs? In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT). Our investigation begins with an unexpected observation: directly distilling safe responses from DeepSeek-R1 fails to significantly enhance safety. We analyze this phenomenon and identify three key failure patterns that contribute to it. We then demonstrate that explicitly addressing these issues during the data distillation process can lead to substantial safety improvements. Next, we explore whether a long and complex reasoning process is necessary for achieving safety. Interestingly, we find that simply using short or template-based reasoning process can attain comparable safety performance-and are significantly easier for models to learn than more intricate reasoning chains. These findings prompt a deeper reflection on the role of reasoning in ensuring safety. Finally, we find that mixing math reasoning data during safety fine-tuning is helpful to balance safety and over-refusal. Overall, we hope our empirical study could provide a more holistic picture on enhancing the safety of LRMs. The code and data used in our experiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
            "score": 6,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "d681b198a7360d3b",
            "authors": [
                "Zhexin Zhang",
                "Xian Qi Loye",
                "Victor Shea-Jay Huang",
                "Junxiao Yang",
                "Qi Zhu",
                "Shiyao Cui",
                "Fei Mi",
                "Lifeng Shang",
                "Yingkang Wang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "The Conversational AI (CoAI) group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15404.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#math",
                    "#reasoning",
                    "#training",
                    "#safety"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LRM: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ĞºĞ»ÑÑ‡Ğ¾Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ ĞšÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (SFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¸Ğ»Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ»ÑŒ Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğº Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºĞ°Ğ·."
                },
                "en": {
                    "title": "Enhancing Safety in Large Reasoning Models through Simplified Reasoning",
                    "desc": "This paper investigates how to improve the safety of Large Reasoning Models (LRMs) while maintaining their reasoning capabilities. The authors find that directly distilling safe responses does not significantly enhance safety and identify three failure patterns that contribute to this issue. They demonstrate that addressing these patterns during data distillation can lead to better safety outcomes. Additionally, the study reveals that simpler reasoning processes can achieve similar safety performance as complex ones, suggesting a reevaluation of reasoning's role in safety enhancement."
                },
                "zh": {
                    "title": "æå‡å¤§å‹æ¨ç†æ¨¡å‹å®‰å…¨æ€§çš„ç ”ç©¶",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†èƒ½åŠ›å¢å¼ºå¹¶ä¸ä¸€å®šèƒ½æé«˜å®‰å…¨æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šé™ä½å®‰å…¨æ€§ã€‚æœ¬æ–‡é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹å¦‚ä½•å¢å¼ºLRMsçš„å®‰å…¨æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå‘ç°ç›´æ¥ä»DeepSeek-R1æå–å®‰å…¨å“åº”å¹¶æœªæ˜¾è‘—æå‡å®‰å…¨æ€§ï¼Œå¹¶è¯†åˆ«å‡ºä¸‰ç§å…³é”®çš„å¤±è´¥æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ç®€å•çš„çŸ­æ¨ç†è¿‡ç¨‹å¯ä»¥å®ç°ä¸å¤æ‚æ¨ç†è¿‡ç¨‹ç›¸å½“çš„å®‰å…¨æ€§èƒ½ï¼Œä¸”æ›´æ˜“äºæ¨¡å‹å­¦ä¹ ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡æ–°æ€è€ƒæ¨ç†åœ¨ç¡®ä¿å®‰å…¨æ€§ä¸­çš„ä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13529",
            "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
            "url": "https://huggingface.co/papers/2505.13529",
            "abstract": "Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with \"I don't know\". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.",
            "score": 5,
            "issue_id": 3892,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "b0175ab5c60ceaee",
            "authors": [
                "Junxiao Yang",
                "Jinzhe Tu",
                "Haoran Liu",
                "Xiaoce Wang",
                "Chujie Zheng",
                "Zhexin Zhang",
                "Shiyao Cui",
                "Caishun Chen",
                "Tiantian He",
                "Hongning Wang",
                "Yew-Soon Ong",
                "Minlie Huang"
            ],
            "affiliations": [
                "Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore",
                "The College of Computing and Data Science, Nanyang Technological University",
                "The Conversational AI (CoAI) group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13529.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#reasoning",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¿Ğ¾ÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ğ´Ğ¾Ğ³Ğ°Ğ´ĞºĞ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ¸ ÑĞ¿Ğ¸Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ overthinking. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BARREL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ BARREL Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1-Distill-Llama-8B Ñ 39.33% Ğ´Ğ¾ 61.48%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ R1."
                },
                "en": {
                    "title": "Enhancing Confidence and Reliability in Large Reasoning Models with BARREL",
                    "desc": "This paper discusses the limitations of Large Reasoning Models (LRMs) in handling mathematical and logical reasoning, particularly their tendency to provide incorrect answers with excessive confidence. The authors identify two problematic reasoning behaviors: last-minute guessing and second-thought spiraling, which lead to these overconfident mistakes. To combat this, they introduce BARREL, a new framework designed to enhance factual reasoning by encouraging models to be more concise and aware of their boundaries. Their experiments show that training with BARREL significantly improves the reliability of a specific LRM, DeepSeek-R1-Distill-Llama-8B, while maintaining competitive accuracy levels."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ¨¡å‹çš„å¯é æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†æ–¹é¢å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„LRMså¾ˆå°‘æ‰¿è®¤è‡ªå·±çš„æ— çŸ¥ï¼Œé€šå¸¸ä¼šåœ¨é”™è¯¯çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œè¿™å¼•å‘äº†å¯¹å…¶äº‹å®å¯é æ€§çš„æ‹…å¿§ã€‚æœ¬æ–‡è¯†åˆ«äº†ä¸¤ç§ç—…æ€æ¨ç†æ¨¡å¼ï¼Œå¯¼è‡´äº†è¿‡åº¦è‡ªä¿¡å’Œé”™è¯¯ç­”æ¡ˆçš„äº§ç”Ÿï¼šä¸´æ—¶çŒœæµ‹å’Œåå¤æ€è€ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BARRELæ¡†æ¶ï¼Œä¿ƒè¿›ç®€æ´ä¸”è¾¹ç•Œæ„è¯†å¼ºçš„äº‹å®æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15781",
            "title": "dKV-Cache: The Cache for Diffusion Language Models",
            "url": "https://huggingface.co/papers/2505.15781",
            "abstract": "Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.",
            "score": 4,
            "issue_id": 3893,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "516300496b94028a",
            "authors": [
                "Xinyin Ma",
                "Runpeng Yu",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15781.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DLM) - Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (dKV-Cache). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: dKV-Cache-Decode, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ¸ dKV-Cache-Greedy Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 2-10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° DLM, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Accelerating Diffusion Language Models with Delayed KV-Cache",
                    "desc": "This paper introduces a new mechanism called delayed KV-Cache to improve the inference speed of Diffusion Language Models (DLMs), which traditionally suffer from slow decoding due to their non-autoregressive nature. The authors identify that different tokens exhibit unique representation dynamics during the diffusion process, leading to the development of a caching strategy that optimizes key and value states. They propose two variants of the caching mechanism: dKV-Cache-Decode, which enhances performance on long sequences, and dKV-Cache-Greedy, which prioritizes speed at the expense of some accuracy. Overall, the proposed dKV-Cache achieves a significant speedup of 2-10x in inference, making DLMs more competitive with autoregressive models."
                },
                "zh": {
                    "title": "åŠ é€Ÿæ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ¨ç†é€Ÿåº¦",
                    "desc": "æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰è¢«è§†ä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æœ‰åŠ›ç«äº‰è€…ï¼Œä½†å…¶æ¨ç†é€Ÿåº¦è¾ƒæ…¢æ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å»¶è¿Ÿé”®å€¼ç¼“å­˜ï¼ˆdKV-Cacheï¼‰æœºåˆ¶ï¼Œä»¥è§£å†³DLMsåœ¨å»å™ªè¿‡ç¨‹ä¸­çš„ç“¶é¢ˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§äº’è¡¥çš„ç¼“å­˜å˜ä½“ï¼Œåˆ†åˆ«ä¸ºdKV-Cache-Decodeå’ŒdKV-Cache-Greedyï¼Œå‰è€…åœ¨é•¿åºåˆ—ä¸Šå‡ ä¹æ— æŸåŠ é€Ÿï¼Œåè€…åˆ™ä»¥æ›´é«˜çš„é€Ÿåº¦å®ç°äº†äºŒæ¬¡æ—¶é—´å¤æ‚åº¦çš„åŠ é€Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒdKV-Cacheåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œç¼©å°äº†è‡ªå›å½’æ¨¡å‹ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14231",
            "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.14231",
            "abstract": "Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
            "score": 4,
            "issue_id": 3891,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "cd98eb52e95427f4",
            "authors": [
                "Sule Bai",
                "Mingxing Li",
                "Yong Liu",
                "Jing Tang",
                "Haoji Zhang",
                "Lei Sun",
                "Xiangxiang Chu",
                "Yansong Tang"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14231.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UniVG-R1 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. UniVG-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MIG-Bench Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Visual Grounding with Advanced Reasoning",
                    "desc": "This paper introduces UniVG-R1, a multimodal large language model designed for universal visual grounding, which is the task of linking images to complex textual instructions. The model enhances its reasoning abilities through a combination of supervised fine-tuning on a newly created Chain-of-Thought dataset and reinforcement learning techniques. To address challenges in training, the authors implement a difficulty-aware weight adjustment strategy that helps the model focus on more complex reasoning tasks as it learns. Experimental results show that UniVG-R1 outperforms previous methods, demonstrating significant improvements in both general performance and zero-shot capabilities across various benchmarks."
                },
                "zh": {
                    "title": "æå‡è§†è§‰å®šä½çš„æ¨ç†èƒ½åŠ›",
                    "desc": "ä¼ ç»Ÿçš„è§†è§‰å®šä½æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒåœºæ™¯å’Œç®€å•æ–‡æœ¬å¼•ç”¨ä¸Šã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°æ¶‰åŠéšå«å’Œå¤æ‚æŒ‡ä»¤çš„çœŸå®åœºæ™¯ï¼Œå°¤å…¶æ˜¯å¤šå›¾åƒçš„æƒ…å†µä¸‹ï¼Œé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé«˜çº§æ¨ç†çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†UniVG-R1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨ç†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†·å¯åŠ¨æ•°æ®æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVG-R1åœ¨MIG-Benchä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æé«˜äº†9.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13934",
            "title": "RLVR-World: Training World Models with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.13934",
            "abstract": "World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.",
            "score": 4,
            "issue_id": 3891,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "bb3f20501a24c607",
            "authors": [
                "Jialong Wu",
                "Shaofeng Yin",
                "Ningya Feng",
                "Mingsheng Long"
            ],
            "affiliations": [
                "School of Software, BNRist, Tsinghua University",
                "Zhili College, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13934.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#reasoning",
                    "#games",
                    "#rl",
                    "#agents",
                    "#video"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "RLVR-World: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RLVR-World - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ, RLVR-World Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹, Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°."
                },
                "en": {
                    "title": "Optimizing World Models with Reinforcement Learning for Better Predictions",
                    "desc": "This paper introduces RLVR-World, a new framework that improves world models by using reinforcement learning with verifiable rewards. Traditional training methods like maximum likelihood estimation often do not align well with the specific goals of these models, such as accuracy in predicting state transitions. RLVR-World addresses this by optimizing world models directly for metrics that matter, like perceptual quality and accuracy. The authors show that this approach leads to significant improvements in performance for both language and video-based models across various tasks, including text games and robot manipulation."
                },
                "zh": {
                    "title": "é€šè¿‡å¯éªŒè¯å¥–åŠ±ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRLVR-Worldçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹ã€‚ä¼ ç»Ÿçš„è®­ç»ƒç›®æ ‡å¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å¾€å¾€ä¸ç‰¹å®šä»»åŠ¡çš„ç›®æ ‡ä¸ä¸€è‡´ï¼Œè€ŒRLVR-Worldç›´æ¥é’ˆå¯¹è¿‡æ¸¡é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬å°†ä¸–ç•Œå»ºæ¨¡è§†ä¸ºå¯¹æ ‡è®°åºåˆ—çš„è‡ªå›å½’é¢„æµ‹ï¼Œå¹¶é€šè¿‡è§£ç é¢„æµ‹çš„åº¦é‡ä½œä¸ºå¯éªŒè¯å¥–åŠ±è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLVR-Worldåœ¨è¯­è¨€å’Œè§†é¢‘åŸºç¡€çš„ä¸–ç•Œæ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œé€‚ç”¨äºæ–‡æœ¬æ¸¸æˆã€ç½‘é¡µå¯¼èˆªå’Œæœºå™¨äººæ“ä½œç­‰å¤šä¸ªé¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12650",
            "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
            "url": "https://huggingface.co/papers/2505.12650",
            "abstract": "Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, converting these images into simulation-ready formats remains labor-intensive and error-prone, creating a bottleneck for model training and validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that automatically transforms scanning transmission electron microscopy (STEM) images into atomic crystal structures and predicts their physical properties. AutoMat combines pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation and property prediction via MatterSim, and coordinated orchestration across all stages. We propose the first dedicated STEM2Mat-Bench for this task and evaluate performance using lattice RMSD, formation energy MAE, and structure-matching success rate. By orchestrating external tool calls, AutoMat enables a text-only LLM to outperform vision-language models in this domain, achieving closed-loop reasoning throughout the pipeline. In large-scale experiments over 450 structure samples, AutoMat substantially outperforms existing multimodal large language models and tools. These results validate both AutoMat and STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic simulation in materials science.The code and dataset are publicly available at https://github.com/yyt-2378/AutoMat and https://huggingface.co/datasets/yaotianvector/STEM2Mat.",
            "score": 4,
            "issue_id": 3891,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "b02918769ea5f226",
            "authors": [
                "Yaotian Yang",
                "Yiwen Tang",
                "Yizhe Chen",
                "Xiao Chen",
                "Jiangjie Qiu",
                "Hao Xiong",
                "Haoyu Yin",
                "Zhiyao Luo",
                "Yifei Zhang",
                "Sijia Tao",
                "Wentao Li",
                "Qinghua Zhang",
                "Yuqiang Li",
                "Wanli Ouyang",
                "Bin Zhao",
                "Xiaonan Wang",
                "Fei Wei"
            ],
            "affiliations": [
                "Department of Chemical Engineering, Tsinghua University, Beijing, China",
                "School of Computer Science, Northwestern Polytechnical University, Xian, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12650.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "AutoMat: Ğ¾Ñ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼",
                    "desc": "AutoMat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞºĞ°Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑĞ²ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ (STEM) Ğ² Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸ÑÑ‚Ğ°Ğ»Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ², Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MatterSim. AutoMat Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ STEM-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ­Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Automating Atomic Structure Extraction from STEM Images with AutoMat",
                    "desc": "This paper presents AutoMat, a machine learning pipeline designed to convert scanning transmission electron microscopy (STEM) images into atomic crystal structures efficiently. It addresses the challenge of limited experimental data by automating the transformation process, which traditionally requires significant manual effort. AutoMat integrates various techniques such as denoising, template retrieval, and property prediction to streamline the workflow and enhance model training. The authors also introduce the STEM2Mat-Bench for evaluating performance, demonstrating that AutoMat significantly outperforms existing models in this domain."
                },
                "zh": {
                    "title": "AutoMatï¼šæ˜¾å¾®é•œä¸åŸå­æ¨¡æ‹Ÿçš„æ¡¥æ¢",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAutoMatçš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œæ—¨åœ¨å°†æ‰«æé€å°„ç”µå­æ˜¾å¾®é•œï¼ˆSTEMï¼‰å›¾åƒè½¬æ¢ä¸ºåŸå­æ™¶ä½“ç»“æ„ï¼Œå¹¶é¢„æµ‹å…¶ç‰©ç†æ€§è´¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”å»å™ªã€ç‰©ç†å¼•å¯¼çš„æ¨¡æ¿æ£€ç´¢å’Œå¯¹ç§°æ„ŸçŸ¥çš„åŸå­é‡å»ºï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†æ•°æ®ã€‚é€šè¿‡å¼•å…¥STEM2Mat-Benchè¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼ŒAutoMatåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚æ­¤ç ”ç©¶ä¸ºææ–™ç§‘å­¦ä¸­æ˜¾å¾®é•œä¸åŸå­çº§æ¨¡æ‹Ÿä¹‹é—´çš„æ¡¥æ¢å»ºè®¾æä¾›äº†é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15816",
            "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
            "url": "https://huggingface.co/papers/2505.15816",
            "abstract": "Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further. The code will be made public at this https://github.com/penghao-wu/ProxyV URL.",
            "score": 2,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "96ff09e995e7c437",
            "authors": [
                "Penghao Wu",
                "Lewei Lu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "1S-Lab, Nanyang Technological University",
                "SenseTime"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15816.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ProxyV: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ProxyV Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ProxyV Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Efficiency in Multimodal Models with Proxy Vision Tokens",
                    "desc": "This paper addresses the computational challenges faced by large multimodal models, particularly in processing visual tokens. Instead of focusing on reducing the number of tokens, the authors explore computation-level redundancy, revealing that not all heavy operations are necessary for vision tokens in decoder-only models. They introduce ProxyV, a method that uses proxy vision tokens to reduce the computational load while maintaining or even improving performance. The study shows that ProxyV can be combined with existing token reduction techniques for even greater efficiency gains."
                },
                "zh": {
                    "title": "ProxyVï¼šå‡è½»è§†è§‰è®¡ç®—è´Ÿæ‹…çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†è§†è§‰æ ‡è®°æ—¶çš„è®¡ç®—å†—ä½™é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å¹¶ä¸éœ€è¦åœ¨è§£ç å™¨ä¸­æ‰§è¡Œæ‰€æœ‰é‡è®¡ç®—æ“ä½œã€‚é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†ProxyVæ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç†è§†è§‰æ ‡è®°æ¥å‡è½»åŸå§‹è§†è§‰æ ‡è®°çš„è®¡ç®—è´Ÿæ‹…ã€‚ProxyVåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¸å½±å“æ€§èƒ½ï¼Œç”šè‡³åœ¨é€‚åº¦æé«˜æ•ˆç‡çš„æƒ…å†µä¸‹è¿˜èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15791",
            "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL",
            "url": "https://huggingface.co/papers/2505.15791",
            "abstract": "Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.",
            "score": 2,
            "issue_id": 3892,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "48280e44cc4e905f",
            "authors": [
                "Fengyuan Dai",
                "Zifeng Zhuang",
                "Yufei Huang",
                "Siteng Huang",
                "Bangyan Liao",
                "Donglin Wang",
                "Fajie Yuan"
            ],
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15791.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#rl",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "VARD: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VARD (Value-based Reinforced Diffusion) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. VARD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞµĞµ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VARD ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Value-Based Reinforcement Learning",
                    "desc": "This paper introduces VAlue-based Reinforced Diffusion (VARD), a new method for fine-tuning diffusion models using reinforcement learning. The approach addresses the challenges of stable and efficient training while dealing with non-differentiable rewards by incorporating a value function that predicts expected rewards from intermediate states. By applying KL regularization, VARD provides dense supervision throughout the generation process, enhancing the model's performance. Experimental results show that VARD improves trajectory guidance and training efficiency, making it suitable for complex reward functions in diffusion models."
                },
                "zh": {
                    "title": "åŸºäºä»·å€¼çš„å¼ºåŒ–æ‰©æ•£ï¼šæå‡ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸä¸­æˆä¸ºå¼ºå¤§çš„ç”Ÿæˆå·¥å…·ï¼Œä½†å°†é¢„è®­ç»ƒæ¨¡å‹è°ƒæ•´ä¸ºå…·æœ‰ç‰¹å®šæœŸæœ›å±æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å½“å‰æ–¹æ³•åœ¨å®ç°ç¨³å®šã€é«˜æ•ˆçš„å¾®è°ƒå’Œæ”¯æŒéå¯å¾®åˆ†å¥–åŠ±æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œç¨€ç–å¥–åŠ±åœ¨ä¸­é—´æ­¥éª¤æä¾›çš„ç›‘ç£ä¸è¶³ï¼Œå¸¸å¸¸å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä»·å€¼çš„å¼ºåŒ–æ‰©æ•£ï¼ˆVARDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ ä»·å€¼å‡½æ•°æ¥é¢„æµ‹ä¸­é—´çŠ¶æ€çš„å¥–åŠ±æœŸæœ›ï¼Œå¹¶ä½¿ç”¨KLæ­£åˆ™åŒ–æä¾›å¯†é›†ç›‘ç£ï¼Œä»è€Œæé«˜ç”Ÿæˆè¿‡ç¨‹çš„è´¨é‡å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15047",
            "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
            "url": "https://huggingface.co/papers/2505.15047",
            "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce PiFlow, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\\% compared to a vanilla agent system. Overall, PiFlow serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our https://github.com/amair-lab/PiFlow{GitHub}.",
            "score": 2,
            "issue_id": 3891,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "12f9a805fafedcf5",
            "authors": [
                "Yingming Pu",
                "Tao Lin",
                "Hongyu Chen"
            ],
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15047.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "PiFlow: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸",
                    "desc": "PiFlow - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ…. PiFlow Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ½Ğ°Ğ½Ğ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹ Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ½Ğ¸ĞºĞ¸. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, PiFlow Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 94.06% Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸ Ğ¿Ğ¾Ğ´ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° 73.55%."
                },
                "en": {
                    "title": "PiFlow: Revolutionizing Automated Scientific Discovery through Uncertainty Reduction",
                    "desc": "This paper presents PiFlow, a new framework for improving automated scientific discovery using Large Language Model (LLM)-based multi-agent systems. It addresses the limitations of existing methods that often lack rationality and fail to connect hypotheses with evidence, leading to inefficient exploration. By framing the discovery process as a structured uncertainty reduction problem, PiFlow enhances the efficiency and quality of scientific discoveries across various domains. The results show significant improvements in discovery efficiency and solution quality, marking a shift towards more effective AI-driven research."
                },
                "zh": {
                    "title": "PiFlowï¼šé«˜æ•ˆçš„è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°æ–°èŒƒå¼",
                    "desc": "åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨ç§‘å­¦å‘ç°ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é¢„å®šä¹‰çš„å·¥ä½œæµç¨‹æ¥è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ï¼Œè¿™äº›æµç¨‹ç¼ºä¹åˆç†æ€§çº¦æŸï¼Œå¯¼è‡´å‡è®¾æ— ç›®çš„ä¸”æ— æ³•æœ‰æ•ˆé“¾æ¥å‡è®¾ä¸è¯æ®ï¼Œä»è€Œé˜»ç¢ç³»ç»Ÿæ€§çš„ä¸ç¡®å®šæ€§å‡å°‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PiFlowï¼Œä¸€ä¸ªä¿¡æ¯ç†è®ºæ¡†æ¶ï¼Œå°†è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°è§†ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„ä¸ç¡®å®šæ€§å‡å°‘é—®é¢˜ï¼Œå¹¶ä»¥ç§‘å­¦åŸåˆ™ä¸ºæŒ‡å¯¼ã€‚åœ¨ä¸‰ä¸ªä¸åŒçš„ç§‘å­¦é¢†åŸŸä¸­è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†å‘ç°æ•ˆç‡ï¼Œå¹¶ä¸”è§£å†³æ–¹æ¡ˆè´¨é‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14827",
            "title": "Text Generation Beyond Discrete Token Sampling",
            "url": "https://huggingface.co/papers/2505.14827",
            "abstract": "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.",
            "score": 2,
            "issue_id": 3893,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "c5866f38a43da092",
            "authors": [
                "Yufan Zhuang",
                "Liyuan Liu",
                "Chandan Singh",
                "Jingbo Shang",
                "Jianfeng Gao"
            ],
            "affiliations": [
                "Microsoft Research",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14827.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mixture of Inputs (MoI). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, MoI ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹. MoI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Autoregressive Generation with Mixture of Inputs",
                    "desc": "This paper introduces a novel method called Mixture of Inputs (MoI) for enhancing autoregressive generation in large language models (LLMs). Instead of discarding the token distribution after sampling a token, MoI combines the generated token with the previously discarded distribution to create a richer input representation. By using Bayesian estimation, MoI treats the token distribution as a prior and the sampled token as an observation, allowing for a continuous posterior expectation to be used as input. This approach leads to improved performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering without requiring additional training."
                },
                "zh": {
                    "title": "æ··åˆè¾“å…¥ï¼šæå‡è‡ªå›å½’ç”Ÿæˆçš„è´¨é‡ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "åœ¨æ ‡å‡†çš„è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œè¯­è¨€æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°çš„åˆ†å¸ƒï¼Œé‡‡æ ·ä¸€ä¸ªç¦»æ•£æ ‡è®°ï¼Œç„¶åä¸¢å¼ƒè¯¥åˆ†å¸ƒï¼Œä»…å°†é‡‡æ ·çš„æ ‡è®°ä½œä¸ºæ–°è¾“å…¥ã€‚ä¸ºäº†ä¿ç•™è¿™ä¸ªåˆ†å¸ƒçš„ä¸°å¯Œä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ··åˆè¾“å…¥ï¼ˆMoIï¼‰çš„æ–¹æ³•ï¼Œå®ƒä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆæ ‡è®°åï¼Œæ„å»ºä¸€ä¸ªæ–°çš„è¾“å…¥ï¼Œå°†ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°ä¸ä¹‹å‰ä¸¢å¼ƒçš„æ ‡è®°åˆ†å¸ƒæ··åˆã€‚MoIä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒæ›´ä¸°å¯Œçš„å†…éƒ¨è¡¨ç¤ºï¼Œä»è€Œæé«˜æ–‡æœ¬è´¨é‡å’Œæ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-21.html",
    "link_next": "2025-05-23.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "21.05",
        "en": "05/21",
        "zh": "5æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.05",
        "en": "05/23",
        "zh": "5æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 6,
        "#cv": 0,
        "#rl": 9,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 4,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 1,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 10,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0,
        "#safety": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç»Ÿä¸€å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡å¼çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚BAGELé€šè¿‡åœ¨å¤§è§„æ¨¡çš„å¤šæ¨¡å¼äº¤é”™æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¤æ‚çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚å®ƒåœ¨å¤šæ¨¡å¼ç”Ÿæˆå’Œç†è§£æ–¹é¢è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶å…·å¤‡é«˜çº§çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®åŠä»£ç å’Œæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡å¼ç ”ç©¶ã€‚",
        "title": "Emerging Properties in Unified Multimodal Pretraining",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«gÃ¨ mÃ­ngwÃ¨i BAGEL de kÄiyuÃ¡n jÄ«chÇ” mÃ³xÃ­ng.\n\nå®ƒèƒ½ç»Ÿä¸€å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡å¼çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚\nTÄ nÃ©ng tÇ’ngyÄ« chÇ”lÇ hÃ© shÄ“ngchÃ©ng duÅzhÇ’ng mÃ³shÃ¬ de shÃ¹jÃ¹, rÃº wÃ©nbÄ›n, tÃºxiÃ ng hÃ© shÃ¬pÃ­n.\n\nBAGELé€šè¿‡åœ¨å¤§è§„æ¨¡çš„å¤šæ¨¡å¼äº¤é”™æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¤æ‚çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚\nBAGEL tÅngguÃ² zÃ i dÃ guÄ«mÃ³ de duÅmÃ³shÃ¬ jiÄocuÃ² shÃ¹jÃ¹ shÃ ng yÃ¹xÃ¹nliÃ n, zhÇnxiÃ n chÅ« fÃ¹zÃ¡ de duÅmÃ³shÃ¬ tuÄ«lÇ nÃ©nglÃ¬.\n\nå®ƒåœ¨å¤šæ¨¡å¼ç”Ÿæˆå’Œç†è§£æ–¹é¢è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶å…·å¤‡é«˜çº§çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚\nTÄ zÃ i duÅmÃ³shÃ¬ shÄ“ngchÃ©ng hÃ© lÇjiÄ› fÄngmiÃ n chÄoyuÃ¨ le qÃ­tÄ kÄiyuÃ¡n mÃ³xÃ­ng, bÃ¬ng jÃ¹bÃ¨i gÄojÃ­ de duÅmÃ³shÃ¬ tuÄ«lÇ nÃ©nglÃ¬.\n\nç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®åŠä»£ç å’Œæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡å¼ç ”ç©¶ã€‚\nYÃ¡njiÅ« tuÃ¡nduÃ¬ gÅngkÄi le guÇnjiÃ n fÄxiÃ n, yÃ¹xÃ¹nliÃ n xÃ¬jiÃ¨, shÃ¹jÃ¹ chuÃ ngjiÃ n xiÃ©yÃ¬ jÃ­ dÃ imÇ hÃ© jiÇnchÃ¡diÇn, yÇ cÃ¹jÃ¬n duÅmÃ³shÃ¬ yÃ¡njiÅ«.",
        "vocab": "[\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open source\"},\n    {\"word\": \"åŸºç¡€æ¨¡å‹\", \"pinyin\": \"jÄ« chÇ” mÃ³ xÃ­ng\", \"trans\": \"foundational model\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unify\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å¼\", \"pinyin\": \"mÃ³ shÃ¬\", \"trans\": \"mode\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹ jÃ¹\", \"trans\": \"data\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pretrain\"},\n    {\"word\": \"å±•ç°\", \"pinyin\": \"zhÇn xiÃ n\", \"trans\": \"demonstrate\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"æ–¹é¢\", \"pinyin\": \"fÄng miÃ n\", \"trans\": \"aspect\"},\n    {\"word\": \"é«˜çº§\", \"pinyin\": \"gÄo jÃ­\", \"trans\": \"advanced\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å›¢é˜Ÿ\", \"pinyin\": \"tuÃ¡n duÃ¬\", \"trans\": \"team\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discovery\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬ jiÄ›\", \"trans\": \"detail\"},\n    {\"word\": \"åè®®\", \"pinyin\": \"xiÃ© yÃ¬\", \"trans\": \"protocol\"},\n    {\"word\": \"æ£€æŸ¥ç‚¹\", \"pinyin\": \"jiÇn chÃ¡ diÇn\", \"trans\": \"checkpoint\"},\n    {\"word\": \"ä¿ƒè¿›\", \"pinyin\": \"cÃ¹ jÃ¬n\", \"trans\": \"promote\"}\n]",
        "trans": "This article introduces an open-source foundational model called BAGEL. It is capable of uniformly processing and generating data in various modalities, such as text, images, and videos. BAGEL, through pre-training on large-scale interleaved multimodal data, demonstrates complex multimodal reasoning capabilities. It outperforms other open-source models in multimodal generation and understanding and possesses advanced multimodal reasoning abilities. The research team has made key findings, pre-training details, data creation protocols, and code and checkpoints publicly available to promote multimodal research.",
        "update_ts": "2025-05-21 09:13"
    }
}