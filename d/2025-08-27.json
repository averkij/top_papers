{
    "date": {
        "ru": "27 августа",
        "en": "August 27",
        "zh": "8月27日"
    },
    "time_utc": "2025-08-27 08:15",
    "weekday": 2,
    "issue_id": 5568,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.18124",
            "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
            "url": "https://huggingface.co/papers/2508.18124",
            "abstract": "CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench.",
            "score": 30,
            "issue_id": 5564,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "20823f1294d8217d",
            "authors": [
                "Weida Wang",
                "Dongchen Huang",
                "Jiatong Li",
                "Tengchao Yang",
                "Ziyang Zheng",
                "Di Zhang",
                "Dong Han",
                "Benteng Chen",
                "Binzhao Luo",
                "Zhiyu Liu",
                "Kunling Liu",
                "Zhiyuan Gao",
                "Shiqi Geng",
                "Wei Ma",
                "Jiaming Su",
                "Xin Li",
                "Shuchen Pu",
                "Yuhan Shui",
                "Qianjia Cheng",
                "Zhihao Dou",
                "Dongfei Cui",
                "Changyong He",
                "Jin Zeng",
                "Zeke Xie",
                "Mao Su",
                "Dongzhan Zhou",
                "Yuqiang Li",
                "Wanli Ouyang",
                "Yunqi Cai",
                "Xi Dai",
                "Shufei Zhang",
                "Lei Bai",
                "Jinguang Cheng",
                "Zhong Fang",
                "Hongming Weng"
            ],
            "affiliations": [
                "Beijing National Laboratory for Condensed Matter Physics and Institute of Physics, Chinese Academy of Sciences",
                "Condensed Matter Physics Data Center, Chinese Academy of Sciences",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shanghai Artificial Intelligence Laboratory",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18124.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#dataset"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый вызов для ИИ: решение задач по физике конденсированного состояния",
                    "desc": "CMPhysBench - это новый бенчмарк для оценки способностей больших языковых моделей (LLM) в области физики конденсированного состояния. Он содержит более 520 вопросов уровня аспирантуры, охватывающих различные подобласти и теоретические основы. Бенчмарк фокусируется на расчетных задачах, требуя от моделей самостоятельно генерировать полные решения. Введена новая метрика SEED для более точной оценки частичной правильности ответов."
                },
                "en": {
                    "title": "Assessing LLMs in Condensed Matter Physics with CMPhysBench",
                    "desc": "CMPhysBench is a new benchmark designed to evaluate the performance of Large Language Models (LLMs) in the field of Condensed Matter Physics. It consists of over 520 carefully selected graduate-level questions that cover key topics such as magnetism and superconductivity. The benchmark focuses on calculation problems, requiring LLMs to produce detailed solutions independently. To assess their performance, a novel scoring method called the Scalable Expression Edit Distance (SEED) score is introduced, which allows for partial credit and provides a nuanced evaluation of the models' outputs, revealing significant gaps in their capabilities."
                },
                "zh": {
                    "title": "评估大型语言模型在凝聚态物理中的能力差距",
                    "desc": "CMPhysBench是一个新颖的基准测试，旨在评估大型语言模型（LLMs）在凝聚态物理学中的能力。该基准包含520多个经过精心挑选的研究生水平问题，涵盖了凝聚态物理的代表性子领域和基础理论框架。我们专注于计算问题，要求LLMs独立生成全面的解决方案，并引入了可扩展表达编辑距离（SEED）分数，以提供更精细的部分评分。结果显示，即使是最好的模型Grok-4，在CMPhysBench上的平均SEED分数仅为36，准确率为28%，这表明在这一前沿领域存在显著的能力差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19205",
            "title": "VibeVoice Technical Report",
            "url": "https://huggingface.co/papers/2508.19205",
            "abstract": "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.",
            "score": 26,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "fc5dc3d2ea656b66",
            "authors": [
                "Zhiliang Peng",
                "Jianwei Yu",
                "Wenhui Wang",
                "Yaoyao Chang",
                "Yutao Sun",
                "Li Dong",
                "Yi Zhu",
                "Weijiang Xu",
                "Hangbo Bao",
                "Zehua Wang",
                "Shaohan Huang",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19205.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "VibeVoice: революция в синтезе длительной многоголосой речи",
                    "desc": "VibeVoice - это новая модель для синтеза длительной многоголосой речи, использующая диффузию следующего токена и эффективный непрерывный токенизатор речи. Модель способна генерировать до 90 минут речи с участием до 4 говорящих, сохраняя естественность разговора. По сравнению с популярной моделью Encodec, токенизатор VibeVoice улучшает сжатие данных в 80 раз при сопоставимом качестве. VibeVoice превосходит как открытые, так и проприетарные модели диалогов по качеству и достоверности синтезированной речи."
                },
                "en": {
                    "title": "VibeVoice: Revolutionizing Multi-Speaker Speech Synthesis",
                    "desc": "VibeVoice is a new model that creates long speeches with multiple speakers using a technique called next-token diffusion. This method generates speech by predicting the next part of the audio in a smart way, making it efficient for continuous data. The model also introduces a special speech tokenizer that compresses data much better than existing models, allowing it to maintain high audio quality while being faster. With VibeVoice, users can generate up to 90 minutes of realistic multi-speaker conversations, outperforming other dialogue systems."
                },
                "zh": {
                    "title": "VibeVoice：高效合成多说话人长语音的创新模型",
                    "desc": "VibeVoice是一种新型模型，旨在合成长时间的多说话人语音。它采用了下一步扩散的方法，通过自回归生成潜在向量来建模连续数据。为了实现这一点，我们引入了一种新型的连续语音标记器，与流行的Encodec模型相比，数据压缩提高了80倍，同时保持了相似的性能。VibeVoice能够合成最长可达90分钟的语音，捕捉真实的对话氛围，超越了开源和专有的对话模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17661",
            "title": "Spacer: Towards Engineered Scientific Inspiration",
            "url": "https://huggingface.co/papers/2508.17661",
            "abstract": "Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs.",
            "score": 19,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "5ecb3211ea5e7d8d",
            "authors": [
                "Minhyeong Lee",
                "Suyoung Hwang",
                "Seunghyun Moon",
                "Geonho Nah",
                "Donghyun Koh",
                "Youngjun Cho",
                "Johyun Park",
                "Hojin Yoo",
                "Jiho Park",
                "Haneul Choi",
                "Sungbin Moon",
                "Taehoon Hwang",
                "Seungwon Kim",
                "Jaeyeong Kim",
                "Seongjun Kim",
                "Juneau Jung"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.17661.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Spacer: Революция в автоматизированных научных открытиях",
                    "desc": "Spacer - это система научных открытий, использующая намеренную деконтекстуализацию для генерации креативных и фактически обоснованных научных концепций из наборов ключевых слов. Система состоит из двух компонентов: Nuri, создающего наборы ключевых слов, и Manifesting Pipeline, преобразующего эти наборы в развернутые научные утверждения. Эксперименты показали высокую точность классификации высокоимпактных публикаций и успешную реконструкцию ключевых концепций из статей ведущих журналов. Анализ векторного пространства выявил, что результаты Spacer значительно ближе к ведущим публикациям по сравнению с современными языковыми моделями."
                },
                "en": {
                    "title": "Spacer: Unleashing Creativity in Scientific Discovery",
                    "desc": "Spacer is a scientific discovery system that generates innovative and accurate scientific concepts by using a method called deliberate decontextualization. This process breaks down information into basic units, or keywords, and explores new connections between them to foster creativity. The system includes Nuri, which creates keyword sets from a vast database of academic publications, and the Manifesting Pipeline, which refines these sets into coherent scientific statements. Experimental results show that Spacer's outputs are highly similar to top-tier publications, outperforming existing language models in terms of relevance and originality."
                },
                "zh": {
                    "title": "Spacer：创新科学概念的发现系统",
                    "desc": "Spacer是一个科学发现系统，通过故意去上下文化的方法，从关键词集合中生成创造性且事实基础的科学概念。该系统将信息拆解为原子单位——关键词，并从它们之间未被探索的联系中汲取创造力。Spacer包括两个部分：Nuri，一个灵感引擎，用于构建关键词集合；以及Manifesting Pipeline，用于将这些集合精炼成详细的科学陈述。实验结果表明，Nuri能够准确分类高影响力的出版物，而Manifesting Pipeline能够成功重建最新顶级期刊文章的核心概念。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19209",
            "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation",
            "url": "https://huggingface.co/papers/2508.19209",
            "abstract": "A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive. Our model, OmniHuman-1.5, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: https://omnihuman-lab.github.io/v1_5/",
            "score": 18,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "a23b1da7bec45638",
            "authors": [
                "Jianwen Jiang",
                "Weihong Zeng",
                "Zerong Zheng",
                "Jiaqi Yang",
                "Chao Liang",
                "Wang Liao",
                "Han Liang",
                "Yuan Zhang",
                "Mingyuan Gao"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19209.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#optimization",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Семантически осмысленная анимация персонажей с помощью мультимодального ИИ",
                    "desc": "Эта статья представляет новую модель OmniHuman-1.5 для генерации анимации персонажей, которая использует мультимодальные языковые модели (Multimodal LLM) для создания семантически связных движений. Авторы предлагают специализированную архитектуру Multimodal DiT с дизайном Pseudo Last Frame для эффективного объединения мультимодальных входных данных. Модель способна интерпретировать совместную семантику аудио, изображений и текста, создавая движения, согласованные с характером персонажа, сценой и лингвистическим содержанием. Эксперименты показывают превосходную производительность модели по различным метрикам, включая точность синхронизации губ и семантическую согласованность с текстовыми подсказками."
                },
                "en": {
                    "title": "Animating Characters with Emotion and Context",
                    "desc": "This paper presents a framework called OmniHuman-1.5 that enhances character animation by integrating Multimodal Large Language Models with a specialized Multimodal DiT architecture. Unlike traditional models that focus on physical likeness, this framework aims to create animations that reflect deeper emotional and contextual understanding. By synthesizing structured textual representations, the model guides motion generation to produce actions that resonate with the character's intent and the surrounding context. The results show significant improvements in lip-sync accuracy, video quality, and overall motion naturalness, making it adaptable for complex scenarios involving multiple characters or non-human entities."
                },
                "zh": {
                    "title": "生成富有表现力的角色动画",
                    "desc": "本论文提出了一种框架，利用多模态大型语言模型和专门的多模态DiT架构，从多模态输入生成语义连贯且富有表现力的角色动画。现有的视频头像模型虽然能够生成流畅的人类动画，但往往无法捕捉角色的真实本质，动作多依赖于低级线索如音频节奏。我们的模型OmniHuman-1.5通过合成结构化的文本表示，提供高层次的语义指导，使得动作生成超越简单的节奏同步，能够产生与上下文和情感相符的动作。实验结果表明，该模型在唇同步精度、视频质量、动作自然性和与文本提示的语义一致性等多个指标上表现优异，且在复杂场景中具有良好的扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18756",
            "title": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning",
            "url": "https://huggingface.co/papers/2508.18756",
            "abstract": "UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.",
            "score": 16,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "baef9d7d7f92b5b0",
            "authors": [
                "Zihao Huang",
                "Yu Bao",
                "Qiyang Min",
                "Siyan Chen",
                "Ran Guo",
                "Hongzhi Huang",
                "Defa Zhu",
                "Yutao Zeng",
                "Banggu Wu",
                "Xun Zhou",
                "Siyuan Qiao"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18756.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "UltraMemV2: Эффективность MoE без высоких затрат на память",
                    "desc": "UltraMemV2 - это новая архитектура слоя памяти для нейронных сетей, которая достигает производительности 8-экспертных моделей Mixture of Experts (MoE) при значительно меньших затратах на доступ к памяти. Ключевые улучшения включают интеграцию слоев памяти в каждый блок трансформера, упрощение расширения значений и принципиальную инициализацию параметров. UltraMemV2 показывает превосходную производительность на задачах, требующих интенсивного использования памяти, таких как запоминание длинного контекста и обучение в контексте. Эта архитектура представляет собой перспективную альтернативу моделям MoE для эффективных разреженных вычислений."
                },
                "en": {
                    "title": "UltraMemV2: Bridging Memory Efficiency and Expert Performance",
                    "desc": "UltraMemV2 is a new memory-layer architecture designed to improve the efficiency of machine learning models by reducing memory access costs. It achieves performance similar to advanced 8-expert Mixture of Experts (MoE) models while using fewer resources. The architecture incorporates several enhancements, such as integrating memory layers into transformer blocks and optimizing value processing. This results in better performance on tasks that require extensive memory, making UltraMemV2 a strong contender for efficient sparse computation."
                },
                "zh": {
                    "title": "UltraMemV2：高效内存层架构的突破",
                    "desc": "UltraMemV2是一种重新设计的内存层架构，能够在显著降低内存访问成本的同时，实现与8专家混合专家（MoE）模型的性能相当。该架构通过将内存层集成到每个变换器块中、简化值扩展、采用基于前馈神经网络的值处理等五个关键改进，成功缩小了与高性能MoE模型之间的差距。经过广泛评估，UltraMemV2在相同计算和参数下，显著降低了内存访问，同时在内存密集型任务上表现优越。我们的研究表明，激活密度对性能的影响大于稀疏参数的总数，展示了内存层架构在高效稀疏计算中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17445",
            "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
            "url": "https://huggingface.co/papers/2508.17445",
            "abstract": "TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\\% up to 43\\% of the sampling design for the trained models, meanwhile showing up to 40\\% reduction at trajectory-level and 35\\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.",
            "score": 16,
            "issue_id": 5564,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 августа",
                "en": "August 24",
                "zh": "8月24日"
            },
            "hash": "1425b1aed1182fe6",
            "authors": [
                "Yizhi Li",
                "Qingshui Gu",
                "Zhoufutu Wen",
                "Ziniu Li",
                "Tianshun Xing",
                "Shuyue Guo",
                "Tianyu Zheng",
                "Xin Zhou",
                "Xingwei Qu",
                "Wangchunshu Zhou",
                "Zheng Zhang",
                "Wei Shen",
                "Qian Liu",
                "Chenghua Lin",
                "Jian Yang",
                "Ge Zhang",
                "Wenhao Huang"
            ],
            "affiliations": [
                "ByteDance",
                "M-A-P",
                "UoM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17445.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "TreePO: Эффективное обучение с подкреплением для языковых моделей",
                    "desc": "Статья представляет TreePO - алгоритм самонаправляемой генерации последовательностей для обучения с подкреплением больших языковых моделей. TreePO рассматривает генерацию как древовидный процесс поиска, используя динамическую политику выборки дерева и декодирование сегментов фиксированной длины. Алгоритм снижает вычислительные затраты, сохраняя или улучшая разнообразие исследования пространства решений. TreePO показывает значительное повышение эффективности на тестах рассуждений и экономию вычислительных ресурсов до 43%."
                },
                "en": {
                    "title": "TreePO: Efficient Exploration in Sequence Generation",
                    "desc": "TreePO is a novel self-guided rollout algorithm designed to improve sequence generation in reinforcement learning for large language models. It treats sequence generation as a tree search, allowing for better exploration of diverse reasoning paths while reducing computational costs. By utilizing dynamic tree sampling and fixed-length segment decoding, TreePO efficiently manages computation and enhances exploration through local uncertainty. The algorithm demonstrates significant efficiency gains, reducing GPU usage by up to 43% and improving performance on reasoning benchmarks."
                },
                "zh": {
                    "title": "TreePO：高效的序列生成与探索多样性",
                    "desc": "TreePO是一种自指导的回滚算法，旨在提高大语言模型在强化学习中的计算效率和探索多样性。它将序列生成视为树状搜索过程，通过动态树采样策略和固定长度段解码来实现。TreePO利用局部不确定性来生成额外的分支，并通过在公共前缀上分摊计算和提前修剪低价值路径，显著降低每次更新的计算负担。实验结果表明，TreePO在推理基准测试中表现出色，GPU计算效率提高了22%到43%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19247",
            "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
            "url": "https://huggingface.co/papers/2508.19247",
            "abstract": "VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.",
            "score": 9,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "1b1c0dd833778383",
            "authors": [
                "Lin Li",
                "Zehuan Huang",
                "Haoran Feng",
                "Gengxiong Zhuang",
                "Rui Chen",
                "Chunchao Guo",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19247.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🔨",
                "ru": {
                    "title": "Точное 3D-редактирование без компромиссов",
                    "desc": "VoxHammer - это метод редактирования 3D-моделей в латентном пространстве без дополнительного обучения. Он обеспечивает точное и согласованное редактирование указанных областей, сохраняя при этом неизмененные части модели. VoxHammer предсказывает траекторию инверсии 3D-модели и использует инвертированные латентные представления и токены для сохранения контекстуальных особенностей. Эксперименты показывают, что метод превосходит существующие подходы по качеству и согласованности результатов редактирования."
                },
                "en": {
                    "title": "VoxHammer: Precision and Coherence in 3D Latent Space Editing",
                    "desc": "VoxHammer is a novel method for editing 3D models in latent space without the need for training. It focuses on maintaining the consistency of unedited regions while ensuring high-quality results in the edited areas. By predicting an inversion trajectory and utilizing key-value tokens, VoxHammer effectively integrates changes while preserving contextual features. This approach outperforms existing techniques, making it valuable for applications in the gaming industry and robotics."
                },
                "zh": {
                    "title": "VoxHammer：无训练的精确3D编辑新方法",
                    "desc": "VoxHammer是一种无需训练的方法，能够在潜在空间中进行精确且连贯的3D编辑。该方法确保了保留区域的一致性和整体结果的高质量。与传统方法不同，VoxHammer直接在3D潜在空间中进行编辑，避免了多视图图像渲染和重建模型的复杂性。实验结果表明，VoxHammer在保留区域的3D一致性和整体质量方面显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17437",
            "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
            "url": "https://huggingface.co/papers/2508.17437",
            "abstract": "PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  \t\t\t\t\tAI-generated summary \t\t\t\t Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/",
            "score": 9,
            "issue_id": 5563,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 августа",
                "en": "August 20",
                "zh": "8月20日"
            },
            "hash": "113a80a24546f00c",
            "authors": [
                "Long Le",
                "Ryan Lucas",
                "Chen Wang",
                "Chuhao Chen",
                "Dinesh Jayaraman",
                "Eric Eaton",
                "Lingjie Liu"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17437.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#inference",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Быстрое предсказание физических свойств 3D-сцен с помощью нейронных сетей",
                    "desc": "PIXIE - это нейросетевой метод, который предсказывает физические свойства 3D-сцен на основе визуальных признаков. Он использует обучение с учителем и предобученные визуальные признаки для быстрого и реалистичного физического моделирования. PIXIE превосходит методы оптимизации во время теста по точности и скорости работы. Метод способен к обобщению на реальные сцены, несмотря на обучение только на синтетических данных."
                },
                "en": {
                    "title": "Fast and Realistic Physics Simulation with PIXIE",
                    "desc": "PIXIE is a neural network approach designed to predict the physical properties of 3D scenes from visual features, which enhances the speed and realism of physics simulations. Unlike traditional methods that require slow optimization for each scene, PIXIE uses supervised learning to create a generalizable model that can infer material characteristics across various environments. The model is trained on a large dataset called PIXIEVERSE, which includes 3D assets and their corresponding physical property annotations. By utilizing pretrained visual features, PIXIE can also apply its knowledge to real-world scenes, achieving significant improvements in performance and efficiency compared to existing methods."
                },
                "zh": {
                    "title": "PIXIE：快速预测三维场景物理属性的神经网络",
                    "desc": "PIXIE是一种神经网络方法，可以从视觉特征中预测三维场景的物理属性，从而实现快速而真实的物理模拟。该方法通过监督学习和预训练的视觉特征，克服了传统方法在每个场景上优化的慢速限制。PIXIE训练出一个可泛化的神经网络，能够在多个场景中预测物理属性，并且在推理时速度极快。通过结合学习到的静态场景表示，PIXIE能够在外力作用下进行真实的物理模拟。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19242",
            "title": "Autoregressive Universal Video Segmentation Model",
            "url": "https://huggingface.co/papers/2508.19242",
            "abstract": "AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences.",
            "score": 6,
            "issue_id": 5564,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "d20ae3e4567983a7",
            "authors": [
                "Miran Heo",
                "Sukjun Hwang",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Albert Gu",
                "Seon Joo Kim",
                "Ryo Hachiuma"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "NVIDIA",
                "National Taiwan University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19242.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Универсальная сегментация видео как последовательное предсказание масок",
                    "desc": "AUSM - это модель универсальной сегментации видео, объединяющая подходы с подсказками и без них. Она рассматривает задачу как последовательное предсказание масок, аналогично языковому моделированию. AUSM превосходит существующие методы на стандартных бенчмарках и обучается быстрее. Модель основана на недавних достижениях в области моделей пространства состояний и масштабируется на видеопотоки произвольной длины."
                },
                "en": {
                    "title": "Unifying Video Segmentation with AUSM: Fast and Flexible!",
                    "desc": "The Autoregressive Universal Segmentation Model (AUSM) is a novel approach that combines prompted and unprompted video segmentation into a single framework. By treating video segmentation as sequential mask prediction, AUSM leverages techniques similar to language modeling, allowing it to effectively detect and track objects without needing external prompts. This model is built on state-space architectures, enabling it to handle video streams of any length while maintaining a fixed-size spatial state. AUSM also features parallel training across frames, resulting in significant speed improvements and superior performance on various standard benchmarks."
                },
                "zh": {
                    "title": "自回归通用分割模型：统一视频分割的未来",
                    "desc": "AUSM是一种自回归通用分割模型，通过将视频分割视为顺序掩码预测，统一了有提示和无提示的视频分割。该模型在标准基准测试中表现优异，并且训练速度更快。AUSM基于最新的状态空间模型，能够处理任意长度的视频流，并保持固定大小的空间状态。所有组件都设计为可以在帧之间并行训练，从而显著提高训练速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18621",
            "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
            "url": "https://huggingface.co/papers/2508.18621",
            "abstract": "Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.",
            "score": 6,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "0e23abc799529ace",
            "authors": [
                "Xin Gao",
                "Li Hu",
                "Siqi Hu",
                "Mingyang Huang",
                "Chaonan Ji",
                "Dechao Meng",
                "Jinwei Qi",
                "Penchong Qiao",
                "Zhen Shen",
                "Yafei Song",
                "Ke Sun",
                "Linrui Tian",
                "Guangyuan Wang",
                "Qi Wang",
                "Zhongjian Wang",
                "Jiayu Xiao",
                "Sheng Xu",
                "Bang Zhang",
                "Peng Zhang",
                "Xindi Zhang",
                "Zhe Zhang",
                "Jingren Zhou",
                "Lian Zhuo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18621.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#audio",
                    "#games",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Wan-S2V: Революция в анимации кинематографических персонажей на основе аудио",
                    "desc": "Модель Wan-S2V, основанная на аудио, улучшает выразительность и точность анимации кинематографических персонажей по сравнению с существующими методами. Она преодолевает ограничения современных подходов в сложных сценариях фильмов и телепередач, требующих нюансированного взаимодействия персонажей и реалистичных движений тела. Экспериментальные результаты показывают, что Wan-S2V значительно превосходит передовые модели, такие как Hunyuan-Avatar и Omnihuman. Модель также демонстрирует универсальность в генерации длинных видео и точном редактировании синхронизации губ."
                },
                "en": {
                    "title": "Elevating Cinematic Animation with Wan-S2V",
                    "desc": "The paper introduces Wan-S2V, an advanced audio-driven model designed to improve character animation in cinematic contexts. Unlike current state-of-the-art methods that excel mainly in speech and singing, Wan-S2V addresses the complexities of film and television, including intricate character interactions and realistic movements. Through rigorous benchmarking against leading models like Hunyuan-Avatar and Omnihuman, the results show that Wan-S2V significantly enhances expressiveness and fidelity. Furthermore, the model's versatility is highlighted through its applications in long-form video generation and accurate lip-sync editing."
                },
                "zh": {
                    "title": "提升电影角色动画的音频驱动模型",
                    "desc": "Wan-S2V是一种基于音频驱动的模型，旨在提升电影角色动画的表现力和真实感。与现有方法相比，它在复杂的影视制作中表现更佳，能够处理细腻的角色互动、真实的身体动作和动态的镜头运作。我们通过与最先进的模型进行广泛实验，证明了Wan-S2V在动画效果上的显著优势。此外，该模型还具有在长视频生成和精确视频口型同步编辑中的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15774",
            "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
            "url": "https://huggingface.co/papers/2508.15774",
            "abstract": "CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.",
            "score": 6,
            "issue_id": 5563,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 августа",
                "en": "August 21",
                "zh": "8月21日"
            },
            "hash": "520a6e202fa3979c",
            "authors": [
                "Haonan Qiu",
                "Ning Yu",
                "Ziqi Huang",
                "Paul Debevec",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Netflix Eyeline Studios",
                "Scanline VFX"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15774.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#inference",
                    "#cv",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "CineScale: Прорыв в генерации высококачественного визуального контента",
                    "desc": "CineScale - это новая парадигма вывода, позволяющая генерировать изображения и видео высокого разрешения без обширной дополнительной настройки. Она решает проблемы повторяющихся паттернов и накопления высокочастотной информации, характерные для существующих методов. CineScale предлагает специализированные варианты для различных архитектур генерации видео, расширяя возможности создания визуального контента высокого разрешения. Эксперименты подтверждают превосходство этого подхода, позволяя генерировать 8K изображения без дополнительной настройки и 4K видео с минимальной настройкой LoRA."
                },
                "en": {
                    "title": "CineScale: Elevating Visual Generation to New Resolutions",
                    "desc": "CineScale is a new method for generating high-resolution images and videos without needing extensive adjustments to existing models. It addresses common problems like repetitive patterns and the buildup of high-frequency details that can occur when generating visuals at resolutions higher than those used during training. By introducing specialized versions of the model for different types of video generation, CineScale enhances the capabilities of pre-trained models, allowing for high-quality outputs. The results show that it can produce 8k images and 4k videos with minimal fine-tuning, significantly improving visual fidelity."
                },
                "zh": {
                    "title": "CineScale：高分辨率视觉生成的新范式",
                    "desc": "CineScale是一种新颖的推理范式，能够在不进行大量微调的情况下实现高分辨率的图像和视频生成。该方法解决了生成内容中重复模式和高频信息积累的问题，提升了视觉生成的质量。通过专门设计的变体，CineScale扩展了高分辨率图像到视频的合成能力，超越了现有的基线方法。实验结果表明，CineScale在高分辨率视觉生成方面具有显著优势，能够生成8k图像和4k视频。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15804",
            "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
            "url": "https://huggingface.co/papers/2508.15804",
            "abstract": "ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench",
            "score": 5,
            "issue_id": 5566,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "c52252190b40f5fb",
            "authors": [
                "Minghao Li",
                "Ying Zeng",
                "Zhihao Cheng",
                "Cong Ma",
                "Kai Jia"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15804.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#survey",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "ReportBench: новый стандарт оценки AI-исследований",
                    "desc": "ReportBench - это новый метод оценки качества исследовательских отчетов, сгенерированных большими языковыми моделями (LLM). Он фокусируется на качестве цитируемой литературы и достоверности утверждений в отчетах. ReportBench использует высококачественные обзорные статьи с arXiv в качестве эталона и применяет обратную инженерию промптов для создания корпуса оценки. Результаты показывают, что коммерческие агенты Deep Research генерируют более надежные отчеты, чем standalone LLM, хотя все еще есть возможности для улучшения."
                },
                "en": {
                    "title": "Evaluating Research Quality in AI-Generated Reports",
                    "desc": "ReportBench is a new tool that assesses the quality of research reports created by large language models (LLMs). It focuses on two main aspects: the quality of the literature cited and the accuracy of the statements made in the reports. By using high-quality survey papers as references, ReportBench checks if the generated content is both comprehensive and trustworthy. The findings show that advanced Deep Research agents outperform standalone LLMs in producing reliable reports, but there is still a need for improvement in research coverage and factual accuracy."
                },
                "zh": {
                    "title": "评估AI生成研究报告的质量",
                    "desc": "本论文提出了ReportBench，这是一个系统化的基准，用于评估大型语言模型生成的研究报告的内容质量。评估主要集中在两个方面：引用文献的质量和相关性，以及生成报告中陈述的真实性和可靠性。研究表明，商业深度研究代理（如OpenAI和Google开发的）生成的报告比独立的语言模型更全面和可靠。尽管如此，研究覆盖的广度和深度以及事实一致性仍有很大的改进空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18773",
            "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2508.18773",
            "abstract": "ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.",
            "score": 3,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "2f5e648d646cae5b",
            "authors": [
                "Qianyu He",
                "Siyu Yuan",
                "Xuefeng Li",
                "Mingxuan Wang",
                "Jiangjie Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University",
                "SIA-Lab of Tsinghua AIR",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18773.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#agi",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ThinkDial: контролируемое рассуждение в больших языковых моделях",
                    "desc": "ThinkDial - это фреймворк с открытым исходным кодом, который реализует контролируемое рассуждение в больших языковых моделях через дискретные режимы работы. Система позволяет переключаться между тремя режимами рассуждения: полным, средним (50% сокращение токенов) и низким (75% сокращение). Это достигается с помощью сквозной парадигмы обучения, включающей контролируемую тонкую настройку и обучение с подкреплением. ThinkDial демонстрирует хорошие результаты по соотношению сжатия и производительности, а также обобщающую способность на новых задачах."
                },
                "en": {
                    "title": "ThinkDial: Control Your AI's Thinking Power!",
                    "desc": "ThinkDial is an innovative open-source framework designed to enhance large language models (LLMs) by enabling controllable reasoning through discrete operational modes. It allows users to switch between three reasoning modes: High, Medium, and Low, which vary in computational effort and performance. This framework incorporates budget-mode control during training, ensuring that reasoning capabilities are embedded into the model's learning process. Extensive testing shows that ThinkDial effectively balances performance and efficiency, making it a valuable tool for practical applications of LLMs."
                },
                "zh": {
                    "title": "ThinkDial：可控推理的新开源框架",
                    "desc": "ThinkDial是一个开源框架，旨在通过离散操作模式实现大型语言模型的可控推理。该系统允许在三种不同的推理模式之间无缝切换：高模式（完全推理能力）、中模式（减少50%的令牌，性能下降不到10%）和低模式（减少75%的令牌，性能下降不到15%）。通过端到端的训练方法，ThinkDial在整个流程中集成了预算模式控制，确保了推理能力的可控性。实验结果表明，ThinkDial在压缩性能权衡方面表现出色，同时在处理超出分布的任务时也展现了强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18672",
            "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
            "url": "https://huggingface.co/papers/2508.18672",
            "abstract": "MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.",
            "score": 2,
            "issue_id": 5566,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "d56645cd1da5b6a9",
            "pdf_title_img": "assets/pdf/title_img/2508.18672.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Разреженность в MoE: компромисс между запоминанием и рассуждением",
                    "desc": "Статья исследует влияние разреженности в моделях Mixture-of-Experts (MoE) на способности больших языковых моделей к запоминанию и рассуждению. Авторы обнаружили, что увеличение общего числа параметров улучшает запоминание, но может ухудшать способность к рассуждениям. Изменение параметра top-k маршрутизации мало влияет при постоянном числе активных параметров. Исследование показывает, что чрезмерная разреженность может негативно сказываться на некоторых аспектах производительности языковых моделей."
                },
                "en": {
                    "title": "Balancing Memorization and Reasoning in Sparse MoE Models",
                    "desc": "This paper explores how Mixture-of-Experts (MoE) models, which introduce sparsity in large language models (LLMs), impact their ability to memorize and reason. While increasing the number of parameters improves memorization performance, reasoning capabilities can actually decline despite these gains. The authors systematically analyze various configurations of MoE Transformers, focusing on how active parameters and routing strategies affect model performance. Their findings indicate that traditional hyperparameters and post-training techniques do not effectively address the reasoning deficits caused by excessive sparsity in these models."
                },
                "zh": {
                    "title": "MoE模型：稀疏性对记忆与推理的双重影响",
                    "desc": "MoE模型引入了稀疏性，这对大型语言模型的记忆和推理能力产生了不同的影响。尽管参数增加，推理性能可能会下降，而记忆能力则随着总参数的增加而持续改善。我们研究了MoE稀疏性如何影响记忆和推理这两种能力模式，并通过训练不同参数配置的MoE Transformer来分析其效果。结果表明，经典超参数如学习率和初始化对泛化差距的影响与稀疏性方向一致，但过于稀疏的模型在推理能力上存在明显不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18370",
            "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
            "url": "https://huggingface.co/papers/2508.18370",
            "abstract": "CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.",
            "score": 2,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "cd60ce12b233220d",
            "authors": [
                "Terry Yue Zhuo",
                "Dingmin Wang",
                "Hantian Ding",
                "Varun Kumar",
                "Zijian Wang"
            ],
            "affiliations": [
                "Amazon",
                "Monash University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18370.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#games",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "CTF-Dojo: революция в обучении ИИ-агентов через исполняемую среду",
                    "desc": "CTF-Dojo - это крупномасштабная исполняемая среда с 658 задачами CTF, позволяющая быстро обучать агентов на основе больших языковых моделей с проверяемой обратной связью. Разработанный авторами конвейер CTF-Forge автоматизирует процесс создания сред выполнения из общедоступных артефактов. Обучение на всего 486 высококачественных траекториях из CTF-Dojo позволило достичь значительных улучшений по сравнению с сильными базовыми моделями в нескольких конкурентных бенчмарках. Лучшая 32-миллиардная модель авторов достигла нового открытого state-of-the-art результата, соперничая с передовыми моделями."
                },
                "en": {
                    "title": "CTF-Dojo: Revolutionizing LLM Training with Executable Challenges",
                    "desc": "CTF-Dojo is a large-scale platform designed to train large language models (LLMs) using executable runtime environments. It features 658 Capture-The-Flag (CTF) challenges that provide verifiable feedback, which is crucial for improving the performance of machine learning agents. The introduction of CTF-Forge allows for the rapid creation of these training environments from publicly available resources, significantly reducing setup time. As a result, LLM-based agents trained on CTF-Dojo have achieved state-of-the-art performance on competitive benchmarks, showcasing the effectiveness of execution-grounded training methods."
                },
                "zh": {
                    "title": "CTF-Dojo：高效训练智能体的新平台",
                    "desc": "CTF-Dojo是一个大型可执行运行环境，包含658个CTF挑战，旨在快速训练基于大语言模型（LLM）的智能体，并提供可验证的反馈。该平台通过自动化管道CTF-Forge，将公开可用的资源转化为可用的执行环境，显著减少了配置时间。研究表明，使用CTF-Dojo训练的智能体在多个基准测试中表现优异，取得了最高11.6%的绝对提升。CTF-Dojo展示了执行基础训练信号在提升高性能机器学习智能体方面的重要性，且不依赖于昂贵的专有系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16697",
            "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features\n  for No-Regret Rewriting",
            "url": "https://huggingface.co/papers/2508.16697",
            "abstract": "QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation.",
            "score": 2,
            "issue_id": 5564,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 августа",
                "en": "August 22",
                "zh": "8月22日"
            },
            "hash": "33856afd1b846ac5",
            "authors": [
                "Nicole Cho",
                "William Watson",
                "Alec Koppel",
                "Sumitra Ganesh",
                "Manuela Veloso"
            ],
            "affiliations": [
                "JP Morgan AI Research, New York, NY"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16697.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#hallucinations",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умное переписывание запросов побеждает галлюцинации ИИ",
                    "desc": "QueryBandits - это фреймворк, использующий алгоритмы многоруких бандитов для снижения галлюцинаций в больших языковых моделях (LLM). Он переписывает запросы на основе лингвистических особенностей, чтобы предотвратить генерацию недостоверной информации. Эксперименты показали, что QueryBandits превосходит статические стратегии переформулирования запросов на различных наборах данных для вопросно-ответных систем. Исследование подтверждает эффективность адаптивного переписывания запросов для уменьшения галлюцинаций в LLM."
                },
                "en": {
                    "title": "Proactive Query Rewriting to Combat Hallucinations in LLMs",
                    "desc": "The paper introduces QueryBandits, a novel framework designed to reduce hallucinations in Large Language Models (LLMs) by rewriting input queries based on linguistic features. Unlike traditional methods that filter out hallucinations after they occur, QueryBandits proactively modifies queries to minimize the likelihood of generating incorrect information. The framework employs a reward model that evaluates the potential for hallucination based on 17 linguistic characteristics, leading to improved performance across various question-answering benchmarks. Experimental results show that QueryBandits significantly outperforms static prompting techniques, demonstrating the importance of dynamic query rewriting in enhancing LLM reliability."
                },
                "zh": {
                    "title": "QueryBandits：主动重写查询以减少幻觉现象",
                    "desc": "QueryBandits 是一个新的框架，旨在通过根据语言特征主动重写查询来减少大型语言模型（LLMs）中的幻觉现象。与传统的静态提示策略相比，QueryBandits 通过设计重写策略来最大化奖励模型，从而有效地引导 LLMs 避免生成幻觉。实验结果显示，QueryBandits 在多个问答基准测试中表现优异，显著提高了模型的准确性。该研究表明，利用语义特征进行引导重写可以显著改善输出行为，而无需重新训练模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19188",
            "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
            "url": "https://huggingface.co/papers/2508.19188",
            "abstract": "A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8times faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.",
            "score": 1,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "31ec4c8e0ef6b3e8",
            "authors": [
                "Jeonghwan Kim",
                "Yushi Lan",
                "Armando Fortes",
                "Yongwei Chen",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19188.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Эффективная генерация 3D-моделей: разделяй и властвуй",
                    "desc": "Предложена эффективная система генерации художественных 3D-моделей, разделяющая процессы создания вершин и граней. Для вершин используется авторегрессионная модель, а для граней - двунаправленный трансформер, что значительно сокращает избыточность. Введены улучшатель точности для оптимизации расположения вершин и постобработка для устранения нежелательных соединений рёбер. Эксперименты показали 8-кратное ускорение генерации и повышение качества моделей по сравнению с современными методами."
                },
                "en": {
                    "title": "Efficient Artistic Mesh Generation: Less Redundancy, More Quality!",
                    "desc": "This paper presents a new framework for generating artistic meshes that improves efficiency by separating the processes of vertex and face generation. It uses an autoregressive model to generate vertices, which reduces the number of tokens needed by about 77% compared to traditional methods. For face generation, a bidirectional transformer is employed to quickly construct the mesh by understanding the relationships between vertices. Additionally, a fidelity enhancer and post-processing techniques are introduced to enhance the quality and speed of the generated meshes."
                },
                "zh": {
                    "title": "高效艺术网格生成的新框架",
                    "desc": "本文提出了一种高效的艺术网格生成框架，通过将顶点和面生成分开，减少了冗余。我们使用自回归模型生成顶点，显著降低了所需的令牌数量。接着，利用双向变换器一次性完成网格构建，捕捉顶点间的关系。最后，通过引入保真度增强器和后处理框架，进一步提高生成质量和速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19026",
            "title": "MovieCORE: COgnitive REasoning in Movies",
            "url": "https://huggingface.co/papers/2508.19026",
            "abstract": "MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.",
            "score": 1,
            "issue_id": 5564,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "ad2f2e8c67dd7dd8",
            "authors": [
                "Gueter Josmy Faure",
                "Min-Hung Chen",
                "Jia-Fong Yeh",
                "Ying Cheng",
                "Hung-Ting Su",
                "Yung-Hao Tang",
                "Shang-Hong Lai",
                "Winston H. Hsu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Chengchi University",
                "National Taiwan University",
                "National Tsing Hua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19026.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#reasoning",
                    "#alignment",
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MovieCORE: Глубокое понимание фильмов с помощью ИИ",
                    "desc": "MovieCORE - это новый набор данных для ответов на вопросы по видео, который использует несколько больших языковых моделей для генерации глубоких когнитивных вопросов. Он фокусируется на вопросах, требующих системного мышления 2-го типа, в отличие от существующих наборов данных, ориентированных на поверхностное понимание. Авторы представляют инновационный подход с использованием ЛЛМ в качестве агентов мышления для создания и уточнения пар вопрос-ответ. Также предложен модуль агентного улучшения ACE, который повышает способности моделей к рассуждению после обучения на 25%."
                },
                "en": {
                    "title": "Deepening Movie Understanding with MovieCORE",
                    "desc": "MovieCORE is a new video question answering (VQA) dataset that focuses on deeper cognitive understanding of movies. It uses multiple large language models to create complex questions that require advanced reasoning, rather than just surface-level comprehension. The paper introduces an agentic enhancement module called Agentic Choice Enhancement (ACE) that significantly boosts the performance of VQA models by improving their reasoning abilities. This work aims to enhance AI's understanding of cinematic content and provides a framework for evaluating VQA models on more challenging questions."
                },
                "zh": {
                    "title": "深度认知电影问答的新突破",
                    "desc": "MovieCORE是一个新的视频问答数据集，旨在深入探讨电影内容的认知理解。与现有数据集不同，MovieCORE强调需要系统二思维的问题，专注于视频材料。我们采用多种大型语言模型作为思维代理，生成和优化高质量的问题-答案对。为了评估数据集的质量，我们开发了一套认知测试，评估问题的深度、思维激发潜力和句法复杂性，同时提出了一个全面的评估方案来评估VQA模型在更深层次认知任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18271",
            "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2508.18271",
            "abstract": "ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .",
            "score": 1,
            "issue_id": 5565,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "7d474b5d9c6dd1d2",
            "authors": [
                "Haitang Feng",
                "Jie Liu",
                "Jie Tang",
                "Gangshan Wu",
                "Beiqi Chen",
                "Jianhuang Lai",
                "Guangcong Wang"
            ],
            "affiliations": [
                "Great Bay University",
                "Harbin Institute of Technology",
                "Nanjing University",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18271.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в 3D-инпейнтинге: от видео к реалистичным объектам",
                    "desc": "ObjFiller-3D - это новый метод для заполнения и редактирования трехмерных объектов высокого качества. Он использует модели редактирования видео для достижения согласованной реконструкции 3D-объектов, преодолевая ограничения традиционных методов 2D-инпейнтинга. ObjFiller-3D анализирует разрыв между 3D и видео представлениями и адаптирует модель видеоинпейнтинга для заполнения 3D-сцен. Эксперименты показывают, что этот метод превосходит предыдущие подходы по точности реконструкции и имеет большой потенциал для практического применения в редактировании 3D-объектов."
                },
                "en": {
                    "title": "Revolutionizing 3D Object Completion with Video Editing Techniques",
                    "desc": "ObjFiller-3D is a new method for completing and editing 3D objects using advanced video editing models. It addresses the common issues of blurred textures and visual artifacts that arise from traditional 2D image inpainting methods. By adapting video inpainting techniques, ObjFiller-3D achieves higher reconstruction fidelity and structural coherence in 3D object completion. Experiments show that it significantly outperforms previous methods, making it suitable for real-world 3D editing applications."
                },
                "zh": {
                    "title": "高质量3D物体补全的新方法",
                    "desc": "ObjFiller-3D是一种新颖的方法，旨在实现高质量和一致性的3D物体补全。与传统的2D图像修复模型不同，该方法利用先进的视频编辑模型来填补3D物体的遮挡区域。通过分析3D与视频之间的表示差距，ObjFiller-3D能够有效克服多视角修复中的不一致性问题。实验结果表明，该方法在重建精度和实际应用潜力方面均优于以往的技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19202",
            "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
            "url": "https://huggingface.co/papers/2508.19202",
            "abstract": "SciReas and SciReas-Pro benchmarks, along with KRUX framework, provide insights into the distinct roles of knowledge and reasoning in scientific tasks, highlighting critical bottlenecks and improvements for LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.",
            "score": 0,
            "issue_id": 5567,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "fe91970689f2e015",
            "authors": [
                "Alan Li",
                "Yixin Liu",
                "Arpan Sarkar",
                "Doug Downey",
                "Arman Cohan"
            ],
            "affiliations": [
                "Allen Institute of AI",
                "Harvard University",
                "Northwestern University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19202.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#science"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Разделяй и властвуй: новый подход к оценке научного мышления ЯМ",
                    "desc": "Статья представляет новые бенчмарки SciReas и SciReas-Pro для оценки научного мышления у языковых моделей. Авторы также предлагают фреймворк KRUX для анализа роли знаний и рассуждений в научных задачах. Исследование показывает, что извлечение релевантной информации из параметров модели является узким местом для ЯМ в научном мышлении. Обнаружено, что улучшение вербализованных рассуждений повышает способность ЯМ извлекать релевантные знания."
                },
                "en": {
                    "title": "Unlocking Scientific Reasoning in LLMs with SciReas and KRUX",
                    "desc": "The paper introduces SciReas and SciReas-Pro, benchmarks designed to evaluate the performance of large language models (LLMs) in scientific reasoning tasks. It highlights the importance of both knowledge retrieval and reasoning capabilities, identifying key challenges that LLMs face in these areas. The KRUX framework is proposed to analyze how knowledge and reasoning interact in scientific problem-solving. The findings suggest that improving knowledge retrieval and enhancing reasoning processes can significantly boost the effectiveness of LLMs in scientific contexts."
                },
                "zh": {
                    "title": "揭示科学推理中的知识与推理角色",
                    "desc": "本论文介绍了SciReas和SciReas-Pro基准，以及KRUX框架，旨在揭示知识和推理在科学任务中的不同角色。科学问题解决对大型语言模型（LLMs）提出了独特的挑战，需要深厚的领域知识和复杂的推理能力。我们提出的SciReas是一个多样化的基准套件，而SciReas-Pro则是一个需要更复杂推理的子集。通过综合评估，我们发现知识检索和推理模型的外部知识增强对科学推理的性能至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18192",
            "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
            "url": "https://huggingface.co/papers/2508.18192",
            "abstract": "A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.",
            "score": 0,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "6ce511ea51abb060",
            "authors": [
                "Kushal Raj Bhandari",
                "Pin-Yu Chen",
                "Jianxi Gao"
            ],
            "affiliations": [
                "IBM Research",
                "Rensselaer Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18192.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#science",
                    "#training",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Сетевой анализ раскрывает когнитивные паттерны в языковых моделях",
                    "desc": "Данная статья представляет сетевую модель, связывающую когнитивные навыки, архитектуры языковых моделей (LLM) и наборы данных. Исследование выявляет уникальные паттерны появления навыков в LLM, которые выигрывают от динамических взаимодействий между различными областями модели. Авторы обнаружили, что распределение навыков в модулях LLM частично отражает распределенную, но взаимосвязанную когнитивную организацию, наблюдаемую в мозге птиц и мелких млекопитающих. Результаты подчеркивают важность пластичности и межрегиональных взаимодействий в процессе обучения LLM, что отличает их от биологических систем."
                },
                "en": {
                    "title": "Unlocking LLMs: A Networked Approach to Cognitive Skills and Architecture",
                    "desc": "This paper presents a network-based framework that connects cognitive skills with the architectures of Large Language Models (LLMs) and the datasets they are trained on. It reveals that LLMs exhibit unique emergent skill patterns that are influenced by dynamic interactions across different regions of their architecture, similar to cognitive processes in biological systems. The study highlights that while LLMs do not strictly mimic the specialization found in biological brains, they show a distributed and interconnected organization of skills. The findings suggest that understanding these emergent skills can improve LLM interpretability and inform better fine-tuning strategies that utilize flexible learning dynamics."
                },
                "zh": {
                    "title": "揭示大型语言模型的认知技能与架构的联系",
                    "desc": "这篇论文提出了一个基于网络的框架，连接了认知技能、语言模型架构和数据集，揭示了大型语言模型（LLMs）中独特的技能模式。这些模型在动态的跨区域交互中受益，展现出与生物系统不同的技能获取方式。研究表明，LLMs的模块社区虽然不完全与特定生物系统的专业化相似，但它们的技能模式部分反映了鸟类和小型哺乳动物大脑的分布式认知组织。通过将认知科学原理与机器学习结合，该框架为LLMs的可解释性提供了新见解，并建议有效的微调策略应利用分布式学习动态，而非僵化的模块干预。"
                }
            }
        }
    ],
    "link_prev": "2025-08-26.html",
    "link_next": "2025-08-28.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "26.08",
        "en": "08/26",
        "zh": "8月26日"
    },
    "short_date_next": {
        "ru": "28.08",
        "en": "08/28",
        "zh": "8月28日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 3,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 4,
        "#audio": 2,
        "#video": 6,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 4,
        "#low_resource": 0
    }
}