{
    "date": {
        "ru": "10 февраля",
        "en": "February 10",
        "zh": "2月10日"
    },
    "time_utc": "2025-02-10 04:13",
    "weekday": 0,
    "issue_id": 2118,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.05173",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "url": "https://huggingface.co/papers/2502.05173",
            "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "score": 9,
            "issue_id": 2118,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 февраля",
                "en": "February 7",
                "zh": "2月7日"
            },
            "hash": "ba284ed1a62b3c2c",
            "authors": [
                "Xilin Wei",
                "Xiaoran Liu",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Jian Tong",
                "Haodong Duan",
                "Qipeng Guo",
                "Jiaqi Wang",
                "Xipeng Qiu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05173.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VideoRoPE: Эффективное позиционное кодирование для глубокого обучения на видео",
                    "desc": "Статья представляет VideoRoPE - новый метод позиционного кодирования для видео, основанный на Rotary Position Embedding. Авторы провели анализ и выявили 4 ключевые характеристики для эффективной адаптации RoPE к видео. Они предложили сложную задачу V-NIAH-D для демонстрации недостатков существующих вариантов RoPE. VideoRoPE имеет 3D-структуру, сохраняющую пространственно-временные отношения, и превосходит предыдущие варианты RoPE в различных задачах обработки видео."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VideoRoPE",
                    "desc": "This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data."
                },
                "zh": {
                    "title": "VideoRoPE：视频中的旋转位置嵌入新突破",
                    "desc": "本文探讨了如何将旋转位置嵌入（RoPE）有效地扩展到视频数据中。研究分析了四个关键特性，这些特性对于RoPE在视频中的适应性至关重要。我们提出了一个新的任务V-NIAH-D，展示了现有RoPE变体在处理视频时容易受到干扰的缺陷。基于这些分析，我们提出了VideoRoPE，它通过3D结构来保持时空关系，并在多个下游任务中表现优于之前的RoPE变体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04403",
            "title": "Agency Is Frame-Dependent",
            "url": "https://huggingface.co/papers/2502.04403",
            "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 февраля",
                "en": "February 6",
                "zh": "2月6日"
            },
            "hash": "32ceb8df4d77794a",
            "authors": [
                "David Abel",
                "André Barreto",
                "Michael Bowling",
                "Will Dabney",
                "Shi Dong",
                "Steven Hansen",
                "Anna Harutyunyan",
                "Khimya Khetarpal",
                "Clare Lyle",
                "Razvan Pascanu",
                "Georgios Piliouras",
                "Doina Precup",
                "Jonathan Richens",
                "Mark Rowland",
                "Tom Schaul",
                "Satinder Singh"
            ],
            "affiliations": [
                "Amii, University of Alberta",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04403.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Агентность: все зависит от точки зрения",
                    "desc": "Статья рассматривает концепцию агентности в контексте обучения с подкреплением. Авторы утверждают, что агентность фундаментально зависит от системы отсчета. Они поддерживают этот тезис, анализируя ключевые свойства агентности, предложенные в предыдущих исследованиях. Статья подчеркивает необходимость учета зависимости от системы отсчета в изучении агентности и обсуждает последствия для обучения с подкреплением."
                },
                "en": {
                    "title": "Agency in Reinforcement Learning: A Frame-Dependent Perspective",
                    "desc": "This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning."
                },
                "zh": {
                    "title": "能动性：依赖于框架的系统能力",
                    "desc": "本文探讨了系统的能动性，特别是在强化学习的背景下。能动性是指系统朝着目标引导结果的能力，但判断一个系统是否具备能动性是一个复杂的问题。我们认为，能动性是依赖于参考框架的，任何对系统能动性的测量都必须相对于某个参考框架进行。通过哲学论证，我们支持这一观点，并讨论了这一结论对强化学习的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04404",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
            "url": "https://huggingface.co/papers/2502.04404",
            "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 февраля",
                "en": "February 6",
                "zh": "2月6日"
            },
            "hash": "a7bd201755c7ea1d",
            "authors": [
                "Xiao-Wen Yang",
                "Xuan-Yi Zhu",
                "Wen-Da Wei",
                "Ding-Chu Zhang",
                "Jie-Jing Shao",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University, China",
                "School of Artificial Intelligence, Nanjing University, China",
                "School of Intelligence Science and Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04404.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#inference",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самостоятельный возврат: путь к более разумным ИИ",
                    "desc": "Статья представляет новый механизм самостоятельного возврата (self-backtracking) для больших языковых моделей (LLM). Этот механизм позволяет LLM автономно определять, когда и где нужно вернуться назад в процессе рассуждений. Авторы утверждают, что это улучшает способности LLM к рассуждению и повышает эффективность, превращая медленное мышление в быстрое через самосовершенствование. Эмпирические оценки показывают значительное улучшение возможностей рассуждения LLM с использованием этого подхода."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Backtracking for Enhanced Reasoning",
                    "desc": "This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods."
                },
                "zh": {
                    "title": "自我回溯机制：提升语言模型推理能力的关键",
                    "desc": "将慢思考机制整合到大型语言模型（LLMs）中，为实现二级AGI推理器提供了有希望的途径。当前的挑战包括低效的过度思考和对辅助奖励模型的过度依赖。我们指出，这些限制源于LLMs无法内化搜索过程，而搜索过程是有效推理的关键组成部分。我们提出了一种自我回溯机制，使LLMs能够在训练和推理过程中自主决定何时以及如何回溯，从而显著提升推理能力和效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04350",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "url": "https://huggingface.co/papers/2502.04350",
            "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "ad7829c09c28de41",
            "authors": [
                "Yongchao Chen",
                "Yilun Hao",
                "Yueying Liu",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard University, Boston, MA, USA",
                "MIT-IBM Watson AI Lab, Boston, MA, USA",
                "Massachusetts Institute of Technology, Boston, MA, USA",
                "University of Illinois Urbana-Champaign, Urbana, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04350.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "CodeSteer: Умное управление для раскрытия потенциала LLM в символьных вычислениях",
                    "desc": "CodeSteer - это новый метод для эффективного управления генерацией кода и текста в больших языковых моделях (LLM). Исследователи создали комплексный бенчмарк SymBench и синтезировали наборы данных для обучения модели. Они дообучили модель Llama-3-8B с использованием многораундового обучения с учителем и прямой оптимизации предпочтений. Результирующая модель CodeSteerLLM значительно улучшает производительность других LLM в задачах символьных вычислений, превосходя даже лучшие существующие модели."
                },
                "en": {
                    "title": "CodeSteer: Guiding LLMs to Master Code and Reasoning!",
                    "desc": "This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks."
                },
                "zh": {
                    "title": "CodeSteer：引导LLM实现符号计算的突破",
                    "desc": "现有的方法无法有效引导大型语言模型（LLMs）在文本推理和代码生成之间切换，导致符号计算能力未得到充分利用。我们提出了一种名为CodeSteer的方法，能够有效指导LLM的代码和文本生成。我们构建了一个全面的基准SymBench，包含37个具有可调复杂度的符号任务，并合成了包含1.2万多轮指导/生成轨迹和5500对指导比较的数据集。通过对Llama-3-8B模型进行多轮监督微调（SFT）和直接偏好优化（DPO），我们得到的CodeSteerLLM模型能够有效引导更大模型的代码/文本生成。"
                }
            }
        }
    ],
    "link_prev": "2025-02-07.html",
    "link_next": "2025-02-11.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2月7日"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2月11日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了一种新方法，系统地映射稀疏自编码器在大语言模型的连续层中发现的特征。我们使用无数据余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。该方法产生特征演变的细粒度流图，提供细粒度的可解释性和机制洞察。关键是，我们展示了这些跨层特征图如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成的定向主题控制。总的来说，我们的发现突显了因果、跨层可解释性框架的实用性，不仅澄清了特征通过前向传递的发展，还提供了透明操作大语言模型的新方法。",
        "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "pinyin": "我们介绍了一种新方法，系统地映射稀疏自编码器在大语言模型的连续层中发现的特征。我们使用无数据余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。该方法产生特征演变的细粒度流图，提供细粒度的可解释性和机制洞察。关键是，我们展示了这些跨层特征图如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成的定向主题控制。总的来说，我们的发现突显了因果、跨层可解释性框架的实用性，不仅澄清了特征通过前向传递的发展，还提供了透明操作大语言模型的新方法。\n\nWǒmen jièshào le yī zhǒng xīn fāngfǎ, xìtǒng de yìngshè xīshū zìbiānmǎqì zài dà yǔyán móxíng de liánxù céng zhōng fāxiàn de tèzhēng. Wǒmen shǐyòng wú shùjù yúxiàn xiàngsìdù jìshù, zhuīzōng tèdìng tèzhēng zài měi ge jiēduàn de chíxù, zhuǎnbiàn huò shǒucì chūxiàn. Gǎi fāngfǎ chǎnshēng tèzhēng yǎnbiàn de xìlìdù liú tú, tígōng xìlìdù de kě jiěshìxìng hé jīzhì dòngchà. Guǎnjiàn shì, wǒmen zhǎnshì le zhèxiē kuàcéng tèzhēng tú rúhé tōngguò fàngdà huò yìzhì xuǎndìng tèzhēng, zhíjiē yǐndǎo móxíng xíngwéi, shíxiàn wénběn shēngchéng de dìngxiàng zhǔtí kòngzhì. Zǒng de lái shuō, wǒmen de fāxiàn tūxì le yīnguǒ, kuàcéng kě jiěshìxìng kuāngjià de shíyòngxìng, bùjǐn chéngqīng le tèzhēng tōngguò qiánxiàng chuándì de fāzhǎn, hái tígōng le tòumíng cāozuò dà yǔyán móxíng de xīn fāngfǎ.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"系统地\", \"pinyin\": \"xì tǒng de\", \"trans\": \"systematically\"},\n    {\"word\": \"映射\", \"pinyin\": \"yìng shè\", \"trans\": \"map\"},\n    {\"word\": \"稀疏\", \"pinyin\": \"xī shū\", \"trans\": \"sparse\"},\n    {\"word\": \"自编码器\", \"pinyin\": \"zì biān mǎ qì\", \"trans\": \"autoencoder\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"连续层\", \"pinyin\": \"lián xù céng\", \"trans\": \"continuous layer\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"feature\"},\n    {\"word\": \"无数据\", \"pinyin\": \"wú shù jù\", \"trans\": \"data-free\"},\n    {\"word\": \"余弦相似度\", \"pinyin\": \"yú xiàn xiāng sì dù\", \"trans\": \"cosine similarity\"},\n    {\"word\": \"技术\", \"pinyin\": \"jì shù\", \"trans\": \"technique\"},\n    {\"word\": \"追踪\", \"pinyin\": \"zhuī zōng\", \"trans\": \"track\"},\n    {\"word\": \"持续\", \"pinyin\": \"chí xù\", \"trans\": \"persist\"},\n    {\"word\": \"转变\", \"pinyin\": \"zhuǎn biàn\", \"trans\": \"transform\"},\n    {\"word\": \"首次\", \"pinyin\": \"shǒu cì\", \"trans\": \"first time\"},\n    {\"word\": \"出现\", \"pinyin\": \"chū xiàn\", \"trans\": \"appear\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"产生\", \"pinyin\": \"chǎn shēng\", \"trans\": \"generate\"},\n    {\"word\": \"演变\", \"pinyin\": \"yǎn biàn\", \"trans\": \"evolution\"},\n    {\"word\": \"细粒度\", \"pinyin\": \"xì lì dù\", \"trans\": \"fine-grained\"},\n    {\"word\": \"流图\", \"pinyin\": \"liú tú\", \"trans\": \"flow chart\"},\n    {\"word\": \"提供\", \"pinyin\": \"tí gōng\", \"trans\": \"provide\"},\n    {\"word\": \"可解释性\", \"pinyin\": \"kě jiě shì xìng\", \"trans\": \"interpretability\"},\n    {\"word\": \"机制\", \"pinyin\": \"jī zhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"洞察\", \"pinyin\": \"dòng chā\", \"trans\": \"insight\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"跨层\", \"pinyin\": \"kuà céng\", \"trans\": \"cross-layer\"},\n    {\"word\": \"特征图\", \"pinyin\": \"tè zhēng tú\", \"trans\": \"feature map\"},\n    {\"word\": \"放大\", \"pinyin\": \"fàng dà\", \"trans\": \"amplify\"},\n    {\"word\": \"抑制\", \"pinyin\": \"yì zhì\", \"trans\": \"suppress\"},\n    {\"word\": \"选定\", \"pinyin\": \"xuǎn dìng\", \"trans\": \"select\"},\n    {\"word\": \"引导\", \"pinyin\": \"yǐn dǎo\", \"trans\": \"guide\"},\n    {\"word\": \"模型行为\", \"pinyin\": \"mó xíng xíng wéi\", \"trans\": \"model behavior\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"文本生成\", \"pinyin\": \"wén běn shēng chéng\", \"trans\": \"text generation\"},\n    {\"word\": \"定向\", \"pinyin\": \"dìng xiàng\", \"trans\": \"directed\"},\n    {\"word\": \"主题控制\", \"pinyin\": \"zhǔ tí kòng zhì\", \"trans\": \"theme control\"},\n    {\"word\": \"总的来说\", \"pinyin\": \"zǒng de lái shuō\", \"trans\": \"in summary\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"突显\", \"pinyin\": \"tū xiǎn\", \"trans\": \"highlight\"},\n    {\"word\": \"因果\", \"pinyin\": \"yīn guǒ\", \"trans\": \"causal\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"实用性\", \"pinyin\": \"shí yòng xìng\", \"trans\": \"practicality\"},\n    {\"word\": \"澄清\", \"pinyin\": \"chéng qīng\", \"trans\": \"clarify\"},\n    {\"word\": \"前向传递\", \"pinyin\": \"qián xiàng chuán dì\", \"trans\": \"forward propagation\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"develop\"},\n    {\"word\": \"提供\", \"pinyin\": \"tí gōng\", \"trans\": \"provide\"},\n    {\"word\": \"透明\", \"pinyin\": \"tòu míng\", \"trans\": \"transparent\"},\n    {\"word\": \"操作\", \"pinyin\": \"cāo zuò\", \"trans\": \"operate\"},\n    {\"word\": \"新方法\", \"pinyin\": \"xīn fāng fǎ\", \"trans\": \"new method\"}\n]",
        "trans": "We introduce a new method that systematically maps the features discovered by sparse autoencoders in the continuous layers of large language models. Utilizing data-free cosine similarity techniques, we track the persistence, transformation, or initial appearance of specific features at each stage. This method generates fine-grained flow maps of feature evolution, offering detailed interpretability and mechanistic insights. Crucially, we demonstrate how these cross-layer feature maps directly guide model behavior by amplifying or suppressing selected features, achieving directed thematic control over text generation. Overall, our findings highlight the practicality of a causal, cross-layer interpretability framework, not only clarifying the development of features through forward propagation but also providing new methods for transparently operating large language models.",
        "update_ts": "2025-02-09 12:37"
    }
}