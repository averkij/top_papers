{
    "date": {
        "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
        "en": "February 10",
        "zh": "2Êúà10Êó•"
    },
    "time_utc": "2025-02-10 16:12",
    "weekday": 0,
    "issue_id": 2130,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.05173",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "url": "https://huggingface.co/papers/2502.05173",
            "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "score": 44,
            "issue_id": 2118,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ba284ed1a62b3c2c",
            "authors": [
                "Xilin Wei",
                "Xiaoran Liu",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Jian Tong",
                "Haodong Duan",
                "Qipeng Guo",
                "Jiaqi Wang",
                "Xipeng Qiu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05173.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "VideoRoPE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoRoPE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ Rotary Position Embedding. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—è–≤–∏–ª–∏ 4 –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ RoPE –∫ –≤–∏–¥–µ–æ. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É V-NIAH-D –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ RoPE. VideoRoPE –∏–º–µ–µ—Ç 3D-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ—Ö—Ä–∞–Ω—è—é—â—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã RoPE –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VideoRoPE",
                    "desc": "This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data."
                },
                "zh": {
                    "title": "VideoRoPEÔºöËßÜÈ¢ë‰∏≠ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•Êñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊúâÊïàÂú∞Êâ©Â±ïÂà∞ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠„ÄÇÁ†îÁ©∂ÂàÜÊûê‰∫ÜÂõõ‰∏™ÂÖ≥ÈîÆÁâπÊÄßÔºåËøô‰∫õÁâπÊÄßÂØπ‰∫éRoPEÂú®ËßÜÈ¢ë‰∏≠ÁöÑÈÄÇÂ∫îÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ‰ªªÂä°V-NIAH-DÔºåÂ±ïÁ§∫‰∫ÜÁé∞ÊúâRoPEÂèò‰ΩìÂú®Â§ÑÁêÜËßÜÈ¢ëÊó∂ÂÆπÊòìÂèóÂà∞Âπ≤Êâ∞ÁöÑÁº∫Èô∑„ÄÇÂü∫‰∫éËøô‰∫õÂàÜÊûêÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVideoRoPEÔºåÂÆÉÈÄöËøá3DÁªìÊûÑÊù•‰øùÊåÅÊó∂Á©∫ÂÖ≥Á≥ªÔºåÂπ∂Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰πãÂâçÁöÑRoPEÂèò‰Ωì„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04507",
            "title": "Fast Video Generation with Sliding Tile Attention",
            "url": "https://huggingface.co/papers/2502.04507",
            "abstract": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.",
            "score": 34,
            "issue_id": 2120,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "dcbf1070dac1b391",
            "authors": [
                "Peiyuan Zhang",
                "Yongqi Chen",
                "Runlong Su",
                "Hangliang Ding",
                "Ion Stoica",
                "Zhenghong Liu",
                "Hao Zhang"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego",
                "University of Michigan, Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04507.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "üéûÔ∏è",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –ø–ª–∏—Ç–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –ø–ª–∏—Ç–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (STA) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. STA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö 3D-–æ–∫–Ω–∞—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–º —É—Ä–æ–≤–Ω–µ. STA —É—Å–∫–æ—Ä—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤ 2.8-17 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention-2 –∏ –≤ 1.6-10 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention-3, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Efficient Video Generation with Sliding Tile Attention",
                    "desc": "This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks."
                },
                "zh": {
                    "title": "ÊªëÂä®Áì¶ÁâáÊ≥®ÊÑèÂäõÔºöÈ´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊªëÂä®Áì¶ÁâáÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàSTAÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàêÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÊâ©Êï£ÂèòÊç¢Âô®Âú®ÁîüÊàêËßÜÈ¢ëÊó∂ËÆ°ÁÆóÊàêÊú¨È´òÔºåËÄåSTAÈÄöËøáÂÖ≥Ê≥®Â±ÄÈÉ®ÁöÑÊó∂Á©∫Âå∫ÂüüÊù•ÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆó„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®‰∫ÜÁ°¨‰ª∂ÂèãÂ•ΩÁöÑËÆæËÆ°ÔºåÈÄêÂùóÂ§ÑÁêÜÔºå‰øùÊåÅ‰∫ÜË°®ËææËÉΩÂäõÁöÑÂêåÊó∂ÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇÁªèËøá‰ºòÂåñÔºåSTAÂú®ËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÊòæËëóÂä†ÈÄü‰∫ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÈôç‰Ωé‰∫ÜÂª∂ËøüÔºåÂêåÊó∂‰∏çÂΩ±ÂìçÁîüÊàêË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05003",
            "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
            "url": "https://huggingface.co/papers/2502.05003",
            "abstract": "One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the \"true\" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.",
            "score": 24,
            "issue_id": 2122,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "c011c3548ad7a5dd",
            "authors": [
                "Andrei Panferov",
                "Jiale Chen",
                "Soroush Tabesh",
                "Roberto L. Castro",
                "Mahdi Nikdan",
                "Dan Alistarh"
            ],
            "affiliations": [
                "ISTA",
                "Red Hat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05003.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ QuEST –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. QuEST –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å –≤–µ—Å–∞–º–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏ –≤ 4 –±–∏—Ç–∞ –∏–ª–∏ –º–µ–Ω—å—à–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FP16. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–π –æ—Ü–µ–Ω—â–∏–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–æ–≤–µ—Ä–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ QuEST –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "QuEST: Revolutionizing Quantization for Efficient Language Models",
                    "desc": "This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support."
                },
                "zh": {
                    "title": "QuESTÔºö‰ΩéÁ≤æÂ∫¶È´òÊïàËÆ≠ÁªÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫QuESTÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÊù•ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇQuESTËÉΩÂ§üÂú®4‰ΩçÊàñÊõ¥‰ΩéÁöÑÁ≤æÂ∫¶‰∏ãËÆ≠ÁªÉÊ®°ÂûãÔºåÂêåÊó∂‰øùÊåÅ‰∏éFP16Á≤æÂ∫¶Áõ∏ÂΩìÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊîπËøõÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÈáèÂåñËøáÁ®ãÔºå‰ª•ÂèäÂºïÂÖ•Êñ∞ÁöÑ‰ø°‰ªªÊ¢ØÂ∫¶‰º∞ËÆ°Âô®ÔºåÊù•ÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQuESTÂú®ÂêÑÁßçÁ°¨‰ª∂ÊîØÊåÅÁöÑÁ≤æÂ∫¶ËåÉÂõ¥ÂÜÖÈÉΩËÉΩÂÆûÁé∞Á®≥ÂÆöÁöÑÊâ©Â±ïÊÄßÔºåÂπ∂‰∏îÂèØ‰ª•ÊúâÊïàÊâßË°å„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04896",
            "title": "Goku: Flow Based Video Generative Foundation Models",
            "url": "https://huggingface.co/papers/2502.04896",
            "abstract": "This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",
            "score": 23,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ad6ef6eed233cc90",
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Yuqi Zhang",
                "Yida Zhang",
                "Fengda Zhu",
                "Hao Yang",
                "Hongxiang Hao",
                "Hui Wu",
                "Zhichao Lai",
                "Yifei Hu",
                "Ting-Che Lin",
                "Shilong Zhang",
                "Fu Li",
                "Chuan Li",
                "Xing Wang",
                "Yanghua Peng",
                "Peize Sun",
                "Ping Luo",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "Bytedance Inc",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04896.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#video",
                    "#architecture",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üêâ",
                "ru": {
                    "title": "Goku: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Goku –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Goku –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Goku: Revolutionizing Image and Video Generation with Transformers",
                    "desc": "This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models."
                },
                "zh": {
                    "title": "GokuÔºöÂõæÂÉè‰∏éËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜGokuÔºåËøôÊòØ‰∏ÄÁßçÂÖàËøõÁöÑËÅîÂêàÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÂà©Áî®‰∫Ü‰øÆÊ≠£ÊµÅTransformer‰ª•ÂÆûÁé∞Ë°å‰∏öÈ¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ËØ¶ÁªÜÈòêËø∞‰∫ÜÈ´òË¥®ÈáèËßÜËßâÁîüÊàêÁöÑÂü∫Á°ÄË¶ÅÁ¥†ÔºåÂåÖÊã¨Êï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ã„ÄÅÊ®°ÂûãÊû∂ÊûÑËÆæËÆ°„ÄÅÊµÅÁöÑÂÖ¨ÂºèÂåñ‰ª•ÂèäÈ´òÊïàÁ®≥ÂÅ•ÁöÑÂ§ßËßÑÊ®°ËÆ≠ÁªÉÂü∫Á°ÄËÆæÊñΩ„ÄÇGokuÊ®°ÂûãÂú®ÂÆöÊÄßÂíåÂÆöÈáèËØÑ‰º∞‰∏≠Ë°®Áé∞‰ºòË∂äÔºå‰∏∫‰∏ªË¶Å‰ªªÂä°ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåGokuÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ËææÂà∞‰∫Ü0.76ÁöÑGenEvalÂíå83.65ÁöÑDPG-BenchÔºåÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ë‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü84.85ÁöÑVBench„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05176",
            "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360¬∞ Unbounded Scene Inpainting",
            "url": "https://huggingface.co/papers/2502.05176",
            "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.",
            "score": 22,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "9b52f2788f53c3c0",
            "authors": [
                "Chung-Ho Wu",
                "Yang-Jung Chen",
                "Ying-Huan Chen",
                "Jie-Ying Lee",
                "Bo-Hsu Ke",
                "Chun-Wei Tuan Mu",
                "Yi-Chuan Huang",
                "Chin-Yang Lin",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: AuraFusion360 –¥–ª—è –±–µ–∑—É–ø—Ä–µ—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω",
                    "desc": "AuraFusion360 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian Splatting. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Å–æ–∫ –Ω–µ–≤–∏–¥–∏–º—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã, –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≥–ª—É–±–∏–Ω—ã –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SDEdit –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–æ—á–∫–∏ –æ–±–∑–æ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 360-USID –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω —Å –æ—Ö–≤–∞—Ç–æ–º 360 –≥—Ä–∞–¥—É—Å–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Inpainting with AuraFusion360",
                    "desc": "AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy."
                },
                "zh": {
                    "title": "AuraFusion360Ôºö‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÂú®ËôöÊãüÁé∞ÂÆûÂíåÂª∫Á≠ëÂèØËßÜÂåñÁ≠âÂ∫îÁî®‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®360Â∫¶Êó†ÁïåÂú∫ÊôØ‰∏≠Èù¢‰∏¥ËßÜÂõæ‰∏ÄËá¥ÊÄßÂíåÂá†‰ΩïÁ≤æÂ∫¶ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAuraFusion360ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂèÇËÄÉÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®È´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ‰∏≠ËøõË°åÁâ©‰ΩìÁßªÈô§ÂíåÂ≠îÂ°´ÂÖÖ„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊ∑±Â∫¶ÊÑüÁü•ÁöÑÊú™ËßÅÊé©Á†ÅÁîüÊàê„ÄÅÈÄÇÂ∫îÊÄßÂºïÂØºÊ∑±Â∫¶Êâ©Êï£ÂíåÂü∫‰∫éSDEditÁöÑÁªÜËäÇÂ¢ûÂº∫ÔºåÁ°Æ‰øùÂ§öËßÜÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåAuraFusion360Âú®ÊÑüÁü•Ë¥®ÈáèÂíåÂá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§üÂú®ÂâßÁÉàËßÜËßíÂèòÂåñ‰∏≠‰øùÊåÅÈ´òË¥®ÈáèÁöÑ‰øÆÂ§çÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05171",
            "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
            "url": "https://huggingface.co/papers/2502.05171",
            "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
            "score": 20,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "4386159312d9856b",
            "authors": [
                "Jonas Geiping",
                "Sean McLeish",
                "Neel Jain",
                "John Kirchenbauer",
                "Siddharth Singh",
                "Brian R. Bartoldson",
                "Bhavya Kailkhura",
                "Abhinav Bhatele",
                "Tom Goldstein"
            ],
            "affiliations": [
                "ELLIS Institute T√ºbingen, Max-Planck Institute for Intelligent Systems, T√ºbingen AI Center",
                "Lawrence Livermore National Laboratory",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ì–ª—É–±–æ–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ–º –Ω–µ—è–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞, —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—è—Å—å –¥–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–æ 3,5 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ 800 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ–∫–∞–∑–∞–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Scaling Reasoning with Latent Space Computation",
                    "desc": "This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts."
                },
                "zh": {
                    "title": "ÈöêÂºèÊé®ÁêÜÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ°ÁÆóËÉΩÂäõ",
                    "desc": "Êàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑÔºåËØ•Êû∂ÊûÑËÉΩÂ§üÈÄöËøáÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ÈöêÂºèÊé®ÁêÜÊù•Êâ©Â±ïÊµãËØïÊó∂ÁöÑËÆ°ÁÆóËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáËø≠‰ª£ÈÄíÂΩíÂùóÂ∑•‰ΩúÔºå‰ªéËÄåÂú®ÊµãËØïÊó∂ÂèØ‰ª•Â±ïÂºÄÂà∞‰ªªÊÑèÊ∑±Â∫¶„ÄÇËøô‰∏é‰∏ªÊµÅÊé®ÁêÜÊ®°Âûã‰∏çÂêåÔºåÂêéËÄÖÈÄöËøáÁîüÊàêÊõ¥Â§öÁöÑÊ†áËÆ∞Êù•Â¢ûÂä†ËÆ°ÁÆóÈáè„ÄÇÊàë‰ª¨ÁöÑÊ®°Âûã‰∏çÈúÄË¶ÅÁâπÊÆäÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåËÉΩÂ§üÂ§ÑÁêÜÂ∞èÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂπ∂‰∏îËÉΩÂ§üÊçïÊçâ‰∏çÊòìÁî®ËØ≠Ë®ÄË°®Á§∫ÁöÑÊé®ÁêÜÁ±ªÂûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05163",
            "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
            "url": "https://huggingface.co/papers/2502.05163",
            "abstract": "The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.",
            "score": 14,
            "issue_id": 2120,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ae863d89ab71ab51",
            "authors": [
                "Yihe Deng",
                "Yu Yang",
                "Junkai Zhang",
                "Wei Wang",
                "Bo Li"
            ],
            "affiliations": [
                "University of California, Los Angeles",
                "University of Illinois at Urbana-Champaign",
                "VirtueAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05163.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#inference",
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#multilingual",
                    "#rl"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–µ–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç framework —Å –¥–≤—É–º—è –∏–≥—Ä–æ–∫–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≥–¥–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å-–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫–∞–∫ –∏–≥—Ä–∞ –¥–≤—É—Ö –∏–≥—Ä–æ–∫–æ–≤ —Å –¥–æ–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –∫ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—é –ù—ç—à–∞. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –º–µ–Ω—å—à–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Multilingual Safety in LLMs with Synthetic Data Generation",
                    "desc": "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."
                },
                "zh": {
                    "title": "Â§öËØ≠Ë®ÄÊä§Ê†èÊ®°ÂûãÁöÑÂàõÊñ∞ËøõÂ±ï",
                    "desc": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÁ°Æ‰øùÂÖ∂Ë¥üË¥£‰ªª‰ΩøÁî®ÁöÑÊä§Ê†èÊ®°ÂûãÈúÄÊ±ÇÂ¢ûÂä†ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê£ÄÊµã‰∏çÂÆâÂÖ®ÂíåÈùûÊ≥ïÂÜÖÂÆπÊñπÈù¢„ÄÇËôΩÁÑ∂Ëã±ËØ≠ÁöÑÂÆâÂÖ®Êï∞ÊçÆÁõ∏ÂØπ‰∏∞ÂØåÔºå‰ΩÜÁî±‰∫éÂÖ∂‰ªñËØ≠Ë®ÄÂºÄÊîæÊ∫ê‰ª£Á†ÅÂÆâÂÖ®Êï∞ÊçÆÁöÑÁ®ÄÁº∫ÔºåÂ§öËØ≠Ë®ÄÊä§Ê†èÂª∫Ê®°‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÁé©ÂÆ∂Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂÖ∂‰∏≠ÁîüÊàêÂô®ÂíåÊä§Ê†èÊ®°ÂûãÂØπÊäóÊÄßÂú∞ÂÖ±ÂêåËøõÂåñÔºå‰ª•ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂêàÊàêÊï∞ÊçÆÁî®‰∫éÂ§öËØ≠Ë®ÄÊä§Ê†èËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§öËØ≠Ë®ÄÂÆâÂÖ®‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑ‰∏çÂπ≥Ë°°ÈóÆÈ¢ò‰∏ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04403",
            "title": "Agency Is Frame-Dependent",
            "url": "https://huggingface.co/papers/2502.04403",
            "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
            "score": 11,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "32ceb8df4d77794a",
            "authors": [
                "David Abel",
                "Andr√© Barreto",
                "Michael Bowling",
                "Will Dabney",
                "Shi Dong",
                "Steven Hansen",
                "Anna Harutyunyan",
                "Khimya Khetarpal",
                "Clare Lyle",
                "Razvan Pascanu",
                "Georgios Piliouras",
                "Doina Precup",
                "Jonathan Richens",
                "Mark Rowland",
                "Tom Schaul",
                "Satinder Singh"
            ],
            "affiliations": [
                "Amii, University of Alberta",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04403.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ê–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å: –≤—Å–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞. –û–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —ç—Ç–æ—Ç —Ç–µ–∑–∏—Å, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∫–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É—á–µ—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞ –≤ –∏–∑—É—á–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –æ–±—Å—É–∂–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º."
                },
                "en": {
                    "title": "Agency in Reinforcement Learning: A Frame-Dependent Perspective",
                    "desc": "This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning."
                },
                "zh": {
                    "title": "ËÉΩÂä®ÊÄßÔºö‰æùËµñ‰∫éÊ°ÜÊû∂ÁöÑÁ≥ªÁªüËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁ≥ªÁªüÁöÑËÉΩÂä®ÊÄßÔºåÁâπÂà´ÊòØÂú®Âº∫ÂåñÂ≠¶‰π†ÁöÑËÉåÊôØ‰∏ã„ÄÇËÉΩÂä®ÊÄßÊòØÊåáÁ≥ªÁªüÊúùÁùÄÁõÆÊ†áÂºïÂØºÁªìÊûúÁöÑËÉΩÂäõÔºå‰ΩÜÂà§Êñ≠‰∏Ä‰∏™Á≥ªÁªüÊòØÂê¶ÂÖ∑Â§áËÉΩÂä®ÊÄßÊòØ‰∏Ä‰∏™Â§çÊùÇÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÉΩÂä®ÊÄßÊòØ‰æùËµñ‰∫éÂèÇËÄÉÊ°ÜÊû∂ÁöÑÔºå‰ªª‰ΩïÂØπÁ≥ªÁªüËÉΩÂä®ÊÄßÁöÑÊµãÈáèÈÉΩÂøÖÈ°ªÁõ∏ÂØπ‰∫éÊüê‰∏™ÂèÇËÄÉÊ°ÜÊû∂ËøõË°å„ÄÇÈÄöËøáÂì≤Â≠¶ËÆ∫ËØÅÔºåÊàë‰ª¨ÊîØÊåÅËøô‰∏ÄËßÇÁÇπÔºåÂπ∂ËÆ®ËÆ∫‰∫ÜËøô‰∏ÄÁªìËÆ∫ÂØπÂº∫ÂåñÂ≠¶‰π†ÁöÑÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05179",
            "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
            "url": "https://huggingface.co/papers/2502.05179",
            "abstract": "DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .",
            "score": 8,
            "issue_id": 2120,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "c3147244e03af4a6",
            "authors": [
                "Shilong Zhang",
                "Wenbo Li",
                "Shoufa Chen",
                "Chongjian Ge",
                "Peize Sun",
                "Yida Zhang",
                "Yi Jiang",
                "Zehuan Yuan",
                "Binyue Peng",
                "Ping Luo"
            ],
            "affiliations": [
                "ByteDance",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05179.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FlashVideo. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –ø—Ä–æ–º–ø—Ç—É, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –≤–∏–¥–µ–æ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–¥ –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫—É—é –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏."
                },
                "en": {
                    "title": "FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework",
                    "desc": "The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use."
                },
                "zh": {
                    "title": "FlashVideoÔºöÈ´òÊïàÁîüÊàêÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "DiTÊâ©Êï£Ê®°ÂûãÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÈ´òÂÜÖÂÆπÂíåËøêÂä®‰øùÁúüÂ∫¶ÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÊ®°ÂûãÂèÇÊï∞ÂíåÂáΩÊï∞ËØÑ‰º∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õËÆ°ÁÆóÈúÄÊ±ÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂FlashVideoÔºåÊó®Âú®Âπ≥Ë°°ÁîüÊàêÁöÑ‰øùÁúüÂ∫¶ÂíåË¥®Èáè„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÈÄöËøá‰ΩéÂàÜËæ®ÁéáÁîüÊàêËøáÁ®ã‰ºòÂÖàËÄÉËôëÊèêÁ§∫‰øùÁúüÂ∫¶ÔºåÂà©Áî®Â§ßÂèÇÊï∞ÂíåË∂≥Â§üÁöÑÂáΩÊï∞ËØÑ‰º∞ÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂàôÂú®‰ΩéÂàÜËæ®ÁéáÂíåÈ´òÂàÜËæ®Áéá‰πãÈó¥Âª∫Á´ãÊµÅÂåπÈÖçÔºåÊúâÊïàÁîüÊàêÁªÜËäÇÔºå‰∏îÊâÄÈúÄÁöÑÂáΩÊï∞ËØÑ‰º∞ÊúÄÂ∞èÂåñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04728",
            "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
            "url": "https://huggingface.co/papers/2502.04728",
            "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.",
            "score": 8,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "cd97668cd0eee0ee",
            "authors": [
                "Zhouliang Yu",
                "Yuhuan Yuan",
                "Tim Z. Xiao",
                "Fuxiang Frank Xia",
                "Jie Fu",
                "Ge Zhang",
                "Ge Lin",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Environmental Systems Research Institute, Inc.",
                "Max Planck Institute for Intelligent Systems, T√ºbingen",
                "SEED, Bytedance",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04728.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é PDDL –∏ LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (PDDL) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω–æ–π —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Best-of-N –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–±–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–æ–≤ PDDL –∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è."
                },
                "en": {
                    "title": "Enhancing LLMs for Optimal Planning with PDDL",
                    "desc": "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."
                },
                "zh": {
                    "title": "Âà©Áî®PDDLÊèêÂçáËßÑÂàíÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËß£ÂÜ≥Â§çÊùÇÁöÑËßÑÂàíÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçËßÑÂàôËøùÂèçÂíåÁ°Æ‰øùÊúÄ‰ºòÊÄßÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºïÂÖ•‰∫ÜËßÑÂàíÈ¢ÜÂüüÂÆö‰πâËØ≠Ë®ÄÔºàPDDLÔºâÔºå‰Ωú‰∏∫‰∏ÄÁßçÁ≤æÁ°ÆÁöÑÁä∂ÊÄÅÊèèËø∞Â∑•ÂÖ∑„ÄÇÈÄöËøáPDDLÔºåÂèØ‰ª•ÁîüÊàêÁ¨¶Âè∑‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Â∫îÁî®ÁªèÂÖ∏ÊêúÁ¥¢ÁÆóÊ≥ïÔºàÂ¶ÇA*ÔºâÊù•ÂØªÊâæÊúÄ‰ºòËÆ°Âàí„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÁÆóÊ≥ïÔºåÈÄöËøáBest-of-NÈááÊ†∑ÂíåÁªÜËá¥ÁöÑÊú∫Âô®Â≠¶‰π†‰ºòÂåñÔºåÊòæËëóÊèêÈ´ò‰∫ÜPDDLÈ¢ÜÂüüÁöÑÁîüÊàêË¥®ÈáèÔºåÊàêÂäüÁéáË∂ÖËøá50%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04959",
            "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces",
            "url": "https://huggingface.co/papers/2502.04959",
            "abstract": "Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .",
            "score": 6,
            "issue_id": 2127,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "a73679e79ff14b7c",
            "authors": [
                "Daniel Marczak",
                "Simone Magistri",
                "Sebastian Cygert",
                "Bart≈Çomiej Twardowski",
                "Andrew D. Bagdanov",
                "Joost van de Weijer"
            ],
            "affiliations": [
                "Computer Vision Center, Barcelona, Spain",
                "Department of Computer Science, Universitat Autonoma de Barcelona, Spain",
                "Department of Information Engineering, University of Florence, Italy",
                "Gdansk University of Technology, Poland",
                "IDEAS NCBR, Warsaw, Poland",
                "Warsaw University of Technology, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04959.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üîÄ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –≤–µ—Å–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ –æ–¥–Ω—É –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—É—é –º–æ–¥–µ–ª—å. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–∞—Ç—Ä–∏—Ü –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –º–æ–¥–µ–ª–∏. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏–∑–æ—Ç—Ä–æ–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–∞—Ç—Ä–∏—Ü –∑–∞–¥–∞—á –∏ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –∑–∞–¥–∞—á –∏ –º–∞—Å—à—Ç–∞–±—ã –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Bridging the Performance Gap in Model Merging",
                    "desc": "This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios."
                },
                "zh": {
                    "title": "ÊúâÊïàÁöÑÊ®°ÂûãÂêàÂπ∂ÊñπÊ≥ïÊèêÂçáÂ§ö‰ªªÂä°ÊÄßËÉΩ",
                    "desc": "Ê®°ÂûãÂêàÂπ∂ÊòØÂ∞ÜÂ§ö‰∏™ÁâπÂÆö‰ªªÂä°Ê®°ÂûãÁöÑÊùÉÈáçÊï¥Âêà‰∏∫‰∏Ä‰∏™Â§ö‰ªªÂä°Ê®°ÂûãÁöÑËøáÁ®ã„ÄÇÂ∞ΩÁÆ°Ëøô‰∏ÄÈóÆÈ¢òÂèóÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®Ôºå‰ΩÜÂêàÂπ∂Ê®°Âûã‰∏éÂçï‰ªªÂä°Ê®°Âûã‰πãÈó¥‰ªçÂ≠òÂú®ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫Ü‰ªªÂä°Áü©ÈòµÁöÑÂÖ≥ÈîÆÁâπÊÄßÔºåËøô‰∫õÁü©ÈòµÊòØÂ∫îÁî®‰∫éÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊùÉÈáçÊõ¥Êñ∞Áü©ÈòµÔºåÂèëÁé∞‰ªªÂä°ÁâπÂÆöÁü©Èòµ‰∏éÂêàÂπ∂Áü©Èòµ‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶‰∏éÊÄßËÉΩÊèêÂçáÂØÜÂàáÁõ∏ÂÖ≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêÑÂêëÂêåÊÄßÂêàÂπ∂Ê°ÜÊû∂ÔºåÈÄöËøáÂπ≥Âù¶Âåñ‰ªªÂä°Áü©ÈòµÁöÑÂ•áÂºÇÂÄºË∞±ÔºåÂ¢ûÂº∫ÂØπÈΩêÊÄßÔºå‰ªéËÄåÁº©Â∞èÊÄßËÉΩÂ∑ÆË∑ùÔºåÂπ∂Âú®Â§ö‰∏™Âú∫ÊôØ‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04520",
            "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
            "url": "https://huggingface.co/papers/2502.04520",
            "abstract": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.",
            "score": 6,
            "issue_id": 2119,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "41ef9027d1533f06",
            "authors": [
                "Letian Peng",
                "Chenyang An",
                "Shibo Hao",
                "Chengyu Dong",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04520.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#data",
                    "#hallucinations"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω –ª–∏–Ω–µ–π–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä–æ–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –æ—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∫ –¥—Ä—É–≥–æ–º—É. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ –∫ –º–∞—Å—à—Ç–∞–±–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –æ–±–æ–±—â–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–∞–∫–∏–µ –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤–∞—Ä—è."
                },
                "en": {
                    "title": "Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition",
                    "desc": "This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge."
                },
                "zh": {
                    "title": "ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥ÊÄß‰∏éÁü•ËØÜÁªÑÂêà",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂú®Áü•ËØÜÁªÑÂêà‰∏≠ÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥Áé∞Ë±°„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊüê‰∫õÁõ∏ÂÖ≥Áü•ËØÜ‰πãÈó¥Â≠òÂú®Á∫øÊÄßÂèòÊç¢ÔºåËøôÁßçÂèòÊç¢ÂèØ‰ª•Â∞Ü‰∏Ä‰∏™ÊèêÁ§∫ÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµã‰ªé‰∏Ä‰∏™Êò†Â∞ÑÂà∞Âè¶‰∏Ä‰∏™„ÄÇÊØîÂ¶ÇÔºå‰ªé\"X ‰ΩèÂú®ÂüéÂ∏Ç\"ÂèØ‰ª•ËΩ¨Âèò‰∏∫\"X ‰ΩèÂú®ÂõΩÂÆ∂\"„ÄÇÁªìÊûúË°®ÊòéÔºåÁ∫øÊÄßÂèòÊç¢Âú®Â§ßËßÑÊ®°ÂæÆË∞É‰∏≠ÂÖ∑ÊúâÈüßÊÄßÔºåÂπ∂‰∏îÂΩì‰∏éÁé∞ÂÆû‰∏ñÁïåÂÖ≥Á≥ª‰∏ÄËá¥Êó∂ËÉΩÂ§üÊé®ÂπøÊõ¥Êñ∞ÁöÑÁü•ËØÜÔºå‰ΩÜÂΩìÂÅèÁ¶ªÊó∂Âàô‰ºöÂØºËá¥ÂπªËßâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04416",
            "title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
            "url": "https://huggingface.co/papers/2502.04416",
            "abstract": "Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.",
            "score": 5,
            "issue_id": 2125,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "0b4520b0860c835c",
            "authors": [
                "Zehua Pei",
                "Lancheng Zou",
                "Hui-Ling Zhen",
                "Xianzhi Yu",
                "Wulong Liu",
                "Sinno Jialin Pan",
                "Mingxuan Yuan",
                "Bei Yu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04416.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#inference"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω—ã—Ö LLM –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ MoE –º–æ–¥–µ–ª–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ CMoE (Carved MoE) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts –∏–∑ –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). CMoE –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –Ω–µ–π—Ä–æ–Ω—ã –≤ –æ–±—â–∏–µ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏. CMoE –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ —Å–æ–∑–¥–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é MoE –º–æ–¥–µ–ª—å –∏–∑ –ø–ª–æ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Efficiently Carving Mixture-of-Experts for Large Language Models",
                    "desc": "This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning."
                },
                "zh": {
                    "title": "È´òÊïàÈõïÂàªÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöËøáÂ¢ûÂä†Ê®°ÂûãÂèÇÊï∞ÂÆûÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑÊé®ÁêÜÂºÄÈîÄ„ÄÇÂâçÈ¶àÁΩëÁªúÔºàFFNsÔºâÂú®LLMÂèÇÊï∞‰∏≠Âç†‰∏ªÂØºÂú∞‰ΩçÔºåÈöêËóèÁ•ûÁªèÂÖÉÁöÑÊøÄÊ¥ªÁ®ÄÁñèÊÄßÂæàÈ´ò„ÄÇ‰∏∫Ê≠§ÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÔºå‰ªÖÊøÄÊ¥ª‰∏ÄÈÉ®ÂàÜÂèÇÊï∞„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåËµÑÊ∫êÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCMoEÔºàCarved MoEÔºâÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•È´òÊïàÂú∞‰ªéÁ®†ÂØÜÊ®°Âûã‰∏≠ÈõïÂàªÂá∫MoEÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03738",
            "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
            "url": "https://huggingface.co/papers/2502.03738",
            "abstract": "Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.",
            "score": 5,
            "issue_id": 2122,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "aa76478090a36c04",
            "authors": [
                "Feng Wang",
                "Yaodong Yu",
                "Guoyizhe Wei",
                "Wei Shao",
                "Yuyin Zhou",
                "Alan Yuille",
                "Cihang Xie"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "UC Berkeley",
                "UC Santa Cruz",
                "University of Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03738.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ü–∏–∫—Å–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–∞—Ç—á–∏ –≤ vision-–º–æ–¥–µ–ª—è—Ö",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É –≤–ª–∏—è–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–µ–π –≤ Vision Transformer (ViT) –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–µ–π –¥–æ 1x1 –ø–∏–∫—Å–µ–ª—è (–ø–∏–∫—Å–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è) —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç —ç—Ñ—Ñ–µ–∫—Ç –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∫–ª—é—á–∞—è ViT –∏ Mamba. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å –±–∞–∑–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –¥–ª–∏–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 50 176 —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 84,6% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ImageNet-1k."
                },
                "en": {
                    "title": "Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers",
                    "desc": "This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models."
                },
                "zh": {
                    "title": "Â∞èÂùóÊõ¥‰ºòÔºåËßÜËßâÁêÜËß£Êõ¥Âº∫ÔºÅ",
                    "desc": "Êú¨ÊñáÁ†îÁ©∂‰∫ÜËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâ‰∏≠ÂõæÂÉèÂàÜÂùóÔºàpatchificationÔºâÂØπ‰ø°ÊÅØÊçüÂ§±ÁöÑÂΩ±Âìç„ÄÇÈÄöËøáÁº©Â∞èÂõæÂÉèÁöÑÁ©∫Èó¥Â§ßÂ∞èÔºåÂàÜÂùóÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂáèÂ∞ë‰ª§ÁâåÂ∫èÂàóÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈöèÁùÄÂàÜÂùóÂ§ßÂ∞èÁöÑÂáèÂ∞èÔºåÊ®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩÊåÅÁª≠ÊèêÈ´òÔºåÁõ¥Âà∞ËææÂà∞ÊúÄÂ∞èÁöÑ1x1ÂÉèÁ¥†ÂàÜÂùó„ÄÇËØ•Á†îÁ©∂ÁªìÊûúÈÄÇÁî®‰∫éÂ§öÁßçËßÜËßâ‰ªªÂä°Âíå‰∏çÂêåÁöÑÊ®°ÂûãÊû∂ÊûÑÔºå‰∏∫Êú™Êù•ÊûÑÂª∫ÈùûÂéãÁº©ËßÜËßâÊ®°ÂûãÊèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04363",
            "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
            "url": "https://huggingface.co/papers/2502.04363",
            "abstract": "We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.",
            "score": 5,
            "issue_id": 2119,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 5",
                "zh": "2Êúà5Êó•"
            },
            "hash": "339b45dee733174c",
            "authors": [
                "Bosung Kim",
                "Kyuhwan Lee",
                "Isu Jeong",
                "Jungmin Cheon",
                "Yeojin Lee",
                "Seulki Lee"
            ],
            "affiliations": [
                "Ulsan National Institute of Science and Technology South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04363.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "üì±",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –ø—Ä—è–º–æ –Ω–∞ –≤–∞—à–µ–º —Å–º–∞—Ä—Ç—Ñ–æ–Ω–µ",
                    "desc": "On-device Sora –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–µ–µ –Ω–∞ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∏ –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: Linear Proportional Leap (LPL) –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, Temporal Dimension Token Merging (TDTM) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è, –∏ Concurrent Inference with Dynamic Loading (CI-DL) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ iPhone 15 Pro –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ On-device Sora —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ Open-Sora –Ω–∞ –º–æ—â–Ω—ã—Ö GPU."
                },
                "en": {
                    "title": "Empowering Video Creation on Your Smartphone!",
                    "desc": "On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services."
                },
                "zh": {
                    "title": "ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÈ´òÊïàËßÜÈ¢ëÁîüÊàêÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫ÜOn-device SoraÔºåËøôÊòØÈ¶ñ‰∏™Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÁßªÂä®ËÆæÂ§áÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÔºåËÉΩÂ§üÈ´òÊïàÂú∞Âú®Êô∫ËÉΩÊâãÊú∫‰∏äËøêË°å„ÄÇËØ•Á≥ªÁªüÈááÁî®‰∫Ü‰∏âÁßçÊñ∞ÊäÄÊúØÊù•Ëß£ÂÜ≥ÁßªÂä®ËÆæÂ§áÂú®ËÆ°ÁÆóÂíåÂÜÖÂ≠òÊñπÈù¢ÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÁ∫øÊÄßÊØî‰æãË∑≥Ë∑ÉÔºàLPLÔºâÈÄöËøáÈ´òÊïàÁöÑË∑≥Ë∑ÉÊñπÊ≥ïÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÊâ©Êï£‰∏≠ÊâÄÈúÄÁöÑÂéªÂô™Ê≠•È™§„ÄÇÂÖ∂Ê¨°ÔºåÊó∂Èó¥Áª¥Â∫¶‰ª§ÁâåÂêàÂπ∂ÔºàTDTMÔºâÈÄöËøáÊ≤øÊó∂Èó¥Áª¥Â∫¶ÂêàÂπ∂ËøûÁª≠‰ª§ÁâåÔºåÈôç‰Ωé‰∫ÜÊ≥®ÊÑèÂäõÂ±Ç‰∏≠ÂØÜÈõÜÁöÑ‰ª§ÁâåÂ§ÑÁêÜËÆ°ÁÆó„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04404",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
            "url": "https://huggingface.co/papers/2502.04404",
            "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.",
            "score": 5,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "a7bd201755c7ea1d",
            "authors": [
                "Xiao-Wen Yang",
                "Xuan-Yi Zhu",
                "Wen-Da Wei",
                "Ding-Chu Zhang",
                "Jie-Jing Shao",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University, China",
                "School of Artificial Intelligence, Nanjing University, China",
                "School of Intelligence Science and Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04404.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#inference",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –≤–æ–∑–≤—Ä–∞—Ç: –ø—É—Ç—å –∫ –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–º –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ (self-backtracking) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –∏ –≥–¥–µ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–≤—Ä–∞—â–∞—è –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –±—ã—Å—Ç—Ä–æ–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Backtracking for Enhanced Reasoning",
                    "desc": "This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods."
                },
                "zh": {
                    "title": "Ëá™ÊàëÂõûÊ∫ØÊú∫Âà∂ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Â∞ÜÊÖ¢ÊÄùËÄÉÊú∫Âà∂Êï¥ÂêàÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Ôºå‰∏∫ÂÆûÁé∞‰∫åÁ∫ßAGIÊé®ÁêÜÂô®Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæÑ„ÄÇÂΩìÂâçÁöÑÊåëÊàòÂåÖÊã¨‰ΩéÊïàÁöÑËøáÂ∫¶ÊÄùËÄÉÂíåÂØπËæÖÂä©Â•ñÂä±Ê®°ÂûãÁöÑËøáÂ∫¶‰æùËµñ„ÄÇÊàë‰ª¨ÊåáÂá∫ÔºåËøô‰∫õÈôêÂà∂Ê∫ê‰∫éLLMsÊó†Ê≥ïÂÜÖÂåñÊêúÁ¥¢ËøáÁ®ãÔºåËÄåÊêúÁ¥¢ËøáÁ®ãÊòØÊúâÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÊàëÂõûÊ∫ØÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠Ëá™‰∏ªÂÜ≥ÂÆö‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰ΩïÂõûÊ∫ØÔºå‰ªéËÄåÊòæËëóÊèêÂçáÊé®ÁêÜËÉΩÂäõÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05092",
            "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2502.05092",
            "abstract": "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.",
            "score": 4,
            "issue_id": 2128,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ea9f14d34d4cbb60",
            "authors": [
                "Rohit Saxena",
                "Aryo Pradipta Gema",
                "Pasquale Minervini"
            ],
            "affiliations": [
                "ILCC, School of Informatics, University of Edinburgh",
                "Miniml.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05092.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#interpretability"
                ],
                "emoji": "‚è∞",
                "ru": {
                    "title": "–í—Ä–µ–º—è –±—Ä–æ—Å–∞–µ—Ç –≤—ã–∑–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–∞—Ç —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–æ–≥–æ–≤—ã–µ —á–∞—Å—ã –∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π ClockQA —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏ —á–∞—Å–æ–≤ –∏ CalendarQA —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫ MLLM –≤—ã–ø–æ–ª–Ω—è—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —á–∏—Å–ª–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–¥–µ–∂–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –¥–ª—è MLLM, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Unlocking Time: Challenges for Multimodal Language Models",
                    "desc": "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."
                },
                "zh": {
                    "title": "ÁêÜËß£Êó∂Èó¥ÁöÑÊåëÊàòÔºöÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄß",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁêÜËß£Êó∂Èó¥ÂíåÊó•ÊúüÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁâπÂà´ÊòØÈÄöËøáÊ®°ÊãüÊó∂ÈíüÂíåÂπ¥Â∫¶Êó•ÂéÜÁöÑËßÜËßâË°®Á§∫„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨‰∏§ÈÉ®ÂàÜÔºöClockQAÔºåÂåÖÂê´‰∏çÂêåÈ£éÊ†ºÁöÑÊó∂ÈíüÂèäÁõ∏ÂÖ≥Êó∂Èó¥ÈóÆÈ¢òÔºõCalendarQAÔºåÂåÖÂê´Âπ¥Â∫¶Êó•ÂéÜÂõæÂÉèÂèäÂ∏∏ËßÅÊó•ÊúüÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂàÜÊûêMLLMsÂú®Â§ÑÁêÜ‰∏éÊó∂Èó¥Áõ∏ÂÖ≥ÁöÑËßÜËßâÊï∞ÊçÆÊó∂ÁöÑËßÜËßâËØÜÂà´„ÄÅÊï∞ÂÄºÊé®ÁêÜÂíåÊó∂Èó¥Êé®Êñ≠ËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜMLLMsÂú®ÂèØÈù†ÁêÜËß£Êó∂Èó¥ÊñπÈù¢‰ªçÈù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05178",
            "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2502.05178",
            "abstract": "We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.",
            "score": 4,
            "issue_id": 2121,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "bbc1f1a0b7f5423f",
            "authors": [
                "Yue Zhao",
                "Fuzhao Xue",
                "Scott Reed",
                "Linxi Fan",
                "Yuke Zhu",
                "Jan Kautz",
                "Zhiding Yu",
                "Philipp Kr√§henb√ºhl",
                "De-An Huang"
            ],
            "affiliations": [
                "NVIDIA",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05178.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "QLIP: –ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "QLIP - —ç—Ç–æ –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Å –±–∏–Ω–∞—Ä–Ω–æ-—Å—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ—á–µ—Ç–∞–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –±–æ–ª—å—à–∏–º –±–∞—Ç—á–∞–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –ø–∞–º—è—Ç–∏. QLIP –º–æ–∂–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é –∏–ª–∏ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "QLIP: Bridging Visual and Language Understanding with Efficient Tokenization",
                    "desc": "The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation."
                },
                "zh": {
                    "title": "ÈáèÂåñËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºöÂ§öÊ®°ÊÄÅÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÈáèÂåñËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºàQLIPÔºâÔºåËøôÊòØ‰∏ÄÁßçËßÜËßâÊ†áËÆ∞ÂåñÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÊúÄÂÖàËøõÁöÑÈáçÂª∫Ë¥®ÈáèÂíåÈõ∂-shotÂõæÂÉèÁêÜËß£ËÉΩÂäõ„ÄÇQLIP‰ΩøÁî®Âü∫‰∫é‰∫åÂÖÉÁêÉÈù¢ÈáèÂåñÁöÑËá™ÁºñÁ†ÅÂô®ËøõË°åËÆ≠ÁªÉÔºåÂêåÊó∂‰ºòÂåñÈáçÂª∫ÂíåËØ≠Ë®Ä-ÂõæÂÉèÂØπÈΩêÁõÆÊ†á„ÄÇÊàë‰ª¨È¶ñÊ¨°ËØÅÊòéËøô‰∏§‰∏™ÁõÆÊ†áÂèØ‰ª•Âä®ÊÄÅÂπ≥Ë°°ÔºåËÄå‰∏çÊòØÁõ∏‰∫íÂØπÁ´ã„ÄÇQLIPÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÊñáÊú¨Êù°‰ª∂ÂõæÂÉèÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂèØ‰ª•‰Ωú‰∏∫LLaVAÁöÑËßÜËßâÁºñÁ†ÅÂô®ÂíåLlamaGenÁöÑÂõæÂÉèÊ†áËÆ∞Âô®ÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊÄßËÉΩÁõ∏ÂΩìÊàñÊõ¥Â•Ω„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04350",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "url": "https://huggingface.co/papers/2502.04350",
            "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.",
            "score": 4,
            "issue_id": 2118,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 4",
                "zh": "2Êúà4Êó•"
            },
            "hash": "ad7829c09c28de41",
            "authors": [
                "Yongchao Chen",
                "Yilun Hao",
                "Yueying Liu",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard University, Boston, MA, USA",
                "MIT-IBM Watson AI Lab, Boston, MA, USA",
                "Massachusetts Institute of Technology, Boston, MA, USA",
                "University of Illinois Urbana-Champaign, Urbana, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04350.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "üß≠",
                "ru": {
                    "title": "CodeSteer: –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ LLM –≤ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö",
                    "desc": "CodeSteer - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–¥–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SymBench –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –û–Ω–∏ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Llama-3-8B —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –º–æ–¥–µ–ª—å CodeSteerLLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "CodeSteer: Guiding LLMs to Master Code and Reasoning!",
                    "desc": "This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks."
                },
                "zh": {
                    "title": "CodeSteerÔºöÂºïÂØºLLMÂÆûÁé∞Á¨¶Âè∑ËÆ°ÁÆóÁöÑÁ™ÅÁ†¥",
                    "desc": "Áé∞ÊúâÁöÑÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂºïÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰πãÈó¥ÂàáÊç¢ÔºåÂØºËá¥Á¨¶Âè∑ËÆ°ÁÆóËÉΩÂäõÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CodeSteerÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÊåáÂØºLLMÁöÑ‰ª£Á†ÅÂíåÊñáÊú¨ÁîüÊàê„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜSymBenchÔºåÂåÖÂê´37‰∏™ÂÖ∑ÊúâÂèØË∞ÉÂ§çÊùÇÂ∫¶ÁöÑÁ¨¶Âè∑‰ªªÂä°ÔºåÂπ∂ÂêàÊàê‰∫ÜÂåÖÂê´1.2‰∏áÂ§öËΩÆÊåáÂØº/ÁîüÊàêËΩ®ËøπÂíå5500ÂØπÊåáÂØºÊØîËæÉÁöÑÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂØπLlama-3-8BÊ®°ÂûãËøõË°åÂ§öËΩÆÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÔºåÊàë‰ª¨ÂæóÂà∞ÁöÑCodeSteerLLMÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂºïÂØºÊõ¥Â§ßÊ®°ÂûãÁöÑ‰ª£Á†Å/ÊñáÊú¨ÁîüÊàê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03512",
            "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
            "url": "https://huggingface.co/papers/2502.03512",
            "abstract": "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.",
            "score": 3,
            "issue_id": 2122,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 5",
                "zh": "2Êúà5Êó•"
            },
            "hash": "ebf6482c46f8fe2f",
            "authors": [
                "Amitava Das",
                "Yaswanth Narsupalli",
                "Gurpreet Singh",
                "Vinija Jain",
                "Vasu Sharma",
                "Suranjana Trivedy",
                "Aman Chadha",
                "Amit Sheth"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "Artificial Intelligence Institute, University of South Carolina, USA",
                "Meta AI, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03512.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#rlhf",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ë–∞–ª–∞–Ω—Å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: YinYangAlign –Ω–∞ —Å—Ç—Ä–∞–∂–µ —Ç–æ—á–Ω–æ—Å—Ç–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç YinYangAlign - –ø–µ—Ä–µ–¥–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ-–∏–∑–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö (T2I) –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —à–µ—Å—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö —Ü–µ–ª—è—Ö –¥–∏–∑–∞–π–Ω–∞, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. YinYangAlign –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –ò–ò, –∞ —Ç–∞–∫–∂–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π. –°–∏—Å—Ç–µ–º–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–∏–º–µ–Ω—è—è –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, —É—Å–ø–µ—à–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö."
                },
                "en": {
                    "title": "Enhancing Image Generation Fidelity with YinYangAlign",
                    "desc": "This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation."
                },
                "zh": {
                    "title": "ÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁ≥ªÁªüÁöÑÂØπÈΩêÁ≤æÂ∫¶",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÔºàT2IÔºâÁ≥ªÁªü‰∏≠Á≤æÁ°ÆÂØπÈΩêÁöÑÈáçË¶ÅÊÄßÔºå‰ª•Á°Æ‰øùÁîüÊàêÁöÑÂõæÂÉèÊó¢ËÉΩÂáÜÁ°ÆÂèçÊò†Áî®Êà∑ÊÑèÂõæÔºåÂèàÁ¨¶Âêà‰º¶ÁêÜÂíåÁæéÂ≠¶Ê†áÂáÜ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÉèGoogle GeminiËøôÊ†∑ÁöÑ‰∫ã‰ª∂Âá∏Êòæ‰∫ÜÂº∫Â§ßÂØπÈΩêÊú∫Âà∂ÁöÑÂøÖË¶ÅÊÄß„ÄÇ‰∏éÊ≠§Áõ∏ÊØîÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂØπÈΩêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºåÁ†îÁ©∂‰∫∫ÂëòÂ∏åÊúõÂ∞ÜÁ±ª‰ººÁöÑÂØπÈΩêÊäÄÊúØÂ∫îÁî®‰∫éT2IÁ≥ªÁªüÔºå‰ª•ÊèêÈ´òÂõæÂÉèÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜYinYangAlignÔºåËøôÊòØ‰∏Ä‰∏™ÂÖàËøõÁöÑÂü∫ÂáÜÊ°ÜÊû∂ÔºåÁ≥ªÁªüÂú∞ÈáèÂåñT2IÁ≥ªÁªüÁöÑÂØπÈΩê‰øùÁúüÂ∫¶ÔºåËß£ÂÜ≥‰∫ÜÂÖ≠‰∏™Âü∫Êú¨‰∏îÂÜÖÂú®ÁüõÁõæÁöÑËÆæËÆ°ÁõÆÊ†á„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04689",
            "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
            "url": "https://huggingface.co/papers/2502.04689",
            "abstract": "Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (\"think step by step\"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.",
            "score": 2,
            "issue_id": 2123,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "a052e3be1fe147cd",
            "authors": [
                "Yuwei Yin",
                "Giuseppe Carenini"
            ],
            "affiliations": [
                "University of British Columbia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04689.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "ARR: –ù–æ–≤—ã–π —à–∞–≥ –≤ —É–ª—É—á—à–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ARR. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —à–∞–≥–∞: –∞–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. ARR –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –∏ –º–µ—Ç–æ–¥–æ–º Chain-of-Thought –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å ARR –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ç–∏–ø–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "ARR: A Structured Approach to Boost LLM Reasoning",
                    "desc": "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."
                },
                "zh": {
                    "title": "ARRÔºöÊèêÂçáÈóÆÁ≠îÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈõ∂-shotÊèêÁ§∫ÊñπÊ≥ïARRÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÈ°πÈÄâÊã©ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇARRÊòéÁ°ÆÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÊ≠•È™§ÔºöÂàÜÊûêÈóÆÈ¢òÊÑèÂõæ„ÄÅÊ£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØÂíåÈÄêÊ≠•Êé®ÁêÜ„ÄÇÈÄöËøáÂú®Â§öÁßçÂ§çÊùÇÈóÆÁ≠î‰ªªÂä°‰∏äÁöÑÂÆûÈ™åÔºåARR consistently outperform‰∫Ü‰º†ÁªüÁöÑÂü∫Á∫øÊñπÊ≥ïÂíåChain-of-ThoughtÔºàCoTÔºâÊèêÁ§∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊÑèÂõæÂàÜÊûêÂú®ARR‰∏≠Ëµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®ÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÊØè‰∏™ÁªÑÊàêÈÉ®ÂàÜÁöÑÁßØÊûÅË¥°ÁåÆ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04376",
            "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
            "url": "https://huggingface.co/papers/2502.04376",
            "abstract": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.",
            "score": 1,
            "issue_id": 2121,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 5",
                "zh": "2Êúà5Êó•"
            },
            "hash": "049ab6200a9d2eae",
            "authors": [
                "Lingxiang Hu",
                "Shurun Yuan",
                "Xiaoting Qin",
                "Jue Zhang",
                "Qingwei Lin",
                "Dongmei Zhang",
                "Saravan Rajmohan",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Northeastern University, China",
                "Peking University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04376.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#agi",
                    "#benchmark"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "LLM –∫–∞–∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –¥–µ–ª–µ–≥–∞—Ç—ã: –±—É–¥—É—â–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–µ—â–∞–Ω–∏–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —É—á–∞—Å—Ç–∏—è –≤ —Å–æ–≤–µ—â–∞–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–º—ã —Å–æ–≤–µ—â–∞–Ω–∏–π, –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –≤ —Ä–æ–ª–∏ –¥–µ–ª–µ–≥–∞—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–∫–æ–ª–æ 60% –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–Ω–∏–∂–µ–Ω–∏–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–ª–µ–≥–∞—Ç–æ–≤ –Ω–∞ —Å–æ–≤–µ—â–∞–Ω–∏—è—Ö, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Empowering Meetings with LLMs: Balancing Engagement and Efficiency",
                    "desc": "This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications."
                },
                "zh": {
                    "title": "Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñ‰ºöËÆÆÂèÇ‰∏é",
                    "desc": "Âú®Áé∞‰ª£Â∑•‰ΩúÂú∫ÊâÄÔºå‰ºöËÆÆÊòØ‰∫§ÊµÅÊÄùÊÉ≥ÂíåÁ°Æ‰øùÂõ¢Èòü‰∏ÄËá¥ÊÄßÁöÑÈáçË¶ÅÁéØËäÇÔºå‰ΩÜÂ∏∏Â∏∏Èù¢‰∏¥Êó∂Èó¥Ê∂àËÄó„ÄÅÊó•Á®ãÂÜ≤Á™ÅÂíåÂèÇ‰∏éÊïàÁéá‰Ωé‰∏ãÁ≠âÊåëÊàò„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ºöËÆÆ‰∏≠ÊúâÊïàÂàÜÈÖçÂèÇ‰∏éËÄÖÁöÑËÉΩÂäõÔºåÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âü∫‰∫éLLMÁöÑ‰ºöËÆÆ‰ª£ÁêÜÁ≥ªÁªüÂéüÂûãÔºåÂπ∂‰ΩøÁî®ÁúüÂÆû‰ºöËÆÆËÆ∞ÂΩïÂàõÂª∫‰∫ÜÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØï„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåGPT-4/4oÂú®ÁßØÊûÅÂíåË∞®ÊÖéÁöÑÂèÇ‰∏éÁ≠ñÁï•‰πãÈó¥‰øùÊåÅ‰∫ÜÂπ≥Ë°°ÔºåËÄåGemini 1.5 ProÂàôÊõ¥ÂÄæÂêë‰∫éË∞®ÊÖéÔºåGemini 1.5 FlashÂíåLlama3-8B/70BÂàôË°®Áé∞Âá∫Êõ¥ÁßØÊûÅÁöÑÂÄæÂêë„ÄÇÂ∞ΩÁÆ°Á∫¶60%ÁöÑÂõûÂ∫îÊ∂µÁõñ‰∫ÜËá≥Â∞ë‰∏Ä‰∏™ÂÖ≥ÈîÆÁÇπÔºå‰ΩÜ‰ªçÈúÄÊîπËøõ‰ª•ÂáèÂ∞ëÊó†ÂÖ≥ÊàñÈáçÂ§çÂÜÖÂÆπÔºåÂπ∂ÊèêÈ´òÂØπÁúüÂÆûÂú∫ÊôØ‰∏≠ËΩ¨ÂΩïÈîôËØØÁöÑÂÆπÂøçÂ∫¶„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-02-07.html",
    "link_next": "2025-02-11.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2Êúà7Êó•"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2Êúà11Êó•"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 14,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 0,
        "#agi": 4,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÊªëÂä®Á£ÅË¥¥Ê≥®ÊÑèÂäõÔºàSTAÔºâÔºå‰ª•Ëß£ÂÜ≥Êâ©Êï£ÂèòÂéãÂô®ÔºàDiTsÔºâÂú®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÂÖ®Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÁîüÊàêÁü≠ËßÜÈ¢ëÊó∂ËÄóÊó∂ÊûÅÈïø„ÄÇSTAÈÄöËøáÂú®Â±ÄÈÉ®3DÁ™óÂè£ÂÜÖÊªëÂä®ÂíåÂÖ≥Ê≥®ÔºåÊ∂àÈô§‰∫ÜÂÖ®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑÂÜó‰Ωô„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®Á°¨‰ª∂ÊÑüÁü•ÁöÑËÆæËÆ°ÔºåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåSTAÂú®‰∏çÈôç‰ΩéË¥®ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑÂª∂Ëøü„ÄÇ",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÊªëÂä®Á£ÅË¥¥Ê≥®ÊÑèÂäõÔºàSTAÔºâÔºå‰ª•Ëß£ÂÜ≥Êâ©Êï£ÂèòÂéãÂô®ÔºàDiTsÔºâÂú®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÂÖ®Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÁîüÊàêÁü≠ËßÜÈ¢ëÊó∂ËÄóÊó∂ÊûÅÈïø„ÄÇSTAÈÄöËøáÂú®Â±ÄÈÉ®3DÁ™óÂè£ÂÜÖÊªëÂä®ÂíåÂÖ≥Ê≥®ÔºåÊ∂àÈô§‰∫ÜÂÖ®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑÂÜó‰Ωô„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®Á°¨‰ª∂ÊÑüÁü•ÁöÑËÆæËÆ°ÔºåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåSTAÂú®‰∏çÈôç‰ΩéË¥®ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑÂª∂Ëøü„ÄÇ\n\nzh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le yƒ´ zh«íng xƒ´n de zh√π y√¨ l√¨ jƒ´ zh√¨, chƒìng w√©i hu√° d√≤ng c√≠ tiƒì zh√π y√¨ l√¨ (STA), y«ê jiƒõ ju√© ku√≤ s√†n bi√†n sh≈´ q√¨ (DiTs) z√†i sh√¨ p√≠n shƒìng ch√©ng zh≈çng de j√¨ su√†n ch√©ng bƒõn w√®n t√≠. chu√°n t«íng de qu√°n zh√π y√¨ l√¨ jƒ´ zh√¨ z√†i shƒìng ch√©ng du«én sh√¨ p√≠n sh√≠ h√†o sh√≠ j√≠ ch√°ng. STA t≈çng gu√≤ z√†i j√∫ b√π 3D chuƒÅng k«íu n√®i hu√° d√≤ng h√© guƒÅn zh√π, xiƒÅo ch√∫ le qu√°n zh√π y√¨ l√¨ zh≈çng de r√≥ng y√π. y«î chu√°n t«íng de hu√° d√≤ng chuƒÅng k«íu zh√π y√¨ l√¨ b√π t√≥ng, STA c«éi y√≤ng y√¨ng ji√†n g«én zhƒ´ de sh√® j√¨, t√≠ gƒÅo le xi√†o l«ú. sh√≠ y√†n xi«én sh√¨, STA z√†i b√π ji√†ng dƒ´ zh√¨ li√†ng de q√≠ng ku√†ng xi√†, xi«én zh√π ji«én sh«éo le sh√¨ p√≠n shƒìng ch√©ng de y√°n ch√≠.",
        "vocab": "[{'word': 'Ê≥®ÊÑèÂäõ', 'pinyin': 'zh√πy√¨l√¨', 'trans': 'attention'},\n{'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´zh√¨', 'trans': 'mechanism'},\n{'word': 'ÊªëÂä®', 'pinyin': 'hu√°d√≤ng', 'trans': 'sliding'},\n{'word': 'Á£ÅË¥¥', 'pinyin': 'c√≠tiƒì', 'trans': 'magnetic tile'},\n{'word': 'Êâ©Êï£', 'pinyin': 'ku√≤s√†n', 'trans': 'diffusion'},\n{'word': 'ÂèòÂéãÂô®', 'pinyin': 'bi√†nyƒÅq√¨', 'trans': 'transformer'},\n{'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'},\n{'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ngbƒõn', 'trans': 'cost'},\n{'word': '‰º†Áªü', 'pinyin': 'chu√°nt«íng', 'trans': 'traditional'},\n{'word': 'ÂÖ®', 'pinyin': 'qu√°n', 'trans': 'full'},\n{'word': 'ËÄóÊó∂', 'pinyin': 'h√†osh√≠', 'trans': 'time-consuming'},\n{'word': 'ÊûÅÈïø', 'pinyin': 'j√≠ch√°ng', 'trans': 'extremely long'},\n{'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫b√π', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': 'Á™óÂè£', 'pinyin': 'chuƒÅngk«íu', 'trans': 'window'},\n{'word': 'ÂÜó‰Ωô', 'pinyin': 'r«íngy√∫', 'trans': 'redundancy'},\n{'word': 'Á°¨‰ª∂', 'pinyin': 'y√¨ngji√†n', 'trans': 'hardware'},\n{'word': 'ÊÑüÁü•', 'pinyin': 'g«énzhƒ´', 'trans': 'perception'},\n{'word': 'ËÆæËÆ°', 'pinyin': 'sh√®j√¨', 'trans': 'design'},\n{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},\n{'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'display'},\n{'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'},\n{'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«énsh«éo', 'trans': 'reduce'},\n{'word': 'Âª∂Ëøü', 'pinyin': 'y√°nch√≠', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}