{
    "date": {
        "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 31",
        "zh": "10æœˆ31æ—¥"
    },
    "time_utc": "2025-10-31 03:40",
    "weekday": 4,
    "issue_id": 6714,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.26583",
            "title": "Emu3.5: Native Multimodal Models are World Learners",
            "url": "https://huggingface.co/papers/2510.26583",
            "abstract": "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.",
            "score": 20,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "e453f75d16182f5e",
            "authors": [
                "Yufeng Cui",
                "Honghao Chen",
                "Haoge Deng",
                "Xu Huang",
                "Xinghang Li",
                "Jirong Liu",
                "Yang Liu",
                "Zhuoyan Luo",
                "Jinsheng Wang",
                "Wenxuan Wang",
                "Yueze Wang",
                "Chengyuan Wang",
                "Fan Zhang",
                "Yingli Zhao",
                "Ting Pan",
                "Xianduo Li",
                "Zecheng Hao",
                "Wenxuan Ma",
                "Zhuo Chen",
                "Yulong Ao",
                "Tiejun Huang",
                "Zhongyuan Wang",
                "Xinlong Wang"
            ],
            "affiliations": [
                "BAAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26583.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#inference",
                    "#multimodal",
                    "#rl",
                    "#agi",
                    "#cv",
                    "#games"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Emu3.5 â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 10 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Discrete Diffusion Adaptation (DiDA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Gemini 2.5 Flash."
                },
                "en": {
                    "title": "Emu3.5: Revolutionizing Multimodal Predictions with Speed and Precision",
                    "desc": "Emu3.5 is a large-scale multimodal world model that predicts future states using both vision and language inputs. It is trained on a massive dataset of over 10 trillion tokens, allowing it to generate outputs that seamlessly combine visual and textual information. The model employs a novel technique called Discrete Diffusion Adaptation (DiDA) to enhance inference speed, achieving a 20x improvement in efficiency. Emu3.5 demonstrates advanced capabilities in multimodal reasoning, world modeling, and generation tasks, outperforming existing models in various benchmarks."
                },
                "zh": {
                    "title": "Emu3.5ï¼šå¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹çš„æœªæ¥",
                    "desc": "Emu3.5æ˜¯ä¸€ç§å¤§è§„æ¨¡çš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œåœ¨åŒ…å«è¶…è¿‡10ä¸‡äº¿ä¸ªæ ‡è®°çš„è§†è§‰-è¯­è¨€æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚è¯¥æ¨¡å‹æ”¯æŒäº¤é”™çš„è§†è§‰-è¯­è¨€è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒEmu3.5å¼•å…¥äº†ç¦»æ•£æ‰©æ•£é€‚åº”ï¼ˆDiDAï¼‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ï¼Œä½¿æ¯å¼ å›¾åƒçš„æ¨ç†é€Ÿåº¦æé«˜çº¦20å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26802",
            "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
            "url": "https://huggingface.co/papers/2510.26802",
            "abstract": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",
            "score": 13,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "b84e32b67921db6d",
            "authors": [
                "Ziyu Guo",
                "Xinyan Chen",
                "Renrui Zhang",
                "Ruichuan An",
                "Yu Qi",
                "Dongzhi Jiang",
                "Xiangtai Li",
                "Manyuan Zhang",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "IMIXR",
                "MMLab",
                "Northeastern University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26802.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¸: Ğ¾Ğ±ĞµÑ‰Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ»Ğ¸, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Veo-3 Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ zero-shot reasoner Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ 12 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ embodied Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MME-CoF. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸, ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ reasoning ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Exploring the Reasoning Limits of Video Generation Models",
                    "desc": "This paper investigates the reasoning capabilities of advanced video generation models, particularly focusing on Veo-3. The authors assess the model's performance across various reasoning dimensions, such as spatial and temporal logic, using a newly created benchmark called MME-CoF. While the findings show that these models can handle short-term spatial coherence and local dynamics well, they struggle with long-term causal reasoning and abstract logic. Ultimately, the study concludes that while these video models are not yet reliable as independent zero-shot reasoners, they show potential as supportive tools in visual reasoning tasks."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç ”ç©¶",
                    "desc": "æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘ï¼Œè¡¨æ˜å®ƒä»¬å¯èƒ½ç¼–ç äº†å¤§é‡çš„ä¸–ç•ŒçŸ¥è¯†ã€‚é™¤äº†ç°å®åˆæˆå¤–ï¼Œè¿™äº›æ¨¡å‹è¿˜è¡¨ç°å‡ºè§†è§‰æ„ŸçŸ¥ã€å»ºæ¨¡å’Œæ“æ§çš„è¡Œä¸ºã€‚æœ¬æ–‡é€šè¿‡å¯¹é¢†å…ˆçš„Veo-3æ¨¡å‹è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¯„ä¼°å…¶åœ¨12ä¸ªç»´åº¦ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´ã€å‡ ä½•ã€ç‰©ç†ã€æ—¶é—´å’Œå…·èº«é€»è¾‘ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å½“å‰è§†é¢‘æ¨¡å‹åœ¨çŸ­æœŸç©ºé—´ä¸€è‡´æ€§å’Œå±€éƒ¨åŠ¨æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é•¿æœŸå› æœæ¨ç†å’ŒæŠ½è±¡é€»è¾‘æ–¹é¢ä»ç„¶æœ‰é™ï¼Œå°šä¸å…·å¤‡ä½œä¸ºç‹¬ç«‹é›¶-shotæ¨ç†å™¨çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26800",
            "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
            "url": "https://huggingface.co/papers/2510.26800",
            "abstract": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
            "score": 8,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "5ddc588a37cb2d17",
            "authors": [
                "Yukun Huang",
                "Jiwen Yu",
                "Yanning Zhou",
                "Jianan Wang",
                "Xintao Wang",
                "Pengfei Wan",
                "Xihui Liu"
            ],
            "affiliations": [
                "Astribot",
                "Kuaishou Technology",
                "Tencent",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26800.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#3d",
                    "#synthetic",
                    "#games"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ 3D-ÑÑ†ĞµĞ½Ğ°Ğ¼ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniX â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D generative Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… prior'Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ PBR-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ cross-modal Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ completion Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ 3D-ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ physically based rendering, Ñ€ĞµĞ»Ğ°Ğ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "OmniX: Bridging 2D Generative Models for Realistic 3D Scene Creation",
                    "desc": "This paper introduces OmniX, a novel framework that enhances panorama-based 2D lifting for generating realistic 3D scenes. It utilizes advanced 2D generative models to perceive and create geometry, textures, and materials suitable for physically based rendering (PBR). Unlike traditional methods that focus solely on visual appearance, OmniX integrates intrinsic properties into the generation process. The authors also present a large-scale synthetic panorama dataset to support various panoramic vision tasks, demonstrating the model's effectiveness in creating immersive virtual environments."
                },
                "zh": {
                    "title": "OmniXï¼šå…¨æ™¯è§†è§‰çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ„å»º3Dåœºæ™¯çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šç¨‹åºç”Ÿæˆå’Œ2Dæå‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºOmniXçš„ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºå¤§çš„2Dç”Ÿæˆæ¨¡å‹æ¥ç”Ÿæˆé€‚åˆç‰©ç†åŸºç¡€æ¸²æŸ“çš„3Dåœºæ™¯ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOmniXä¸ä»…å…³æ³¨å¤–è§‚ç”Ÿæˆï¼Œè¿˜é‡è§†å‡ ä½•ã€çº¹ç†å’Œææ–™çš„å†…åœ¨å±æ€§æ„ŸçŸ¥ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆå…¨æ™¯æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒOmniXåœ¨å…¨æ™¯è§†è§‰æ„ŸçŸ¥å’Œ3Dåœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†æ²‰æµ¸å¼è™šæ‹Ÿä¸–ç•Œçš„ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26794",
            "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
            "url": "https://huggingface.co/papers/2510.26794",
            "abstract": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
            "score": 7,
            "issue_id": 6714,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "930f4b60c73c7768",
            "authors": [
                "Jing Lin",
                "Ruisi Wang",
                "Junzhe Lu",
                "Ziqi Huang",
                "Guorui Song",
                "Ailing Zeng",
                "Xian Liu",
                "Chen Wei",
                "Wanqi Yin",
                "Qingping Sun",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Nanyang Technological University",
                "SenseTime Research",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26794.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#open_source",
                    "#cv",
                    "#multimodal",
                    "#diffusion",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ViMoGen-228K Ğ¸Ğ· 228 Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ motion capture, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¾Ñ‚ video generation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ViMoGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ diffusion transformer Ñ flow matching Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ conditioning Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MBench Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging Video and Motion: A New Era for 3D Human Motion Generation",
                    "desc": "This paper addresses the limitations of current 3D human motion generation (MoGen) models in generalization by leveraging insights from video generation (ViGen). The authors introduce a new dataset, ViMoGen-228K, which contains 228,000 high-quality motion samples that combine motion capture data with semantically rich annotations from web videos. They propose a novel model, ViMoGen, which utilizes a flow-matching-based diffusion transformer to integrate knowledge from both MoGen and ViGen, and also introduce a lighter version, ViMoGen-light, for improved efficiency. Finally, they create MBench, a benchmark for evaluating motion generation quality, demonstrating that their framework significantly enhances performance in various evaluation metrics."
                },
                "zh": {
                    "title": "çŸ¥è¯†è½¬ç§»ï¼Œæå‡ä¸‰ç»´åŠ¨ä½œç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†è§†é¢‘ç”Ÿæˆï¼ˆViGenï¼‰é¢†åŸŸçš„çŸ¥è¯†è½¬ç§»åˆ°ä¸‰ç»´äººç±»åŠ¨ä½œç”Ÿæˆï¼ˆMoGenï¼‰ä¸­ï¼Œä»¥æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ViMoGen-228Kï¼ŒåŒ…å«228,000ä¸ªé«˜è´¨é‡åŠ¨ä½œæ ·æœ¬ï¼Œç»“åˆäº†é«˜ä¿çœŸå…‰å­¦åŠ¨ä½œæ•æ‰æ•°æ®å’Œè¯­ä¹‰æ³¨é‡Šçš„åŠ¨ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ViMoGenï¼Œä¸€ä¸ªåŸºäºæµåŒ¹é…çš„æ‰©æ•£å˜æ¢å™¨ï¼Œé€šè¿‡é—¨æ§å¤šæ¨¡æ€æ¡ä»¶åŒ–æ¥ç»Ÿä¸€MoCapæ•°æ®å’ŒViGenæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†MBenchï¼Œä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼Œç”¨äºå¯¹åŠ¨ä½œè´¨é‡ã€æç¤ºä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›è¿›è¡Œç»†è‡´è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶åœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26692",
            "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
            "url": "https://huggingface.co/papers/2510.26692",
            "abstract": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.",
            "score": 7,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "0b8a017f9816e001",
            "authors": [
                "Kimi Team",
                "Yu Zhang",
                "Zongyu Lin",
                "Xingcheng Yao",
                "Jiaxi Hu",
                "Fanqing Meng",
                "Chengyin Liu",
                "Xin Men",
                "Songlin Yang",
                "Zhiyuan Li",
                "Wentao Li",
                "Enzhe Lu",
                "Weizhou Liu",
                "Yanru Chen",
                "Weixin Xu",
                "Longhui Yu",
                "Yejie Wang",
                "Yu Fan",
                "Longguang Zhong",
                "Enming Yuan",
                "Dehao Zhang",
                "Yizhi Zhang",
                "T. Y. Liu",
                "Haiming Wang",
                "Shengjun Fang",
                "Weiran He",
                "Shaowei Liu",
                "Yiwei Li",
                "Jianlin Su",
                "Jiezhong Qiu",
                "Bo Pang",
                "Junjie Yan",
                "Zhejun Jiang",
                "Weixiao Huang",
                "Bohong Yin",
                "Jiacheng You",
                "Chu Wei",
                "Zhengtao Wang",
                "Chao Hong",
                "Yutian Chen",
                "Guanduo Chen",
                "Yucheng Wang",
                "Huabin Zheng",
                "Feng Wang",
                "Yibo Liu",
                "Mengnan Dong",
                "Zheng Zhang",
                "Siyuan Pan",
                "Wenhao Wu",
                "Yuhao Wu",
                "Longyu Guan",
                "Jiawen Tao",
                "Guohong Fu",
                "Xinran Xu",
                "Yuzhi Wang",
                "Guokun Lai",
                "Yuxin Wu",
                "Xinyu Zhou",
                "Zhilin Yang",
                "Yulun Du"
            ],
            "affiliations": [
                "MoonshotAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26692.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#long_context",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Kimi Linear Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… â€” Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Kimi Delta Attention (KDA) â€” Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ RNN. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 3B Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ½Ğ° 75% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² 6 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 1M Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Kimi Linear: Revolutionizing Attention with Efficiency and Performance",
                    "desc": "Kimi Linear is a new hybrid linear attention architecture that surpasses traditional full attention methods in various contexts, including short and long sequences, as well as reinforcement learning scenarios. It features Kimi Delta Attention (KDA), which enhances the Gated DeltaNet model with a more precise gating mechanism, optimizing the use of limited RNN memory. The architecture employs a unique chunkwise algorithm that leverages Diagonal-Plus-Low-Rank (DPLR) transition matrices to significantly lower computational costs while adhering to classical delta rules. Experimental results indicate that Kimi Linear not only outperforms full Multi-Head Latent Attention (MLA) but also reduces key-value cache usage and increases decoding speed, making it a highly efficient alternative for attention-based tasks."
                },
                "zh": {
                    "title": "Kimi Linearï¼šè¶…è¶Šå…¨æ³¨æ„åŠ›çš„é«˜æ•ˆæ¶æ„",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKimi Linearçš„æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œå®ƒé¦–æ¬¡åœ¨å…¬å¹³æ¯”è¾ƒä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‚ç”¨äºçŸ­ä¸Šä¸‹æ–‡ã€é•¿ä¸Šä¸‹æ–‡å’Œå¼ºåŒ–å­¦ä¹ ç­‰å¤šç§åœºæ™¯ã€‚å…¶æ ¸å¿ƒæ˜¯Kimi Delta Attentionï¼ˆKDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¨è¾¾èƒ½åŠ›å¼ºçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡æ›´ç»†ç²’åº¦çš„é—¨æ§æœºåˆ¶ï¼Œæå‡äº†æœ‰é™çŠ¶æ€RNNå†…å­˜çš„ä½¿ç”¨æ•ˆç‡ã€‚æˆ‘ä»¬è®¾è®¡çš„åˆ†å—ç®—æ³•é€šè¿‡ç‰¹æ®Šçš„å¯¹è§’åŠ ä½ç§©ï¼ˆDPLRï¼‰è½¬ç§»çŸ©é˜µå˜ä½“ï¼Œå®ç°äº†é«˜æ•ˆçš„ç¡¬ä»¶åˆ©ç”¨ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ç»å…¸å¢é‡è§„åˆ™çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKimi Linearåœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå…¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼Œå¹¶ä¸”åœ¨å¤„ç†é•¿è¾“å…¥å’Œè¾“å‡ºæ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25992",
            "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.25992",
            "abstract": "Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.",
            "score": 2,
            "issue_id": 6713,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "ab88bca35e9e2ff1",
            "authors": [
                "Yihe Deng",
                "I-Hung Hsu",
                "Jun Yan",
                "Zifeng Wang",
                "Rujun Han",
                "Gufeng Zhang",
                "Yanfei Chen",
                "Wei Wang",
                "Tomas Pfister",
                "Chen-Yu Lee"
            ],
            "affiliations": [
                "Google Cloud",
                "Google Cloud AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25992.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Supervised Reinforcement Learning (SRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², SRL ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, ÑĞ»ĞµĞ´ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ reinforcement learning Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs with Supervised Reinforcement Learning for Better Reasoning",
                    "desc": "This paper introduces Supervised Reinforcement Learning (SRL) as a new approach to improve the reasoning capabilities of Large Language Models (LLMs). SRL reformulates problem-solving into generating a sequence of logical actions, allowing the model to engage in internal reasoning before making decisions. By providing smoother rewards based on the similarity to expert actions, SRL enhances learning even when initial attempts are incorrect. The framework not only improves performance on reasoning tasks but also generalizes well to software engineering applications, making it a versatile tool for training LLMs."
                },
                "zh": {
                    "title": "ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼šæå‡å¤šæ­¥æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦å¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹å°è§„æ¨¡å¼€æºæ¨¡å‹ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ­£ç¡®è§£ç­”ç¨€å°‘çš„æƒ…å†µä¸‹æ•ˆæœä¸ä½³ï¼Œè€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆ™å®¹æ˜“é€šè¿‡é€å­—æ¨¡ä»¿å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆSRLï¼‰ï¼Œè¯¥æ¡†æ¶å°†é—®é¢˜è§£å†³é‡æ–°å®šä¹‰ä¸ºç”Ÿæˆä¸€ç³»åˆ—é€»è¾‘â€œåŠ¨ä½œâ€ã€‚SRLé€šè¿‡åœ¨æ¯ä¸ªåŠ¨ä½œä¹‹å‰ç”Ÿæˆå†…éƒ¨æ¨ç†ç‹¬ç™½ï¼Œæä¾›åŸºäºæ¨¡å‹åŠ¨ä½œä¸ä¸“å®¶åŠ¨ä½œç›¸ä¼¼åº¦çš„å¹³æ»‘å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆæå‡å°æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26787",
            "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
            "url": "https://huggingface.co/papers/2510.26787",
            "abstract": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",
            "score": 1,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "4b733966c35e82f0",
            "authors": [
                "Mantas Mazeika",
                "Alice Gatti",
                "Cristina Menghini",
                "Udari Madhushani Sehwag",
                "Shivam Singhal",
                "Yury Orlovskiy",
                "Steven Basart",
                "Manasi Sharma",
                "Denis Peskoff",
                "Elaine Lau",
                "Jaehyuk Lim",
                "Lachlan Carroll",
                "Alice Blair",
                "Vinaya Sivakumar",
                "Sumana Basu",
                "Brad Kenstler",
                "Yuntao Ma",
                "Julian Michael",
                "Xiaoke Li",
                "Oliver Ingebretsen",
                "Aditya Mehta",
                "Jean Mottola",
                "John Teichmann",
                "Kevin Yu",
                "Zaina Shaik",
                "Adam Khoja",
                "Richard Ren",
                "Jason Hausenloy",
                "Long Phan",
                "Ye Htet",
                "Ankit Aich",
                "Tahseen Rabbani",
                "Vivswan Shah",
                "Andriy Novykov",
                "Felix Binder",
                "Kirill Chugunov",
                "Luis Ramirez",
                "Matias Geralnik",
                "HernÃ¡n Mesura",
                "Dean Lee",
                "Ed-Yeremai Hernandez Cardona",
                "Annette Diamond",
                "Summer Yue",
                "Alexandr Wang",
                "Bing Liu",
                "Ernesto Hernandez",
                "Dan Hendrycks"
            ],
            "affiliations": [
                "Center for AI Safety",
                "Scale AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26787.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#science",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¢",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ÑƒĞ´Ğ°: AI Ğ¿Ğ¾ĞºĞ° ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ 2.5% Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Remote Labor Index (RLI) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 2.5% Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ AI Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ÑƒĞ´Ğ°, Ğ¾Ñ‚Ğ´ĞµĞ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° AI Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ Ñ‚Ñ€ÑƒĞ´Ğ°."
                },
                "en": {
                    "title": "Measuring AI's Real-World Impact on Labor Automation",
                    "desc": "This paper introduces the Remote Labor Index (RLI), a new benchmark designed to assess the performance of AI agents in real-world economic tasks. The RLI evaluates how well AI can automate valuable projects across various sectors, providing a practical measure of AI's capabilities. The findings reveal that current AI agents perform poorly on the RLI, with the best achieving only a 2.5% automation rate. This research aims to provide empirical evidence for discussions about AI's impact on labor and help stakeholders understand and manage the implications of AI-driven automation."
                },
                "zh": {
                    "title": "é‡åŒ–AIè‡ªåŠ¨åŒ–çš„ç»æµä»·å€¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œç§°ä¸ºè¿œç¨‹åŠ³åŠ¨æŒ‡æ•°ï¼ˆRLIï¼‰ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½åœ¨å®é™…ç»æµé¡¹ç›®ä¸­çš„è¡¨ç°ã€‚RLIæ˜¯ä¸€ä¸ªå¤šè¡Œä¸šçš„åŸºå‡†ï¼Œæ—¨åœ¨è¡¡é‡AIä»£ç†åœ¨çœŸå®ä¸–ç•Œä¸­çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒAIä»£ç†åœ¨RLIä¸Šçš„è¡¨ç°æ¥è¿‘æœ€ä½æ°´å¹³ï¼Œæœ€é«˜çš„è‡ªåŠ¨åŒ–ç‡ä»…ä¸º2.5%ã€‚è¿™äº›ç»“æœä¸ºAIè‡ªåŠ¨åŒ–çš„è®¨è®ºæä¾›äº†å®è¯ä¾æ®ï¼Œå¸®åŠ©åˆ©ç›Šç›¸å…³è€…æ›´å¥½åœ°ç†è§£å’Œåº”å¯¹AIé©±åŠ¨çš„åŠ³åŠ¨è‡ªåŠ¨åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26298",
            "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
            "url": "https://huggingface.co/papers/2510.26298",
            "abstract": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.",
            "score": 1,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "17bebfc38acb1c32",
            "authors": [
                "Jingran Zhang",
                "Ning Li",
                "Justin Cui"
            ],
            "affiliations": [
                "UC San Diego",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26298.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞÑ‚Ğ»Ğ°Ñ Ğ¾Ñ‚ OpenAI: ÑĞ¸Ğ»Ñ‘Ğ½ Ğ² Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ, ÑĞ»Ğ°Ğ± Ğ² Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ChatGPT Atlas, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑƒÑ€ÑĞ¾Ñ€ Ğ¸ ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…: T-Rex Runner, Sudoku, Flappy Bird Ğ¸ Stein.world. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ ÑÑƒĞ´Ğ¾ĞºÑƒ, Ñ€ĞµÑˆĞ°Ñ Ğ¸Ñ… Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ»ÑĞ´ĞµĞ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ°ÑÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ AI Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ° Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Evaluating Atlas: Strong in Logic, Struggling in Real-Time Interaction",
                    "desc": "OpenAI's ChatGPT Atlas enhances web interaction by allowing the model to analyze webpages and perform user actions like clicking and typing. This paper evaluates Atlas's performance in browser-based games to understand its capabilities in dynamic environments. The results indicate that Atlas excels in logical reasoning tasks, completing puzzles like Sudoku faster than humans, but struggles with real-time games that require quick reflexes and precise control. These findings highlight the model's strengths in analytical tasks while revealing significant limitations in interactive scenarios that demand immediate responses."
                },
                "zh": {
                    "title": "Atlasï¼šç½‘é¡µäº¤äº’çš„æ–°æ¢ç´¢",
                    "desc": "OpenAIçš„ChatGPT Atlaså¼•å…¥äº†æ–°çš„ç½‘é¡µäº¤äº’èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚å°½ç®¡å…¶åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„èƒ½åŠ›å·²å¾—åˆ°éªŒè¯ï¼Œä½†åœ¨åŠ¨æ€äº¤äº’ç¯å¢ƒä¸­çš„è¡¨ç°ä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨åŸºäºæµè§ˆå™¨çš„æ¸¸æˆï¼ˆå¦‚è°·æ­Œçš„T-Rex Runnerã€æ•°ç‹¬ã€Flappy Birdå’ŒStein.worldï¼‰è¿›è¡Œæ—©æœŸè¯„ä¼°ï¼Œé‡‡ç”¨æ¸¸æˆå†…è¡¨ç°åˆ†æ•°ä½œä¸ºé‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°ä¸åŒä»»åŠ¡ç±»å‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒAtlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°ç‹¬ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®Œæˆéš¾é¢˜çš„é€Ÿåº¦æ˜¾è‘—å¿«äºäººç±»åŸºçº¿ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®æ—¶æœºå’Œè¿åŠ¨æ§åˆ¶çš„å®æ—¶æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸æ— æ³•çªç ´åˆå§‹éšœç¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25628",
            "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
            "url": "https://huggingface.co/papers/2510.25628",
            "abstract": "Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.",
            "score": 1,
            "issue_id": 6714,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "68de771aa478bf84",
            "authors": [
                "Yusheng Liao",
                "Chaoyi Wu",
                "Junwei Liu",
                "Shuyang Jiang",
                "Pengcheng Qiu",
                "Haowen Wang",
                "Yun Yue",
                "Shuai Zhen",
                "Jian Wang",
                "Qianrui Fan",
                "Jinjie Gu",
                "Ya Zhang",
                "Yanfeng Wang",
                "Yu Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China",
                "Intelligence Healthcare Department, AntGroup, Hangzhou, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25628.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#healthcare",
                    "#reasoning",
                    "#training",
                    "#science"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "EHR-R1: AI-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EHR-R1 â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ EHR-Ins â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 300 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ 42 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ reinforcement learning. EHR-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 30 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MIMIC-Bench Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° 10% Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ AUROC Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½Ğ° EHRSHOT."
                },
                "en": {
                    "title": "Revolutionizing EHR Analysis with Enhanced Reasoning Models",
                    "desc": "This paper addresses the challenges of analyzing Electronic Health Records (EHRs) using large language models (LLMs). It introduces EHR-Ins, a comprehensive dataset designed for EHR reasoning, which includes 300,000 reasoning cases and 4 million non-reasoning cases across 42 tasks. The authors develop EHR-R1, a series of reasoning-enhanced LLMs that utilize a multi-stage training approach to improve their reasoning capabilities and domain knowledge for EHR analysis. The results demonstrate that EHR-R1 outperforms existing models, providing a significant advancement in the reliability and relevance of EHR analysis in clinical settings."
                },
                "zh": {
                    "title": "æå‡ç”µå­å¥åº·è®°å½•åˆ†æçš„æ™ºèƒ½æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ¨ç†æŒ‡ä»¤æ•°æ®é›†EHR-Insï¼ŒåŒ…å«30ä¸‡ä¸ªé«˜è´¨é‡æ¨ç†æ¡ˆä¾‹å’Œ400ä¸‡ä¸ªéæ¨ç†æ¡ˆä¾‹ï¼Œè¦†ç›–42ä¸ªä¸åŒçš„EHRä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ€ç»´å›¾çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†æ•°æ®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†EHR-R1ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹EHRåˆ†æçš„å¢å¼ºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡é«˜è¾¾720äº¿ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬é¢†åŸŸé€‚åº”ã€æ¨ç†å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒEHR-R1ç³»ç»Ÿæ€§åœ°è·å–äº†é¢†åŸŸçŸ¥è¯†å’Œå¤šæ ·çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†EHRåˆ†æçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25132",
            "title": "EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation",
            "url": "https://huggingface.co/papers/2510.25132",
            "abstract": "Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\\% in designability and 13\\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.",
            "score": 1,
            "issue_id": 6714,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "289ff7b39a55aab4",
            "authors": [
                "Chao Song",
                "Zhiyuan Liu",
                "Han Huang",
                "Liang Wang",
                "Qiong Wang",
                "Jianyu Shi",
                "Hui Yu",
                "Yihang Zhou",
                "Yang Zhang"
            ],
            "affiliations": [
                "Institute of Automation at CAS",
                "National University of Singapore",
                "Northwestern Polytechnical University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25132.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ EnzyControl â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EnzyBind Ñ 11,100 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚-ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ EnzyAdapter, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ backbone Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 13% Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ¸ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Enzyme Design with EnzyControl",
                    "desc": "This paper presents EnzyBind, a new dataset containing 11,100 validated enzyme-substrate pairs to enhance enzyme design in computational protein engineering. The authors introduce EnzyControl, a method that allows for the generation of enzyme backbones that are specifically tailored to bind certain substrates. EnzyControl utilizes a lightweight component called EnzyAdapter, which integrates with a pretrained model to improve substrate awareness during backbone generation. The results demonstrate significant improvements in designability and catalytic efficiency, outperforming existing models on benchmark tests."
                },
                "zh": {
                    "title": "æ™ºèƒ½è®¾è®¡ç‰¹å®šåŠŸèƒ½é…¶éª¨æ¶çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•EnzyControlï¼Œç”¨äºè®¾è®¡å…·æœ‰ç‰¹å®šåº•ç‰©åŠŸèƒ½çš„é…¶éª¨æ¶ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºEnzyBindçš„æ•°æ®é›†ï¼ŒåŒ…å«11,100å¯¹ç»è¿‡å®éªŒéªŒè¯çš„é…¶-åº•ç‰©é…å¯¹ï¼Œä»¥æ”¯æŒé…¶è®¾è®¡ã€‚EnzyControlé€šè¿‡åˆ©ç”¨å¤šåºåˆ—æ¯”å¯¹ï¼ˆMSAï¼‰æ³¨é‡Šçš„å‚¬åŒ–ä½ç‚¹å’Œç›¸åº”çš„åº•ç‰©ï¼Œç”Ÿæˆæ¡ä»¶åŒ–çš„é…¶éª¨æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEnzyControlåœ¨ç»“æ„å’ŒåŠŸèƒ½æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®¾è®¡èƒ½åŠ›å’Œå‚¬åŒ–æ•ˆç‡åˆ†åˆ«æé«˜äº†13%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26474",
            "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
            "url": "https://huggingface.co/papers/2510.26474",
            "abstract": "Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew effect\"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.",
            "score": 0,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "d7c19ec426ccd7e8",
            "authors": [
                "Xin Guo",
                "Zhiheng Xi",
                "Yiwen Ding",
                "Yitao Zhai",
                "Xiaowei Shi",
                "Xunliang Cai",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Fudan University",
                "Meituan",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26474.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ¼ ĞœĞ°Ñ‚Ñ„ĞµÑ Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLMs): Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ \"ÑÑ„Ñ„ĞµĞºÑ‚ ĞœĞ°Ñ‚Ñ„ĞµÑ\", ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‘ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑƒÑ‰ĞµÑ€Ğ± ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2-VL Ğ¸ InternVL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… reasoning ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° 3.86 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Balancing Reasoning Skills in Vision-Language Models",
                    "desc": "This paper addresses a significant challenge in self-improvement for large vision-language models (LVLMs), where the models tend to excel at simple queries but struggle with complex ones, leading to an imbalance in their reasoning capabilities. This phenomenon, referred to as the \"Matthew effect,\" results in models focusing on easier tasks and neglecting more difficult reasoning challenges. To mitigate this issue, the authors propose four strategies aimed at re-balancing the model's learning process by reshaping data distribution and resampling trajectories. Their experiments show that these strategies enhance the models' visual reasoning abilities, achieving an average improvement of 3.86 points over traditional self-improvement methods."
                },
                "zh": {
                    "title": "å¹³è¡¡æ¨ç†èƒ½åŠ›ï¼Œæå‡æ¨¡å‹è¡¨ç°",
                    "desc": "è‡ªæˆ‘æå‡å·²æˆä¸ºæé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ï¼Œæ¨¡å‹é€šè¿‡è¿­ä»£æ¢ç´¢å’Œå­¦ä¹ æˆåŠŸçš„è½¨è¿¹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ¨¡å‹åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æŸ¥è¯¢ä¸Šå´åŠ›ä¸ä»å¿ƒã€‚è¿™å¯¼è‡´äº†ä¼˜åŒ–çš„ä¸å¹³è¡¡ï¼Œä½¿æ¨¡å‹æ›´å€¾å‘äºç®€å•æ¨ç†æŠ€èƒ½ï¼Œè€ŒæŠ‘åˆ¶äº†å…¶å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å››ç§é«˜æ•ˆç­–ç•¥ï¼Œä»¥å®ç°æ¢ç´¢å’Œå­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤´å°¾é‡å¹³è¡¡ï¼Œä»è€Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26160",
            "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
            "url": "https://huggingface.co/papers/2510.26160",
            "abstract": "Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.",
            "score": 0,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "6be5612dcf7842dd",
            "authors": [
                "Jiaqi Wang",
                "Xiao Yang",
                "Kai Sun",
                "Parth Suresh",
                "Sanat Sharma",
                "Adam Czyzewski",
                "Derek Andersen",
                "Surya Appini",
                "Arkav Banerjee",
                "Sajal Choudhary",
                "Shervin Ghasemlou",
                "Ziqiang Guan",
                "Akil Iyer",
                "Haidar Khan",
                "Lingkun Kong",
                "Roy Luo",
                "Tiffany Ma",
                "Zhen Qiao",
                "David Tran",
                "Wenfang Xu",
                "Skyler Yeatman",
                "Chen Zhou",
                "Gunveer Gujral",
                "Yinglong Xia",
                "Shane Moon",
                "Nicolas Scheffer",
                "Nirav Shah",
                "Eun Chang",
                "Yue Liu",
                "Florian Metze",
                "Tammy Stark",
                "Zhaleh Feizollahi",
                "Andrea Jessee",
                "Mangesh Pujari",
                "Ahmed Aly",
                "Babak Damavandi",
                "Rakesh Wanga",
                "Anuj Kumar",
                "Rohit Patel",
                "Wen-tau Yih",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "FAIR, Meta",
                "Meta",
                "Meta Reality Labs",
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26160.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ‘“",
                "ru": {
                    "title": "CRAG-MM: Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ² Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRAG-MM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ²Ñ€Ğ¾Ğ´Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 6,5 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 6,2 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑŠÑ‘Ğ¼ĞºÑƒ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 32-45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² KDD Cup 2025, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 28%."
                },
                "en": {
                    "title": "Advancing Multi-Modal Conversations with CRAG-MM Benchmark",
                    "desc": "This paper introduces CRAG-MM, a new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG) specifically designed for wearable devices like smart glasses. It includes 6.5K triplets of images, questions, and answers, along with 2K visual-based multi-turn conversations across various domains. The benchmark addresses real-world challenges by incorporating diverse question types and image-quality issues, facilitating the evaluation of retrieval methods. Results indicate that current RAG approaches have significant room for improvement, as evidenced by the performance metrics achieved in the KDD Cup 2025 competition."
                },
                "zh": {
                    "title": "å¯ç©¿æˆ´è®¾å¤‡çš„å¤šæ¨¡æ€å¯¹è¯æ–°åŸºå‡†",
                    "desc": "å¯ç©¿æˆ´è®¾å¤‡å¦‚æ™ºèƒ½çœ¼é•œæ­£åœ¨æ”¹å˜äººä»¬ä¸å‘¨å›´ç¯å¢ƒçš„äº’åŠ¨æ–¹å¼ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè·å–è§†é‡ä¸­å®ä½“çš„ä¿¡æ¯ã€‚å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰åœ¨æ”¯æŒæ­¤ç±»é—®é¢˜ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½†ç›®å‰å°šç¼ºä¹é’ˆå¯¹å¯ç©¿æˆ´åœºæ™¯çš„å…¨é¢åŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRAG-MMâ€”â€”ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤šè½®å¯¹è¯çš„ç»¼åˆRAGåŸºå‡†ï¼ŒåŒ…å«6500ä¸ªï¼ˆå›¾åƒã€é—®é¢˜ã€ç­”æ¡ˆï¼‰ä¸‰å…ƒç»„å’Œ2000ä¸ªåŸºäºè§†è§‰çš„å¤šè½®å¯¹è¯ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„RAGæ–¹æ³•åœ¨CRAG-MMçš„å•è½®å’Œå¤šè½®é—®ç­”ä¸­ä»…å®ç°äº†32%å’Œ43%çš„çœŸå®åº¦ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26140",
            "title": "FullPart: Generating each 3D Part at Full Resolution",
            "url": "https://huggingface.co/papers/2510.26140",
            "abstract": "Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.",
            "score": 0,
            "issue_id": 6713,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "37da8a03be0a8489",
            "authors": [
                "Lihe Ding",
                "Shaocong Dong",
                "Yaokun Li",
                "Chenjian Gao",
                "Xiao Chen",
                "Rui Han",
                "Yihao Kuang",
                "Hong Zhang",
                "Bo Huang",
                "Zhanpeng Huang",
                "Zibin Wang",
                "Dan Xu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "CUHK",
                "Chongqing University",
                "HKUST",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.26140.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹: ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ²Ğ¾Ñ‘ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FullPart â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ (implicit) Ğ¸ ÑĞ²Ğ½Ñ‹Ğµ (explicit) Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾ĞºÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ²ÑĞµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PartVerse-XL â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ 40 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 320 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Generation with FullPart",
                    "desc": "This paper introduces FullPart, a new framework for generating 3D parts that combines implicit and explicit methods. It uses an implicit diffusion process to create a bounding box layout, which is effective for low-detail shapes. Each part is then generated in its own high-resolution voxel grid, allowing for detailed and intricate designs without the limitations of shared low-resolution spaces. Additionally, the authors present PartVerse-XL, a large dataset to improve the training of models in 3D part generation, and demonstrate that FullPart achieves top performance in this area."
                },
                "zh": {
                    "title": "FullPartï¼šæå‡3Déƒ¨åˆ†ç”Ÿæˆçš„å…¨æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFullPartçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›åŸºäºéƒ¨åˆ†çš„3Dç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼çš„è¡¨ç¤ºæ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡éšå¼ç›’å­å‘é‡é›†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆè¾¹ç•Œæ¡†å¸ƒå±€ã€‚ç„¶åï¼Œä¸ºæ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆè¯¦ç»†çš„3Då½¢çŠ¶ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½åœ¨å…¶è‡ªå·±çš„å…¨åˆ†è¾¨ç‡ä½“ç´ ç½‘æ ¼ä¸­ç”Ÿæˆï¼Œé¿å…äº†å°éƒ¨åˆ†å ç”¨è¿‡å°‘ä½“ç´ çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸­å¿ƒç‚¹ç¼–ç ç­–ç•¥ï¼Œä»¥è§£å†³ä¸åŒå¤§å°éƒ¨åˆ†ä¹‹é—´ä¿¡æ¯äº¤æ¢æ—¶çš„å¯¹é½é—®é¢˜ï¼Œå¹¶å‘å¸ƒäº†PartVerse-XLæ•°æ®é›†ï¼Œä»¥æ”¯æŒ3Déƒ¨åˆ†ç”Ÿæˆçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25779",
            "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets",
            "url": "https://huggingface.co/papers/2510.25779",
            "abstract": "As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.",
            "score": 0,
            "issue_id": 6714,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 27",
                "zh": "10æœˆ27æ—¥"
            },
            "hash": "fc91b6d1ec75911d",
            "authors": [
                "Gagan Bansal",
                "Wenyue Hua",
                "Zezhou Huang",
                "Adam Fourney",
                "Amanda Swearngin",
                "Will Epperson",
                "Tyler Payne",
                "Jake M. Hofman",
                "Brendan Lucier",
                "Chinmay Singh",
                "Markus Mobius",
                "Akshay Nambi",
                "Archana Yadav",
                "Kevin Gao",
                "David M. Rothschild",
                "Aleksandrs Slivkins",
                "Daniel G. Goldstein",
                "Hussein Mozannar",
                "Nicole Immorlica",
                "Maya Murad",
                "Matthew Vogel",
                "Subbarao Kambhampati",
                "Eric Horvitz",
                "Saleema Amershi"
            ],
            "affiliations": [
                "Arizona State University",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25779.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ñ€Ñ‹Ğ½ĞºĞ°Ñ…, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ â€” ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑÑ‹. Ğ”Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Magentic-Marketplace, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğº Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ (first-proposal bias), ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ² 10-30 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ°Ğ´ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Navigating the Complexities of Agentic Marketplaces",
                    "desc": "This paper explores how large language model (LLM) agents can influence economic decisions in real-world markets, where they act on behalf of users. It highlights the need to understand agent behavior in complex, dynamic environments rather than simplified settings. The authors introduce a simulated environment called Magentic-Marketplace to analyze interactions between consumer-representing Assistants and competing Service agents. Their findings indicate that while advanced models can achieve good outcomes under ideal conditions, they struggle with scale and exhibit biases that can significantly impact market efficiency."
                },
                "zh": {
                    "title": "æ¢ç´¢ä»£ç†å¸‚åœºçš„å…¬å¹³ä¸æ•ˆç‡",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å‘å±•ï¼Œå®ƒä»¬åœ¨ç»æµå†³ç­–ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼ŒåŒ…æ‹¬äº§å“å‘ç°å’Œäº¤æ˜“ã€‚è¿™äº›åº”ç”¨è™½ç„¶å¸¦æ¥äº†å¥½å¤„ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºä»£ç†è´£ä»»å’Œç”¨æˆ·ä»·å€¼çš„è®¸å¤šé—®é¢˜ã€‚ä¸ºäº†ç†è§£ä»£ç†åœ¨ç°å®å¸‚åœºæ¡ä»¶ä¸‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¼€å‘äº†Magentic-Marketplaceï¼Œä¸€ä¸ªæ¨¡æ‹Ÿç¯å¢ƒï¼Œç ”ç©¶æ¶ˆè´¹è€…ä»£ç†å’Œç«äº‰ä¼ä¸šä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å‰æ²¿æ¨¡å‹åœ¨ç†æƒ³æœç´¢æ¡ä»¶ä¸‹å¯ä»¥æ¥è¿‘æœ€ä½³ç¦åˆ©ï¼Œä½†åœ¨è§„æ¨¡æ‰©å¤§æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºä¸¥é‡çš„é¦–æ¬¡ææ¡ˆåè§ï¼Œå¯¼è‡´å“åº”é€Ÿåº¦ç›¸å¯¹äºè´¨é‡æœ‰10-30å€çš„ä¼˜åŠ¿ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-30.html",
    "link_next": "2025-11-03.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "03.11",
        "en": "11/03",
        "zh": "11æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}