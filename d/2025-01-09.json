{
    "date": {
        "ru": "9 января",
        "en": "January 9",
        "zh": "1月9日"
    },
    "time_utc": "2025-01-09 04:12",
    "weekday": 3,
    "issue_id": 1573,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.04519",
            "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
            "url": "https://huggingface.co/papers/2501.04519",
            "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising \"deep thinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",
            "score": 12,
            "issue_id": 1572,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "b065003de5fa3bde",
            "authors": [
                "Xinyu Guan",
                "Li Lyna Zhang",
                "Yifei Liu",
                "Ning Shang",
                "Youran Sun",
                "Yi Zhu",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04519.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Малые модели решают большие задачи: rStar-Math превосходит гигантов в математике",
                    "desc": "Статья представляет rStar-Math - подход, позволяющий малым языковым моделям (SLM) достичь или превзойти способности крупных моделей в математических рассуждениях. Метод использует поиск по методу Монте-Карло (MCTS) с двумя специально обученными SLM: политикой и моделью вознаграждения. Авторы вводят новые методы синтеза обучающих данных, обучения модели вознаграждения и итеративного улучшения моделей. В результате rStar-Math значительно повышает эффективность SLM на математических тестах, превосходя более крупные модели."
                },
                "en": {
                    "title": "Empowering Small Models to Excel in Math Reasoning",
                    "desc": "The paper introduces rStar-Math, a framework that enhances the math reasoning abilities of small language models (SLMs) without relying on larger models. It employs Monte Carlo Tree Search (MCTS) to enable deep thinking, allowing the SLM to perform guided search during problem-solving. Key innovations include a code-augmented Chain of Thought (CoT) data synthesis method for generating verified reasoning paths, a refined process preference model (PPM) for better reward training, and a self-evolution strategy for iterative improvement. As a result, rStar-Math significantly boosts the performance of SLMs on math benchmarks, achieving state-of-the-art results in various assessments."
                },
                "zh": {
                    "title": "小型语言模型的数学推理新突破",
                    "desc": "rStar-Math展示了小型语言模型（SLMs）在数学推理能力上可以与OpenAI的o1相媲美，甚至超越它，而无需从更强大的模型中蒸馏。该方法通过蒙特卡洛树搜索（MCTS）实现“深度思考”，在测试时由SLM驱动的过程奖励模型指导数学策略SLM进行搜索。rStar-Math引入了三项创新来解决训练两个SLM的挑战，包括新颖的代码增强的链式推理数据合成方法和更有效的过程偏好模型（PPM）训练方法。经过四轮自我进化，rStar-Math在747,000个数学问题上生成了数百万个合成解，使SLMs的数学推理能力达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02772",
            "title": "GeAR: Generation Augmented Retrieval",
            "url": "https://huggingface.co/papers/2501.02772",
            "abstract": "Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called Generation Augmented Retrieval (GeAR) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to \"focus on\" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.",
            "score": 3,
            "issue_id": 1572,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "dafa87428ce906b5",
            "authors": [
                "Haoyu Liu",
                "Shaohan Huang",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Weiwei Deng",
                "Feng Sun",
                "Furu Wei",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02772.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#data",
                    "#rag",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "GeAR: Новый взгляд на извлечение документов через генерацию",
                    "desc": "Статья предлагает новый метод извлечения документов под названием Generation Augmented Retrieval (GeAR). В отличие от традиционных би-энкодеров, GeAR использует модули слияния и декодирования для генерации релевантного текста на основе запроса и документа. Это позволяет модели фокусироваться на детальной информации, не увеличивая вычислительную нагрузку. Авторы также разработали конвейер для синтеза качественных данных с помощью больших языковых моделей для обучения GeAR."
                },
                "en": {
                    "title": "GeAR: Enhancing Document Retrieval with Fine-Grained Semantic Focus",
                    "desc": "This paper introduces a new method called Generation Augmented Retrieval (GeAR) that enhances document retrieval techniques by focusing on fine-grained semantic relationships. Unlike traditional bi-encoders that primarily assess global semantics, GeAR generates relevant text from documents by fusing the query and document representations. This approach allows for a deeper understanding of retrieval results without increasing computational costs. Additionally, the authors provide a pipeline for synthesizing high-quality training data using large language models, leading to improved performance across various datasets."
                },
                "zh": {
                    "title": "生成增强检索：关注细粒度信息的创新方法",
                    "desc": "本文提出了一种新的文档检索方法，称为生成增强检索（GeAR）。GeAR通过融合查询和文档的表示，生成相关文本，从而关注细粒度信息。与传统的双编码器方法相比，GeAR在检索时不会增加计算负担，同时在多种场景和数据集上表现出竞争力的检索和定位性能。该方法还通过利用大型语言模型合成高质量数据，支持新框架的训练。"
                }
            }
        }
    ],
    "link_prev": "2025-01-08.html",
    "link_next": "2025-01-10.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1月8日"
    },
    "short_date_next": {
        "ru": "10.01",
        "en": "01/10",
        "zh": "1月10日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的强化学习方法，叫做 REINFORCE++。它改进了经典的 REINFORCE 算法，结合了 PPO 的优化技术，但不需要评论网络。REINFORCE++ 有三个主要目标：简单、提高训练稳定性和减少计算开销。实验结果显示，REINFORCE++ 比 GRPO 更稳定，比 PPO 更高效，性能也相当。代码可以在 https://github.com/OpenRLHF/OpenRLHF 找到。",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
        "pinyin": "这篇文章介绍了一种新的强化学习方法，叫做 REINFORCE++。它改进了经典的 REINFORCE 算法，结合了 PPO 的优化技术，但不需要评论网络。REINFORCE++ 有三个主要目标：简单、提高训练稳定性和减少计算开销。实验结果显示，REINFORCE++ 比 GRPO 更稳定，比 PPO 更高效，性能也相当。代码可以在 https://github.com/OpenRLHF/OpenRLHF 找到。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de qiáng huà xuéxí fāngfǎ, jiàozuò REINFORCE++. Tā gǎijìn le jīngdiǎn de REINFORCE suànfǎ, jiéhé le PPO de yōuhuà jìshù, dàn bù xūyào pínglùn wǎngluò. REINFORCE++ yǒu sān gè zhǔyào mùbiāo: jiǎndān, tígāo xùnliàn wěndìngxìng hé jiǎnshǎo jìsuàn kāixiāo. Shíyàn jiéguǒ xiǎnshì, REINFORCE++ bǐ GRPO gèng wěndìng, bǐ PPO gèng gāoxiào, xìngnéng yě xiāngdāng. Dàimǎ kěyǐ zài https://github.com/OpenRLHF/OpenRLHF zhǎo dào.",
        "vocab": "[{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'}, {'word': '经典', 'pinyin': 'jīng diǎn', 'trans': 'classic'}, {'word': '算法', 'pinyin': 'suàn fǎ', 'trans': 'algorithm'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'}, {'word': '评论', 'pinyin': 'píng lùn', 'trans': 'comment'}, {'word': '网络', 'pinyin': 'wǎng luò', 'trans': 'network'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}, {'word': '稳定性', 'pinyin': 'wěn dìng xìng', 'trans': 'stability'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'calculate'}, {'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '找到', 'pinyin': 'zhǎo dào', 'trans': 'find'}]",
        "trans": "This article introduces a new reinforcement learning method called REINFORCE++. It improves upon the classic REINFORCE algorithm by incorporating optimization techniques from PPO, without the need for a critic network. REINFORCE++ has three main objectives: simplicity, enhanced training stability, and reduced computational overhead. Experimental results show that REINFORCE++ is more stable than GRPO and more efficient than PPO, with comparable performance. The code can be found at https://github.com/OpenRLHF/OpenRLHF.",
        "update_ts": "2025-01-08 09:10"
    }
}