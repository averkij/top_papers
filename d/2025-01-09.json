{
    "date": {
        "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 9",
        "zh": "1æœˆ9æ—¥"
    },
    "time_utc": "2025-01-09 04:12",
    "weekday": 3,
    "issue_id": 1573,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.04519",
            "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
            "url": "https://huggingface.co/papers/2501.04519",
            "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising \"deep thinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",
            "score": 12,
            "issue_id": 1572,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "b065003de5fa3bde",
            "authors": [
                "Xinyu Guan",
                "Li Lyna Zhang",
                "Yifei Liu",
                "Ning Shang",
                "Youran Sun",
                "Yi Zhu",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04519.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: rStar-Math Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ rStar-Math - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (SLM) Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS) Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ SLM: Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ rStar-Math Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SLM Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Empowering Small Models to Excel in Math Reasoning",
                    "desc": "The paper introduces rStar-Math, a framework that enhances the math reasoning abilities of small language models (SLMs) without relying on larger models. It employs Monte Carlo Tree Search (MCTS) to enable deep thinking, allowing the SLM to perform guided search during problem-solving. Key innovations include a code-augmented Chain of Thought (CoT) data synthesis method for generating verified reasoning paths, a refined process preference model (PPM) for better reward training, and a self-evolution strategy for iterative improvement. As a result, rStar-Math significantly boosts the performance of SLMs on math benchmarks, achieving state-of-the-art results in various assessments."
                },
                "zh": {
                    "title": "å°å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†æ–°çªç ´",
                    "desc": "rStar-Mathå±•ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šå¯ä»¥ä¸OpenAIçš„o1ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šå®ƒï¼Œè€Œæ— éœ€ä»æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­è’¸é¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å®ç°â€œæ·±åº¦æ€è€ƒâ€ï¼Œåœ¨æµ‹è¯•æ—¶ç”±SLMé©±åŠ¨çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æŒ‡å¯¼æ•°å­¦ç­–ç•¥SLMè¿›è¡Œæœç´¢ã€‚rStar-Mathå¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°æ¥è§£å†³è®­ç»ƒä¸¤ä¸ªSLMçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–°é¢–çš„ä»£ç å¢å¼ºçš„é“¾å¼æ¨ç†æ•°æ®åˆæˆæ–¹æ³•å’Œæ›´æœ‰æ•ˆçš„è¿‡ç¨‹åå¥½æ¨¡å‹ï¼ˆPPMï¼‰è®­ç»ƒæ–¹æ³•ã€‚ç»è¿‡å››è½®è‡ªæˆ‘è¿›åŒ–ï¼ŒrStar-Mathåœ¨747,000ä¸ªæ•°å­¦é—®é¢˜ä¸Šç”Ÿæˆäº†æ•°ç™¾ä¸‡ä¸ªåˆæˆè§£ï¼Œä½¿SLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02772",
            "title": "GeAR: Generation Augmented Retrieval",
            "url": "https://huggingface.co/papers/2501.02772",
            "abstract": "Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called Generation Augmented Retrieval (GeAR) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to \"focus on\" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.",
            "score": 3,
            "issue_id": 1572,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "dafa87428ce906b5",
            "authors": [
                "Haoyu Liu",
                "Shaohan Huang",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Weiwei Deng",
                "Feng Sun",
                "Furu Wei",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02772.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#data",
                    "#rag",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "GeAR: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Generation Augmented Retrieval (GeAR). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², GeAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ GeAR."
                },
                "en": {
                    "title": "GeAR: Enhancing Document Retrieval with Fine-Grained Semantic Focus",
                    "desc": "This paper introduces a new method called Generation Augmented Retrieval (GeAR) that enhances document retrieval techniques by focusing on fine-grained semantic relationships. Unlike traditional bi-encoders that primarily assess global semantics, GeAR generates relevant text from documents by fusing the query and document representations. This approach allows for a deeper understanding of retrieval results without increasing computational costs. Additionally, the authors provide a pipeline for synthesizing high-quality training data using large language models, leading to improved performance across various datasets."
                },
                "zh": {
                    "title": "ç”Ÿæˆå¢å¼ºæ£€ç´¢ï¼šå…³æ³¨ç»†ç²’åº¦ä¿¡æ¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æ¡£æ£€ç´¢æ–¹æ³•ï¼Œç§°ä¸ºç”Ÿæˆå¢å¼ºæ£€ç´¢ï¼ˆGeARï¼‰ã€‚GeARé€šè¿‡èåˆæŸ¥è¯¢å’Œæ–‡æ¡£çš„è¡¨ç¤ºï¼Œç”Ÿæˆç›¸å…³æ–‡æœ¬ï¼Œä»è€Œå…³æ³¨ç»†ç²’åº¦ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„åŒç¼–ç å™¨æ–¹æ³•ç›¸æ¯”ï¼ŒGeARåœ¨æ£€ç´¢æ—¶ä¸ä¼šå¢åŠ è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶åœ¨å¤šç§åœºæ™¯å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›çš„æ£€ç´¢å’Œå®šä½æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œæ”¯æŒæ–°æ¡†æ¶çš„è®­ç»ƒã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-08.html",
    "link_next": "2025-01-10.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.01",
        "en": "01/10",
        "zh": "1æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå«åš REINFORCE++ã€‚å®ƒæ”¹è¿›äº†ç»å…¸çš„ REINFORCE ç®—æ³•ï¼Œç»“åˆäº† PPO çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†ä¸éœ€è¦è¯„è®ºç½‘ç»œã€‚REINFORCE++ æœ‰ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šç®€å•ã€æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREINFORCE++ æ¯” GRPO æ›´ç¨³å®šï¼Œæ¯” PPO æ›´é«˜æ•ˆï¼Œæ€§èƒ½ä¹Ÿç›¸å½“ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/OpenRLHF/OpenRLHF æ‰¾åˆ°ã€‚",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå«åš REINFORCE++ã€‚å®ƒæ”¹è¿›äº†ç»å…¸çš„ REINFORCE ç®—æ³•ï¼Œç»“åˆäº† PPO çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†ä¸éœ€è¦è¯„è®ºç½‘ç»œã€‚REINFORCE++ æœ‰ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šç®€å•ã€æé«˜è®­ç»ƒç¨³å®šæ€§å’Œå‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREINFORCE++ æ¯” GRPO æ›´ç¨³å®šï¼Œæ¯” PPO æ›´é«˜æ•ˆï¼Œæ€§èƒ½ä¹Ÿç›¸å½“ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/OpenRLHF/OpenRLHF æ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de qiÃ¡ng huÃ  xuÃ©xÃ­ fÄngfÇ, jiÃ ozuÃ² REINFORCE++. TÄ gÇijÃ¬n le jÄ«ngdiÇn de REINFORCE suÃ nfÇ, jiÃ©hÃ© le PPO de yÅuhuÃ  jÃ¬shÃ¹, dÃ n bÃ¹ xÅ«yÃ o pÃ­nglÃ¹n wÇngluÃ². REINFORCE++ yÇ’u sÄn gÃ¨ zhÇ”yÃ o mÃ¹biÄo: jiÇndÄn, tÃ­gÄo xÃ¹nliÃ n wÄ›ndÃ¬ngxÃ¬ng hÃ© jiÇnshÇo jÃ¬suÃ n kÄixiÄo. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, REINFORCE++ bÇ GRPO gÃ¨ng wÄ›ndÃ¬ng, bÇ PPO gÃ¨ng gÄoxiÃ o, xÃ¬ngnÃ©ng yÄ› xiÄngdÄng. DÃ imÇ kÄ›yÇ zÃ i https://github.com/OpenRLHF/OpenRLHF zhÇo dÃ o.",
        "vocab": "[{'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'}, {'word': 'ç»å…¸', 'pinyin': 'jÄ«ng diÇn', 'trans': 'classic'}, {'word': 'ç®—æ³•', 'pinyin': 'suÃ n fÇ', 'trans': 'algorithm'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ© hÃ©', 'trans': 'combine'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimize'}, {'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'}, {'word': 'è¯„è®º', 'pinyin': 'pÃ­ng lÃ¹n', 'trans': 'comment'}, {'word': 'ç½‘ç»œ', 'pinyin': 'wÇng luÃ²', 'trans': 'network'}, {'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'goal'}, {'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›n dÃ¬ng xÃ¬ng', 'trans': 'stability'}, {'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'calculate'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'show'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'æ‰¾åˆ°', 'pinyin': 'zhÇo dÃ o', 'trans': 'find'}]",
        "trans": "This article introduces a new reinforcement learning method called REINFORCE++. It improves upon the classic REINFORCE algorithm by incorporating optimization techniques from PPO, without the need for a critic network. REINFORCE++ has three main objectives: simplicity, enhanced training stability, and reduced computational overhead. Experimental results show that REINFORCE++ is more stable than GRPO and more efficient than PPO, with comparable performance. The code can be found at https://github.com/OpenRLHF/OpenRLHF.",
        "update_ts": "2025-01-08 09:10"
    }
}