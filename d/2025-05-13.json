{
    "date": {
        "ru": "13 –º–∞—è",
        "en": "May 13",
        "zh": "5Êúà13Êó•"
    },
    "time_utc": "2025-05-13 02:30",
    "weekday": 1,
    "issue_id": 3722,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07062",
            "title": "Seed1.5-VL Technical Report",
            "url": "https://huggingface.co/papers/2505.07062",
            "abstract": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
            "score": 10,
            "issue_id": 3722,
            "pub_date": "2025-05-11",
            "pub_date_card": {
                "ru": "11 –º–∞—è",
                "en": "May 11",
                "zh": "5Êúà11Êó•"
            },
            "hash": "c3406b40cc21820d",
            "authors": [
                "Dong Guo",
                "Faming Wu",
                "Feida Zhu",
                "Fuxing Leng",
                "Guang Shi",
                "Haobin Chen",
                "Haoqi Fan",
                "Jian Wang",
                "Jianyu Jiang",
                "Jiawei Wang",
                "Jingji Chen",
                "Jingjia Huang",
                "Kang Lei",
                "Liping Yuan",
                "Lishu Luo",
                "Pengfei Liu",
                "Qinghao Ye",
                "Rui Qian",
                "Shen Yan",
                "Shixiong Zhao",
                "Shuai Peng",
                "Shuangye Li",
                "Sihang Yuan",
                "Sijin Wu",
                "Tianheng Cheng",
                "Weiwei Liu",
                "Wenqian Wang",
                "Xianhan Zeng",
                "Xiao Liu",
                "Xiaobo Qin",
                "Xiaohan Ding",
                "Xiaojun Xiao",
                "Xiaoying Zhang",
                "Xuanwei Zhang",
                "Xuehan Xiong",
                "Yanghua Peng",
                "Yangrui Chen",
                "Yanwei Li",
                "Yanxu Hu",
                "Yi Lin",
                "Yiyuan Hu",
                "Yiyuan Zhang",
                "Youbin Wu",
                "Yu Li",
                "Yudong Liu",
                "Yue Ling",
                "Yujia Qin",
                "Zanbo Wang",
                "Zhiwu He",
                "Aoxue Zhang",
                "Bairen Yi",
                "Bencheng Liao",
                "Can Huang",
                "Can Zhang",
                "Chaorui Deng",
                "Chaoyi Deng",
                "Cheng Lin",
                "Cheng Yuan",
                "Chenggang Li",
                "Chenhui Gou",
                "Chenwei Lou",
                "Chengzhi Wei",
                "Chundian Liu",
                "Chunyuan Li",
                "Deyao Zhu",
                "Donghong Zhong",
                "Feng Li",
                "Feng Zhang",
                "Gang Wu",
                "Guodong Li",
                "Guohong Xiao",
                "Haibin Lin",
                "Haihua Yang",
                "Haoming Wang",
                "Heng Ji",
                "Hongxiang Hao",
                "Hui Shen",
                "Huixia Li",
                "Jiahao Li",
                "Jialong Wu",
                "Jianhua Zhu",
                "Jianpeng Jiao",
                "Jiashi Feng",
                "Jiaze Chen",
                "Jianhui Duan",
                "Jihao Liu",
                "Jin Zeng",
                "Jingqun Tang",
                "Jingyu Sun",
                "Joya Chen",
                "Jun Long",
                "Junda Feng",
                "Junfeng Zhan",
                "Junjie Fang",
                "Junting Lu",
                "Kai Hua",
                "Kai Liu",
                "Kai Shen",
                "Kaiyuan Zhang",
                "Ke Shen",
                "Ke Wang",
                "Keyu Pan",
                "Kun Zhang",
                "Kunchang Li",
                "Lanxin Li",
                "Lei Li",
                "Lei Shi",
                "Li Han",
                "Liang Xiang",
                "Liangqiang Chen",
                "Lin Chen",
                "Lin Li",
                "Lin Yan",
                "Liying Chi",
                "Longxiang Liu",
                "Mengfei Du",
                "Mingxuan Wang",
                "Ningxin Pan",
                "Peibin Chen",
                "Pengfei Chen",
                "Pengfei Wu",
                "Qingqing Yuan",
                "Qingyao Shuai",
                "Qiuyan Tao",
                "Renjie Zheng",
                "Renrui Zhang",
                "Ru Zhang",
                "Rui Wang",
                "Rui Yang",
                "Rui Zhao",
                "Shaoqiang Xu",
                "Shihao Liang",
                "Shipeng Yan",
                "Shu Zhong",
                "Shuaishuai Cao",
                "Shuangzhi Wu",
                "Shufan Liu",
                "Shuhan Chang",
                "Songhua Cai",
                "Tenglong Ao",
                "Tianhao Yang",
                "Tingting Zhang",
                "Wanjun Zhong",
                "Wei Jia",
                "Wei Weng",
                "Weihao Yu",
                "Wenhao Huang",
                "Wenjia Zhu",
                "Wenli Yang",
                "Wenzhi Wang",
                "Xiang Long",
                "XiangRui Yin",
                "Xiao Li",
                "Xiaolei Zhu",
                "Xiaoying Jia",
                "Xijin Zhang",
                "Xin Liu",
                "Xinchen Zhang",
                "Xinyu Yang",
                "Xiongcai Luo",
                "Xiuli Chen",
                "Xuantong Zhong",
                "Xuefeng Xiao",
                "Xujing Li",
                "Yan Wu",
                "Yawei Wen",
                "Yifan Du",
                "Yihao Zhang",
                "Yining Ye",
                "Yonghui Wu",
                "Yu Liu",
                "Yu Yue",
                "Yufeng Zhou",
                "Yufeng Yuan",
                "Yuhang Xu",
                "Yuhong Yang",
                "Yun Zhang",
                "Yunhao Fang",
                "Yuntao Li",
                "Yurui Ren",
                "Yuwen Xiong",
                "Zehua Hong",
                "Zehua Wang",
                "Zewei Sun",
                "Zeyu Wang",
                "Zhao Cai",
                "Zhaoyue Zha",
                "Zhecheng An",
                "Zhehui Zhao",
                "Zhengzhuo Xu",
                "Zhipeng Chen",
                "Zhiyong Wu",
                "Zhuofan Zheng",
                "Zihao Wang",
                "Zilong Huang",
                "Ziyu Zhu",
                "Zuquan Song"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07062.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#survey",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤—ã–¥–∞—é—â–∏–º–∏—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏",
                    "desc": "Seed1.5-VL - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ—á–µ—Ç–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ 532 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å 20 –º–ª—Ä–¥ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è state-of-the-art –Ω–∞ 38 –∏–∑ 60 –ø—É–±–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. Seed1.5-VL –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏, –∏–≥—Ä–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –≤–µ–¥—É—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã."
                },
                "en": {
                    "title": "Empowering Multimodal Understanding with Seed1.5-VL",
                    "desc": "Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles."
                },
                "zh": {
                    "title": "Seed1.5-VLÔºöÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜSeed1.5-VLÔºåËøôÊòØ‰∏ÄÁßçÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÊé®ÁêÜÁöÑËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°Âûã„ÄÇSeed1.5-VLÁî±‰∏Ä‰∏™532MÂèÇÊï∞ÁöÑËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™ÂÖ∑Êúâ20BÊ¥ªË∑ÉÂèÇÊï∞ÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoE LLMÔºâÁªÑÊàê„ÄÇÂ∞ΩÁÆ°ÂÖ∂Êû∂ÊûÑÁõ∏ÂØπÁ¥ßÂáëÔºå‰ΩÜÂú®Â§ö‰∏™ÂÖ¨ÂÖ±VLMÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®60‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏≠Êúâ38‰∏™ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂú®‰ª•‰ª£ÁêÜ‰∏∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÊéßÂà∂ÂíåÊ∏∏ÊàèÁé©Ê≥ïÔºåSeed1.5-VLË∂ÖË∂ä‰∫ÜÈ¢ÜÂÖàÁöÑÂ§öÊ®°ÊÄÅÁ≥ªÁªüÔºåÂåÖÊã¨OpenAI CUAÂíåClaude 3.7„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07787",
            "title": "Learning from Peers in Reasoning Models",
            "url": "https://huggingface.co/papers/2505.07787",
            "abstract": "Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the \"Prefix Dominance Trap\". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .",
            "score": 7,
            "issue_id": 3722,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 –º–∞—è",
                "en": "May 12",
                "zh": "5Êúà12Êó•"
            },
            "hash": "350f28f20ab516fc",
            "authors": [
                "Tongxu Luo",
                "Wenyu Du",
                "Jiaxi Bi",
                "Stephen Chung",
                "Zhengyang Tang",
                "Hao Yang",
                "Min Zhang",
                "Benyou Wang"
            ],
            "affiliations": [
                "DualityRL",
                "Huawei",
                "The Chinese University of Hong Kong, Shenzhen",
                "USTB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07787.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º: –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –º–æ–≥—É—Ç –ø–æ–ø–∞–¥–∞—Ç—å –≤ '–ª–æ–≤—É—à–∫—É –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–∞', –∫–æ–≥–¥–∞ –ø–ª–æ—Ö–æ–µ –Ω–∞—á–∞–ª–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–µ—à–∞–µ—Ç —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ '–û–±—É—á–µ–Ω–∏–µ —É —Å–≤–µ—Ä—Å—Ç–Ω–∏–∫–æ–≤' (LeaP), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–æ–¥–µ–ª—è–º –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π LeaP-T, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ –æ–±–æ–±—â–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LeaP."
                },
                "en": {
                    "title": "Empowering LRMs through Peer Collaboration",
                    "desc": "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."
                },
                "zh": {
                    "title": "Âêå‰º¥Â≠¶‰π†ÔºöÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑËá™ÊàëÁ∫†Ê≠£ËÉΩÂäõ",
                    "desc": "Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂÖ∑ÊúâËá™ÊàëÁ∫†Ê≠£ÁöÑËÉΩÂäõÔºå‰ΩÜÂΩìÊé®ÁêÜËøáÁ®ã‰ª•Áü≠ËÄåÂ∑ÆÁöÑÂºÄÂ§¥ÂºÄÂßãÊó∂ÔºåÊ®°ÂûãÂæàÈöæÊÅ¢Â§ç„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÁé∞Ë±°‰∏∫‚ÄúÂâçÁºÄ‰∏ªÂØºÈô∑Èò±‚Äù„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‚Äú‰ªéÂêå‰º¥Â≠¶‰π†‚ÄùÔºàLeaPÔºâÔºåÈÄöËøáË∑ØÁî±Êú∫Âà∂ËÆ©ÊØè‰∏™Êé®ÁêÜË∑ØÂæÑÊÄªÁªìÂÖ∂‰∏≠Èó¥Êé®ÁêÜÂπ∂‰∏éÂÖ∂‰ªñË∑ØÂæÑÂÖ±‰∫´Ôºå‰ªéËÄåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËûçÂÖ•Âêå‰º¥ÁöÑËßÅËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLeaPÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜ‰∏çÂêå‰ªªÂä°ÈöæÂ∫¶Êó∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈîôËØØÂÆπÂøçËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03733",
            "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
            "url": "https://huggingface.co/papers/2505.03733",
            "abstract": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.",
            "score": 7,
            "issue_id": 3722,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 –º–∞—è",
                "en": "May 6",
                "zh": "5Êúà6Êó•"
            },
            "hash": "1d5e56d00ea8d485",
            "authors": [
                "Zimu Lu",
                "Yunqiao Yang",
                "Houxing Ren",
                "Haotian Hou",
                "Han Xiao",
                "Ke Wang",
                "Weikang Shi",
                "Aojun Zhou",
                "Mingjie Zhan",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03733.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "WebGen-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏",
                    "desc": "WebGen-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ –≤–µ–±-—Å–∞–π—Ç—ã —Å –Ω—É–ª—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ –ø–æ—á—Ç–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Ç–∏–ø—ã –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –ö–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö GPT-4o –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é. –õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è - Bolt.diy —Å DeepSeek-R1 - –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ª—å–∫–æ 27,8% —Ç–æ—á–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–∞."
                },
                "en": {
                    "title": "Benchmarking LLMs for Website Code Generation",
                    "desc": "This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models."
                },
                "zh": {
                    "title": "ËØÑ‰º∞LLM‰ª£ÁêÜÁîüÊàêÁΩëÁ´ô‰ª£Á†ÅÁöÑÊåëÊàò",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïWebGen-BenchÔºåÊó®Âú®ËØÑ‰º∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰ª£ÁêÜÂú®‰ªéÈõ∂ÂºÄÂßãÂàõÂª∫Â§öÊñá‰ª∂ÁΩëÁ´ô‰ª£Á†ÅÂ∫ìÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÁΩëÁ´ôÁîüÊàêÊåá‰ª§ÔºåÊ∂µÁõñ‰∫Ü‰∏âÂ§ßÁ±ªÂíåÂçÅ‰∏âÂ∞èÁ±ªÔºåÂá†‰πéÂåÖÊã¨ÊâÄÊúâÈáçË¶ÅÁ±ªÂûãÁöÑWebÂ∫îÁî®Á®ãÂ∫è„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑË¥®ÈáèÔºå‰ΩøÁî®GPT-4oÁîüÊàêÈíàÂØπÊØè‰∏™ÂäüËÉΩÁöÑÊµãËØïÁî®‰æãÔºåÂπ∂ÊâãÂä®ËøáÊª§ÂíåË∞ÉÊï¥ÔºåÊúÄÁªàÂΩ¢Êàê647‰∏™ÊµãËØïÁî®‰æã„ÄÇÈÄöËøáÂº∫Â§ßÁöÑÁΩëÈ°µÂØºËà™‰ª£ÁêÜËá™Âä®ÊâßË°åÊµãËØïÔºåËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑÂìçÂ∫îÊòØÂê¶Á¨¶ÂêàÈ¢ÑÊúüÁªìÊûúÔºåÁªìÊûúÊòæÁ§∫ÊúÄ‰Ω≥Ê®°ÂûãÁªÑÂêàÁöÑÂáÜÁ°ÆÁéá‰ªÖ‰∏∫27.8%ÔºåÊòæÁ§∫Âá∫Âü∫ÂáÜÁöÑÊåëÊàòÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06548",
            "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
            "url": "https://huggingface.co/papers/2505.06548",
            "abstract": "Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.",
            "score": 5,
            "issue_id": 3722,
            "pub_date": "2025-05-10",
            "pub_date_card": {
                "ru": "10 –º–∞—è",
                "en": "May 10",
                "zh": "5Êúà10Êó•"
            },
            "hash": "db28335cad79db53",
            "authors": [
                "Aniruddha Roy",
                "Pretam Ray",
                "Abhilash Nandy",
                "Somak Aditya",
                "Pawan Goyal"
            ],
            "affiliations": [
                "Indian Institute of Technology, Kharagpur"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06548.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#rl",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ RL —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ø–ú",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLaMA 2-7B, LLama 2-13B, Mistral 7B) –¥–ª—è –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–º–µ–Ω—å—à–∞—é—â–∏–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω–∏ —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å RL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ 63-66% –∑–∞–¥–∞—á."
                },
                "en": {
                    "title": "Empowering LLMs with Cost-Effective Instruction Generation",
                    "desc": "This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods."
                },
                "zh": {
                    "title": "È´òÊïàÁîüÊàêÊåá‰ª§Êï∞ÊçÆÔºåÊèêÂçáLLMsÊÄßËÉΩ",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊåá‰ª§È©±Âä®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂçäËá™Âä®ÂåñÊ°ÜÊû∂ÔºåÂà©Áî®ÂºÄÊ∫êÁöÑÂ∞èÂûãLLMsÔºàÂ¶ÇLLaMA 2-7B„ÄÅLLaMA 2-13BÂíåMistral 7BÔºâÊù•ÁîüÊàêÊåá‰ª§Êï∞ÊçÆÈõÜÔºå‰ªéËÄåÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢ÑÂíåÊàêÊú¨„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÁÆóÊ≥ïÔºåÁ†îÁ©∂Ë°®ÊòéËøôÁßçÊñπÊ≥ïÂú®63-66%ÁöÑ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇËØ•Á†îÁ©∂‰∏∫ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊåá‰ª§Êï∞ÊçÆÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-05-12.html",
    "link_next": "2025-05-14.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "12.05",
        "en": "05/12",
        "zh": "5Êúà12Êó•"
    },
    "short_date_next": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5Êúà14Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü Bielik v3ÔºåËøôÊòØ‰∏ÄÁ≥ªÂàó‰∏ì‰∏∫Ê≥¢ÂÖ∞ËØ≠Â§ÑÁêÜ‰ºòÂåñÁöÑÂèÇÊï∞È´òÊïàÁîüÊàêÊñáÊú¨Ê®°ÂûãÔºà1.5B Âíå 4.5BÔºâ„ÄÇËøô‰∫õÊ®°ÂûãÂ±ïÁ§∫‰∫ÜËæÉÂ∞è‰ΩÜ‰ºòÂåñËâØÂ•ΩÁöÑÊû∂ÊûÑÂèØ‰ª•ÂÆûÁé∞‰∏éÂ§ßÂæóÂ§öÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÈúÄË¶ÅÊõ¥Â∞ëÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨Ëá™ÂÆö‰πâÊ≥¢ÂÖ∞ËØ≠ÂàÜËØçÂô®ÔºàAPT4Ôºâ„ÄÅÂπ≥Ë°°‰∏çÂêåÊåá‰ª§Á±ªÂûãÂ≠¶‰π†ÁöÑÂä†ÊùÉÊåá‰ª§‰∫§ÂèâÁÜµÊçüÂ§±ÂíåÊ†πÊçÆËÆ≠ÁªÉËøõÂ∫¶Âä®ÊÄÅË∞ÉÊï¥ÁöÑËá™ÈÄÇÂ∫îÂ≠¶‰π†Áéá„ÄÇÁªèËøáÁ≤æÂøÉÁ≠ñÂàíÁöÑ 292 ‰∫ø‰∏™Ê†áËÆ∞Âíå 303 Áôæ‰∏áÊñáÊ°£ÁöÑËÆ≠ÁªÉÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ4.5B ÂèÇÊï∞Ê®°ÂûãÁöÑÁªìÊûú‰∏éÂÖ∂ 2-3 ÂÄçÂ§ßÂ∞èÁöÑÊ®°ÂûãÁ´û‰∫âÂäõÁõ∏ÂΩìÔºåËÄå 1.5B Ê®°ÂûãÂàôÂú®ÂÖ∂ÊûÅÂÖ∂Á¥ßÂáëÁöÑÈÖçÁΩÆ‰∏ãË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇ",
        "title": "Bielik v3 Small: Technical Report",
        "pinyin": "W«ímen ji√®sh√†o le Bielik v3, zh√® sh√¨ yƒ´ x√¨li√® zhuƒÅn w√®i B≈çl√°n y«î ch«îl«ê y≈çuhu√† de cƒÅnsh√π gƒÅoxi√†o shƒìngch√©ng w√©nbƒõn m√≥x√≠ng (1.5B h√© 4.5B). Zh√®xiƒì m√≥x√≠ng zh«énsh√¨ le ji√†o xi«éo d√†n y≈çuhu√† li√°ng h«éo de ji√†g√≤u kƒõy«ê sh√≠xi√†n y«î d√† d√© du≈ç m√≥x√≠ng xi√†ngdƒÅng de x√≠ngn√©ng, t√≥ngsh√≠ x≈´y√†o g√®ng sh«éo de j√¨su√†n zƒ´yu√°n. W«ímen de fƒÅngf«é bƒÅoku√≤ z√¨d√¨ngy√¨ B≈çl√°n y«î fƒìnc√≠q√¨ (APT4), p√≠ng h√©ng b√πt√≥ng zh«êl√¨ng l√®ix√≠ng xu√©x√≠ de jiƒÅqu√°n zh«êl√¨ng jiƒÅochƒÅ shƒÅngs«în y«î gƒìnj√π x√πnli√†n j√¨nd√π d√≤ngt√†i ti√°oji√© de z√¨sh√¨y√¨ng xu√©x√≠ l«ú. Jƒ´ngxƒ´n c√®hu√† de 292 y√¨ g√® biƒÅoj√¨ h√© 303 b«éi w√†n w√©nji√†n de x√πnli√†n, zh√®xiƒì m√≥x√≠ng z√†i du≈çg√® jƒ´zh«în c√®sh√¨ zh≈çng bi«éoxi√†n ch≈´s√®. 4.5B cƒÅnsh√π m√≥x√≠ng de ji√©gu«í y«î q√≠ 2-3 b√®i d√†x√¨ao de m√≥x√≠ng j√¨ngzhƒìngl√¨ xi√†ngdƒÅng, √©r 1.5B m√≥x√≠ng z√© z√†i q√≠ q√≠ t√® j«ênk«íu de p√®izh√¨ xi√† bi«éoxi√†n ch≈´ qi√°ngd√† de x√≠ngn√©ng.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"Á≥ªÂàó\", \"pinyin\": \"x√¨ li√®\", \"trans\": \"series\"},\n    {\"word\": \"‰∏ì‰∏∫\", \"pinyin\": \"zhuƒÅn w√®i\", \"trans\": \"specially for\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimize\"},\n    {\"word\": \"ÂèÇÊï∞\", \"pinyin\": \"cƒÅn sh√π\", \"trans\": \"parameters\"},\n    {\"word\": \"È´òÊïà\", \"pinyin\": \"gƒÅo xi√†o\", \"trans\": \"efficient\"},\n    {\"word\": \"ÁîüÊàê\", \"pinyin\": \"shƒìng ch√©ng\", \"trans\": \"generate\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"Â±ïÁ§∫\", \"pinyin\": \"zh«én sh√¨\", \"trans\": \"demonstrate\"},\n    {\"word\": \"Êû∂ÊûÑ\", \"pinyin\": \"ji√† g√≤u\", \"trans\": \"architecture\"},\n    {\"word\": \"Áõ∏ÂΩì\", \"pinyin\": \"xiƒÅng dƒÅng\", \"trans\": \"equivalent\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ËÆ°ÁÆó\", \"pinyin\": \"j√¨ su√†n\", \"trans\": \"compute\"},\n    {\"word\": \"ËµÑÊ∫ê\", \"pinyin\": \"zƒ´ yu√°n\", \"trans\": \"resources\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"Ëá™ÂÆö‰πâ\", \"pinyin\": \"z√¨ d√¨ng y√¨\", \"trans\": \"customize\"},\n    {\"word\": \"ÂàÜËØçÂô®\", \"pinyin\": \"fƒìn c√≠ q√¨\", \"trans\": \"tokenizer\"},\n    {\"word\": \"Âπ≥Ë°°\", \"pinyin\": \"p√≠ng h√©ng\", \"trans\": \"balance\"},\n    {\"word\": \"Êåá‰ª§\", \"pinyin\": \"zh«ê l√¨ng\", \"trans\": \"instruction\"},\n    {\"word\": \"Á±ªÂûã\", \"pinyin\": \"l√®i x√≠ng\", \"trans\": \"type\"},\n    {\"word\": \"Â≠¶‰π†\", \"pinyin\": \"xu√© x√≠\", \"trans\": \"learn\"},\n    {\"word\": \"Âä†ÊùÉ\", \"pinyin\": \"jiƒÅ qu√°n\", \"trans\": \"weighted\"},\n    {\"word\": \"‰∫§ÂèâÁÜµ\", \"pinyin\": \"jiƒÅo chƒÅ shƒÅng\", \"trans\": \"cross-entropy\"},\n    {\"word\": \"ÊçüÂ§±\", \"pinyin\": \"s«în shƒ´\", \"trans\": \"loss\"},\n    {\"word\": \"Ëá™ÈÄÇÂ∫î\", \"pinyin\": \"z√¨ sh√¨ y√¨ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"Â≠¶‰π†Áéá\", \"pinyin\": \"xu√© x√≠ l«ú\", \"trans\": \"learning rate\"},\n    {\"word\": \"Á≠ñÂàí\", \"pinyin\": \"c√® hu√†\", \"trans\": \"plan\"},\n    {\"word\": \"Ê†áËÆ∞\", \"pinyin\": \"biƒÅo j√¨\", \"trans\": \"token\"},\n    {\"word\": \"ÊñáÊ°£\", \"pinyin\": \"w√©n d√†ng\", \"trans\": \"document\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"train\"},\n    {\"word\": \"Âü∫ÂáÜ\", \"pinyin\": \"jƒ´ zh«în\", \"trans\": \"benchmark\"},\n    {\"word\": \"ÊµãËØï\", \"pinyin\": \"c√® sh√¨\", \"trans\": \"test\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"perform\"},\n    {\"word\": \"Âá∫Ëâ≤\", \"pinyin\": \"ch≈´ s√®\", \"trans\": \"outstanding\"},\n    {\"word\": \"ÁªìÊûú\", \"pinyin\": \"ji√© gu«í\", \"trans\": \"result\"},\n    {\"word\": \"Á´û‰∫âÂäõ\", \"pinyin\": \"j√¨ng zhƒìng l√¨\", \"trans\": \"competitiveness\"},\n    {\"word\": \"ÈÖçÁΩÆ\", \"pinyin\": \"p√®i zh√¨\", \"trans\": \"configuration\"},\n    {\"word\": \"Á¥ßÂáë\", \"pinyin\": \"j«ên c√≤u\", \"trans\": \"compact\"},\n    {\"word\": \"Âº∫Â§ß\", \"pinyin\": \"qi√°ng d√†\", \"trans\": \"powerful\"}\n]",
        "trans": "We introduced Bielik v3, a series of parameter-efficient text generation models optimized specifically for Polish language processing (1.5B and 4.5B). These models demonstrate that smaller but well-optimized architectures can achieve performance comparable to much larger models while requiring fewer computational resources. Our approach includes a custom Polish tokenizer (APT4), weighted instruction cross-entropy loss to balance learning across different instruction types, and adaptive learning rates dynamically adjusted according to training progress. With meticulously planned training on 292 billion tokens and 303 million documents, these models perform excellently on multiple benchmarks. The 4.5B parameter model's results are competitive with models 2-3 times its size, while the 1.5B model shows strong performance in its extremely compact configuration.",
        "update_ts": "2025-05-12 10:13"
    }
}