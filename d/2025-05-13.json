{
    "date": {
        "ru": "13 Ğ¼Ğ°Ñ",
        "en": "May 13",
        "zh": "5æœˆ13æ—¥"
    },
    "time_utc": "2025-05-13 06:16",
    "weekday": 1,
    "issue_id": 3726,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07062",
            "title": "Seed1.5-VL Technical Report",
            "url": "https://huggingface.co/papers/2505.07062",
            "abstract": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
            "score": 48,
            "issue_id": 3722,
            "pub_date": "2025-05-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ",
                "en": "May 11",
                "zh": "5æœˆ11æ—¥"
            },
            "hash": "c3406b40cc21820d",
            "authors": [
                "Dong Guo",
                "Faming Wu",
                "Feida Zhu",
                "Fuxing Leng",
                "Guang Shi",
                "Haobin Chen",
                "Haoqi Fan",
                "Jian Wang",
                "Jianyu Jiang",
                "Jiawei Wang",
                "Jingji Chen",
                "Jingjia Huang",
                "Kang Lei",
                "Liping Yuan",
                "Lishu Luo",
                "Pengfei Liu",
                "Qinghao Ye",
                "Rui Qian",
                "Shen Yan",
                "Shixiong Zhao",
                "Shuai Peng",
                "Shuangye Li",
                "Sihang Yuan",
                "Sijin Wu",
                "Tianheng Cheng",
                "Weiwei Liu",
                "Wenqian Wang",
                "Xianhan Zeng",
                "Xiao Liu",
                "Xiaobo Qin",
                "Xiaohan Ding",
                "Xiaojun Xiao",
                "Xiaoying Zhang",
                "Xuanwei Zhang",
                "Xuehan Xiong",
                "Yanghua Peng",
                "Yangrui Chen",
                "Yanwei Li",
                "Yanxu Hu",
                "Yi Lin",
                "Yiyuan Hu",
                "Yiyuan Zhang",
                "Youbin Wu",
                "Yu Li",
                "Yudong Liu",
                "Yue Ling",
                "Yujia Qin",
                "Zanbo Wang",
                "Zhiwu He",
                "Aoxue Zhang",
                "Bairen Yi",
                "Bencheng Liao",
                "Can Huang",
                "Can Zhang",
                "Chaorui Deng",
                "Chaoyi Deng",
                "Cheng Lin",
                "Cheng Yuan",
                "Chenggang Li",
                "Chenhui Gou",
                "Chenwei Lou",
                "Chengzhi Wei",
                "Chundian Liu",
                "Chunyuan Li",
                "Deyao Zhu",
                "Donghong Zhong",
                "Feng Li",
                "Feng Zhang",
                "Gang Wu",
                "Guodong Li",
                "Guohong Xiao",
                "Haibin Lin",
                "Haihua Yang",
                "Haoming Wang",
                "Heng Ji",
                "Hongxiang Hao",
                "Hui Shen",
                "Huixia Li",
                "Jiahao Li",
                "Jialong Wu",
                "Jianhua Zhu",
                "Jianpeng Jiao",
                "Jiashi Feng",
                "Jiaze Chen",
                "Jianhui Duan",
                "Jihao Liu",
                "Jin Zeng",
                "Jingqun Tang",
                "Jingyu Sun",
                "Joya Chen",
                "Jun Long",
                "Junda Feng",
                "Junfeng Zhan",
                "Junjie Fang",
                "Junting Lu",
                "Kai Hua",
                "Kai Liu",
                "Kai Shen",
                "Kaiyuan Zhang",
                "Ke Shen",
                "Ke Wang",
                "Keyu Pan",
                "Kun Zhang",
                "Kunchang Li",
                "Lanxin Li",
                "Lei Li",
                "Lei Shi",
                "Li Han",
                "Liang Xiang",
                "Liangqiang Chen",
                "Lin Chen",
                "Lin Li",
                "Lin Yan",
                "Liying Chi",
                "Longxiang Liu",
                "Mengfei Du",
                "Mingxuan Wang",
                "Ningxin Pan",
                "Peibin Chen",
                "Pengfei Chen",
                "Pengfei Wu",
                "Qingqing Yuan",
                "Qingyao Shuai",
                "Qiuyan Tao",
                "Renjie Zheng",
                "Renrui Zhang",
                "Ru Zhang",
                "Rui Wang",
                "Rui Yang",
                "Rui Zhao",
                "Shaoqiang Xu",
                "Shihao Liang",
                "Shipeng Yan",
                "Shu Zhong",
                "Shuaishuai Cao",
                "Shuangzhi Wu",
                "Shufan Liu",
                "Shuhan Chang",
                "Songhua Cai",
                "Tenglong Ao",
                "Tianhao Yang",
                "Tingting Zhang",
                "Wanjun Zhong",
                "Wei Jia",
                "Wei Weng",
                "Weihao Yu",
                "Wenhao Huang",
                "Wenjia Zhu",
                "Wenli Yang",
                "Wenzhi Wang",
                "Xiang Long",
                "XiangRui Yin",
                "Xiao Li",
                "Xiaolei Zhu",
                "Xiaoying Jia",
                "Xijin Zhang",
                "Xin Liu",
                "Xinchen Zhang",
                "Xinyu Yang",
                "Xiongcai Luo",
                "Xiuli Chen",
                "Xuantong Zhong",
                "Xuefeng Xiao",
                "Xujing Li",
                "Yan Wu",
                "Yawei Wen",
                "Yifan Du",
                "Yihao Zhang",
                "Yining Ye",
                "Yonghui Wu",
                "Yu Liu",
                "Yu Yue",
                "Yufeng Zhou",
                "Yufeng Yuan",
                "Yuhang Xu",
                "Yuhong Yang",
                "Yun Zhang",
                "Yunhao Fang",
                "Yuntao Li",
                "Yurui Ren",
                "Yuwen Xiong",
                "Zehua Hong",
                "Zehua Wang",
                "Zewei Sun",
                "Zeyu Wang",
                "Zhao Cai",
                "Zhaoyue Zha",
                "Zhecheng An",
                "Zhehui Zhao",
                "Zhengzhuo Xu",
                "Zhipeng Chen",
                "Zhiyong Wu",
                "Zhuofan Zheng",
                "Zihao Wang",
                "Zilong Huang",
                "Ziyu Zhu",
                "Zuquan Song"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07062.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#survey",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Seed1.5-VL - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 532 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ 20 Ğ¼Ğ»Ñ€Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ğ½Ğ° 38 Ğ¸Ğ· 60 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Seed1.5-VL Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸, Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Multimodal Understanding with Seed1.5-VL",
                    "desc": "Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles."
                },
                "zh": {
                    "title": "Seed1.5-VLï¼šå¤šæ¨¡æ€ç†è§£ä¸æ¨ç†çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Seed1.5-VLï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚Seed1.5-VLç”±ä¸€ä¸ª532Må‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ªå…·æœ‰20Bæ´»è·ƒå‚æ•°çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoE LLMï¼‰ç»„æˆã€‚å°½ç®¡å…¶æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†åœ¨å¤šä¸ªå…¬å…±VLMåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨60ä¸ªå…¬å…±åŸºå‡†ä¸­æœ‰38ä¸ªè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾å½¢ç”¨æˆ·ç•Œé¢æ§åˆ¶å’Œæ¸¸æˆç©æ³•ï¼ŒSeed1.5-VLè¶…è¶Šäº†é¢†å…ˆçš„å¤šæ¨¡æ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬OpenAI CUAå’ŒClaude 3.7ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07608",
            "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
            "url": "https://huggingface.co/papers/2505.07608",
            "abstract": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.",
            "score": 32,
            "issue_id": 3723,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "9db5f7b72add3369",
            "authors": [
                "Xiaomi LLM-Core Team",
                ":",
                "Bingquan Xia",
                "Bowen Shen",
                "Cici",
                "Dawei Zhu",
                "Di Zhang",
                "Gang Wang",
                "Hailin Zhang",
                "Huaqiu Liu",
                "Jiebao Xiao",
                "Jinhao Dong",
                "Liang Zhao",
                "Peidian Li",
                "Peng Wang",
                "Shihua Yu",
                "Shimao Chen",
                "Weikun Wang",
                "Wenhan Ma",
                "Xiangwei Deng",
                "Yi Huang",
                "Yifan Song",
                "Zihan Jiang",
                "Bowen Ye",
                "Can Cai",
                "Chenhong He",
                "Dong Zhang",
                "Duo Zhang",
                "Guoan Wang",
                "Hao Tian",
                "Haochen Zhao",
                "Heng Qu",
                "Hongshen Xu",
                "Jun Shi",
                "Kainan Bao",
                "QingKai Fang",
                "Kang Zhou",
                "Kangyang Zhou",
                "Lei Li",
                "Menghang Zhu",
                "Nuo Chen",
                "Qiantong Wang",
                "Shaohui Liu",
                "Shicheng Li",
                "Shuhao Gu",
                "Shuhuai Ren",
                "Shuo Liu",
                "Sirui Deng",
                "Weiji Zhuang",
                "Weiwei Lv",
                "Wenyu Yang",
                "Xin Zhang",
                "Xing Yong",
                "Xing Zhang",
                "Xingchen Song",
                "Xinzhe Xu",
                "Xu Wang",
                "Yihan Yan",
                "Yu Tu",
                "Yuanyuan Tian",
                "Yudong Wang",
                "Yue Yu",
                "Zhenru Lin",
                "Zhichao Song",
                "Zihao Yue"
            ],
            "affiliations": [
                "Xiaomi LLM-Core Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07608.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#reasoning",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#rl",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MiMo-7B: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MiMo-7B - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ»Ğ¾ÑÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 130 Ñ‚Ñ‹ÑÑÑ‡ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MiMo-7B-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques",
                    "desc": "MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks."
                },
                "zh": {
                    "title": "MiMo-7Bï¼šæ¨ç†ä»»åŠ¡çš„å¼ºå¤§è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–äº†é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¢å¼ºäº†æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥æå‡åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚MiMo-7B-Baseåœ¨25ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¢åŠ äº†å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ•´ç†äº†130Kä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜æ•°æ®é›†ï¼Œç»“åˆæµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æœºåˆ¶ï¼Œè§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æˆ˜ç•¥æ€§æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07787",
            "title": "Learning from Peers in Reasoning Models",
            "url": "https://huggingface.co/papers/2505.07787",
            "abstract": "Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the \"Prefix Dominance Trap\". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .",
            "score": 24,
            "issue_id": 3722,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "350f28f20ab516fc",
            "authors": [
                "Tongxu Luo",
                "Wenyu Du",
                "Jiaxi Bi",
                "Stephen Chung",
                "Zhengyang Tang",
                "Hao Yang",
                "Min Zhang",
                "Benyou Wang"
            ],
            "affiliations": [
                "DualityRL",
                "Huawei",
                "The Chinese University of Hong Kong, Shenzhen",
                "USTB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07787.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ² 'Ğ»Ğ¾Ğ²ÑƒÑˆĞºÑƒ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°', ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑˆĞ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñƒ ÑĞ²ĞµÑ€ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²' (LeaP), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LeaP-T, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LeaP."
                },
                "en": {
                    "title": "Empowering LRMs through Peer Collaboration",
                    "desc": "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."
                },
                "zh": {
                    "title": "åŒä¼´å­¦ä¹ ï¼šæå‡æ¨ç†æ¨¡å‹çš„è‡ªæˆ‘çº æ­£èƒ½åŠ›",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·æœ‰è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä½†å½“æ¨ç†è¿‡ç¨‹ä»¥çŸ­è€Œå·®çš„å¼€å¤´å¼€å§‹æ—¶ï¼Œæ¨¡å‹å¾ˆéš¾æ¢å¤ã€‚æˆ‘ä»¬ç§°è¿™ç§ç°è±¡ä¸ºâ€œå‰ç¼€ä¸»å¯¼é™·é˜±â€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»åŒä¼´å­¦ä¹ â€ï¼ˆLeaPï¼‰ï¼Œé€šè¿‡è·¯ç”±æœºåˆ¶è®©æ¯ä¸ªæ¨ç†è·¯å¾„æ€»ç»“å…¶ä¸­é—´æ¨ç†å¹¶ä¸å…¶ä»–è·¯å¾„å…±äº«ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥åŒä¼´çš„è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeaPæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åŒä»»åŠ¡éš¾åº¦æ—¶å±•ç°å‡ºå¼ºå¤§çš„é”™è¯¯å®¹å¿èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07747",
            "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
            "url": "https://huggingface.co/papers/2505.07747",
            "abstract": "While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.",
            "score": 23,
            "issue_id": 3723,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "d9ffe741ebae4acb",
            "authors": [
                "Weiyu Li",
                "Xuanyang Zhang",
                "Zheng Sun",
                "Di Qi",
                "Hao Li",
                "Wei Cheng",
                "Weiwei Cai",
                "Shihao Wu",
                "Jiarui Liu",
                "Zihao Wang",
                "Xiao Chen",
                "Feipeng Tian",
                "Jianxiong Pan",
                "Zeming Li",
                "Gang Yu",
                "Xiangyu Zhang",
                "Daxin Jiang",
                "Ping Tan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07747.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#data",
                    "#diffusion",
                    "#transfer_learning",
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Step1X-3D - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ VAE-DiT Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Step1X-3D Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸Ğ· 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº 3D-ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with Step1X-3D",
                    "desc": "The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation."
                },
                "zh": {
                    "title": "Step1X-3Dï¼šå¼€åˆ›å¯æ§3Dç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Step1X-3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³3Dç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºã€ç®—æ³•é™åˆ¶å’Œç”Ÿæ€ç³»ç»Ÿç¢ç‰‡åŒ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œå¤„ç†è¶…è¿‡500ä¸‡èµ„äº§ï¼Œåˆ›å»ºäº†ä¸€ä¸ª200ä¸‡é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ ‡å‡†åŒ–çš„å‡ ä½•å’Œçº¹ç†å±æ€§ã€‚å®ƒç»“åˆäº†æ··åˆVAE-DiTå‡ ä½•ç”Ÿæˆå™¨å’ŒåŸºäºæ‰©æ•£çš„çº¹ç†åˆæˆæ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„3Dæ¨¡å‹ã€‚Step1X-3Dè¿˜æ”¯æŒå°†2Dæ§åˆ¶æŠ€æœ¯ç›´æ¥è½¬ç§»åˆ°3Dåˆæˆï¼Œæ¨åŠ¨äº†å¯æ§3Dèµ„äº§ç”Ÿæˆçš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06548",
            "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
            "url": "https://huggingface.co/papers/2505.06548",
            "abstract": "Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.",
            "score": 15,
            "issue_id": 3722,
            "pub_date": "2025-05-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ",
                "en": "May 10",
                "zh": "5æœˆ10æ—¥"
            },
            "hash": "db28335cad79db53",
            "authors": [
                "Aniruddha Roy",
                "Pretam Ray",
                "Abhilash Nandy",
                "Somak Aditya",
                "Pawan Goyal"
            ],
            "affiliations": [
                "Indian Institute of Technology, Kharagpur"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06548.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#rl",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğœ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLaMA 2-7B, LLama 2-13B, Mistral 7B) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² 63-66% Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Empowering LLMs with Cost-Effective Instruction Generation",
                    "desc": "This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œæå‡LLMsæ€§èƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æŒ‡ä»¤é©±åŠ¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºçš„å°å‹LLMsï¼ˆå¦‚LLaMA 2-7Bã€LLaMA 2-13Bå’ŒMistral 7Bï¼‰æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®é›†ï¼Œä»è€Œå‡å°‘äººå·¥å¹²é¢„å’Œæˆæœ¬ã€‚é€šè¿‡å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç®—æ³•ï¼Œç ”ç©¶è¡¨æ˜è¿™ç§æ–¹æ³•åœ¨63-66%çš„ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07447",
            "title": "Unified Continuous Generative Models",
            "url": "https://huggingface.co/papers/2505.07447",
            "abstract": "Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.",
            "score": 11,
            "issue_id": 3725,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "8d18ef028e9905b1",
            "authors": [
                "Peng Sun",
                "Yi Jiang",
                "Tao Lin"
            ],
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07447.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸ flow-matching, Ñ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ consistency models. Ğ˜Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, UCGM-{T,S}, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet 256x256, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, UCGM-S ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ FID Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Generative Models for Superior Performance",
                    "desc": "This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„è¿ç»­ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆå¤šæ­¥å’Œå°‘æ­¥ç”Ÿæˆæ–¹æ³•çš„è®­ç»ƒå’Œé‡‡æ ·ã€‚é€šè¿‡å¼•å…¥ç»Ÿä¸€çš„è®­ç»ƒå’Œé‡‡æ ·å™¨ï¼ˆUCGM-{T,S}ï¼‰ï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒUCGM-Tèƒ½å¤Ÿåœ¨20æ­¥å†…å°†å¤šæ­¥æ¨¡å‹çš„FIDé™ä½åˆ°1.30ï¼Œè€Œå°‘æ­¥æ¨¡å‹åœ¨ä»…2æ­¥å†…è¾¾åˆ°1.42çš„FIDã€‚æ­¤å¤–ï¼Œä½¿ç”¨UCGM-Så¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼ŒFIDä»250æ­¥çš„1.26é™è‡³ä»…40æ­¥çš„1.06ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07818",
            "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
            "url": "https://huggingface.co/papers/2505.07818",
            "abstract": "Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.",
            "score": 10,
            "issue_id": 3723,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "023078250e0d651f",
            "authors": [
                "Zeyue Xue",
                "Jie Wu",
                "Yu Gao",
                "Fangyuan Kong",
                "Lingting Zhu",
                "Mengzhao Chen",
                "Zhiheng Liu",
                "Wei Liu",
                "Qiushan Guo",
                "Weilin Huang",
                "Ping Luo"
            ],
            "affiliations": [
                "ByteDance Seed",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07818.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#video",
                    "#multimodal",
                    "#rl",
                    "#diffusion",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DanceGRPO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DanceGRPO - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Group Relative Policy Optimization (GRPO) Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. DanceGRPO ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. DanceGRPO ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DanceGRPO: Unifying Reinforcement Learning for Visual Generation",
                    "desc": "This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis."
                },
                "zh": {
                    "title": "DanceGRPOï¼šè§†è§‰ç”Ÿæˆçš„ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åº”ç”¨äºè§†è§‰ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨ç°ä»£å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·ã€è®­ç»ƒç¨³å®šæ€§å’Œè§†é¢‘ç”ŸæˆéªŒè¯æ–¹é¢çš„å±€é™æ€§ã€‚DanceGRPOèƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹å’Œä¿®æ­£æµç­‰å¤šç§ç”ŸæˆèŒƒå¼ä¸­æ— ç¼é€‚åº”ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŸºçº¿ï¼Œå±•ç¤ºäº†åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03733",
            "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
            "url": "https://huggingface.co/papers/2505.03733",
            "abstract": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.",
            "score": 10,
            "issue_id": 3722,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "1d5e56d00ea8d485",
            "authors": [
                "Zimu Lu",
                "Yunqiao Yang",
                "Houxing Ren",
                "Haotian Hou",
                "Han Xiao",
                "Ke Wang",
                "Weikang Shi",
                "Aojun Zhou",
                "Mingjie Zhan",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03733.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "WebGen-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "WebGen-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ñ‹ Ñ Ğ½ÑƒĞ»Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GPT-4o Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ. Ğ›ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ - Bolt.diy Ñ DeepSeek-R1 - Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 27,8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°."
                },
                "en": {
                    "title": "Benchmarking LLMs for Website Code Generation",
                    "desc": "This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models."
                },
                "zh": {
                    "title": "è¯„ä¼°LLMä»£ç†ç”Ÿæˆç½‘ç«™ä»£ç çš„æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•WebGen-Benchï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨ä»é›¶å¼€å§‹åˆ›å»ºå¤šæ–‡ä»¶ç½‘ç«™ä»£ç åº“çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«å¤šæ ·åŒ–çš„ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ï¼Œæ¶µç›–äº†ä¸‰å¤§ç±»å’Œåä¸‰å°ç±»ï¼Œå‡ ä¹åŒ…æ‹¬æ‰€æœ‰é‡è¦ç±»å‹çš„Webåº”ç”¨ç¨‹åºã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆç½‘ç«™çš„è´¨é‡ï¼Œä½¿ç”¨GPT-4oç”Ÿæˆé’ˆå¯¹æ¯ä¸ªåŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æ‰‹åŠ¨è¿‡æ»¤å’Œè°ƒæ•´ï¼Œæœ€ç»ˆå½¢æˆ647ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚é€šè¿‡å¼ºå¤§çš„ç½‘é¡µå¯¼èˆªä»£ç†è‡ªåŠ¨æ‰§è¡Œæµ‹è¯•ï¼Œè¯„ä¼°ç”Ÿæˆç½‘ç«™çš„å“åº”æ˜¯å¦ç¬¦åˆé¢„æœŸç»“æœï¼Œç»“æœæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ç»„åˆçš„å‡†ç¡®ç‡ä»…ä¸º27.8%ï¼Œæ˜¾ç¤ºå‡ºåŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07796",
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "url": "https://huggingface.co/papers/2505.07796",
            "abstract": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.",
            "score": 8,
            "issue_id": 3723,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "c63d617be0b4d13a",
            "authors": [
                "Xingjin Wang",
                "Howe Tissue",
                "Lu Wang",
                "Linjing Li",
                "Daniel Dajun Zeng"
            ],
            "affiliations": [
                "Ritzz-AI",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07796.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ (CPT) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ CPT, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ CPT."
                },
                "en": {
                    "title": "Unlocking the Dynamics of Continual Pre-Training",
                    "desc": "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."
                },
                "zh": {
                    "title": "æŒç»­é¢„è®­ç»ƒçš„åŠ¨æ€ä¸ä¼˜åŒ–æ³•åˆ™",
                    "desc": "æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ˜¯ä¸€ç§å°†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œç‰¹åˆ«å…³æ³¨åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œé€šç”¨æ€§èƒ½å’Œä¸‹æ¸¸é¢†åŸŸæ€§èƒ½çš„æ¼”å˜ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°CPTæŸå¤±æ›²çº¿æœ¬è´¨ä¸Šæè¿°äº†ä»ä¸€ä¸ªæ›²çº¿åˆ°å¦ä¸€ä¸ªéšè—æ›²çº¿çš„è¿‡æ¸¡ï¼Œå¹¶é€šè¿‡è§£è€¦åˆ†å¸ƒå˜åŒ–å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“æ¥è¿›è¡Œæè¿°ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ç§CPTç¼©æ”¾æ³•åˆ™ï¼Œç»“åˆäº†è¿™ä¸¤ä¸ªå› ç´ ï¼Œä½¿å¾—èƒ½å¤Ÿé¢„æµ‹åœ¨ä»»ä½•ï¼ˆæŒç»­ï¼‰è®­ç»ƒæ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ä¸‹çš„æŸå¤±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07293",
            "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection",
            "url": "https://huggingface.co/papers/2505.07293",
            "abstract": "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.",
            "score": 8,
            "issue_id": 3725,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "7403ae602b400fc4",
            "authors": [
                "Kai Hua",
                "Steven Wu",
                "Ge Zhang",
                "Ke Shen"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07293.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¯Ğœ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttentionInfluence Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ AttentionInfluence Ğº ĞºĞ¾Ñ€Ğ¿ÑƒÑÑƒ SmolLM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with AttentionInfluence",
                    "desc": "This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks."
                },
                "zh": {
                    "title": "æ— ç›‘ç£æ¨ç†æ•°æ®é€‰æ‹©çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬è¶Šæ¥è¶Šå…³æ³¨æ”¶é›†æ¨ç†å¯†é›†å‹çš„é¢„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç›‘ç£åˆ†ç±»å™¨æ¥è¯†åˆ«è¿™äº›æ•°æ®ï¼Œè¿™éœ€è¦äººç±»æˆ–LLMsè¿›è¡Œæ ‡æ³¨ï¼Œå¸¸å¸¸å¼•å…¥é¢†åŸŸç‰¹å®šçš„åè§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAttentionInfluenceçš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ— ç›‘ç£è®­ç»ƒæ–¹æ³•ã€‚é€šè¿‡ç®€å•çš„æ³¨æ„åŠ›å¤´å±è”½æ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—ä¸€ä¸ªå°å‹çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿä½œä¸ºå¼ºå¤§çš„æ•°æ®é€‰æ‹©å™¨ï¼Œä»è€Œåœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07596",
            "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
            "url": "https://huggingface.co/papers/2505.07596",
            "abstract": "Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.",
            "score": 7,
            "issue_id": 3723,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "10063104a79da512",
            "authors": [
                "Ziyang Huang",
                "Xiaowei Yuan",
                "Yiming Ju",
                "Jun Zhao",
                "Kang Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07596.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#agents",
                    "#optimization",
                    "#hallucinations",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞºĞ°Ñ‚ÑŒ, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒÑÑ ÑĞµĞ±Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° retrieval-augmented generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° IKEA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IKEA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Optimizing Knowledge Use in Language Models with IKEA",
                    "desc": "This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ£€ç´¢ï¼Œä¼˜åŒ–çŸ¥è¯†åˆ©ç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œåä¸ºIKEAï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢èƒ½åŠ›ã€‚IKEAèƒ½å¤Ÿè¯†åˆ«è‡ªèº«çŸ¥è¯†çš„è¾¹ç•Œï¼Œå¹¶ä¼˜å…ˆä½¿ç”¨å†…éƒ¨çŸ¥è¯†ï¼Œåªæœ‰åœ¨å†…éƒ¨çŸ¥è¯†ä¸è¶³æ—¶æ‰ä¼šè¿›è¡Œå¤–éƒ¨æ£€ç´¢ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°å’Œè®­ç»ƒæ•°æ®é›†ï¼ŒIKEAèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å†—ä½™æ£€ç´¢ï¼Œæé«˜å›ç­”çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIKEAåœ¨å¤šä¸ªçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†æ£€ç´¢é¢‘ç‡ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06324",
            "title": "Document Attribution: Examining Citation Relationships using Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.06324",
            "abstract": "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.",
            "score": 1,
            "issue_id": 3724,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ",
                "en": "May 9",
                "zh": "5æœˆ9æ—¥"
            },
            "hash": "d1b4a407c1a67da8",
            "authors": [
                "Vipula Rawte",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Nedim Lipka"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Adobe Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06324.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² LLM: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ flan-ul2, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 0,27% Ğ¸ 2,4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AttributionBench. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ flan-t5-small, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ»Ğ¸Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ F1 Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑĞ»Ğ¾ÑÑ…, ĞºÑ€Ğ¾Ğ¼Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ…."
                },
                "en": {
                    "title": "Enhancing Trust in LLMs through Improved Attribution Techniques",
                    "desc": "This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¿¡æ€§ä¸å¯è§£é‡Šæ€§",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æ¡£æ‘˜è¦ã€é—®ç­”å’Œä¿¡æ¯æå–ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œç¡®ä¿è¿™äº›ç³»ç»Ÿçš„å¯ä¿¡æ€§å’Œå¯è§£é‡Šæ€§å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å½’å› æ–¹æ³•ï¼Œé€šè¿‡è¿½è¸ªç”Ÿæˆçš„è¾“å‡ºå›åˆ°å…¶æºæ–‡æ¡£æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æŠ€æœ¯ï¼šä¸€ç§æ˜¯é›¶æ ·æœ¬æ–¹æ³•ï¼Œå°†å½’å› è§†ä¸ºç®€å•çš„æ–‡æœ¬è•´å«ä»»åŠ¡ï¼Œå¦ä¸€ç§æ˜¯æ¢ç´¢æ³¨æ„åŠ›æœºåˆ¶åœ¨å¢å¼ºå½’å› è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07086",
            "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design",
            "url": "https://huggingface.co/papers/2505.07086",
            "abstract": "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.",
            "score": 0,
            "issue_id": 3725,
            "pub_date": "2025-05-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ",
                "en": "May 11",
                "zh": "5æœˆ11æ—¥"
            },
            "hash": "a2fce171208a1e7a",
            "authors": [
                "Tong Chen",
                "Yinuo Zhang",
                "Sophia Tang",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Department of Computer Science, Fudan University",
                "Management and Technology Program, University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07086.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#science",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸. MOG-DFM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğº ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ°Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ”ĞĞš Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. MOG-DFM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞ±Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Biomolecule Design with MOG-DFM",
                    "desc": "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."
                },
                "zh": {
                    "title": "å¤šç›®æ ‡ä¼˜åŒ–ï¼ŒåŠ©åŠ›ç”Ÿç‰©åˆ†å­è®¾è®¡",
                    "desc": "åœ¨ç”Ÿç‰©åˆ†å­å·¥ç¨‹ä¸­ï¼Œè®¾è®¡æ»¡è¶³å¤šç§åŠŸèƒ½å’Œç”Ÿç‰©ç‰©ç†æ ‡å‡†çš„ç”Ÿç‰©åºåˆ—ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šç›®æ ‡å¼•å¯¼ç¦»æ•£æµåŒ¹é…ï¼ˆMOG-DFMï¼‰çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ ‡é‡ç›®æ ‡ä¹‹é—´å®ç°å¸•ç´¯æ‰˜æœ‰æ•ˆçš„æƒè¡¡ã€‚MOG-DFMé€šè¿‡è®¡ç®—æ··åˆæ’åæ–¹å‘åˆ†æ•°å’Œåº”ç”¨è‡ªé€‚åº”è¶…é”¥è¿‡æ»¤å™¨ï¼Œæ¥å¼•å¯¼é¢„è®­ç»ƒçš„ç¦»æ•£æ—¶é—´æµåŒ¹é…ç”Ÿæˆå™¨è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚æˆ‘ä»¬å±•ç¤ºäº†MOG-DFMåœ¨ç”Ÿæˆä¼˜åŒ–çš„è‚½ç»“åˆç‰©å’Œç‰¹å®šå¢å¼ºå­ç±»DNAåºåˆ—æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤šå±æ€§å¼•å¯¼çš„ç”Ÿç‰©åˆ†å­åºåˆ—è®¾è®¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-12.html",
    "link_next": "2025-05-14.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "12.05",
        "en": "05/12",
        "zh": "5æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 6,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 3,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† Bielik v3ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºæ³¢å…°è¯­å¤„ç†ä¼˜åŒ–çš„å‚æ•°é«˜æ•ˆç”Ÿæˆæ–‡æœ¬æ¨¡å‹ï¼ˆ1.5B å’Œ 4.5Bï¼‰ã€‚è¿™äº›æ¨¡å‹å±•ç¤ºäº†è¾ƒå°ä½†ä¼˜åŒ–è‰¯å¥½çš„æ¶æ„å¯ä»¥å®ç°ä¸å¤§å¾—å¤šæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„è®¡ç®—èµ„æºã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è‡ªå®šä¹‰æ³¢å…°è¯­åˆ†è¯å™¨ï¼ˆAPT4ï¼‰ã€å¹³è¡¡ä¸åŒæŒ‡ä»¤ç±»å‹å­¦ä¹ çš„åŠ æƒæŒ‡ä»¤äº¤å‰ç†µæŸå¤±å’Œæ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´çš„è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„ 292 äº¿ä¸ªæ ‡è®°å’Œ 303 ç™¾ä¸‡æ–‡æ¡£çš„è®­ç»ƒï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚4.5B å‚æ•°æ¨¡å‹çš„ç»“æœä¸å…¶ 2-3 å€å¤§å°çš„æ¨¡å‹ç«äº‰åŠ›ç›¸å½“ï¼Œè€Œ 1.5B æ¨¡å‹åˆ™åœ¨å…¶æå…¶ç´§å‡‘çš„é…ç½®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚",
        "title": "Bielik v3 Small: Technical Report",
        "pinyin": "WÇ’men jiÃ¨shÃ o le Bielik v3, zhÃ¨ shÃ¬ yÄ« xÃ¬liÃ¨ zhuÄn wÃ¨i BÅlÃ¡n yÇ” chÇ”lÇ yÅuhuÃ  de cÄnshÃ¹ gÄoxiÃ o shÄ“ngchÃ©ng wÃ©nbÄ›n mÃ³xÃ­ng (1.5B hÃ© 4.5B). ZhÃ¨xiÄ“ mÃ³xÃ­ng zhÇnshÃ¬ le jiÃ o xiÇo dÃ n yÅuhuÃ  liÃ¡ng hÇo de jiÃ gÃ²u kÄ›yÇ shÃ­xiÃ n yÇ” dÃ  dÃ© duÅ mÃ³xÃ­ng xiÃ ngdÄng de xÃ­ngnÃ©ng, tÃ³ngshÃ­ xÅ«yÃ o gÃ¨ng shÇo de jÃ¬suÃ n zÄ«yuÃ¡n. WÇ’men de fÄngfÇ bÄokuÃ² zÃ¬dÃ¬ngyÃ¬ BÅlÃ¡n yÇ” fÄ“ncÃ­qÃ¬ (APT4), pÃ­ng hÃ©ng bÃ¹tÃ³ng zhÇlÃ¬ng lÃ¨ixÃ­ng xuÃ©xÃ­ de jiÄquÃ¡n zhÇlÃ¬ng jiÄochÄ shÄngsÇ”n yÇ” gÄ“njÃ¹ xÃ¹nliÃ n jÃ¬ndÃ¹ dÃ²ngtÃ i tiÃ¡ojiÃ© de zÃ¬shÃ¬yÃ¬ng xuÃ©xÃ­ lÇœ. JÄ«ngxÄ«n cÃ¨huÃ  de 292 yÃ¬ gÃ¨ biÄojÃ¬ hÃ© 303 bÇi wÃ n wÃ©njiÃ n de xÃ¹nliÃ n, zhÃ¨xiÄ“ mÃ³xÃ­ng zÃ i duÅgÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨. 4.5B cÄnshÃ¹ mÃ³xÃ­ng de jiÃ©guÇ’ yÇ” qÃ­ 2-3 bÃ¨i dÃ xÃ¬ao de mÃ³xÃ­ng jÃ¬ngzhÄ“nglÃ¬ xiÃ ngdÄng, Ã©r 1.5B mÃ³xÃ­ng zÃ© zÃ i qÃ­ qÃ­ tÃ¨ jÇnkÇ’u de pÃ¨izhÃ¬ xiÃ  biÇoxiÃ n chÅ« qiÃ¡ngdÃ  de xÃ­ngnÃ©ng.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"ç³»åˆ—\", \"pinyin\": \"xÃ¬ liÃ¨\", \"trans\": \"series\"},\n    {\"word\": \"ä¸“ä¸º\", \"pinyin\": \"zhuÄn wÃ¨i\", \"trans\": \"specially for\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimize\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄn shÃ¹\", \"trans\": \"parameters\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"ç›¸å½“\", \"pinyin\": \"xiÄng dÄng\", \"trans\": \"equivalent\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬ suÃ n\", \"trans\": \"compute\"},\n    {\"word\": \"èµ„æº\", \"pinyin\": \"zÄ« yuÃ¡n\", \"trans\": \"resources\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"è‡ªå®šä¹‰\", \"pinyin\": \"zÃ¬ dÃ¬ng yÃ¬\", \"trans\": \"customize\"},\n    {\"word\": \"åˆ†è¯å™¨\", \"pinyin\": \"fÄ“n cÃ­ qÃ¬\", \"trans\": \"tokenizer\"},\n    {\"word\": \"å¹³è¡¡\", \"pinyin\": \"pÃ­ng hÃ©ng\", \"trans\": \"balance\"},\n    {\"word\": \"æŒ‡ä»¤\", \"pinyin\": \"zhÇ lÃ¬ng\", \"trans\": \"instruction\"},\n    {\"word\": \"ç±»å‹\", \"pinyin\": \"lÃ¨i xÃ­ng\", \"trans\": \"type\"},\n    {\"word\": \"å­¦ä¹ \", \"pinyin\": \"xuÃ© xÃ­\", \"trans\": \"learn\"},\n    {\"word\": \"åŠ æƒ\", \"pinyin\": \"jiÄ quÃ¡n\", \"trans\": \"weighted\"},\n    {\"word\": \"äº¤å‰ç†µ\", \"pinyin\": \"jiÄo chÄ shÄng\", \"trans\": \"cross-entropy\"},\n    {\"word\": \"æŸå¤±\", \"pinyin\": \"sÇ”n shÄ«\", \"trans\": \"loss\"},\n    {\"word\": \"è‡ªé€‚åº”\", \"pinyin\": \"zÃ¬ shÃ¬ yÃ¬ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"å­¦ä¹ ç‡\", \"pinyin\": \"xuÃ© xÃ­ lÇœ\", \"trans\": \"learning rate\"},\n    {\"word\": \"ç­–åˆ’\", \"pinyin\": \"cÃ¨ huÃ \", \"trans\": \"plan\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄo jÃ¬\", \"trans\": \"token\"},\n    {\"word\": \"æ–‡æ¡£\", \"pinyin\": \"wÃ©n dÃ ng\", \"trans\": \"document\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨ shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"perform\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"ç«äº‰åŠ›\", \"pinyin\": \"jÃ¬ng zhÄ“ng lÃ¬\", \"trans\": \"competitiveness\"},\n    {\"word\": \"é…ç½®\", \"pinyin\": \"pÃ¨i zhÃ¬\", \"trans\": \"configuration\"},\n    {\"word\": \"ç´§å‡‘\", \"pinyin\": \"jÇn cÃ²u\", \"trans\": \"compact\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ng dÃ \", \"trans\": \"powerful\"}\n]",
        "trans": "We introduced Bielik v3, a series of parameter-efficient text generation models optimized specifically for Polish language processing (1.5B and 4.5B). These models demonstrate that smaller but well-optimized architectures can achieve performance comparable to much larger models while requiring fewer computational resources. Our approach includes a custom Polish tokenizer (APT4), weighted instruction cross-entropy loss to balance learning across different instruction types, and adaptive learning rates dynamically adjusted according to training progress. With meticulously planned training on 292 billion tokens and 303 million documents, these models perform excellently on multiple benchmarks. The 4.5B parameter model's results are competitive with models 2-3 times its size, while the 1.5B model shows strong performance in its extremely compact configuration.",
        "update_ts": "2025-05-12 10:13"
    }
}