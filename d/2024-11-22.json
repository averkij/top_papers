{
    "date": {
        "ru": "22 ноября",
        "en": "November 22",
        "zh": "11月22日"
    },
    "time_utc": "2024-11-22 05:10",
    "weekday": 4,
    "issue_id": 722,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10442",
            "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
            "url": "https://huggingface.co/papers/2411.10442",
            "abstract": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.",
            "score": 15,
            "issue_id": 722,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "3cc3675b352b1634",
            "authors": [
                "Weiyun Wang",
                "Zhe Chen",
                "Wenhai Wang",
                "Yue Cao",
                "Yangzhou Liu",
                "Zhangwei Gao",
                "Jinguo Zhu",
                "Xizhou Zhu",
                "Lewei Lu",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "OpenGVLab, Shanghai AI Laboratory",
                "SenseTime Research",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10442.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление мультимодальных рассуждений ИИ через оптимизацию предпочтений",
                    "desc": "Исследователи представили новый метод улучшения мультимодальных языковых моделей (MLLM) с помощью оптимизации предпочтений (PO). Они создали крупномасштабный набор данных MMPR для мультимодальных рассуждений и разработали метод смешанной оптимизации предпочтений (MPO). Их модель InternVL2-8B-MPO показала значительное улучшение производительности в задачах мультимодальных рассуждений, особенно в тестах типа Chain-of-Thought. Результаты демонстрируют потенциал этого подхода для повышения способностей MLLM к мультимодальным рассуждениям."
                },
                "en": {
                    "title": "Boosting Multimodal Reasoning with Preference Optimization",
                    "desc": "This paper addresses the limitations of existing multimodal large language models (MLLMs) in reasoning tasks due to distribution shifts. It introduces a preference optimization (PO) process to enhance the Chain-of-Thought (CoT) performance of these models. The authors create a high-quality dataset called MMPR through an automated preference data construction pipeline and develop a method called Mixed Preference Optimization (MPO) to integrate PO with MLLMs. Their model, InternVL2-8B-MPO, shows significant improvements in multimodal reasoning tasks, achieving higher accuracy than previous models and demonstrating the potential for further advancements in this field."
                },
                "zh": {
                    "title": "提升多模态推理能力的新方法",
                    "desc": "本文介绍了一种新的方法来提高多模态大语言模型（MLLMs）的推理能力。我们提出了一种偏好优化（PO）过程，旨在解决现有模型在多模态推理中的分布偏移问题。通过构建高质量的大规模多模态推理偏好数据集MMPR，并将PO与MLLMs结合，我们开发了混合偏好优化（MPO）方法，显著提升了模型在多模态链式思维（CoT）任务中的表现。实验结果表明，我们的模型在多个基准测试中表现优异，特别是在多模态推理任务上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12364",
            "title": "Ultra-Sparse Memory Network",
            "url": "https://huggingface.co/papers/2411.12364",
            "abstract": "It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget.",
            "score": 8,
            "issue_id": 721,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "090bf8a39ee13838",
            "authors": [
                "Zihao Huang",
                "Qiyang Min",
                "Hongzhi Huang",
                "Defa Zhu",
                "Yutao Zeng",
                "Ran Guo",
                "Xun Zhou"
            ],
            "affiliations": [
                "Seed-Foundation-Model Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12364.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "UltraMem: сверхбыстрые трансформеры с разреженной памятью",
                    "desc": "В этой статье представлен новый подход UltraMem, который использует сверхразреженные слои памяти для улучшения производительности трансформеров. Метод позволяет значительно снизить задержку при выводе, сохраняя качество модели. Исследованы законы масштабирования новой архитектуры, показывающие её преимущества перед традиционными моделями. Эксперименты с сетями, содержащими до 20 миллионов ячеек памяти, демонстрируют state-of-the-art скорость вывода и качество модели при заданном вычислительном бюджете."
                },
                "en": {
                    "title": "UltraMem: Speeding Up Transformers with Sparse Memory!",
                    "desc": "This paper presents UltraMem, a novel architecture designed to enhance the efficiency of Transformer models by integrating a large-scale, ultra-sparse memory layer. By decoupling the number of parameters from computational complexity, UltraMem addresses the high memory access costs that hinder inference speed in existing models. The authors demonstrate that their approach not only reduces inference latency but also maintains competitive model performance, even with networks containing up to 20 million memory slots. Additionally, the study explores the scaling laws of UltraMem, showing that it outperforms traditional models while adhering to computational constraints."
                },
                "zh": {
                    "title": "UltraMem：提升推理速度的新架构",
                    "desc": "本论文提出了一种名为UltraMem的新架构，旨在解决Transformer模型在推理时的高内存访问成本问题。通过引入大规模的超稀疏内存层，UltraMem能够在保持模型性能的同时显著降低推理延迟。我们还研究了这种新架构的扩展规律，结果表明其具有良好的扩展性，并且在性能上优于传统模型。实验表明，我们的方法在给定的计算预算内实现了最先进的推理速度和模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14432",
            "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2411.14432",
            "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.",
            "score": 5,
            "issue_id": 720,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "0af1bb82d8021d3b",
            "authors": [
                "Yuhao Dong",
                "Zuyan Liu",
                "Hai-Long Sun",
                "Jingkang Yang",
                "Winston Hu",
                "Yongming Rao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, NTU",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14432.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#agents",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение визуальных рассуждений ИИ через длинные цепочки и мультиагентное обучение",
                    "desc": "Статья представляет Insight-V - подход к улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждениям. Авторы предлагают двухэтапный конвейер для создания длинных и структурированных данных для обучения без участия человека. Они также разрабатывают мультиагентную систему, состоящую из агента рассуждений и агента-резюме, для эффективного обучения MLLM. Результаты показывают значительное улучшение производительности на сложных мультимодальных задачах, требующих визуального рассуждения."
                },
                "en": {
                    "title": "Empowering Multi-Modal Reasoning with Insight-V",
                    "desc": "This paper introduces Insight-V, a novel approach to enhance the reasoning capabilities of multi-modal large language models (MLLMs) by generating high-quality long-chain reasoning data. The authors propose a two-step pipeline that creates structured reasoning paths without human intervention and employs a multi-granularity assessment method to ensure the quality of the generated data. They also develop a multi-agent system that includes a reasoning agent for long-chain reasoning and a summary agent to evaluate and condense the reasoning outputs. The results show that Insight-V significantly improves performance on complex multi-modal tasks, particularly those requiring visual reasoning, while maintaining effectiveness on perception-focused tasks."
                },
                "zh": {
                    "title": "提升多模态推理能力的创新方法",
                    "desc": "本文介绍了一种名为Insight-V的系统，旨在提高多模态大语言模型（MLLMs）的推理能力。我们设计了一个两步生成管道，以无人工干预的方式创建长且结构化的推理数据，并采用多粒度评估方法确保数据质量。研究表明，直接用复杂推理数据监督MLLMs并不能达到理想效果，因此我们构建了一个多代理系统，包括专注于长链推理的推理代理和负责评估和总结推理结果的总结代理。通过这种方法，Insight-V在视觉推理等多模态基准测试中表现出显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14199",
            "title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs",
            "url": "https://huggingface.co/papers/2411.14199",
            "abstract": "Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.",
            "score": 5,
            "issue_id": 720,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "f429efe07ec308f2",
            "authors": [
                "Akari Asai",
                "Jacqueline He",
                "Rulin Shao",
                "Weijia Shi",
                "Amanpreet Singh",
                "Joseph Chee Chang",
                "Kyle Lo",
                "Luca Soldaini",
                "Sergey Feldman",
                "Mike D'arcy",
                "David Wadden",
                "Matt Latzke",
                "Minyang Tian",
                "Pan Ji",
                "Shengyan Liu",
                "Hao Tong",
                "Bohao Wu",
                "Yanyu Xiong",
                "Luke Zettlemoyer",
                "Graham Neubig",
                "Dan Weld",
                "Doug Downey",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Carnegie Mellon University",
                "Meta",
                "Stanford University",
                "University of Illinois, Urbana-Champaign",
                "University of North Carolina, Chapel Hill",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14199.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#rag",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "OpenScholar: ИИ-помощник для синтеза научной литературы",
                    "desc": "Статья представляет OpenScholar - специализированную языковую модель с расширенным поиском, которая отвечает на научные запросы, используя 45 миллионов научных статей. Для оценки модели авторы разработали ScholarQABench - первый масштабный мультидоменный бенчмарк для поиска литературы. OpenScholar-8B превосходит GPT-4 и PaperQA2 по точности ответов, несмотря на меньший размер модели. Эксперты предпочли ответы OpenScholar-8B и OpenScholar-GPT4o экспертным ответам в 51% и 70% случаев соответственно."
                },
                "en": {
                    "title": "Empowering Scientific Research with OpenScholar: Accurate, Citation-Backed Insights",
                    "desc": "This paper presents OpenScholar, a retrieval-augmented language model designed to assist researchers in synthesizing scientific literature. OpenScholar effectively identifies relevant passages from a vast collection of 45 million open-access papers and generates citation-backed responses to scientific queries. The model is evaluated using ScholarQABench, a benchmark that includes expert-written queries and answers across multiple domains, demonstrating superior performance in correctness compared to other models like GPT-4o. Additionally, OpenScholar shows a significant reduction in citation hallucination, achieving accuracy comparable to human experts, and enhances the performance of existing models through its innovative architecture."
                },
                "zh": {
                    "title": "OpenScholar：提升科学文献检索的智能助手",
                    "desc": "本论文介绍了一种名为OpenScholar的专用检索增强语言模型，旨在帮助科学家从4500万篇开放获取论文中提取相关信息并生成基于引用的回答。我们开发了ScholarQABench，这是第一个大规模的多领域文献搜索基准，包含2967个专家编写的查询和208个长答案，涵盖计算机科学、物理学、神经科学和生物医学等领域。OpenScholar在准确性上超越了GPT-4o和PaperQA2，尽管其模型规模较小，且在引用准确性方面与人类专家相当。我们还开源了所有代码、模型和数据，提供了公共演示，促进了科学文献的检索和理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14405",
            "title": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
            "url": "https://huggingface.co/papers/2411.14405",
            "abstract": "Currently OpenAI o1 has sparked a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: \"Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?\" Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.",
            "score": 3,
            "issue_id": 720,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "ef4a95abeea69237",
            "authors": [
                "Yu Zhao",
                "Huifeng Yin",
                "Bo Zeng",
                "Hao Wang",
                "Tianqi Shi",
                "Chenyang Lyu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "MarcoPolo Team, Alibaba International Digital Commerce"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14405.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Расширение границ искусственного интеллекта: от стандартных задач к открытым проблемам",
                    "desc": "Статья описывает разработку модели Marco-o1, которая расширяет возможности OpenAI o1 в области рассуждений. Модель нацелена на решение задач с открытым концом, где отсутствуют четкие стандарты и сложно количественно оценить результаты. Marco-o1 использует усовершенствованные методы, такие как обучение с подкреплением, цепочки размышлений и метод Монте-Карло. Основная цель - создать модель, способную эффективно обобщать знания и решать сложные задачи реального мира."
                },
                "en": {
                    "title": "Unlocking Generalization in Large Reasoning Models",
                    "desc": "This paper explores the capabilities of the Marco-o1 model in handling large reasoning tasks across various domains. It emphasizes the model's ability to generalize beyond traditional areas like mathematics and coding, where answers are clear-cut. The research investigates how Marco-o1 can tackle open-ended problems where standard solutions and quantifiable rewards are not readily available. Key techniques employed include Chain-of-Thought fine-tuning and Monte Carlo Tree Search, which enhance the model's reasoning and problem-solving abilities in complex scenarios."
                },
                "zh": {
                    "title": "推动推理模型的广泛应用",
                    "desc": "这篇论文探讨了大型推理模型（LRM）的研究，特别是OpenAI的o1模型。Marco-o1不仅关注数学、物理和编程等有标准答案的学科，还强调开放式问题的解决能力。研究的核心问题是o1模型是否能够有效地推广到缺乏明确标准和难以量化奖励的更广泛领域。Marco-o1结合了链式思维（CoT）微调、蒙特卡洛树搜索（MCTS）、反思机制和创新推理策略，以优化复杂的现实问题解决任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14251",
            "title": "Natural Language Reinforcement Learning",
            "url": "https://huggingface.co/papers/2411.14251",
            "abstract": "Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.",
            "score": 3,
            "issue_id": 719,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "351fc2a705b34aff",
            "authors": [
                "Xidong Feng",
                "Ziyu Wan",
                "Haotian Fu",
                "Bo Liu",
                "Mengyue Yang",
                "Girish A. Koushik",
                "Zhiyuan Hu",
                "Ying Wen",
                "Jun Wang"
            ],
            "affiliations": [
                "Brown University",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "University College London",
                "University of Bristol",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14251.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rl",
                    "#rlhf",
                    "#open_source",
                    "#games"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Обучение с подкреплением заговорило на естественном языке",
                    "desc": "Эта статья представляет новую концепцию - обучение с подкреплением на естественном языке (NLRL). NLRL расширяет традиционные марковские процессы принятия решений, переопределяя основные принципы RL в языковом пространстве. Используя достижения в области больших языковых моделей, NLRL может быть реализовано с помощью промптов или градиентного обучения. Эксперименты на играх Maze, Breakthrough и крестики-нолики демонстрируют эффективность и интерпретируемость этого подхода."
                },
                "en": {
                    "title": "Revolutionizing Decision-Making with Language: Natural Language Reinforcement Learning",
                    "desc": "This paper introduces Natural Language Reinforcement Learning (NLRL), which adapts traditional Reinforcement Learning (RL) methods to work with natural language representations. By extending the Markov Decision Process (MDP) framework, NLRL redefines key RL concepts such as task objectives, policies, and value functions in the context of language. The approach leverages advancements in large language models (LLMs) to enhance policy and value updates through prompting or gradient-based training. Experimental results on games like Maze, Breakthrough, and Tic-Tac-Toe showcase NLRL's effectiveness and interpretability across various applications."
                },
                "zh": {
                    "title": "自然语言强化学习：决策的新视角",
                    "desc": "强化学习（RL）通过马尔可夫决策过程（MDP）来数学化决策制定。本文提出了一种新的可能性，即自然语言强化学习（NLRL），通过将传统的MDP扩展到基于自然语言的表示空间。NLRL创新性地重新定义了强化学习的原则，包括任务目标、策略、价值函数、贝尔曼方程和策略迭代，使其适应语言的对应关系。通过在迷宫、突破和井字棋等游戏中的实验，验证了NLRL框架在多种应用场景中的有效性、效率和可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.13676",
            "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
            "url": "https://huggingface.co/papers/2411.13676",
            "abstract": "We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the \"forced-to-attend\" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.",
            "score": 1,
            "issue_id": 721,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "24009a4acf67d4c7",
            "authors": [
                "Xin Dong",
                "Yonggan Fu",
                "Shizhe Diao",
                "Wonmin Byeon",
                "Zijia Chen",
                "Ameya Sunil Mahabaleshwarkar",
                "Shih-Yang Liu",
                "Matthijs Van Keirsbilck",
                "Min-Hung Chen",
                "Yoshi Suhara",
                "Yingyan Lin",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.13676.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гибридная архитектура Hymba: эффективность малых языковых моделей на новом уровне",
                    "desc": "Авторы представляют Hymba - семейство малых языковых моделей с гибридной параллельной архитектурой, сочетающей механизмы внимания трансформеров и модели пространства состояний (SSM) для повышения эффективности. В модель добавлены обучаемые мета-токены, хранящие важную информацию и снижающие нагрузку на механизмы внимания. Оптимизация включает межслойное разделение ключей и значений, а также частичное скользящее окно внимания. Модель Hymba-1.5B-Base превосходит все публичные модели до 2 млрд параметров и даже Llama-3.2-3B по точности, при значительном уменьшении размера кэша и увеличении пропускной способности."
                },
                "en": {
                    "title": "Hymba: Efficient Language Models with Hybrid Architecture",
                    "desc": "Hymba is a new family of small language models that combines transformer attention with state space models (SSMs) to improve efficiency. The model uses attention heads for detailed recall and SSM heads for summarizing context effectively. It also features learnable meta tokens that help retain important information without overloading the attention mechanism. Our experiments show that Hymba outperforms existing small language models, achieving better accuracy while using significantly less cache and providing faster processing speeds."
                },
                "zh": {
                    "title": "Hymba：高效的小型语言模型新选择",
                    "desc": "我们提出了Hymba，这是一种小型语言模型，采用混合头并行架构，将变换器注意机制与状态空间模型（SSMs）结合，以提高效率。注意头提供高分辨率的回忆，而SSM头则实现高效的上下文总结。此外，我们引入了可学习的元标记，这些标记被添加到提示前，存储关键信息，减轻了与注意机制相关的“强制关注”负担。我们的模型通过跨层键值（KV）共享和部分滑动窗口注意机制进一步优化，结果是缓存大小紧凑。"
                }
            }
        }
    ],
    "link_prev": "2024-11-21.html",
    "link_next": "2024-11-25.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11月21日"
    },
    "short_date_next": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11月25日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种加速注意力计算的方法，称为 SageAttention2。它使用 4-bit 矩阵乘法和精度增强技术。首先，将矩阵 Q 和 K 量化为 INT4，将矩阵 P 和 V 量化为 FP8。然后，提出平滑 Q 和 V 的方法，提高注意力精度。实验证明，这种方法在多种模型上几乎没有性能损失，并且在 RTX4090 上的操作速度是 FlashAttention2 和 xformers 的 3 倍和 5 倍。代码已在 GitHub 上公开。",
        "title": "SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration",
        "pinyin": "这篇文章介绍了一种加速注意力计算的方法，称为 SageAttention2。它使用 4-bit 矩阵乘法和精度增强技术。首先，将矩阵 Q 和 K 量化为 INT4，将矩阵 P 和 V 量化为 FP8。然后，提出平滑 Q 和 V 的方法，提高注意力精度。实验证明，这种方法在多种模型上几乎没有性能损失，并且在 RTX4090 上的操作速度是 FlashAttention2 和 xformers 的 3 倍和 5 倍。代码已在 GitHub 上公开。\n\nzhè piān wén zhāng jiè shào le yī zhǒng jiā sù zhù yì lì jì suàn de fāng fǎ, chēng wéi SageAttention2. tā shǐ yòng 4-bit jǔ zhèn chéng fǎ hé jīng dù zēng qiáng jì shù. shǒu xiān, jiāng jǔ zhèn Q hé K liàng huà wéi INT4, jiāng jǔ zhèn P hé V liàng huà wéi FP8. rán hòu, tí chū píng huá Q hé V de fāng fǎ, tí gāo zhù yì lì jīng dù. shí yàn zhèng míng, zhè zhǒng fāng fǎ zài duō zhǒng mó xíng shàng jī hū méi yǒu xìng néng sǔn shī, bìng qiě zài RTX4090 shàng de cāo zuò sù dù shì FlashAttention2 hé xformers de 3 bèi hé 5 bèi. dài mǎ yǐ zài GitHub shàng gōng kāi.",
        "vocab": "[{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'},\n{'word': '注意力', 'pinyin': 'zhù yì lì', 'trans': 'attention'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'calculation'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '称为', 'pinyin': 'chēng wéi', 'trans': 'called'},\n{'word': '矩阵', 'pinyin': 'jǔ zhèn', 'trans': 'matrix'},\n{'word': '乘法', 'pinyin': 'chén fǎ', 'trans': 'multiplication'},\n{'word': '精度', 'pinyin': 'jīng dù', 'trans': 'precision'},\n{'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhancement'},\n{'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'},\n{'word': '量化', 'pinyin': 'liàng huà', 'trans': 'quantization'},\n{'word': '平滑', 'pinyin': 'píng huá', 'trans': 'smooth'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'},\n{'word': '操作', 'pinyin': 'cāo zuò', 'trans': 'operation'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}]",
        "trans": "This article introduces a method for accelerating attention computation, called SageAttention2. It employs 4-bit matrix multiplication and precision enhancement techniques. First, matrices Q and K are quantized to INT4, while matrices P and V are quantized to FP8. Then, a method for smoothing Q and V is proposed to improve attention precision. Experiments demonstrate that this method incurs almost no performance loss across various models and operates at speeds that are 3 times and 5 times faster than FlashAttention2 and xformers, respectively, on the RTX4090. The code has been made publicly available on GitHub.",
        "update_ts": "2024-11-21 09:11"
    }
}