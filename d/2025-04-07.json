{
    "date": {
        "ru": "7 апреля",
        "en": "April 7",
        "zh": "4月7日"
    },
    "time_utc": "2025-04-07 09:12",
    "weekday": 0,
    "issue_id": 3100,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02605",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "url": "https://huggingface.co/papers/2504.02605",
            "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.",
            "score": 26,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "bcf7d7c20685c914",
            "authors": [
                "Daoguang Zan",
                "Zhirong Huang",
                "Wei Liu",
                "Hanwu Chen",
                "Linhao Zhang",
                "Shulin Xin",
                "Lu Chen",
                "Qi Liu",
                "Xiaojian Zhong",
                "Aoyan Li",
                "Siyao Liu",
                "Yongsheng Xiao",
                "Liangqiang Chen",
                "Yuyu Zhang",
                "Jing Su",
                "Tianyu Liu",
                "Rui Long",
                "Kai Shen",
                "Liang Xiang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02605.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычный бенчмарк для оценки ИИ в решении программных задач",
                    "desc": "Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок."
                },
                "en": {
                    "title": "Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages",
                    "desc": "This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "多语言问题解决基准，推动强化学习研究",
                    "desc": "本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03553",
            "title": "Agentic Knowledgeable Self-awareness",
            "url": "https://huggingface.co/papers/2504.03553",
            "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a \"flood irrigation\" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
            "score": 12,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "4a06cb6959ea30d3",
            "authors": [
                "Shuofei Qiao",
                "Zhisong Qiu",
                "Baochang Ren",
                "Xiaobin Wang",
                "Xiangyuan Ru",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Nanjing University of Aeronautics and Astronautics",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самоосознанность в планировании: эффективное использование знаний языковыми моделями",
                    "desc": "Статья представляет новый подход к обучению языковых моделей для задач планирования, называемый KnowSelf. Он основан на принципе ситуативной самоосознанности, позволяющем модели динамически оценивать ситуацию и стратегически использовать знания. KnowSelf использует двухэтапный процесс обучения и специальные токены для переключения между различными ситуациями. Эксперименты показывают, что этот метод превосходит базовые подходы на различных задачах, минимально используя внешние знания."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Aware Decision Making",
                    "desc": "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."
                },
                "zh": {
                    "title": "自主调节知识使用的智能代理",
                    "desc": "大型语言模型（LLMs）在多种代理规划任务中表现出色。然而，传统的代理规划方法采用了\"洪水灌溉\"的方式，随意注入黄金轨迹、外部反馈和领域知识，这种做法忽视了人类在决策过程中动态评估情境需求的能力。我们提出了代理知识自我意识的概念，旨在填补这一空白，使基于LLM的代理能够自主调节知识的使用。具体而言，我们设计了一种启发式情境判断标准，通过标记代理自我探索轨迹上的特殊标记，收集训练数据，从而实现更高效的规划效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03561",
            "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
            "url": "https://huggingface.co/papers/2504.03561",
            "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
            "score": 8,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "469f9f28c32e1a1a",
            "authors": [
                "Runnan Fang",
                "Xiaobin Wang",
                "Yuan Liang",
                "Shuofei Qiao",
                "Jialong Wu",
                "Zekun Xi",
                "Ningyu Zhang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03561.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "SynWorld: Автономное исследование и обучение агентов в новых средах",
                    "desc": "SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые среды и оптимизировать рабочие процессы. Он использует синтез возможных сценариев с многошаговым вызовом действий и применяет метод Монте-Карло для поиска по дереву (MCTS) для эффективного уточнения знаний о действиях в текущей среде. Эксперименты показывают, что SynWorld является эффективным и универсальным подходом к изучению знаний о действиях в новых средах. Этот метод решает проблему ограниченных возможностей LLM-агентов при работе в незнакомых окружениях или нестандартных пространствах действий."
                },
                "en": {
                    "title": "Empowering Agents to Explore with SynWorld",
                    "desc": "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."
                },
                "zh": {
                    "title": "SynWorld：赋能代理探索新环境的框架",
                    "desc": "在代理与环境的互动中，代理通过规划和执行动作来扩展其能力。然而，基于大型语言模型的代理在新环境中或需要在非常规动作空间中导航时面临重大挑战。为了解决这个问题，我们提出了SynWorld框架，使代理能够合成可能的场景，并在动作空间内进行多步动作调用，同时执行蒙特卡洛树搜索（MCTS）探索，以有效地优化其在当前环境中的动作知识。实验结果表明，SynWorld是学习新环境中动作知识的有效且通用的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02949",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.02949",
            "abstract": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "score": 7,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "71423989b2bed2d8",
            "authors": [
                "Xianwei Zhuang",
                "Yuxin Xie",
                "Yufan Deng",
                "Dongchao Yang",
                "Liming Liang",
                "Jinghan Ru",
                "Yuguo Yin",
                "Yuexian Zou"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#open_source",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, генерации и редактирования изображений",
                    "desc": "VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям."
                },
                "en": {
                    "title": "Unifying Visual Understanding and Generation with VARGPT-v1.1",
                    "desc": "VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation."
                },
                "zh": {
                    "title": "统一视觉自回归模型的突破性进展",
                    "desc": "本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03641",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "url": "https://huggingface.co/papers/2504.03641",
            "abstract": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.",
            "score": 6,
            "issue_id": 3095,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "45da77ffd9c21caf",
            "authors": [
                "Wulin Xie",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Yang Shi",
                "Bingyan Nie",
                "Hongkai Chen",
                "Zhang Zhang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "CASIA",
                "M-M-E Project",
                "NJU",
                "PKU",
                "Vivo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый стандарт оценки мультимодальных языковых моделей",
                    "desc": "Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью."
                },
                "en": {
                    "title": "Enhancing Evaluation for Unified Multimodal Language Models",
                    "desc": "This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs."
                },
                "zh": {
                    "title": "全面评估统一多模态大语言模型的必要性",
                    "desc": "现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03601",
            "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
            "url": "https://huggingface.co/papers/2504.03601",
            "abstract": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io",
            "score": 4,
            "issue_id": 3097,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "05921cbfa42a13b4",
            "authors": [
                "Akshara Prabhakar",
                "Zuxin Liu",
                "Weiran Yao",
                "Jianguo Zhang",
                "Ming Zhu",
                "Shiyu Wang",
                "Zhiwei Liu",
                "Tulika Awalgaonkar",
                "Haolin Chen",
                "Thai Hoang",
                "Juan Carlos Niebles",
                "Shelby Heinecke",
                "Huan Wang",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03601.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#agents",
                    "#open_source",
                    "#data",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в обучении ИИ-агентов: синтетические данные для реалистичного многоходового взаимодействия",
                    "desc": "APIGen-MT - это новый фреймворк для генерации качественных данных для обучения ИИ-агентов многоходовому взаимодействию. Он использует двухфазный подход: сначала создаются детальные планы задач с помощью ревьюеров на основе больших языковых моделей, затем эти планы преобразуются в полные траектории взаимодействия. На основе полученных данных обучено семейство моделей xLAM-2-fc-r, превосходящих по ряду показателей такие модели как GPT-4 и Claude 3.5. Исследователи открыли доступ к синтетическим данным и обученным моделям для дальнейшего развития области ИИ-агентов."
                },
                "en": {
                    "title": "Generating High-Quality Data for AI Agents with APIGen-MT",
                    "desc": "The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research."
                },
                "zh": {
                    "title": "高效生成多轮交互数据的AI代理训练框架",
                    "desc": "为了训练有效的AI代理进行多轮交互，我们提出了APIGen-MT框架，该框架能够生成可验证和多样化的多轮代理数据。该框架分为两个阶段，首先通过大型语言模型（LLM）评审委员会和迭代反馈生成详细的任务蓝图，并提供真实的行动。接着，这些蓝图被转化为完整的交互轨迹，通过模拟人机互动实现。我们的xLAM-2-fc-r系列模型在多个基准测试中表现优异，尤其是在多轮设置中，小模型的表现超过了大模型，展示了我们的方法在生成高质量训练数据方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03011",
            "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
            "url": "https://huggingface.co/papers/2504.03011",
            "abstract": "This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.",
            "score": 3,
            "issue_id": 3096,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "94d3411c37993837",
            "authors": [
                "Junying Wang",
                "Jingyuan Liu",
                "Xin Sun",
                "Krishna Kumar Singh",
                "Zhixin Shu",
                "He Zhang",
                "Jimei Yang",
                "Nanxuan Zhao",
                "Tuanfeng Y. Wang",
                "Simon S. Chen",
                "Ulrich Neumann",
                "Jae Shin Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "Runway",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03011.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Универсальное управление освещением людей на изображениях и видео",
                    "desc": "Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображениях и видео с произвольными частями тела в любых сценах. Авторы используют предобученную диффузионную модель в качестве общего априорного распределения изображений и совместно моделируют перелозировку человека и гармонизацию фона. Для улучшения временной согласованности освещения вводится модель временного освещения, обучаемая без учителя на реальных видео. Эксперименты показывают, что метод превосходит существующие подходы к перелозировке и гармонизации изображений людей."
                },
                "en": {
                    "title": "Revolutionizing Lighting Control in Images and Videos",
                    "desc": "This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images."
                },
                "zh": {
                    "title": "全面重光照：人类图像光照的全能解决方案",
                    "desc": "本文介绍了全面重光照（Comprehensive Relighting），这是首个能够控制和协调来自任意场景中人类图像或视频的光照的全能方法。构建这样一个通用模型非常具有挑战性，因为缺乏数据集，限制了现有基于图像的重光照模型只能应用于特定场景（例如，面部或静态人类）。为了解决这个问题，我们重新利用了一个预训练的扩散模型作为通用图像先验，并在粗到细的框架中联合建模人类重光照和背景协调。实验结果表明，全面重光照在通用性和光照时间一致性方面表现出色，超越了现有的基于图像的人类重光照和协调方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02402",
            "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
            "url": "https://huggingface.co/papers/2504.02402",
            "abstract": "When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.",
            "score": 3,
            "issue_id": 3099,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "ca80ca19171ef86b",
            "authors": [
                "Hao Yin",
                "Shi Guo",
                "Xu Jia",
                "Xudong XU",
                "Lu Zhang",
                "Si Liu",
                "Dong Wang",
                "Huchuan Lu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "Beihang University",
                "Dalian University of Technology",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02402.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#data",
                    "#cv"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "Новый подход к бесконтактному восстановлению звука с помощью событийных камер",
                    "desc": "Статья представляет новый метод бесконтактного восстановления звука с использованием событийных камер. Авторы разработали конвейер, который полностью использует пространственно-временную информацию из потока событий камеры. Они создали большой набор данных для обучения с помощью нового метода симуляции и спроектировали нейронную сеть, использующую разреженность событий и архитектуру Mamba для моделирования долгосрочной временной информации. Экспериментальные результаты на синтетических и реальных данных демонстрируют эффективность предложенного метода."
                },
                "en": {
                    "title": "Revolutionizing Sound Recovery with Event Cameras",
                    "desc": "This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments."
                },
                "zh": {
                    "title": "利用事件流实现高效声音恢复",
                    "desc": "本研究提出了一种新颖的非接触声音恢复方法，充分利用事件流中的时空信息。我们首先通过新的仿真管道生成了一个大型训练集，然后设计了一个网络，利用事件的稀疏性捕捉空间信息，并使用Mamba模型来处理长期的时间信息。最后，我们训练了一个空间聚合模块，以聚合来自不同位置的信息，从而进一步提高信号质量。实验结果表明，我们的方法在合成和真实数据上都表现出良好的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03536",
            "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
            "url": "https://huggingface.co/papers/2504.03536",
            "abstract": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.",
            "score": 2,
            "issue_id": 3097,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "9a6ad8e0086d88eb",
            "authors": [
                "Boyuan Wang",
                "Runqi Ouyang",
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Guosheng Zhao",
                "Chaojun Ni",
                "Guan Huang",
                "Lihong Liu",
                "Xingang Wang"
            ],
            "affiliations": [
                "GigaAI",
                "Institute of Automation, Chinese Academy of Sciences",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03536.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#3d"
                ],
                "emoji": "🧑‍🦰",
                "ru": {
                    "title": "Реалистичные 3D-модели людей из одного фото",
                    "desc": "HumanDreamer-X - это новая система для реконструкции 3D-моделей человека по одному изображению. Она объединяет генерацию мультиракурсных изображений и 3D-реконструкцию в единый процесс, что значительно улучшает геометрическую согласованность и визуальное качество моделей. Система использует 3D Gaussian Splatting для начальной геометрии и HumanFixer для улучшения рендеров. Предложенная стратегия модуляции внимания повышает детализацию и согласованность между ракурсами."
                },
                "en": {
                    "title": "Revolutionizing Human Reconstruction with HumanDreamer-X",
                    "desc": "This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics."
                },
                "zh": {
                    "title": "统一多视图生成与重建，提升人类模型质量",
                    "desc": "单图像人类重建对数字人类建模应用至关重要，但仍然是一个极具挑战性的任务。目前的方法依赖生成模型合成多视图图像以进行后续的3D重建和动画。然而，从单个人体图像直接生成多个视图会导致几何不一致，重建模型中出现肢体碎片或模糊的问题。为了解决这些限制，我们提出了HumanDreamer-X，一个将多视图人类生成和重建整合为统一流程的新框架，显著提高了重建3D模型的几何一致性和视觉真实感。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24067",
            "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
            "url": "https://huggingface.co/papers/2503.24067",
            "abstract": "Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.",
            "score": 2,
            "issue_id": 3100,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "c397bc55eaf9dd26",
            "authors": [
                "Yixing Li",
                "Ruobing Xie",
                "Zhen Yang",
                "Xingwu Sun",
                "Shuaipeng Li",
                "Weidong Han",
                "Zhanhui Kang",
                "Yu Cheng",
                "Chengzhong Xu",
                "Di Wang",
                "Jie Jiang"
            ],
            "affiliations": [
                "Tencent Hunyuan",
                "The Chinese University of Hong Kong",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24067.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Объединение Transformer и Mamba для эффективной обработки длинных последовательностей",
                    "desc": "TransMamba - это новая архитектура, объединяющая Transformer и Mamba через общие матрицы параметров. Она позволяет динамически переключаться между механизмами внимания и моделями пространства состояний (SSM) на разных длинах токенов и слоях. Модель использует конвертер памяти для преобразования выходов внимания в состояния, совместимые с SSM. Эксперименты показали, что TransMamba превосходит базовые модели по эффективности обучения и производительности."
                },
                "en": {
                    "title": "TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing",
                    "desc": "This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks."
                },
                "zh": {
                    "title": "TransMamba：高效的序列建模新方案",
                    "desc": "本文提出了一种新的框架TransMamba，旨在结合Transformer和Mamba模型，以提高长序列处理的效率。通过共享参数矩阵，TransMamba能够在不同的token长度和层次之间动态切换注意力机制和状态空间模型（SSM）。我们设计了记忆转换器，将注意力输出转换为SSM兼容的状态，确保信息在转换点的无缝流动。此外，本文还深入探讨了TransPoint调度，以进一步提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24310",
            "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2503.24310",
            "abstract": "In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.",
            "score": 1,
            "issue_id": 3097,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "890bba46601fef07",
            "authors": [
                "Alok Abhishek",
                "Lisa Erickson",
                "Tushar Bandopadhyay"
            ],
            "affiliations": [
                "Boston, USA",
                "San Francisco, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24310.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "BEATS: комплексная оценка этичности языковых моделей",
                    "desc": "Исследователи представили BEATS - новую систему оценки предвзятости, этики, справедливости и фактической точности в больших языковых моделях (LLM). На основе BEATS разработан эталонный тест, измеряющий производительность LLM по 29 различным метрикам, включая демографические, когнитивные и социальные предубеждения. Результаты показали, что 37,65% выходных данных ведущих моделей содержали некоторую форму предвзятости. BEATS предлагает масштабируемую методологию для оценки LLM, диагностики факторов, вызывающих предвзятость, и разработки стратегий по ее снижению."
                },
                "en": {
                    "title": "BEATS: A Framework for Fair and Ethical AI Evaluation",
                    "desc": "This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics."
                },
                "zh": {
                    "title": "BEATS框架：推动负责任的人工智能评估",
                    "desc": "本研究介绍了BEATS框架，用于评估大型语言模型（LLMs）中的偏见、伦理、公平性和事实性。我们建立了一个偏见基准，涵盖29个不同的指标，评估LLMs在多样性、认知和社会偏见等方面的表现。通过这些指标，可以定量评估LLM生成的响应在多大程度上可能延续社会偏见，强化或扩大系统性不平等。我们的目标是通过BEATS框架，促进更具社会责任感和伦理对齐的人工智能模型的发展。"
                }
            }
        }
    ],
    "link_prev": "2025-04-04.html",
    "link_next": "2025-04-08.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4月4日"
    },
    "short_date_next": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 3,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "pinyin": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de duō yǔ yán wèn tí jiě jué jī zhǔn, chēng wéi Multi-SWE-bench. tā hán gǎi le Java, TypeScript, JavaScript, Go, Rust, C hé C++ děng yǔ yán. gǎi jī zhǔn bāo hán 1,632 gè gāo zhì liàng shí lì, yǒu 68 wèi zhuān jiā zhù shì. jī yú zhè gè jī zhǔn, zuò zhě píng gū le yī xì liè xiān jìn de mó xíng, bìng tí gōng le xiáng xì de fēn xī. cǐ wài, wén zhāng hái xuān bù chéng lì le yī gè kāi yuán shè qū Multi-SWE-RL, zhǐ yú wèi wèn tí jiě jué rèn wù gòu jiàn dà guī mó qiáng huà xué xùn liàn shù jù jí.",
        "vocab": "[\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"注释\", \"pinyin\": \"zhùshì\", \"trans\": \"annotate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"宣布\", \"pinyin\": \"xuānbù\", \"trans\": \"announce\"},\n    {\"word\": \"成立\", \"pinyin\": \"chénglì\", \"trans\": \"establish\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shèqū\", \"trans\": \"community\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"构建\", \"pinyin\": \"gòujiàn\", \"trans\": \"build\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"training\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces a new multilingual problem-solving benchmark called Multi-SWE-bench. It covers languages such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. The benchmark contains 1,632 high-quality instances annotated by 68 experts. Based on this benchmark, the authors evaluated a series of advanced models and provided detailed analyses. Additionally, the article announces the establishment of an open-source community, Multi-SWE-RL, aimed at building large-scale reinforcement learning training datasets for problem-solving tasks.",
        "update_ts": "2025-04-07 09:12"
    }
}