{
    "date": {
        "ru": "7 апреля",
        "en": "April 7",
        "zh": "4月7日"
    },
    "time_utc": "2025-04-07 05:12",
    "weekday": 0,
    "issue_id": 3096,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02605",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "url": "https://huggingface.co/papers/2504.02605",
            "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.",
            "score": 15,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "bcf7d7c20685c914",
            "authors": [
                "Daoguang Zan",
                "Zhirong Huang",
                "Wei Liu",
                "Hanwu Chen",
                "Linhao Zhang",
                "Shulin Xin",
                "Lu Chen",
                "Qi Liu",
                "Xiaojian Zhong",
                "Aoyan Li",
                "Siyao Liu",
                "Yongsheng Xiao",
                "Liangqiang Chen",
                "Yuyu Zhang",
                "Jing Su",
                "Tianyu Liu",
                "Rui Long",
                "Kai Shen",
                "Liang Xiang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02605.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычный бенчмарк для оценки ИИ в решении программных задач",
                    "desc": "Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок."
                },
                "en": {
                    "title": "Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages",
                    "desc": "This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "多语言问题解决基准，推动强化学习研究",
                    "desc": "本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03553",
            "title": "Agentic Knowledgeable Self-awareness",
            "url": "https://huggingface.co/papers/2504.03553",
            "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a \"flood irrigation\" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
            "score": 8,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "4a06cb6959ea30d3",
            "authors": [
                "Shuofei Qiao",
                "Zhisong Qiu",
                "Baochang Ren",
                "Xiaobin Wang",
                "Xiangyuan Ru",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Nanjing University of Aeronautics and Astronautics",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самоосознанность в планировании: эффективное использование знаний языковыми моделями",
                    "desc": "Статья представляет новый подход к обучению языковых моделей для задач планирования, называемый KnowSelf. Он основан на принципе ситуативной самоосознанности, позволяющем модели динамически оценивать ситуацию и стратегически использовать знания. KnowSelf использует двухэтапный процесс обучения и специальные токены для переключения между различными ситуациями. Эксперименты показывают, что этот метод превосходит базовые подходы на различных задачах, минимально используя внешние знания."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Aware Decision Making",
                    "desc": "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."
                },
                "zh": {
                    "title": "自主调节知识使用的智能代理",
                    "desc": "大型语言模型（LLMs）在多种代理规划任务中表现出色。然而，传统的代理规划方法采用了\"洪水灌溉\"的方式，随意注入黄金轨迹、外部反馈和领域知识，这种做法忽视了人类在决策过程中动态评估情境需求的能力。我们提出了代理知识自我意识的概念，旨在填补这一空白，使基于LLM的代理能够自主调节知识的使用。具体而言，我们设计了一种启发式情境判断标准，通过标记代理自我探索轨迹上的特殊标记，收集训练数据，从而实现更高效的规划效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03561",
            "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
            "url": "https://huggingface.co/papers/2504.03561",
            "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
            "score": 7,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "469f9f28c32e1a1a",
            "authors": [
                "Runnan Fang",
                "Xiaobin Wang",
                "Yuan Liang",
                "Shuofei Qiao",
                "Jialong Wu",
                "Zekun Xi",
                "Ningyu Zhang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03561.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "SynWorld: Автономное исследование и обучение агентов в новых средах",
                    "desc": "SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые среды и оптимизировать рабочие процессы. Он использует синтез возможных сценариев с многошаговым вызовом действий и применяет метод Монте-Карло для поиска по дереву (MCTS) для эффективного уточнения знаний о действиях в текущей среде. Эксперименты показывают, что SynWorld является эффективным и универсальным подходом к изучению знаний о действиях в новых средах. Этот метод решает проблему ограниченных возможностей LLM-агентов при работе в незнакомых окружениях или нестандартных пространствах действий."
                },
                "en": {
                    "title": "Empowering Agents to Explore with SynWorld",
                    "desc": "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."
                },
                "zh": {
                    "title": "SynWorld：赋能代理探索新环境的框架",
                    "desc": "在代理与环境的互动中，代理通过规划和执行动作来扩展其能力。然而，基于大型语言模型的代理在新环境中或需要在非常规动作空间中导航时面临重大挑战。为了解决这个问题，我们提出了SynWorld框架，使代理能够合成可能的场景，并在动作空间内进行多步动作调用，同时执行蒙特卡洛树搜索（MCTS）探索，以有效地优化其在当前环境中的动作知识。实验结果表明，SynWorld是学习新环境中动作知识的有效且通用的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03641",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "url": "https://huggingface.co/papers/2504.03641",
            "abstract": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.",
            "score": 3,
            "issue_id": 3095,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "45da77ffd9c21caf",
            "authors": [
                "Wulin Xie",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Yang Shi",
                "Bingyan Nie",
                "Hongkai Chen",
                "Zhang Zhang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "CASIA",
                "M-M-E Project",
                "NJU",
                "PKU",
                "Vivo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый стандарт оценки мультимодальных языковых моделей",
                    "desc": "Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью."
                },
                "en": {
                    "title": "Enhancing Evaluation for Unified Multimodal Language Models",
                    "desc": "This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs."
                },
                "zh": {
                    "title": "全面评估统一多模态大语言模型的必要性",
                    "desc": "现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02949",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.02949",
            "abstract": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "score": 3,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "71423989b2bed2d8",
            "authors": [
                "Xianwei Zhuang",
                "Yuxin Xie",
                "Yufan Deng",
                "Dongchao Yang",
                "Liming Liang",
                "Jinghan Ru",
                "Yuguo Yin",
                "Yuexian Zou"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#open_source",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, генерации и редактирования изображений",
                    "desc": "VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям."
                },
                "en": {
                    "title": "Unifying Visual Understanding and Generation with VARGPT-v1.1",
                    "desc": "VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation."
                },
                "zh": {
                    "title": "统一视觉自回归模型的突破性进展",
                    "desc": "本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03011",
            "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
            "url": "https://huggingface.co/papers/2504.03011",
            "abstract": "This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.",
            "score": 0,
            "issue_id": 3096,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "94d3411c37993837",
            "authors": [
                "Junying Wang",
                "Jingyuan Liu",
                "Xin Sun",
                "Krishna Kumar Singh",
                "Zhixin Shu",
                "He Zhang",
                "Jimei Yang",
                "Nanxuan Zhao",
                "Tuanfeng Y. Wang",
                "Simon S. Chen",
                "Ulrich Neumann",
                "Jae Shin Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "Runway",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03011.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Универсальное управление освещением людей на изображениях и видео",
                    "desc": "Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображениях и видео с произвольными частями тела в любых сценах. Авторы используют предобученную диффузионную модель в качестве общего априорного распределения изображений и совместно моделируют перелозировку человека и гармонизацию фона. Для улучшения временной согласованности освещения вводится модель временного освещения, обучаемая без учителя на реальных видео. Эксперименты показывают, что метод превосходит существующие подходы к перелозировке и гармонизации изображений людей."
                },
                "en": {
                    "title": "Revolutionizing Lighting Control in Images and Videos",
                    "desc": "This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images."
                },
                "zh": {
                    "title": "全面重光照：人类图像光照的全能解决方案",
                    "desc": "本文介绍了全面重光照（Comprehensive Relighting），这是首个能够控制和协调来自任意场景中人类图像或视频的光照的全能方法。构建这样一个通用模型非常具有挑战性，因为缺乏数据集，限制了现有基于图像的重光照模型只能应用于特定场景（例如，面部或静态人类）。为了解决这个问题，我们重新利用了一个预训练的扩散模型作为通用图像先验，并在粗到细的框架中联合建模人类重光照和背景协调。实验结果表明，全面重光照在通用性和光照时间一致性方面表现出色，超越了现有的基于图像的人类重光照和协调方法。"
                }
            }
        }
    ],
    "link_prev": "2025-04-04.html",
    "link_next": "2025-04-08.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4月4日"
    },
    "short_date_next": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "大型语言模型（LLMs）的出现推动了人工智能的变革，产生了具有复杂推理、强大感知和多样行动能力的智能代理。这些代理在AI研究和实际应用中的设计、评估和改进面临复杂的挑战。本文综合概述了智能代理的模块化、脑启发式架构，结合认知科学、神经科学和计算研究的原则。文章分为四个部分，探讨智能代理的模块基础、自我增强机制、多代理系统和安全可靠的AI系统。",
        "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
        "pinyin": "Dàxíng yǔyán móxíng (LLMs) de chūxiàn tuīdòngle réngōng zhìnéng de biànge, chǎnshēngle jùyǒu fùzá xīnglǐ, qiángdà gǎnjué hé duōyàng xíngdòng nénglì de zhìnéng dàilǐ. Zhèxiē dàilǐ zài AI yánjiū hé shíjì yìngyòng zhōng de shèjì, pínggū hé gǎijìn miànlín fùzá de tiǎozhàn. Běnwén zònghé gàishùle zhìnéng dàilǐ de mókuàihuà, nǎo qǐfāshì jiàgòu, jiéhé rénzhī kēxué, shénjīng kēxué hé jìsuàn yánjiū de yuánzé. Wénzhāng fēnwéi sì gè bùfēn, tuàntào zhìnéng dàilǐ de mókuài jīchǔ, zìwǒ zēngqiáng jīzhì, duō dàilǐ xìtǒng hé ānquán kěkào de AI xìtǒng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔyán móxíng\", \"trans\": \"language model\"},\n    {\"word\": \"变革\", \"pinyin\": \"biàngé\", \"trans\": \"transformation\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎnzhī\", \"trans\": \"perception\"},\n    {\"word\": \"行动\", \"pinyin\": \"xíngdòng\", \"trans\": \"action\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"代理\", \"pinyin\": \"dàilǐ\", \"trans\": \"agent\"},\n    {\"word\": \"面临\", \"pinyin\": \"miànlín\", \"trans\": \"face\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎozhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"综合\", \"pinyin\": \"zònghé\", \"trans\": \"comprehensive\"},\n    {\"word\": \"概述\", \"pinyin\": \"gàishù\", \"trans\": \"overview\"},\n    {\"word\": \"模块化\", \"pinyin\": \"mókuàihuà\", \"trans\": \"modularization\"},\n    {\"word\": \"脑启发式\", \"pinyin\": \"nǎo qǐfāshì\", \"trans\": \"brain-inspired\"},\n    {\"word\": \"架构\", \"pinyin\": \"jiàgòu\", \"trans\": \"architecture\"},\n    {\"word\": \"认知科学\", \"pinyin\": \"rènzhī kēxué\", \"trans\": \"cognitive science\"},\n    {\"word\": \"神经科学\", \"pinyin\": \"shénjīng kēxué\", \"trans\": \"neuroscience\"},\n    {\"word\": \"计算\", \"pinyin\": \"jìsuàn\", \"trans\": \"computation\"},\n    {\"word\": \"原则\", \"pinyin\": \"yuánzé\", \"trans\": \"principle\"},\n    {\"word\": \"部分\", \"pinyin\": \"bùfen\", \"trans\": \"part\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàntǎo\", \"trans\": \"discuss\"},\n    {\"word\": \"基础\", \"pinyin\": \"jīchǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"自我增强\", \"pinyin\": \"zìwǒ zēngqiáng\", \"trans\": \"self-enhancement\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"多代理\", \"pinyin\": \"duō dàilǐ\", \"trans\": \"multi-agent\"},\n    {\"word\": \"系统\", \"pinyin\": \"xìtǒng\", \"trans\": \"system\"},\n    {\"word\": \"安全\", \"pinyin\": \"ānquán\", \"trans\": \"safe\"},\n    {\"word\": \"可靠\", \"pinyin\": \"kěkào\", \"trans\": \"reliable\"}\n]",
        "trans": "The emergence of large language models (LLMs) has driven a transformation in artificial intelligence, giving rise to intelligent agents with complex reasoning, powerful perception, and diverse action capabilities. The design, evaluation, and improvement of these agents in AI research and practical applications face complex challenges. This article provides a comprehensive overview of the modular, brain-inspired architecture of intelligent agents, integrating principles from cognitive science, neuroscience, and computational research. The article is divided into four sections, discussing the modular foundations of intelligent agents, self-enhancement mechanisms, multi-agent systems, and secure and reliable AI systems.",
        "update_ts": "2025-04-06 12:41"
    }
}