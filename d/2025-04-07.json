{
    "date": {
        "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 7",
        "zh": "4æœˆ7æ—¥"
    },
    "time_utc": "2025-04-07 04:13",
    "weekday": 0,
    "issue_id": 3095,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02605",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "url": "https://huggingface.co/papers/2504.02605",
            "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.",
            "score": 6,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "bcf7d7c20685c914",
            "authors": [
                "Daoguang Zan",
                "Zhirong Huang",
                "Wei Liu",
                "Hanwu Chen",
                "Linhao Zhang",
                "Shulin Xin",
                "Lu Chen",
                "Qi Liu",
                "Xiaojian Zhong",
                "Aoyan Li",
                "Siyao Liu",
                "Yongsheng Xiao",
                "Liangqiang Chen",
                "Yuyu Zhang",
                "Jing Su",
                "Tianyu Liu",
                "Rui Long",
                "Kai Shen",
                "Liang Xiang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02605.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multi-SWE-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1632 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²: Agentless, SWE-agent Ğ¸ OpenHands. Ğ—Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Multi-SWE-RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages",
                    "desc": "This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œç§°ä¸ºMulti-SWE-benchï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–äº†Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ç­‰ä¸ƒç§ç¼–ç¨‹è¯­è¨€ï¼Œå…±åŒ…å«1,632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œæ—¨åœ¨ä¸ºé—®é¢˜è§£å†³ä»»åŠ¡æ„å»ºå¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å‘å¸ƒäº†4,723ä¸ªç»“æ„è‰¯å¥½çš„å®ä¾‹ã€‚é€šè¿‡å¼€æ”¾æ•°æ®ç”Ÿäº§æµç¨‹å’Œè¯¦ç»†æ•™ç¨‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¿€åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03641",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "url": "https://huggingface.co/papers/2504.03641",
            "abstract": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.",
            "score": 1,
            "issue_id": 3095,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 4",
                "zh": "4æœˆ4æ—¥"
            },
            "hash": "45da77ffd9c21caf",
            "authors": [
                "Wulin Xie",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Yang Shi",
                "Bingyan Nie",
                "Hongkai Chen",
                "Zhang Zhang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "CASIA",
                "M-M-E Project",
                "NJU",
                "PKU",
                "Vivo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (U-MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 12 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… U-MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… U-MLLM, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing Evaluation for Unified Multimodal Language Models",
                    "desc": "This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§",
                    "desc": "ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆU-MLLMï¼‰åŸºå‡†åœ¨è¯„ä¼°æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡åŸºå‡†å’Œæ··åˆæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°U-MLLMã€‚è¯¥åŸºå‡†åŒ…æ‹¬æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°å’Œäº”ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œå¸¸è¯†é—®ç­”ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ç°æœ‰U-MLLMåœ¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å¼ºå¤§æ¨¡å‹çš„å¿…è¦æ€§ï¼Œä»¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02949",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.02949",
            "abstract": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "score": 1,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "71423989b2bed2d8",
            "authors": [
                "Xianwei Zhuang",
                "Yuxin Xie",
                "Yufan Deng",
                "Dongchao Yang",
                "Liming Liang",
                "Jinghan Ru",
                "Yuguo Yin",
                "Yuexian Zou"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#open_source",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "VARGPT-v1.1 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ VARGPT. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Direct Preference Optimization (DPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 8,3 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2 Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. VARGPT-v1.1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unifying Visual Understanding and Generation with VARGPT-v1.1",
                    "desc": "VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹çš„çªç ´æ€§è¿›å±•",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†VARGPT-v1.1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæˆ‘ä»¬ä¹‹å‰çš„VARGPTæ¡†æ¶ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ç†è§£çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå›¾åƒåˆæˆçš„ä¸‹ä¸€ä¸ªå°ºåº¦ç”Ÿæˆçš„åŒé‡èŒƒå¼ã€‚VARGPT-v1.1é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†è¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒä¼˜å’Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼ŒVARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-04.html",
    "link_next": "2025-04-08.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4æœˆ4æ—¥"
    },
    "short_date_next": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å˜é©ï¼Œäº§ç”Ÿäº†å…·æœ‰å¤æ‚æ¨ç†ã€å¼ºå¤§æ„ŸçŸ¥å’Œå¤šæ ·è¡ŒåŠ¨èƒ½åŠ›çš„æ™ºèƒ½ä»£ç†ã€‚è¿™äº›ä»£ç†åœ¨AIç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­çš„è®¾è®¡ã€è¯„ä¼°å’Œæ”¹è¿›é¢ä¸´å¤æ‚çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ç»¼åˆæ¦‚è¿°äº†æ™ºèƒ½ä»£ç†çš„æ¨¡å—åŒ–ã€è„‘å¯å‘å¼æ¶æ„ï¼Œç»“åˆè®¤çŸ¥ç§‘å­¦ã€ç¥ç»ç§‘å­¦å’Œè®¡ç®—ç ”ç©¶çš„åŸåˆ™ã€‚æ–‡ç« åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œæ¢è®¨æ™ºèƒ½ä»£ç†çš„æ¨¡å—åŸºç¡€ã€è‡ªæˆ‘å¢å¼ºæœºåˆ¶ã€å¤šä»£ç†ç³»ç»Ÿå’Œå®‰å…¨å¯é çš„AIç³»ç»Ÿã€‚",
        "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
        "pinyin": "DÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) de chÅ«xiÃ n tuÄ«dÃ²ngle rÃ©ngÅng zhÃ¬nÃ©ng de biÃ nge, chÇnshÄ“ngle jÃ¹yÇ’u fÃ¹zÃ¡ xÄ«nglÇ, qiÃ¡ngdÃ  gÇnjuÃ© hÃ© duÅyÃ ng xÃ­ngdÃ²ng nÃ©nglÃ¬ de zhÃ¬nÃ©ng dÃ ilÇ. ZhÃ¨xiÄ“ dÃ ilÇ zÃ i AI yÃ¡njiÅ« hÃ© shÃ­jÃ¬ yÃ¬ngyÃ²ng zhÅng de shÃ¨jÃ¬, pÃ­nggÅ« hÃ© gÇijÃ¬n miÃ nlÃ­n fÃ¹zÃ¡ de tiÇozhÃ n. BÄ›nwÃ©n zÃ²nghÃ© gÃ ishÃ¹le zhÃ¬nÃ©ng dÃ ilÇ de mÃ³kuÃ ihuÃ , nÇo qÇfÄshÃ¬ jiÃ gÃ²u, jiÃ©hÃ© rÃ©nzhÄ« kÄ“xuÃ©, shÃ©njÄ«ng kÄ“xuÃ© hÃ© jÃ¬suÃ n yÃ¡njiÅ« de yuÃ¡nzÃ©. WÃ©nzhÄng fÄ“nwÃ©i sÃ¬ gÃ¨ bÃ¹fÄ“n, tuÃ ntÃ o zhÃ¬nÃ©ng dÃ ilÇ de mÃ³kuÃ i jÄ«chÇ”, zÃ¬wÇ’ zÄ“ngqiÃ¡ng jÄ«zhÃ¬, duÅ dÃ ilÇ xÃ¬tÇ’ng hÃ© ÄnquÃ¡n kÄ›kÃ o de AI xÃ¬tÇ’ng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"å˜é©\", \"pinyin\": \"biÃ ngÃ©\", \"trans\": \"transformation\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇnzhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"è¡ŒåŠ¨\", \"pinyin\": \"xÃ­ngdÃ²ng\", \"trans\": \"action\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ ilÇ\", \"trans\": \"agent\"},\n    {\"word\": \"é¢ä¸´\", \"pinyin\": \"miÃ nlÃ­n\", \"trans\": \"face\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇozhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"ç»¼åˆ\", \"pinyin\": \"zÃ²nghÃ©\", \"trans\": \"comprehensive\"},\n    {\"word\": \"æ¦‚è¿°\", \"pinyin\": \"gÃ ishÃ¹\", \"trans\": \"overview\"},\n    {\"word\": \"æ¨¡å—åŒ–\", \"pinyin\": \"mÃ³kuÃ ihuÃ \", \"trans\": \"modularization\"},\n    {\"word\": \"è„‘å¯å‘å¼\", \"pinyin\": \"nÇo qÇfÄshÃ¬\", \"trans\": \"brain-inspired\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"è®¤çŸ¥ç§‘å­¦\", \"pinyin\": \"rÃ¨nzhÄ« kÄ“xuÃ©\", \"trans\": \"cognitive science\"},\n    {\"word\": \"ç¥ç»ç§‘å­¦\", \"pinyin\": \"shÃ©njÄ«ng kÄ“xuÃ©\", \"trans\": \"neuroscience\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬suÃ n\", \"trans\": \"computation\"},\n    {\"word\": \"åŸåˆ™\", \"pinyin\": \"yuÃ¡nzÃ©\", \"trans\": \"principle\"},\n    {\"word\": \"éƒ¨åˆ†\", \"pinyin\": \"bÃ¹fen\", \"trans\": \"part\"},\n    {\"word\": \"æ¢è®¨\", \"pinyin\": \"tÃ ntÇo\", \"trans\": \"discuss\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ«chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"è‡ªæˆ‘å¢å¼º\", \"pinyin\": \"zÃ¬wÇ’ zÄ“ngqiÃ¡ng\", \"trans\": \"self-enhancement\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"å¤šä»£ç†\", \"pinyin\": \"duÅ dÃ ilÇ\", \"trans\": \"multi-agent\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"å®‰å…¨\", \"pinyin\": \"ÄnquÃ¡n\", \"trans\": \"safe\"},\n    {\"word\": \"å¯é \", \"pinyin\": \"kÄ›kÃ o\", \"trans\": \"reliable\"}\n]",
        "trans": "The emergence of large language models (LLMs) has driven a transformation in artificial intelligence, giving rise to intelligent agents with complex reasoning, powerful perception, and diverse action capabilities. The design, evaluation, and improvement of these agents in AI research and practical applications face complex challenges. This article provides a comprehensive overview of the modular, brain-inspired architecture of intelligent agents, integrating principles from cognitive science, neuroscience, and computational research. The article is divided into four sections, discussing the modular foundations of intelligent agents, self-enhancement mechanisms, multi-agent systems, and secure and reliable AI systems.",
        "update_ts": "2025-04-06 12:41"
    }
}