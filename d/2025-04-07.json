{
    "date": {
        "ru": "7 апреля",
        "en": "April 7",
        "zh": "4月7日"
    },
    "time_utc": "2025-04-07 18:14",
    "weekday": 0,
    "issue_id": 3109,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02605",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "url": "https://huggingface.co/papers/2504.02605",
            "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.",
            "score": 29,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "bcf7d7c20685c914",
            "authors": [
                "Daoguang Zan",
                "Zhirong Huang",
                "Wei Liu",
                "Hanwu Chen",
                "Linhao Zhang",
                "Shulin Xin",
                "Lu Chen",
                "Qi Liu",
                "Xiaojian Zhong",
                "Aoyan Li",
                "Siyao Liu",
                "Yongsheng Xiao",
                "Liangqiang Chen",
                "Yuyu Zhang",
                "Jing Su",
                "Tianyu Liu",
                "Rui Long",
                "Kai Shen",
                "Liang Xiang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02605.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычный бенчмарк для оценки ИИ в решении программных задач",
                    "desc": "Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок."
                },
                "en": {
                    "title": "Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages",
                    "desc": "This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "多语言问题解决基准，推动强化学习研究",
                    "desc": "本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03553",
            "title": "Agentic Knowledgeable Self-awareness",
            "url": "https://huggingface.co/papers/2504.03553",
            "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a \"flood irrigation\" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
            "score": 19,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "4a06cb6959ea30d3",
            "authors": [
                "Shuofei Qiao",
                "Zhisong Qiu",
                "Baochang Ren",
                "Xiaobin Wang",
                "Xiangyuan Ru",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Nanjing University of Aeronautics and Astronautics",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самоосознанность в планировании: эффективное использование знаний языковыми моделями",
                    "desc": "Статья представляет новый подход к обучению языковых моделей для задач планирования, называемый KnowSelf. Он основан на принципе ситуативной самоосознанности, позволяющем модели динамически оценивать ситуацию и стратегически использовать знания. KnowSelf использует двухэтапный процесс обучения и специальные токены для переключения между различными ситуациями. Эксперименты показывают, что этот метод превосходит базовые подходы на различных задачах, минимально используя внешние знания."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Aware Decision Making",
                    "desc": "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."
                },
                "zh": {
                    "title": "自主调节知识使用的智能代理",
                    "desc": "大型语言模型（LLMs）在多种代理规划任务中表现出色。然而，传统的代理规划方法采用了\"洪水灌溉\"的方式，随意注入黄金轨迹、外部反馈和领域知识，这种做法忽视了人类在决策过程中动态评估情境需求的能力。我们提出了代理知识自我意识的概念，旨在填补这一空白，使基于LLM的代理能够自主调节知识的使用。具体而言，我们设计了一种启发式情境判断标准，通过标记代理自我探索轨迹上的特殊标记，收集训练数据，从而实现更高效的规划效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02807",
            "title": "MegaMath: Pushing the Limits of Open Math Corpora",
            "url": "https://huggingface.co/papers/2504.02807",
            "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.",
            "score": 19,
            "issue_id": 3102,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "a6dd17864afc6dca",
            "authors": [
                "Fan Zhou",
                "Zengzhi Wang",
                "Nikhil Ranjan",
                "Zhoujun Cheng",
                "Liping Tang",
                "Guowei He",
                "Zhengzhong Liu",
                "Eric P. Xing"
            ],
            "affiliations": [
                "MBZUAI",
                "MegaMath"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02807.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "MegaMath: Большие данные для умных вычислений",
                    "desc": "Статья представляет MegaMath - крупномасштабный открытый датасет для предобучения языковых моделей в области математики. Авторы использовали оптимизированные методы извлечения математических документов из веб-данных, а также включили математический код и синтетические данные. Датасет содержит 371 миллиард токенов и отличается высоким качеством и разнообразием. MegaMath призван улучшить математические рассуждения в больших языковых моделях."
                },
                "en": {
                    "title": "MegaMath: Elevating LLMs with a Massive Math Dataset",
                    "desc": "This paper introduces MegaMath, a comprehensive dataset designed to enhance the mathematical reasoning capabilities of large language models (LLMs). The dataset is created by extracting and optimizing mathematical documents from the web, ensuring high quality through filtering and deduplication. Additionally, it incorporates high-quality math-related code from existing code corpora, further enriching the dataset's diversity. By synthesizing various forms of data, MegaMath provides a substantial resource of 371 billion tokens, making it the largest and highest quality open dataset for math pre-training available."
                },
                "zh": {
                    "title": "MegaMath：数学推理的开放数据集",
                    "desc": "数学推理是人类智能的基石，也是大型语言模型（LLM）高级能力的重要基准。然而，目前研究界缺乏一个开放的大规模高质量数学数据集，以满足数学中心的LLM预训练需求。我们提出了MegaMath，这是一个从多种数学相关来源精心策划的开放数据集，包含3710亿个标记，具有现有开放数学预训练数据集中最大的数量和最佳质量。该数据集通过重新提取网络数据、回收数学相关代码数据和探索合成数据等策略，确保了数据的多样性和高质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03561",
            "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
            "url": "https://huggingface.co/papers/2504.03561",
            "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
            "score": 14,
            "issue_id": 3096,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "469f9f28c32e1a1a",
            "authors": [
                "Runnan Fang",
                "Xiaobin Wang",
                "Yuan Liang",
                "Shuofei Qiao",
                "Jialong Wu",
                "Zekun Xi",
                "Ningyu Zhang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03561.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "SynWorld: Автономное исследование и обучение агентов в новых средах",
                    "desc": "SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые среды и оптимизировать рабочие процессы. Он использует синтез возможных сценариев с многошаговым вызовом действий и применяет метод Монте-Карло для поиска по дереву (MCTS) для эффективного уточнения знаний о действиях в текущей среде. Эксперименты показывают, что SynWorld является эффективным и универсальным подходом к изучению знаний о действиях в новых средах. Этот метод решает проблему ограниченных возможностей LLM-агентов при работе в незнакомых окружениях или нестандартных пространствах действий."
                },
                "en": {
                    "title": "Empowering Agents to Explore with SynWorld",
                    "desc": "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."
                },
                "zh": {
                    "title": "SynWorld：赋能代理探索新环境的框架",
                    "desc": "在代理与环境的互动中，代理通过规划和执行动作来扩展其能力。然而，基于大型语言模型的代理在新环境中或需要在非常规动作空间中导航时面临重大挑战。为了解决这个问题，我们提出了SynWorld框架，使代理能够合成可能的场景，并在动作空间内进行多步动作调用，同时执行蒙特卡洛树搜索（MCTS）探索，以有效地优化其在当前环境中的动作知识。实验结果表明，SynWorld是学习新环境中动作知识的有效且通用的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03641",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "url": "https://huggingface.co/papers/2504.03641",
            "abstract": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.",
            "score": 10,
            "issue_id": 3095,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "45da77ffd9c21caf",
            "authors": [
                "Wulin Xie",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Yang Shi",
                "Bingyan Nie",
                "Hongkai Chen",
                "Zhang Zhang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "CASIA",
                "M-M-E Project",
                "NJU",
                "PKU",
                "Vivo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый стандарт оценки мультимодальных языковых моделей",
                    "desc": "Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью."
                },
                "en": {
                    "title": "Enhancing Evaluation for Unified Multimodal Language Models",
                    "desc": "This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs."
                },
                "zh": {
                    "title": "全面评估统一多模态大语言模型的必要性",
                    "desc": "现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02949",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.02949",
            "abstract": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "score": 9,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "71423989b2bed2d8",
            "authors": [
                "Xianwei Zhuang",
                "Yuxin Xie",
                "Yufan Deng",
                "Dongchao Yang",
                "Liming Liang",
                "Jinghan Ru",
                "Yuguo Yin",
                "Yuexian Zou"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#open_source",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, генерации и редактирования изображений",
                    "desc": "VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям."
                },
                "en": {
                    "title": "Unifying Visual Understanding and Generation with VARGPT-v1.1",
                    "desc": "VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation."
                },
                "zh": {
                    "title": "统一视觉自回归模型的突破性进展",
                    "desc": "本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03601",
            "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
            "url": "https://huggingface.co/papers/2504.03601",
            "abstract": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io",
            "score": 8,
            "issue_id": 3097,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "05921cbfa42a13b4",
            "authors": [
                "Akshara Prabhakar",
                "Zuxin Liu",
                "Weiran Yao",
                "Jianguo Zhang",
                "Ming Zhu",
                "Shiyu Wang",
                "Zhiwei Liu",
                "Tulika Awalgaonkar",
                "Haolin Chen",
                "Thai Hoang",
                "Juan Carlos Niebles",
                "Shelby Heinecke",
                "Huan Wang",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03601.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#agents",
                    "#open_source",
                    "#data",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в обучении ИИ-агентов: синтетические данные для реалистичного многоходового взаимодействия",
                    "desc": "APIGen-MT - это новый фреймворк для генерации качественных данных для обучения ИИ-агентов многоходовому взаимодействию. Он использует двухфазный подход: сначала создаются детальные планы задач с помощью ревьюеров на основе больших языковых моделей, затем эти планы преобразуются в полные траектории взаимодействия. На основе полученных данных обучено семейство моделей xLAM-2-fc-r, превосходящих по ряду показателей такие модели как GPT-4 и Claude 3.5. Исследователи открыли доступ к синтетическим данным и обученным моделям для дальнейшего развития области ИИ-агентов."
                },
                "en": {
                    "title": "Generating High-Quality Data for AI Agents with APIGen-MT",
                    "desc": "The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research."
                },
                "zh": {
                    "title": "高效生成多轮交互数据的AI代理训练框架",
                    "desc": "为了训练有效的AI代理进行多轮交互，我们提出了APIGen-MT框架，该框架能够生成可验证和多样化的多轮代理数据。该框架分为两个阶段，首先通过大型语言模型（LLM）评审委员会和迭代反馈生成详细的任务蓝图，并提供真实的行动。接着，这些蓝图被转化为完整的交互轨迹，通过模拟人机互动实现。我们的xLAM-2-fc-r系列模型在多个基准测试中表现优异，尤其是在多轮设置中，小模型的表现超过了大模型，展示了我们的方法在生成高质量训练数据方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24067",
            "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
            "url": "https://huggingface.co/papers/2503.24067",
            "abstract": "Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.",
            "score": 8,
            "issue_id": 3100,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "c397bc55eaf9dd26",
            "authors": [
                "Yixing Li",
                "Ruobing Xie",
                "Zhen Yang",
                "Xingwu Sun",
                "Shuaipeng Li",
                "Weidong Han",
                "Zhanhui Kang",
                "Yu Cheng",
                "Chengzhong Xu",
                "Di Wang",
                "Jie Jiang"
            ],
            "affiliations": [
                "Tencent Hunyuan",
                "The Chinese University of Hong Kong",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24067.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Объединение Transformer и Mamba для эффективной обработки длинных последовательностей",
                    "desc": "TransMamba - это новая архитектура, объединяющая Transformer и Mamba через общие матрицы параметров. Она позволяет динамически переключаться между механизмами внимания и моделями пространства состояний (SSM) на разных длинах токенов и слоях. Модель использует конвертер памяти для преобразования выходов внимания в состояния, совместимые с SSM. Эксперименты показали, что TransMamba превосходит базовые модели по эффективности обучения и производительности."
                },
                "en": {
                    "title": "TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing",
                    "desc": "This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks."
                },
                "zh": {
                    "title": "TransMamba：高效的序列建模新方案",
                    "desc": "本文提出了一种新的框架TransMamba，旨在结合Transformer和Mamba模型，以提高长序列处理的效率。通过共享参数矩阵，TransMamba能够在不同的token长度和层次之间动态切换注意力机制和状态空间模型（SSM）。我们设计了记忆转换器，将注意力输出转换为SSM兼容的状态，确保信息在转换点的无缝流动。此外，本文还深入探讨了TransPoint调度，以进一步提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03011",
            "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
            "url": "https://huggingface.co/papers/2504.03011",
            "abstract": "This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.",
            "score": 7,
            "issue_id": 3096,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "94d3411c37993837",
            "authors": [
                "Junying Wang",
                "Jingyuan Liu",
                "Xin Sun",
                "Krishna Kumar Singh",
                "Zhixin Shu",
                "He Zhang",
                "Jimei Yang",
                "Nanxuan Zhao",
                "Tuanfeng Y. Wang",
                "Simon S. Chen",
                "Ulrich Neumann",
                "Jae Shin Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "Runway",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03011.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Универсальное управление освещением людей на изображениях и видео",
                    "desc": "Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображениях и видео с произвольными частями тела в любых сценах. Авторы используют предобученную диффузионную модель в качестве общего априорного распределения изображений и совместно моделируют перелозировку человека и гармонизацию фона. Для улучшения временной согласованности освещения вводится модель временного освещения, обучаемая без учителя на реальных видео. Эксперименты показывают, что метод превосходит существующие подходы к перелозировке и гармонизации изображений людей."
                },
                "en": {
                    "title": "Revolutionizing Lighting Control in Images and Videos",
                    "desc": "This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images."
                },
                "zh": {
                    "title": "全面重光照：人类图像光照的全能解决方案",
                    "desc": "本文介绍了全面重光照（Comprehensive Relighting），这是首个能够控制和协调来自任意场景中人类图像或视频的光照的全能方法。构建这样一个通用模型非常具有挑战性，因为缺乏数据集，限制了现有基于图像的重光照模型只能应用于特定场景（例如，面部或静态人类）。为了解决这个问题，我们重新利用了一个预训练的扩散模型作为通用图像先验，并在粗到细的框架中联合建模人类重光照和背景协调。实验结果表明，全面重光照在通用性和光照时间一致性方面表现出色，超越了现有的基于图像的人类重光照和协调方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03536",
            "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
            "url": "https://huggingface.co/papers/2504.03536",
            "abstract": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.",
            "score": 6,
            "issue_id": 3097,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "9a6ad8e0086d88eb",
            "authors": [
                "Boyuan Wang",
                "Runqi Ouyang",
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Guosheng Zhao",
                "Chaojun Ni",
                "Guan Huang",
                "Lihong Liu",
                "Xingang Wang"
            ],
            "affiliations": [
                "GigaAI",
                "Institute of Automation, Chinese Academy of Sciences",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03536.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#3d"
                ],
                "emoji": "🧑‍🦰",
                "ru": {
                    "title": "Реалистичные 3D-модели людей из одного фото",
                    "desc": "HumanDreamer-X - это новая система для реконструкции 3D-моделей человека по одному изображению. Она объединяет генерацию мультиракурсных изображений и 3D-реконструкцию в единый процесс, что значительно улучшает геометрическую согласованность и визуальное качество моделей. Система использует 3D Gaussian Splatting для начальной геометрии и HumanFixer для улучшения рендеров. Предложенная стратегия модуляции внимания повышает детализацию и согласованность между ракурсами."
                },
                "en": {
                    "title": "Revolutionizing Human Reconstruction with HumanDreamer-X",
                    "desc": "This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics."
                },
                "zh": {
                    "title": "统一多视图生成与重建，提升人类模型质量",
                    "desc": "单图像人类重建对数字人类建模应用至关重要，但仍然是一个极具挑战性的任务。目前的方法依赖生成模型合成多视图图像以进行后续的3D重建和动画。然而，从单个人体图像直接生成多个视图会导致几何不一致，重建模型中出现肢体碎片或模糊的问题。为了解决这些限制，我们提出了HumanDreamer-X，一个将多视图人类生成和重建整合为统一流程的新框架，显著提高了重建3D模型的几何一致性和视觉真实感。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02402",
            "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
            "url": "https://huggingface.co/papers/2504.02402",
            "abstract": "When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.",
            "score": 5,
            "issue_id": 3099,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "ca80ca19171ef86b",
            "authors": [
                "Hao Yin",
                "Shi Guo",
                "Xu Jia",
                "Xudong XU",
                "Lu Zhang",
                "Si Liu",
                "Dong Wang",
                "Huchuan Lu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "Beihang University",
                "Dalian University of Technology",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02402.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#data",
                    "#cv"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "Новый подход к бесконтактному восстановлению звука с помощью событийных камер",
                    "desc": "Статья представляет новый метод бесконтактного восстановления звука с использованием событийных камер. Авторы разработали конвейер, который полностью использует пространственно-временную информацию из потока событий камеры. Они создали большой набор данных для обучения с помощью нового метода симуляции и спроектировали нейронную сеть, использующую разреженность событий и архитектуру Mamba для моделирования долгосрочной временной информации. Экспериментальные результаты на синтетических и реальных данных демонстрируют эффективность предложенного метода."
                },
                "en": {
                    "title": "Revolutionizing Sound Recovery with Event Cameras",
                    "desc": "This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments."
                },
                "zh": {
                    "title": "利用事件流实现高效声音恢复",
                    "desc": "本研究提出了一种新颖的非接触声音恢复方法，充分利用事件流中的时空信息。我们首先通过新的仿真管道生成了一个大型训练集，然后设计了一个网络，利用事件的稀疏性捕捉空间信息，并使用Mamba模型来处理长期的时间信息。最后，我们训练了一个空间聚合模块，以聚合来自不同位置的信息，从而进一步提高信号质量。实验结果表明，我们的方法在合成和真实数据上都表现出良好的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03600",
            "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
            "url": "https://huggingface.co/papers/2504.03600",
            "abstract": "Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.",
            "score": 3,
            "issue_id": 3103,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "c1ef5354c6e2cdcb",
            "authors": [
                "Jun Ma",
                "Zongxin Yang",
                "Sumin Kim",
                "Bihui Chen",
                "Mohammed Baharoon",
                "Adibvafa Fallahpour",
                "Reza Asakereh",
                "Hongwei Lyu",
                "Bo Wang"
            ],
            "affiliations": [
                "AI Collaborative Centre, University Health Network",
                "AI Hub, University Health Network",
                "Department of Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA",
                "Department of Computer Science, University of Toronto",
                "Department of Laboratory Medicine and Pathobiology and Department of Computer Science, University of Toronto",
                "Peter Munk Cardiac Centre, University Health Network",
                "University of Toronto, Toronto, Canada",
                "Vector Institute, Toronto, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03600.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#3d",
                    "#dataset",
                    "#training",
                    "#cv"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "MedSAM2: Революция в сегментации медицинских 3D-изображений и видео",
                    "desc": "MedSAM2 - это модель сегментации медицинских 3D-изображений и видео, основанная на fine-tuning Segment Anything Model 2. Модель обучена на большом наборе данных, включающем более 455 000 пар 3D-изображений и масок, а также 76 000 кадров, и превосходит предыдущие модели по широкому спектру органов, поражений и модальностей визуализации. Исследователи реализовали подход human-in-the-loop для создания масштабных датасетов, что позволило сократить затраты на ручную разметку более чем на 85%. MedSAM2 интегрирована в популярные платформы с удобным интерфейсом для локального и облачного развертывания, что делает ее практичным инструментом для эффективной и качественной сегментации в исследованиях и здравоохранении."
                },
                "en": {
                    "title": "MedSAM2: Revolutionizing 3D Medical Segmentation",
                    "desc": "This paper introduces MedSAM2, a new model designed for segmenting 3D medical images and videos, which is essential for precision medicine. It builds on the Segment Anything Model 2 and has been fine-tuned using a vast dataset of over 455,000 3D image-mask pairs and 76,000 video frames. MedSAM2 outperforms existing models in segmenting various organs and lesions, while also significantly reducing manual annotation costs by over 85% through a human-in-the-loop approach. Additionally, it is user-friendly and can be deployed on both local and cloud platforms, making it accessible for research and healthcare applications."
                },
                "zh": {
                    "title": "MedSAM2：高效的3D医学图像分割工具",
                    "desc": "MedSAM2是一种用于3D医学图像和视频分割的可提示分割基础模型。该模型通过在一个包含超过455,000个3D图像-掩膜对和76,000帧的大型医学数据集上微调Segment Anything Model 2而开发。MedSAM2在多个器官、病变和成像模式上超越了之前的模型，并通过人机协作的流程创建了大规模数据集，进行了一项广泛的用户研究。该模型能够将人工成本降低超过85%，并且已集成到广泛使用的平台中，便于本地和云端部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03597",
            "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation",
            "url": "https://huggingface.co/papers/2504.03597",
            "abstract": "Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.",
            "score": 3,
            "issue_id": 3106,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "d1e68cb65f756f3e",
            "authors": [
                "Jad Abou-Chakra",
                "Lingfeng Sun",
                "Krishan Rana",
                "Brandon May",
                "Karl Schmeckpeper",
                "Maria Vittoria Minniti",
                "Laura Herlant"
            ],
            "affiliations": [
                "Queensland University of Technology",
                "Robotics and AI Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03597.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Цифровой двойник для эффективного обучения роботов",
                    "desc": "Эта статья представляет новый подход к обучению роботов сложным манипуляционным задачам, называемый real-is-sim. Метод использует динамический цифровой двойник на основе Embodied Gaussians для сбора данных, обучения и развертывания политик. Real-is-sim позволяет оценивать политики в симуляторе, что значительно ускоряет процесс разработки. Авторы демонстрируют сильную корреляцию между успешностью в симуляторе и реальном мире на задаче PushT."
                },
                "en": {
                    "title": "Bridging the Gap: Real-World Success through Simulated Training",
                    "desc": "This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training."
                },
                "zh": {
                    "title": "动态数字双胞胎提升行为克隆性能",
                    "desc": "最近，行为克隆技术的进步使得机器人能够执行复杂的操作任务。然而，准确评估训练性能仍然具有挑战性，尤其是在实际应用中，因为行为克隆损失与实际任务成功率的相关性较差。因此，研究人员不得不依赖于昂贵且耗时的实际评估来获取成功率指标，这使得识别最佳策略和检测过拟合或欠拟合变得不切实际。为了解决这些问题，我们提出了real-is-sim，一个新颖的行为克隆框架，通过动态数字双胞胎在整个策略开发流程中进行数据收集、训练和部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02534",
            "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery",
            "url": "https://huggingface.co/papers/2504.02534",
            "abstract": "The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything.",
            "score": 3,
            "issue_id": 3107,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "18e13d7f115ab27c",
            "authors": [
                "Mykola Lavreniuk",
                "Nataliia Kussul",
                "Andrii Shelestov",
                "Bohdan Yailymov",
                "Yevhenii Salii",
                "Volodymyr Kuzin",
                "Zoltan Szantoi"
            ],
            "affiliations": [
                "European Space Agency",
                "National Technical University of Ukraine Igor Sikorsky Kyiv Polytechnic Institute",
                "Space Research Institute NASU-SSAU",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02534.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#inference",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Революция в картографировании сельхозугодий: от пикселей к точным границам полей",
                    "desc": "Статья представляет новый подход к точному определению границ сельскохозяйственных полей на спутниковых снимках, переформулируя задачу как сегментацию экземпляров. Авторы создали крупномасштабный набор данных FBIS-22M, содержащий более 22 миллионов масок отдельных полей на снимках разного разрешения. На основе этого набора данных разработана модель Delineate Anything, которая значительно превосходит существующие методы по точности и скорости. Модель демонстрирует сильную обобщающую способность на различных разрешениях изображений и неизвестных географических регионах."
                },
                "en": {
                    "title": "Revolutionizing Agricultural Field Boundary Detection with FBIS-22M",
                    "desc": "This paper focuses on improving the identification of agricultural field boundaries using satellite images, which is crucial for effective land management. The authors introduce a new dataset called Field Boundary Instance Segmentation - 22M (FBIS-22M), which contains a large number of high-resolution satellite image patches and detailed instance masks for individual fields. They reformulate the problem as instance segmentation and develop a model named Delineate Anything, which is trained on this extensive dataset. The model achieves state-of-the-art performance, showing significant improvements in mean Average Precision (mAP) and demonstrating fast inference and strong generalization capabilities across various conditions."
                },
                "zh": {
                    "title": "精准识别农业边界，助力土地管理",
                    "desc": "本文提出了一种新的农业领域边界识别方法，利用卫星图像进行土地管理和作物监测。我们通过实例分割的方式重新定义了这一任务，并引入了FBIS-22M数据集，该数据集包含672,909个高分辨率卫星图像片段和22,926,427个实例掩膜。我们的模型\"Delineate Anything\"在FBIS-22M数据集上训练，达到了新的最先进水平，在mAP@0.5上提高了88.5%，并在不同图像分辨率和未见地理区域上表现出强大的零样本泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24310",
            "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2503.24310",
            "abstract": "In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.",
            "score": 3,
            "issue_id": 3097,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "890bba46601fef07",
            "authors": [
                "Alok Abhishek",
                "Lisa Erickson",
                "Tushar Bandopadhyay"
            ],
            "affiliations": [
                "Boston, USA",
                "San Francisco, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24310.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "BEATS: комплексная оценка этичности языковых моделей",
                    "desc": "Исследователи представили BEATS - новую систему оценки предвзятости, этики, справедливости и фактической точности в больших языковых моделях (LLM). На основе BEATS разработан эталонный тест, измеряющий производительность LLM по 29 различным метрикам, включая демографические, когнитивные и социальные предубеждения. Результаты показали, что 37,65% выходных данных ведущих моделей содержали некоторую форму предвзятости. BEATS предлагает масштабируемую методологию для оценки LLM, диагностики факторов, вызывающих предвзятость, и разработки стратегий по ее снижению."
                },
                "en": {
                    "title": "BEATS: A Framework for Fair and Ethical AI Evaluation",
                    "desc": "This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics."
                },
                "zh": {
                    "title": "BEATS框架：推动负责任的人工智能评估",
                    "desc": "本研究介绍了BEATS框架，用于评估大型语言模型（LLMs）中的偏见、伦理、公平性和事实性。我们建立了一个偏见基准，涵盖29个不同的指标，评估LLMs在多样性、认知和社会偏见等方面的表现。通过这些指标，可以定量评估LLM生成的响应在多大程度上可能延续社会偏见，强化或扩大系统性不平等。我们的目标是通过BEATS框架，促进更具社会责任感和伦理对齐的人工智能模型的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01328",
            "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
            "url": "https://huggingface.co/papers/2504.01328",
            "abstract": "Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.",
            "score": 1,
            "issue_id": 3104,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "5272839c562d78b6",
            "authors": [
                "Min Shi",
                "Shihao Wang",
                "Chieh-Yun Chen",
                "Jitesh Jain",
                "Kai Wang",
                "Junjun Xiong",
                "Guilin Liu",
                "Zhiding Yu",
                "Humphrey Shi"
            ],
            "affiliations": [
                "HKPU",
                "NVIDIA",
                "SHI Labs @ Georgia Tech",
                "SUNY Buffalo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01328.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективное понимание видео с помощью slow-fast архитектуры для MLLM",
                    "desc": "Статья представляет новую архитектуру slow-fast для видео-ориентированных мультимодальных больших языковых моделей (MLLM). Эта архитектура использует двойную стратегию токенов: 'быстрые' визуальные токены для общего обзора и 'медленные' токены для детального анализа. Подход позволяет обрабатывать больше кадров видео, сохраняя пространственные детали, и значительно превосходит базовые модели на основе self-attention. Модель достигает state-of-the-art результатов среди моделей аналогичного размера на пяти бенчмарках по пониманию видео."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Efficient Slow-Fast Architecture",
                    "desc": "This paper addresses the challenge of balancing temporal resolution and spatial detail in video-based multi-modal large language models (MLLMs) while managing computational limits. The authors introduce a slow-fast architecture that allows for the processing of more input frames without losing important spatial information. By using a dual-token strategy, the model incorporates both compressed 'fast' visual tokens for quick overviews and uncompressed 'slow' visual tokens for detailed analysis, enhancing instruction-aware visual extraction. Experimental results demonstrate that this approach significantly improves performance and efficiency, allowing for a greater input capacity with minimal computational cost."
                },
                "zh": {
                    "title": "慢-快架构：提升视频理解的效率与性能",
                    "desc": "在视频基础的多模态大语言模型（MLLMs）中，平衡时间分辨率和空间细节是一个关键挑战。现有方法通常在输入到LLM之前使用预定义规则压缩视频表示，导致不可逆的信息损失，并且常常忽视输入指令。为了解决这个问题，我们提出了一种新颖的慢-快架构，能够在保留空间细节的同时使用更多的输入帧。我们的设计采用双令牌策略，通过快速视觉令牌和慢速视觉令牌的结合，实现了指令感知的相关视觉细节提取，显著提高了模型的性能和效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00396",
            "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic\n  Pollution-Free Fine-tuning",
            "url": "https://huggingface.co/papers/2504.00396",
            "abstract": "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/",
            "score": 1,
            "issue_id": 3104,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "0afcec0b90ba9cfd",
            "authors": [
                "Xiaole Xian",
                "Zhichao Liao",
                "Qingyu Li",
                "Wenyu Qin",
                "Pengfei Wan",
                "Weicheng Xie",
                "Long Zeng",
                "Linlin Shen",
                "Pingfa Feng"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shenzhen University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00396.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точная настройка генерации портретов без семантического загрязнения",
                    "desc": "Статья представляет новый метод SPF-Portrait для настройки моделей генерации изображений по тексту для создания портретов. Авторы предлагают двухпутевой подход с контрастным обучением, который позволяет адаптировать модель к целевым атрибутам, сохраняя при этом поведение исходной модели. Ключевым элементом является семантически-осведомленная карта точного контроля, которая направляет процесс выравнивания между контрастными путями. Метод также включает механизм усиления отклика для улучшения целевых атрибутов и снижения несоответствия представлений."
                },
                "en": {
                    "title": "Preserving Originality in Portrait Customization with SPF-Portrait",
                    "desc": "This paper presents SPF-Portrait, a method for fine-tuning a pre-trained Text-to-Image model specifically for customizing portrait attributes without losing the original model's capabilities. The authors address the problem of Semantic Pollution, which occurs during fine-tuning and affects the model's performance on unrelated attributes. SPF-Portrait employs a dual-path pipeline that uses contrastive learning to align target attributes while maintaining the integrity of the original model. Additionally, a Semantic-Aware Fine Control Map is introduced to guide the alignment process, ensuring effective adaptation and enhancing the performance of the desired attributes."
                },
                "zh": {
                    "title": "消除语义污染，实现肖像定制的创新之路",
                    "desc": "本文提出了一种名为SPF-Portrait的方法，用于在定制肖像属性时消除语义污染。通过引入原始模型作为参考，采用对比学习确保目标属性的适应性，同时保持原始肖像的其他无关属性。我们还引入了一种新的语义感知细控图，精确指导对比路径之间的对齐过程，从而有效保留原始模型的性能。实验结果表明，SPF-Portrait在肖像定制任务中达到了最先进的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-04-04.html",
    "link_next": "2025-04-08.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4月4日"
    },
    "short_date_next": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 4,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 7,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 1,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "pinyin": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de duō yǔ yán wèn tí jiě jué jī zhǔn, chēng wéi Multi-SWE-bench. tā hán gǎi le Java, TypeScript, JavaScript, Go, Rust, C hé C++ děng yǔ yán. gǎi jī zhǔn bāo hán 1,632 gè gāo zhì liàng shí lì, yǒu 68 wèi zhuān jiā zhù shì. jī yú zhè gè jī zhǔn, zuò zhě píng gū le yī xì liè xiān jìn de mó xíng, bìng tí gōng le xiáng xì de fēn xī. cǐ wài, wén zhāng hái xuān bù chéng lì le yī gè kāi yuán shè qū Multi-SWE-RL, zhǐ yú wèi wèn tí jiě jué rèn wù gòu jiàn dà guī mó qiáng huà xué xùn liàn shù jù jí.",
        "vocab": "[\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"注释\", \"pinyin\": \"zhùshì\", \"trans\": \"annotate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"宣布\", \"pinyin\": \"xuānbù\", \"trans\": \"announce\"},\n    {\"word\": \"成立\", \"pinyin\": \"chénglì\", \"trans\": \"establish\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shèqū\", \"trans\": \"community\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"构建\", \"pinyin\": \"gòujiàn\", \"trans\": \"build\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"training\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces a new multilingual problem-solving benchmark called Multi-SWE-bench. It covers languages such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. The benchmark contains 1,632 high-quality instances annotated by 68 experts. Based on this benchmark, the authors evaluated a series of advanced models and provided detailed analyses. Additionally, the article announces the establishment of an open-source community, Multi-SWE-RL, aimed at building large-scale reinforcement learning training datasets for problem-solving tasks.",
        "update_ts": "2025-04-07 09:12"
    }
}