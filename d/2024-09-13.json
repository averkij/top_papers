{
    "date": {
        "ru": "13 сентября",
        "en": "September 13",
        "zh": "9月13日"
    },
    "time_utc": "2024-09-13 09:00",
    "weekday": 4,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-13",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.07703",
            "title": "DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?",
            "url": "https://huggingface.co/papers/2409.07703",
            "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.",
            "score": 66,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "8b2f2eaf3883ea5d",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#cv",
                    "#long_context",
                    "#data",
                    "#agents",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "DSBench: реалистичная оценка возможностей ИИ в анализе данных",
                    "desc": "DSBench - это новый комплексный бенчмарк для оценки агентов в области анализа данных с реалистичными задачами. Он включает 466 задач по анализу данных и 74 задачи по моделированию данных из источников Eloquence и соревнований Kaggle. DSBench предлагает реалистичные условия, включая длинные контексты, мультимодальные задачи, работу с большими файлами данных и многотабличными структурами. Оценка современных языковых моделей и агентов показала, что они справляются лишь с 34,12% задач анализа данных, что подчеркивает необходимость дальнейшего развития более практичных и автономных агентов для data science."
                },
                "en": {
                    "title": "Bridging the Gap: Realistic Benchmarks for Data Science Agents",
                    "desc": "This paper introduces DSBench, a new benchmark aimed at evaluating the performance of data science agents in realistic scenarios. Unlike previous benchmarks, DSBench includes a wide range of tasks, such as data analysis and modeling, that reflect real-world challenges faced by data scientists. The evaluation of current state-of-the-art Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) reveals that they struggle significantly, with the best agent only solving about one-third of the tasks. This highlights the necessity for further improvements in creating more capable and autonomous data science agents."
                },
                "zh": {
                    "title": "构建更智能的数据科学智能体",
                    "desc": "大型语言模型（LLMs）和大型视觉语言模型（LVLMs）在语言和视觉推理方面表现出色，推动了针对特定应用（如购物助手或AI软件工程师）的智能体开发。为了评估这些智能体在数据科学领域的表现，研究者们提出了许多数据科学基准，但现有基准与真实数据科学应用相比仍显不足。为此，我们引入了DSBench，这是一个全面的基准，旨在评估数据科学智能体在现实任务中的表现，包含466个数据分析任务和74个数据建模任务。我们的评估结果显示，当前最先进的模型在大多数任务上表现不佳，强调了开发更实用、智能和自主的数据科学智能体的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04109",
            "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "url": "https://huggingface.co/papers/2409.04109",
            "abstract": "Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",
            "score": 43,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 сентября",
                "en": "September 6",
                "zh": "9月6日"
            },
            "hash": "b1fccf9709fd9871",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM vs Человек: Кто генерирует более инновационные научные идеи?",
                    "desc": "Исследование сравнивает идеи, генерируемые большими языковыми моделями (LLM), с идеями экспертов-исследователей в области обработки естественного языка. Результаты показывают, что идеи LLM оцениваются как более новаторские, но немного менее осуществимые. Выявлены проблемы в самооценке LLM и недостаток разнообразия в генерации идей. Предложен новый дизайн исследования для оценки реальной ценности идей путем их реализации в полноценные проекты."
                },
                "en": {
                    "title": "LLMs Outshine Humans in Novelty of Research Ideas!",
                    "desc": "This paper investigates the ability of large language models (LLMs) to generate novel research ideas compared to human experts in natural language processing (NLP). The authors conducted an experimental study where over 100 NLP researchers generated ideas and reviewed both LLM-generated and human-generated ideas. The results showed that LLM-generated ideas were considered more novel than those from human experts, although they were rated slightly lower in feasibility. The study highlights challenges in evaluating LLMs, such as their self-evaluation capabilities and diversity in idea generation, and suggests further research to assess the impact of these ideas on actual research outcomes."
                },
                "zh": {
                    "title": "大型语言模型在研究创意生成中的潜力与挑战",
                    "desc": "最近大型语言模型（LLMs）的进展引发了人们对其加速科学发现潜力的乐观。本文通过实验设计评估研究创意生成，首次对比了专家NLP研究人员与LLM创意代理的表现。结果显示，LLM生成的创意在新颖性上被评判为优于人类专家的创意，但在可行性上略显不足。我们还发现了构建和评估研究代理的开放问题，并提出了一个完整的研究设计，以进一步验证创意的实际研究成果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08264",
            "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
            "url": "https://huggingface.co/papers/2409.08264",
            "abstract": "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena",
            "score": 43,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "e7d193394c84841c",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#cv",
                    "#training",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Windows Agent Arena: Реалистичное тестирование ИИ-агентов в среде Windows",
                    "desc": "Эта статья представляет Windows Agent Arena - новую среду для тестирования агентов искусственного интеллекта в реальной операционной системе Windows. Среда включает более 150 разнообразных задач, требующих планирования, понимания экрана и использования инструментов. Авторы также представляют нового мультимодального агента Navi, который достигает 19.5% успешности в выполнении задач на Windows. Benchmark позволяет проводить масштабируемое и быстрое тестирование агентов в реалистичной среде Windows."
                },
                "en": {
                    "title": "Empowering AI Agents in Real-World Windows Tasks",
                    "desc": "This paper presents the Windows Agent Arena, a new benchmark designed to evaluate the performance of large language models (LLMs) as computer agents in a realistic Windows operating system environment. The arena allows agents to perform over 150 diverse tasks that require skills in planning, screen understanding, and tool usage, addressing the limitations of existing benchmarks that are often modality-specific and slow to evaluate. The introduced multi-modal agent, Navi, demonstrates a success rate of 19.5% in completing tasks, highlighting the challenges faced by AI agents compared to human performance. The benchmark is scalable and can be evaluated quickly, paving the way for future research in agent development and data generation."
                },
                "zh": {
                    "title": "Windows代理竞技场：提升代理性能的新平台",
                    "desc": "大型语言模型（LLMs）在多模态任务中展现出作为计算机代理的巨大潜力，能够提升人类的生产力和软件的可访问性。然而，在现实环境中评估代理性能仍然面临挑战，因为大多数基准测试仅限于特定的模态或领域，并且完整的基准评估速度较慢。为了解决这些问题，我们引入了Windows代理竞技场，这是一个专注于Windows操作系统的可重复环境，代理可以在其中自由操作，使用各种应用程序和工具。我们创建了150多个多样化的Windows任务，要求代理具备规划、屏幕理解和工具使用的能力，并且我们的基准测试可以在Azure上无缝并行化，快速完成评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08240",
            "title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2409.08240",
            "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "b63d6f4e9ec2890a",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#benchmark",
                    "#open_source",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точное позиционирование и детализация объектов в генеративных моделях изображений",
                    "desc": "Статья представляет новый подход к генерации изображений с множественными объектами - Instance Feature Generation (IFG). Авторы предлагают Instance Feature Adapter (IFAdapter), который улучшает точность позиционирования и детализацию отдельных объектов на изображении. IFAdapter использует дополнительные токены внешнего вида и семантическую карту для лучшего согласования признаков объектов с их пространственным расположением. Экспериментальные результаты показывают превосходство IFAdapter над другими моделями в количественных и качественных оценках."
                },
                "en": {
                    "title": "Enhancing Image Generation with Instance Feature Control",
                    "desc": "This paper addresses the limitations of Text-to-Image (T2I) diffusion models in generating multiple instances with accurate positioning and detailed features. It introduces the Instance Feature Generation (IFG) task, which focuses on improving both the spatial accuracy and the fidelity of features in generated images. To tackle this task, the authors propose the Instance Feature Adapter (IFAdapter), which uses additional appearance tokens and an Instance Semantic Map to better align features with their spatial locations. The paper also presents a benchmark for evaluating the IFG task and shows that the IFAdapter significantly outperforms existing models in generating instances with precise positioning and enhanced features."
                },
                "zh": {
                    "title": "提升图像生成的实例特征与定位精度",
                    "desc": "本文提出了一种新的任务，称为实例特征生成（IFG），旨在提高生成图像中多个实例的定位准确性和特征保真度。为了解决这一任务，作者引入了实例特征适配器（IFAdapter），该模块通过增加外观标记和使用实例语义图来增强特征表现。IFAdapter作为一个可插拔模块，能够适应不同的社区模型，并指导扩散过程。实验结果表明，IFAdapter在定量和定性评估中均优于其他模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08239",
            "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
            "url": "https://huggingface.co/papers/2409.08239",
            "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "7b45c82ece8d90d6",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#data",
                    "#transfer_learning",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтетические данные для улучшения навыков языковых моделей",
                    "desc": "Статья представляет новый метод Source2Synth для обучения больших языковых моделей (LLM) без использования дорогостоящих человеческих аннотаций. Метод генерирует синтетические данные с промежуточными шагами рассуждения, основанными на реальных источниках. Source2Synth улучшает качество набора данных, отбрасывая низкокачественные генерации. Авторы демонстрируют эффективность метода в двух сложных областях: многоэтапные вопросно-ответные системы и использование инструментов в табличных вопросно-ответных системах."
                },
                "en": {
                    "title": "Empowering LLMs with Synthetic Data for Enhanced Reasoning Skills",
                    "desc": "This paper introduces Source2Synth, a novel method designed to enhance the capabilities of Large Language Models (LLMs) in complex tasks involving structured data and reasoning. The approach generates synthetic data points from a custom data source, incorporating intermediate reasoning steps that are based on real-world information. By filtering out low-quality outputs based on their answerability, Source2Synth significantly improves the quality of the training dataset. The effectiveness of this method is demonstrated through substantial performance gains in multi-hop question answering and tabular question answering tasks, achieving improvements of over 22% in accuracy compared to traditional fine-tuning methods."
                },
                "zh": {
                    "title": "Source2Synth：提升大型语言模型的新方法",
                    "desc": "本文提出了一种新方法Source2Synth，用于教导大型语言模型（LLMs）新技能，而无需依赖昂贵的人类标注。该方法通过输入自定义数据源，生成带有中间推理步骤的合成数据点，这些步骤基于真实世界的来源。Source2Synth通过丢弃低质量生成的答案来提高数据集的质量。我们在两个具有挑战性的领域进行了测试，结果显示该方法在表格问答（TQA）和多跳问答（MHQA）上显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08248",
            "title": "TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder",
            "url": "https://huggingface.co/papers/2409.08248",
            "abstract": "Recent breakthroughs in text-to-image models have opened up promising research avenues in personalized image generation, enabling users to create diverse images of a specific subject using natural language prompts. However, existing methods often suffer from performance degradation when given only a single reference image. They tend to overfit the input, producing highly similar outputs regardless of the text prompt. This paper addresses the challenge of one-shot personalization by mitigating overfitting, enabling the creation of controllable images through text prompts. Specifically, we propose a selective fine-tuning strategy that focuses on the text encoder. Furthermore, we introduce three key techniques to enhance personalization performance: (1) augmentation tokens to encourage feature disentanglement and alleviate overfitting, (2) a knowledge-preservation loss to reduce language drift and promote generalizability across diverse prompts, and (3) SNR-weighted sampling for efficient training. Extensive experiments demonstrate that our approach efficiently generates high-quality, diverse images using only a single reference image while significantly reducing memory and storage requirements.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "c7e040a619639ae3",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Персонализированная генерация изображений: качество и разнообразие из одного примера",
                    "desc": "В статье представлен метод улучшения генерации персонализированных изображений по текстовому запросу с использованием всего одного референсного изображения. Авторы предлагают стратегию выборочной тонкой настройки текстового энкодера и вводят три ключевые техники: токены аугментации, функцию потерь для сохранения знаний и взвешенное SNR-сэмплирование. Эти подходы позволяют снизить переобучение модели и улучшить разнообразие генерируемых изображений. Эксперименты показывают, что метод эффективно создает качественные и разнообразные изображения, значительно снижая требования к памяти и хранению."
                },
                "en": {
                    "title": "Enhancing One-Shot Personalization in Image Generation",
                    "desc": "This paper presents a novel approach to improve personalized image generation from text prompts using only one reference image. It tackles the issue of overfitting, which leads to similar outputs regardless of the input text. The authors propose a selective fine-tuning strategy for the text encoder and introduce techniques like augmentation tokens, knowledge-preservation loss, and SNR-weighted sampling to enhance performance. Experimental results show that their method generates diverse, high-quality images efficiently while minimizing memory and storage needs."
                },
                "zh": {
                    "title": "一图多样，个性化生成新突破",
                    "desc": "这篇论文探讨了文本到图像模型在个性化图像生成中的应用，尤其是如何通过自然语言提示生成特定主题的多样化图像。现有方法在仅使用单一参考图像时，常常出现性能下降和过拟合的问题，导致输出图像与输入提示高度相似。为了解决这一挑战，论文提出了一种选择性微调策略，重点关注文本编码器，并引入了三种关键技术来增强个性化性能。通过这些技术，研究表明可以有效生成高质量、多样化的图像，同时显著减少内存和存储需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.07239",
            "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
            "url": "https://huggingface.co/papers/2409.07239",
            "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "63af38e7f029dd60",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#games",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "PiTe: Новый уровень понимания видео с помощью траекторий объектов",
                    "desc": "В этой статье представлена новая модель PiTe, которая объединяет видео и текст на основе траекторий объектов. Авторы создали датасет PiTe-143k с аннотациями траекторий движения объектов на уровне пикселей. Модель PiTe демонстрирует выдающиеся результаты в различных мультимодальных задачах, связанных с видео. Этот подход позволяет достичь более точного согласования видео и текста по сравнению с существующими методами."
                },
                "en": {
                    "title": "Bridging Video and Language with Trajectory-Guided Alignment",
                    "desc": "This paper introduces a new model called PiTe, which stands for trajectory-guided Pixel-Temporal Alignment, aimed at improving the connection between video and language. It addresses the challenges faced by Large Video-Language Models (LVidLMs) in understanding the complex relationships between language and the dynamic nature of video data. The authors present a unique dataset, PiTe-143k, which includes detailed moving trajectories of objects in videos, enhancing the model's ability to align visual and textual information. PiTe outperforms existing models in various multi-modal tasks, showcasing its effectiveness in video-language alignment."
                },
                "zh": {
                    "title": "视频与语言的精细对齐新方法",
                    "desc": "随着大型语言模型（LLMs）的发展，大型视觉语言模型（LVLMs）成为了连接图像和文本的重要进展。然而，视频的复杂性使得LVLMs在处理时面临挑战。本文提出了一种新颖的LVidLM模型，通过轨迹引导的像素时间对齐（PiTe），实现了视频和语言的精细对齐。我们还构建了一个多模态预训练数据集PiTe-143k，提供了视频中所有对象的移动轨迹，以支持多种视频相关的多模态任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08278",
            "title": "DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors",
            "url": "https://huggingface.co/papers/2409.08278",
            "abstract": "We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "5e63dc8bd9635183",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Синтез взаимодействий человека с объектами с помощью ИИ",
                    "desc": "DreamHOI - это новый метод для синтеза взаимодействий человека с объектами без предварительного обучения, используя текстовое описание. Метод использует диффузионные модели для генерации изображений и оптимизирует артикуляцию 3D-модели человека с помощью Score Distillation Sampling. Авторы вводят двойное неявно-явное представление сетки с анимацией, комбинируя нейронные радиальные поля (NeRF) с явной анимацией скелета. Эксперименты показывают эффективность метода в генерации реалистичных взаимодействий человека с объектами."
                },
                "en": {
                    "title": "Realistic Human-Object Interactions from Text Descriptions",
                    "desc": "DreamHOI is a new method that allows a 3D human model to interact with various objects based on text descriptions, even when there is no prior data for those specific interactions. It addresses the challenge of limited datasets by using text-to-image diffusion models that have learned from a vast number of image-caption pairs. The method optimizes the movement of a human model's mesh by using Score Distillation Sampling (SDS) to guide the edits needed for realistic interactions. By combining neural radiance fields with traditional mesh articulation, DreamHOI effectively generates realistic human-object interactions in a zero-shot manner."
                },
                "zh": {
                    "title": "DreamHOI：实现人机交互的零样本合成",
                    "desc": "我们提出了一种新方法DreamHOI，用于零样本合成人体与物体的交互（HOIs）。该方法允许3D人类模型根据文本描述与任何给定物体进行逼真的交互。为了克服数据稀缺的问题，我们利用了在数十亿图像-文本对上训练的文本到图像扩散模型。我们通过引入双重隐式-显式表示，结合神经辐射场（NeRF）和骨架驱动的网格关节，优化了人类模型的关节动作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08270",
            "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
            "url": "https://huggingface.co/papers/2409.08270",
            "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50times faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "a61a3bf3d33858ce",
            "data": {
                "categories": [
                    "#cv",
                    "#math",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#3d"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Молниеносная сегментация 3D Gaussian Splatting с помощью линейного программирования",
                    "desc": "Это исследование предлагает новый метод для сегментации 3D Gaussian Splatting из 2D масок. Вместо итеративного градиентного спуска, авторы разработали глобально оптимальное решение, основанное на линейном программировании. Метод использует альфа-смешивание в процессе сплаттинга для оптимизации в один шаг. Результаты показывают, что предложенный подход работает в 50 раз быстрее существующих методов и демонстрирует высокую эффективность в задачах удаления объектов и инпейнтинга."
                },
                "en": {
                    "title": "Fast and Robust 3D Segmentation with Linear Programming",
                    "desc": "This paper presents a new method for segmenting 3D Gaussian Splatting (3D-GS) from 2D masks, addressing the inefficiencies of traditional iterative gradient descent approaches. The authors introduce a globally optimal solver that leverages the linear relationship between 2D mask rendering and Gaussian labels, allowing for a closed-form solution through linear programming. By incorporating background bias into the objective function, the method enhances robustness against noise in 3D segmentation tasks. The proposed optimization is significantly faster, completing in about 30 seconds, and demonstrates superior performance in various applications, including object removal and inpainting."
                },
                "zh": {
                    "title": "高效鲁棒的3D高斯分割方法",
                    "desc": "本研究解决了从2D掩膜中准确分割3D高斯点云的挑战。传统方法通常依赖迭代梯度下降为每个高斯分配唯一标签，导致优化过程漫长且结果不理想。我们提出了一种简单但全局最优的3D-GS分割求解器，利用线性规划在封闭形式中解决最优标签分配。通过将背景偏差纳入目标函数，我们的方法在3D分割中对噪声表现出更强的鲁棒性，优化过程仅需30秒，速度比现有最佳方法快约50倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.05162",
            "title": "Can OOD Object Detectors Learn from Foundation Models?",
            "url": "https://huggingface.co/papers/2409.05162",
            "abstract": "Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "74ea126cddc6e29e",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Синтез данных для улучшения обнаружения объектов вне распределения",
                    "desc": "Статья представляет новый метод SyncOOD для обнаружения объектов вне распределения (OOD). Этот метод использует генеративные модели, обученные на масштабных открытых данных, для синтеза образцов OOD. SyncOOD автоматически извлекает значимые данные OOD из генеративных моделей текст-изображение. Синтетические образцы OOD используются для улучшения обучения легковесного детектора OOD, оптимизируя границы решений между данными в распределении (ID) и вне его (OOD)."
                },
                "en": {
                    "title": "Enhancing OOD Detection with Synthetic Data from Generative Models",
                    "desc": "This paper addresses the challenge of detecting out-of-distribution (OOD) objects, which is difficult due to the lack of available OOD data. The authors propose SyncOOD, a method that uses generative models like Stable Diffusion to create synthetic OOD samples from large-scale open-set data. By leveraging these synthetic samples, the method enhances the training of a lightweight OOD detector, improving its ability to distinguish between in-distribution (ID) and OOD data. The results show that SyncOOD achieves superior performance compared to existing techniques, setting new benchmarks with minimal reliance on synthetic data."
                },
                "zh": {
                    "title": "利用生成模型提升超出分布物体检测的能力",
                    "desc": "本文研究了如何利用生成模型来改善超出分布（OOD）物体检测。我们提出了一种名为SyncOOD的方法，通过从文本到图像的生成模型中提取有意义的OOD数据，来合成OOD样本。这样可以利用大型基础模型的开放世界知识，增强OOD物体检测的能力。实验结果表明，SyncOOD在多个基准测试中显著优于现有方法，达到了新的最先进性能。"
                }
            }
        }
    ],
    "link_prev": "2024-09-12.html",
    "link_next": "2024-09-16.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9月12日"
    },
    "short_date_next": {
        "ru": "16.09",
        "en": "09/16",
        "zh": "9月16日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 8,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 3,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    }
}