
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. October 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 октября</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-15.html">⬅️ <span id="prev-date">15.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-17.html">➡️ <span id="next-date">17.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'};
        let feedDateNext = {'ru': '17.10', 'en': '10/17', 'zh': '10月17日'};
        let feedDatePrev = {'ru': '15.10', 'en': '10/15', 'zh': '10月15日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.10814', 'title': 'Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free', 'url': 'https://huggingface.co/papers/2410.10814', 'abstract': 'While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.', 'score': 48, 'issue_id': 124, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'c2ddb801d2228a78', 'authors': ['Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Department of Computer Science University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10814.jpg', 'data': {'categories': ['#dataset', '#agi', '#interpretability', '#optimization', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MoE LLM: Скрытый потенциал эмбеддингов без дополнительной настройки', 'desc': 'Исследование показывает, что маршрутизаторы экспертов в моделях Mixture-of-Experts (MoE) LLM могут служить готовой моделью для создания эмбеддингов с многообещающей производительностью на разнообразных задачах, не требуя дополнительной настройки. Анализ демонстрирует, что веса маршрутизации MoE (RW) дополняют скрытое состояние (HS) LLM, широко используемое для эмбеддингов. Предложенный метод MoEE, объединяющий RW и HS, показывает лучшую производительность, чем использование каждого по отдельности. Эксперименты проводились на 6 задачах эмбеддинга с 20 наборами данных из Massive Text Embedding Benchmark (MTEB).'}, 'en': {'title': 'Unlocking the Embedding Power of MoE LLMs: No Finetuning Needed!', 'desc': 'The paper investigates the potential of Mixture-of-Experts (MoE) large language models (LLMs) as effective embedding models without additional finetuning. It reveals that the expert routers in MoE LLMs can be used directly for embedding tasks, showing strong performance across various tasks. The study finds that the routing weights (RW) in MoE models complement the hidden states (HS) of LLMs, offering robustness to prompt variations and focusing on high-level semantics. By combining RW and HS, the proposed MoEE model achieves superior results, as demonstrated on multiple datasets from the Massive Text Embedding Benchmark (MTEB).'}, 'zh': {'title': '专家混合：提升语言模型嵌入能力的新路径', 'desc': '大型语言模型（LLM）在生成任务上表现出色，但其仅解码架构限制了其作为嵌入模型的潜力。我们研究了专家混合（MoE）LLM，发现其专家路由器无需微调即可在多种嵌入任务中表现良好。MoE的路由权重（RW）与隐藏状态（HS）互补，且对提示选择更具鲁棒性。我们提出的MoEE结合了RW和HS，显著提升了嵌入任务的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.09342', 'title': 'LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models', 'url': 'https://huggingface.co/papers/2410.09342', 'abstract': 'Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.', 'score': 37, 'issue_id': 125, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': '214a55e4bcf12eb6', 'authors': ['Zihan Zhou', 'Chong Li', 'Xinyi Chen', 'Shuo Wang', 'Yu Chao', 'Zhili Li', 'Haoyu Wang', 'Rongqiao An', 'Qi Shi', 'Zhixing Tan', 'Xu Han', 'Xiaodong Shi', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['BUPT', 'Beijing National Research Center for Information Science and Technology', 'Dept. of Comp. Sci. & Tech., Tsinghua University', 'Institute for AI, Tsinghua University', 'Nankai University', 'Peking University', 'Xiamen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09342.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#long_context'], 'emoji': '📄', 'ru': {'title': 'Эффективная обработка длинных текстов без переобучения LLM', 'desc': 'Статья представляет новый подход к обработке длинных текстов с помощью больших языковых моделей (LLM). Авторы предлагают фреймворк LLM×MapReduce, который разделяет документ на части для обработки, а затем объединяет промежуточные результаты. Для решения проблем потери информации при разделении текста разработаны протокол структурированной информации и механизм калибровки уверенности в контексте. Эксперименты показывают, что LLM×MapReduce превосходит существующие модели с длинным контекстом и применим к различным LLM.'}, 'en': {'title': 'Divide and Conquer: Mastering Long Texts with LLMtimesMapReduce', 'desc': "The paper introduces a new method called LLMtimesMapReduce to help large language models (LLMs) understand very long texts without needing extra training. It uses a divide-and-conquer approach, breaking the text into smaller parts for the LLMs to process, and then combines the results to form a complete understanding. The main challenge is ensuring that important information isn't lost when the text is split, which the authors address with a structured information protocol and a confidence calibration mechanism. Tests show that this method works better than other similar tools and can be used with different LLMs."}, 'zh': {'title': '分而治之：无训练框架处理超长文本', 'desc': '这篇论文提出了一种新的无训练框架，称为LLMtimesMapReduce，用于处理超长文本。该框架通过分而治之的方法，将文档分成多个部分供大语言模型（LLM）阅读，然后汇总中间答案以生成最终输出。主要挑战在于分割文档时可能丢失重要的长距离信息，导致模型基于分段文本产生不完整或错误的答案。实验结果表明，LLMtimesMapReduce在处理长文本方面优于其他开源和商业长上下文LLM。'}}}, {'id': 'https://huggingface.co/papers/2410.10626', 'title': 'Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts', 'url': 'https://huggingface.co/papers/2410.10626', 'abstract': 'Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.', 'score': 37, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'e8c698bf76856366', 'authors': ['Guorui Zheng', 'Xidong Wang', 'Juhao Liang', 'Nuo Chen', 'Yuping Zheng', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10626.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#healthcare', '#interpretability', '#optimization', '#transfer_learning', '#architecture', '#low_resource'], 'emoji': '🏥', 'ru': {'title': 'Многоязычные медицинские LLM: преодоление языковых барьеров в здравоохранении', 'desc': 'Статья посвящена адаптации больших языковых моделей (LLM) для медицинских целей в различных языках. Авторы создают качественный медицинский датасет и исследуют внутренние механизмы многоязычных LLM с использованием модульности Mixture of Experts (MoE). Они предлагают новый метод маршрутизации MoE с языково-специфичными экспертами и кросс-языковой маршрутизацией. На основе анализа информационного потока разработана архитектура Post-MoE, применяющая разреженную маршрутизацию только в поздних слоях модели.'}, 'en': {'title': 'Breaking Language Barriers in Medical AI', 'desc': "The paper addresses the challenge of adapting medical Large Language Models (LLMs) to low-resource languages by constructing a high-quality medical dataset and analyzing its quality. It introduces a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing to enhance the model's generalization capabilities. Inspired by circuit theory, the authors propose the Post-MoE architecture, which applies sparse routing in later layers to improve language-specific performance while maintaining interpretability. The approach allows the model to scale efficiently to 50 languages by using language family experts, leveraging linguistic priors without increasing parameters."}, 'zh': {'title': '跨语言医疗模型：突破语言障碍的创新之路', 'desc': '这篇论文探讨了如何将大型语言模型（LLM）适应本地语言，以减少获取医疗服务的障碍。研究中，作者首先构建了一个高质量的医疗数据集，并进行了分析以确保其质量。通过使用专家混合（MoE）模块化方法，研究了多语言LLM的内部信息流，提出了一种新的MoE路由方法。实验结果表明，这种方法提高了多语言模型的泛化能力，同时保持了解释性。'}}}, {'id': 'https://huggingface.co/papers/2406.15786', 'title': 'What Matters in Transformers? Not All Attention is Needed', 'url': 'https://huggingface.co/papers/2406.15786', 'abstract': 'While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.', 'score': 29, 'issue_id': 121, 'pub_date': '2024-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': 'fdf872015ec393aa', 'authors': ['Shwai He', 'Guoheng Sun', 'Zheyu Shen', 'Ang Li'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2406.15786.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Оптимизация LLM: меньше слоев, та же мощность', 'desc': 'Это исследование посвящено проблеме избыточности в архитектуре больших языковых моделей (LLM) на базе трансформеров. Авторы обнаружили, что значительная часть слоев внимания (attention layers) обладает высокой схожестью и может быть удалена без существенной потери производительности. Например, модель Llama-2-70B достигла ускорения на 48.4% при снижении производительности всего на 2.4% после удаления половины слоев внимания. Исследователи также предложили метод совместного удаления слоев внимания и MLP, что позволяет еще более агрессивно сокращать архитектуру модели.'}, 'en': {'title': 'Prune to Perform: Streamlining Transformers for Efficiency', 'desc': "This paper explores the redundancy in Transformer-based large language models, focusing on the MLP and Attention layers. The authors found that many attention layers are highly similar and can be pruned without significantly affecting performance, as demonstrated by a 48.4% speedup in Llama-2-70B with minimal performance loss. They also propose a method to jointly prune Attention and MLP layers, achieving substantial efficiency gains while maintaining most of the model's performance. This research offers insights into optimizing Transformer architectures for more efficient real-world applications."}, 'zh': {'title': '优化Transformer：减少冗余，提升效率', 'desc': '这篇论文研究了在Transformer模型中不同模块的冗余问题，特别是注意力层和MLP层。研究发现，尽管注意力层在Transformer中非常重要，但许多层的相似度过高，可以被剪枝而不影响性能。通过剪枝注意力层，模型的速度可以显著提高，而性能损失很小。此外，论文还提出了一种同时剪枝注意力层和MLP层的方法，进一步提高了模型的效率。'}}}, {'id': 'https://huggingface.co/papers/2410.11779', 'title': 'MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation', 'url': 'https://huggingface.co/papers/2410.11779', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.', 'score': 24, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '25a9176442c65ae3', 'authors': ['Chenxi Wang', 'Xiang Chen', 'Ningyu Zhang', 'Bozhong Tian', 'Haoming Xu', 'Shumin Deng', 'Huajun Chen'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11779.jpg', 'data': {'categories': ['#hallucinations', '#training', '#interpretability', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных моделях', 'desc': 'Статья исследует проблему галлюцинаций в мультимодальных больших языковых моделях (MLLM). Авторы обнаружили, что MLLM способны распознавать визуальные объекты в промежуточных слоях, но в итоговом выводе могут их некорректно генерировать. Предполагается, что это связано с подавлением визуальной информации сильными языковыми приорами модели. Для решения проблемы предложен метод декодирования DeCo, адаптивно интегрирующий знания из предыдущих слоев в финальный слой.'}, 'en': {'title': 'DeCo: Correcting Hallucinations in Multimodal Models', 'desc': "This paper investigates why Multimodal Large Language Models (MLLMs) often produce hallucinations, where they generate incorrect outputs. The authors discover that while MLLMs can recognize visual objects in earlier layers, the language model's strong knowledge priors may suppress this visual information, causing hallucinations. To address this, they introduce a new method called dynamic correction decoding (DeCo), which adjusts the integration of knowledge in the final output layer. DeCo is versatile and can be used with different models and decoding strategies, significantly reducing hallucination rates in tests."}, 'zh': {'title': '动态校正解码：减少多模态模型幻觉的创新方法', 'desc': '多模态大语言模型（MLLMs）常常会出现幻觉现象，但其原因尚不明确。本文通过实证分析发现，尽管MLLMs在最终输出中错误生成对象，但它们实际上能够在前面的层中识别视觉对象。我们推测这可能是由于语言模型的强知识先验抑制了视觉信息，导致幻觉现象。为此，我们提出了一种新的动态校正解码方法（DeCo），可以自适应地选择合适的前置层，并将知识按比例整合到最终层中，以调整输出。'}}}, {'id': 'https://huggingface.co/papers/2410.10816', 'title': 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions', 'url': 'https://huggingface.co/papers/2410.10816', 'abstract': 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.', 'score': 19, 'issue_id': 123, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'b7f807e5ff9e1614', 'authors': ['Tianwei Xiong', 'Yuqing Wang', 'Daquan Zhou', 'Zhijie Lin', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10816.jpg', 'data': {'categories': ['#video', '#dataset', '#long_context', '#training', '#data', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'LVD-2M: Новый стандарт для обучения генерации длинных видео', 'desc': 'Статья представляет новый датасет LVD-2M для обучения моделей генерации длинных видео. Датасет содержит 2 миллиона видео длительностью более 10 секунд, снятых одним кадром и сопровождаемых плотными временными подписями. Авторы разработали методику отбора качественных видео и создания аннотаций. Эффективность датасета подтверждена экспериментами по дообучению моделей генерации видео.'}, 'en': {'title': '"Unlocking the Future of Long Video Generation"', 'desc': 'The paper discusses the importance of high-quality datasets for training video generation models, especially for long videos. It introduces a new dataset, LVD-2M, which includes 2 million long-take videos, each over 10 seconds, with diverse content and detailed captions. The authors developed a pipeline to select these videos based on specific quality metrics and to generate temporally dense captions. This dataset aims to advance research in generating long videos with dynamic motions.'}, 'zh': {'title': '推动长视频生成研究的新数据集', 'desc': '这篇论文讨论了视频生成模型的训练数据集质量对模型效果的重要性。作者指出，现有的视频生成模型大多基于短视频片段，而长视频生成的研究受到高质量长视频缺乏的限制。为此，作者提出了一种新的数据集选择流程，专注于长视频的质量评估和密集字幕生成。最终，他们创建了一个名为LVD-2M的数据集，包含200万段长视频，并验证了其在长视频生成中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.11710', 'title': 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.11710', 'abstract': 'Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.', 'score': 18, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '44b55db6ab50c3d4', 'authors': ['Pei Wang', 'Yanan Wu', 'Zekun Wang', 'Jiaheng Liu', 'Xiaoshuai Song', 'Zhongyuan Peng', 'Ken Deng', 'Chenchen Zhang', 'Jiakai Wang', 'Junran Peng', 'Ge Zhang', 'Hangyu Guo', 'Zhaoxiang Zhang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'University of Chinese Academy of Sciences', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11710.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#data', '#benchmark', '#open_source'], 'emoji': '🛠️', 'ru': {'title': 'Многогранная оценка навыков LLM в использовании инструментов', 'desc': 'В этой статье представлен новый бенчмарк MTU-Bench для оценки навыков использования инструментов большими языковыми моделями (LLM). MTU-Bench охватывает пять сценариев использования инструментов, от простых до сложных и нестандартных задач. Оценка производится без использования API GPT или человеческой экспертизы, что снижает затраты. Авторы также предлагают набор данных MTU-Instruct для улучшения способностей LLM в использовании инструментов.'}, 'en': {'title': 'MTU-Bench: Elevating LLM Tool-Use Evaluation', 'desc': "The paper introduces MTU-Bench, a new benchmark designed to evaluate large language models (LLMs) on their ability to use tools effectively. MTU-Bench addresses the limitations of existing datasets by providing a multi-granularity approach, covering various tool-use scenarios such as single or multiple tools and turns, as well as out-of-distribution tasks. Unlike previous benchmarks, MTU-Bench relies solely on prediction results and ground truth for evaluation, avoiding costly GPT or human assessments. Additionally, the authors present MTU-Instruct, a dataset aimed at enhancing LLMs' tool-use capabilities, demonstrating its effectiveness through comprehensive experiments."}, 'zh': {'title': 'MTU-Bench：提升语言模型工具使用能力的新基准', 'desc': '大型语言模型在推理和决策能力上有了显著提升，并能与用户进行自然对话。现有的工具使用基准数据集存在评估场景不足和评估成本高的问题。为了解决这些问题，本文提出了一个多粒度工具使用基准MTU-Bench，涵盖五种工具使用场景。实验结果表明，MTU-Bench在提升语言模型工具使用能力方面效果显著。'}}}, {'id': 'https://huggingface.co/papers/2410.11795', 'title': 'Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices', 'url': 'https://huggingface.co/papers/2410.11795', 'abstract': 'As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey', 'score': 16, 'issue_id': 124, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'e1a8ba0c4193aa99', 'authors': ['Zhiyuan Ma', 'Yuzhu Zhang', 'Guoli Jia', 'Liangliang Zhao', 'Yichao Ma', 'Mingjie Ma', 'Gaofeng Liu', 'Kaiyan Zhang', 'Jianjun Li', 'Bowen Zhou'], 'affiliations': ['HUST', 'SJTU', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11795.jpg', 'data': {'categories': ['#video', '#survey', '#training', '#inference', '#diffusion', '#architecture', '#multimodal', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Эффективные диффузионные модели: от теории к практике', 'desc': 'Эта статья представляет собой обзор диффузионных моделей, одного из самых популярных и востребованных типов генеративных моделей последних лет. Авторы предоставляют комплексный анализ принципов работы и эффективных практик применения диффузионных моделей в различных задачах, таких как синтез изображений, генерация видео и молекулярный дизайн. Особое внимание уделяется архитектуре моделей, методам обучения, быстрому выводу и надежному развертыванию. Статья призвана помочь исследователям и практикам быстро понять и применить диффузионные модели в новых сценариях.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models: Efficiency in Generative Tasks', 'desc': 'Diffusion models have become a popular choice for generative tasks like image and video creation due to their strong theoretical foundations and practical applications. This paper reviews the design principles and methodologies that make diffusion models efficient and effective. It aims to provide a comprehensive understanding of these models to facilitate their application in new scenarios. The survey focuses on architecture design, model training, fast inference, and reliable deployment to guide future research and development.'}, 'zh': {'title': '扩散模型：高效生成的未来', 'desc': '扩散模型是近年来非常受欢迎的生成模型，广泛应用于图像合成、视频生成等任务。它们的成功主要归功于渐进的设计原则和高效的架构、训练、推理及部署方法。本文综述了这些原则和实践，提供了一个以效率为导向的新视角。通过这种方式，帮助读者更好地理解和应用扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2410.11096', 'title': 'SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI', 'url': 'https://huggingface.co/papers/2410.11096', 'abstract': "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'f1b69c45d56f76d8', 'authors': ['Yu Yang', 'Yuzhou Nie', 'Zhun Wang', 'Yuheng Tang', 'Wenbo Guo', 'Bo Li', 'Dawn Song'], 'affiliations': ['University of California, Berkeley', 'University of California, Los Angeles', 'University of California, Santa Barbara', 'University of Illinois Urbana-Champaign', 'Virtue AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11096.jpg', 'data': {'categories': ['#security', '#data', '#plp', '#agents', '#benchmark', '#optimization', '#synthetic'], 'emoji': '🛡️', 'ru': {'title': 'SecCodePLT: Новый стандарт оценки безопасности AI-кодогенераторов', 'desc': 'Статья представляет SecCodePLT - новую платформу для оценки рисков безопасности генеративных моделей AI для кода. Авторы разработали методологию создания данных, сочетающую экспертную оценку и автоматическую генерацию, а также внедрили динамические метрики оценки. SecCodePLT позволяет оценивать как потенциал моделей генерировать небезопасный код, так и их полезность для кибератак. Эксперименты показали превосходство SecCodePLT над существующими бенчмарками в выявлении рисков безопасности современных моделей кодогенерации.'}, 'en': {'title': 'SecCodePLT: Elevating Code GenAI Security Evaluation', 'desc': "The paper introduces SecCodePLT, a new platform designed to evaluate the security risks of Code GenAI models more effectively. It addresses the limitations of existing benchmarks by combining expert input with automatic data generation to ensure high-quality, large-scale data for insecure code evaluation. The platform also uses dynamic evaluation metrics, such as test cases, to assess the models' ability to generate executable attacks in a real environment. Extensive experiments demonstrate that SecCodePLT surpasses current benchmarks in identifying security risks, revealing significant vulnerabilities in advanced coding agents like Cursor."}, 'zh': {'title': 'SecCodePLT：提升代码生成AI安全评估的新平台', 'desc': '这篇论文介绍了一个名为SecCodePLT的新平台，用于评估代码生成AI的安全风险。SecCodePLT通过结合专家和自动生成的方法，创建高质量且大规模的数据集，并使用动态评估方法来检测不安全代码和网络攻击的生成能力。与现有的基准相比，SecCodePLT在安全相关性上表现更好，并能更准确地识别最先进模型的安全风险。研究还首次在先进的代码代理Cursor中发现了显著的安全风险。'}}}, {'id': 'https://huggingface.co/papers/2410.09704', 'title': 'EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation', 'url': 'https://huggingface.co/papers/2410.09704', 'abstract': 'Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '5bf065e3ea6b9f7a', 'authors': ['Milos Vukadinovic', 'Xiu Tang', 'Neal Yuan', 'Paul Cheng', 'Debiao Li', 'Susan Cheng', 'Bryan He', 'David Ouyang'], 'affiliations': ['Biomedical Imaging Research Institute, Cedars-Sinai Medical Center, Los Angeles, CA', 'Department of Bioengineering, University of California Los Angeles, Los Angeles, CA', 'Department of Cardiology, Smidt Heart Institute, Cedars-Sinai Medical Center, Los Angeles, CA', 'Department of Computer Science, Stanford University, Stanford, CA', 'Department of Medicine, University of California, San Francisco, CA', 'Division of Artificial Intelligence in Medicine, Cedars-Sinai Medical Center, Los Angeles, CA', 'Division of Cardiology, Department of Medicine, Stanford University, Palo Alto, CA', 'Division of Cardiology, San Francisco Veterans Affairs Medical Center, San Francisco, CA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09704.jpg', 'data': {'categories': ['#science', '#video', '#dataset', '#cv', '#healthcare', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#multimodal'], 'emoji': '❤️', 'ru': {'title': 'EchoPrime: революция в автоматизированной интерпретации эхокардиограмм', 'desc': 'EchoPrime - это многозадачная модель искусственного интеллекта для анализа эхокардиограмм, обученная на более чем 12 миллионах пар видео-отчетов. Она использует контрастное обучение для создания единой модели вложений для всех стандартных проекций в комплексном эхокардиографическом исследовании. EchoPrime применяет классификацию проекций и анатомическую модель внимания для точной интерпретации взаимосвязи между эхокардиографическими проекциями и анатомическими структурами. Модель превосходит существующие подходы в 23 различных задачах оценки формы и функции сердца, что было подтверждено в двух независимых медицинских системах.'}, 'en': {'title': 'EchoPrime: Revolutionizing Heart Imaging with Multi-View AI', 'desc': 'EchoPrime is a new AI model designed to improve echocardiography by integrating information from multiple ultrasound views, unlike traditional single-view models. It uses contrastive learning to create a unified representation of echocardiogram data, capturing both common and rare cardiac conditions. The model employs view-classification and anatomic attention to accurately interpret the relationship between different views and heart structures. EchoPrime has demonstrated superior performance across various benchmarks, offering a more comprehensive and precise tool for cardiac assessment.'}, 'zh': {'title': 'EchoPrime：多视角心脏超声AI的革新', 'desc': 'EchoPrime 是一个多视角、视图知情的视频语言基础模型，旨在解决传统单视角AI模型的局限性。它通过对比学习训练一个统一的嵌入模型，能够处理标准心脏超声检查中的所有视图。EchoPrime 利用视图分类和解剖注意模型，准确映射超声视图与解剖结构之间的关系。经过严格的临床评估，EchoPrime 在心脏形态和功能的23个基准测试中表现优异，能够帮助医生进行自动化的初步评估。'}}}, {'id': 'https://huggingface.co/papers/2410.11805', 'title': 'NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models', 'url': 'https://huggingface.co/papers/2410.11805', 'abstract': 'Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '9823a7c3dbc76e44', 'authors': ['Han Han', 'Tong Zhu', 'Xiang Zhang', 'Mengsong Wu', 'Hao Xiong', 'Wenliang Chen'], 'affiliations': ['Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11805.jpg', 'data': {'categories': ['#dataset', '#training', '#rag', '#rl', '#optimization', '#benchmark', '#synthetic'], 'emoji': '🛠️', 'ru': {'title': 'NesTools: новый стандарт для оценки вложенного обучения инструментам в LLM', 'desc': 'Статья представляет новый датасет NesTools для оценки способностей больших языковых моделей (LLM) к обучению вложенному использованию инструментов. NesTools включает метод автоматической генерации данных для создания крупномасштабных вложенных вызовов инструментов с различными структурами вложенности. Датасет прошел ручную проверку и доработку, что обеспечивает его высокое качество и соответствие реальным сценариям. Эксперименты на 22 LLM с использованием NesTools показали, что современные модели все еще испытывают трудности со сложными задачами вложенного обучения инструментам.'}, 'en': {'title': "NesTools: Elevating LLMs' Nested Tool Learning Evaluation", 'desc': "The paper introduces NesTools, a new benchmark designed to evaluate the nested tool learning capabilities of large language models (LLMs). NesTools uses an innovative automatic data generation method to create complex nested tool call scenarios, which are then manually reviewed to ensure high quality and real-world relevance. The study highlights that current LLMs struggle with these complex tasks, as demonstrated through extensive experiments on 22 different models. This work aims to fill the gap in existing research by providing a comprehensive dataset for assessing LLMs' ability to handle nested tool learning."}, 'zh': {'title': 'NesTools：提升嵌套工具学习的全新基准', 'desc': '这篇论文介绍了一种名为NesTools的新方法，用于评估大型语言模型（LLMs）的嵌套工具学习能力。NesTools通过自动生成大量不同嵌套结构的工具调用数据，填补了现有基准中缺乏相关数据实例的空白。经过人工审核和改进，数据集质量高且与现实场景紧密结合。实验表明，当前的LLMs在处理复杂的嵌套工具学习任务时仍然存在困难。'}}}, {'id': 'https://huggingface.co/papers/2410.10934', 'title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'url': 'https://huggingface.co/papers/2410.10934', 'abstract': 'Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.', 'score': 10, 'issue_id': 127, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'a25e89cb7be0bed4', 'authors': ['Mingchen Zhuge', 'Changsheng Zhao', 'Dylan Ashley', 'Wenyi Wang', 'Dmitrii Khizbullin', 'Yunyang Xiong', 'Zechun Liu', 'Ernie Chang', 'Raghuraman Krishnamoorthi', 'Yuandong Tian', 'Yangyang Shi', 'Vikas Chandra', 'Jürgen Schmidhuber'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10934.jpg', 'data': {'categories': ['#interpretability', '#plp', '#agents', '#benchmark', '#optimization', '#alignment', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Агент судит агента: новый подход к оценке ИИ-систем', 'desc': 'Статья представляет новый подход к оценке агентных систем искусственного интеллекта - Agent-as-a-Judge. Этот метод использует агентные системы для оценки других агентных систем, что позволяет получать промежуточную обратную связь на протяжении всего процесса решения задачи. Авторы применяют Agent-as-a-Judge к задаче генерации кода и представляют новый бенчмарк DevAI, содержащий 55 реалистичных задач по автоматизированной разработке ИИ. Результаты показывают, что Agent-as-a-Judge значительно превосходит LLM-as-a-Judge и сопоставим с оценкой человека.'}, 'en': {'title': '"Agent-as-a-Judge: Revolutionizing Evaluation for Agentic Systems"', 'desc': 'The paper introduces the Agent-as-a-Judge framework, which uses agentic systems to evaluate other agentic systems, providing intermediate feedback throughout the task-solving process. This approach is applied to code generation and is tested using a new benchmark called DevAI, which includes 55 realistic AI development tasks with detailed annotations. The framework is shown to outperform existing evaluation methods like LLM-as-a-Judge and matches human evaluation reliability. Overall, Agent-as-a-Judge offers a significant advancement in providing dynamic and scalable feedback for agentic systems.'}, 'zh': {'title': '代理即评判者：现代代理系统评估的新方向', 'desc': '传统的评估方法对代理系统来说不够完善，因为它们要么只关注最终结果而忽略了代理系统的逐步特性，要么需要大量的人工劳动。为了解决这个问题，我们提出了“代理即评判者”框架，利用代理系统来评估代理系统。这种方法是“LLM即评判者”框架的自然扩展，加入了代理特性，能够在整个任务解决过程中提供中间反馈。我们应用“代理即评判者”于代码生成任务，并引入了新的基准DevAI，证明其在评估中表现优于传统方法。'}}}, {'id': 'https://huggingface.co/papers/2410.11419', 'title': 'GS^3: Efficient Relighting with Triple Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.11419', 'abstract': 'We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.', 'score': 10, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '8aa8e4a555a54086', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#cv', '#training', '#open_source', '#architecture', '#synthetic', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Реалистичный рендеринг с динамическим освещением на основе гауссианов', 'desc': 'Статья представляет новый метод синтеза изображений с изменением освещения и ракурса в реальном времени. Авторы используют представление на основе пространственных и угловых гауссианов, а также процесс тройного сплаттинга. Для описания сложных визуальных эффектов применяется функция отражения, состоящая из ламбертовской составляющей и смеси угловых гауссианов. Метод демонстрирует высокое качество результатов на разнообразных объектах при быстром обучении и рендеринге.'}, 'en': {'title': '"Illuminate and Transform: Real-Time View Synthesis with Gaussian Magic"', 'desc': 'This paper introduces a novel method for creating high-quality images with new lighting and viewpoints using a spatial and angular Gaussian representation combined with a triple splatting process. The approach models complex appearances by using a Lambertian and angular Gaussian mixture reflectance function for each spatial Gaussian. To handle self-shadowing, spatial Gaussians are projected towards the light source, with shadow values refined by a small neural network, while another network adjusts for global illumination effects. The method is tested on diverse samples and achieves fast training and rendering times, outperforming existing techniques in quality and performance.'}, 'zh': {'title': '实时高质量光照合成的新方法', 'desc': '这篇论文介绍了一种基于空间和角度高斯的表示方法，以及三重喷溅过程，用于从多视点光照输入图像中实时生成高质量的新光照和视图合成。为了描述复杂的外观，作者使用了朗伯反射加上角度高斯混合作为每个空间高斯的有效反射函数。为了生成自阴影，所有空间高斯被喷溅到光源方向以获得阴影值，并通过一个小型多层感知器进一步优化。为了补偿全局光照等其他效果，另一个网络被训练来计算并添加每个空间高斯的RGB值。'}}}, {'id': 'https://huggingface.co/papers/2410.09754', 'title': 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.09754', 'abstract': "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.", 'score': 7, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '04b27b8dbf829651', 'authors': ['Hojoon Lee', 'Dongyoon Hwang', 'Donghu Kim', 'Hyunseung Kim', 'Jun Jet Tai', 'Kaushik Subramanian', 'Peter R. Wurman', 'Jaegul Choo', 'Peter Stone', 'Takuma Seno'], 'affiliations': ['Coventry University', 'KAIST', 'Sony AI', 'UT Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09754.jpg', 'data': {'categories': ['#cv', '#training', '#rl', '#optimization', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'SimBa: Простота и масштабируемость для глубокого обучения с подкреплением', 'desc': 'Статья представляет архитектуру SimBa для масштабирования параметров в глубоком обучении с подкреплением (RL). SimBa включает нормализацию наблюдений, резидуальный блок прямой связи и нормализацию слоев для внедрения смещения к простоте. Эксперименты показывают, что SimBa улучшает эффективность выборки для различных алгоритмов RL, включая off-policy, on-policy и unsupervised методы. Интеграция SimBa в алгоритм SAC позволяет достичь или превзойти современные методы RL на различных средах с высокой вычислительной эффективностью.'}, 'en': {'title': '"SimBa: Scaling Simplicity in Deep Reinforcement Learning"', 'desc': 'The paper introduces SimBa, a new architecture for deep reinforcement learning (RL) that scales up network parameters while maintaining simplicity to avoid overfitting. SimBa includes an observation normalization layer, a residual feedforward block, and a layer normalization to enhance model performance. By using SimBa, various deep RL algorithms show improved sample efficiency and computational effectiveness. The architecture proves its versatility by matching or surpassing state-of-the-art methods across different environments and tasks.'}, 'zh': {'title': 'SimBa：通过简单性偏差提升深度强化学习', 'desc': '近年来，计算机视觉和自然语言处理领域通过增加网络参数数量取得了显著进展，尽管传统理论认为更大的网络容易过拟合。这些大型网络通过引入简单性偏差的组件来避免过拟合，指导模型朝向简单且可推广的解决方案。在深度强化学习中，网络设计和扩展的研究较少。SimBa架构通过注入简单性偏差来扩展深度强化学习中的参数，提高了多种深度强化学习算法的样本效率。'}}}, {'id': 'https://huggingface.co/papers/2410.08001', 'title': 'Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.08001', 'abstract': 'The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/', 'score': 3, 'issue_id': 122, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'f9a57a673dfbc6f6', 'authors': ['Qingwen Bu', 'Hongyang Li', 'Li Chen', 'Jisong Cai', 'Jia Zeng', 'Heming Cui', 'Maoqing Yao', 'Yu Qiao'], 'affiliations': ['AgiBot', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08001.jpg', 'data': {'categories': ['#reasoning', '#training', '#agi', '#inference', '#games', '#open_source', '#diffusion', '#small_models', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboDual: Объединение сильных сторон обобщенной и специализированной политик для универсальных роботов', 'desc': 'RoboDual - это синергетическая двойная система, объединяющая преимущества обобщенной и специализированной политик для роботов. Обобщенная политика на основе vision-language-action обеспечивает высокоуровневое понимание задач, а специализированная политика на основе диффузионного трансформера осуществляет точные многошаговые действия. RoboDual демонстрирует значительное улучшение производительности по сравнению с OpenVLA как в реальных, так и в симулированных средах. Система сохраняет высокую эффективность даже при использовании всего 5% демонстрационных данных и обеспечивает более высокую частоту управления при развертывании в реальном мире.'}, 'en': {'title': 'RoboDual: Merging Generalist Flexibility with Specialist Precision', 'desc': 'The paper introduces RoboDual, a dual-system approach that combines the strengths of both generalist and specialist policies for robotic systems. The generalist policy uses a vision-language-action model to provide high-level task understanding, while the specialist policy, based on a diffusion transformer, focuses on efficient multi-step action rollouts. RoboDual significantly improves performance in real-world settings and specific benchmarks by using a specialist policy with a small number of trainable parameters. This system achieves high efficiency and adaptability, even with limited demonstration data, and enhances control frequency in practical applications.'}, 'zh': {'title': 'RoboDual：通用与专业策略的完美结合', 'desc': 'RoboDual 是一个结合通用策略和专业策略的双系统，旨在提高机器人在多变环境中的适应性。通用策略利用大规模跨体数据，提供高层次的任务理解，而专业策略则专注于特定任务的高效执行。RoboDual 通过引入基于扩散变压器的专业策略，在真实环境中实现了显著的性能提升。该系统在仅使用少量示范数据的情况下，仍能保持高效的控制频率。'}}}, {'id': 'https://huggingface.co/papers/2410.06593', 'title': 'Towards Natural Image Matting in the Wild via Real-Scenario Prior', 'url': 'https://huggingface.co/papers/2410.06593', 'abstract': 'Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.', 'score': 2, 'issue_id': 126, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '656804d454a782ca', 'authors': ['Ruihao Xia', 'Yu Liang', 'Peng-Tao Jiang', 'Hao Zhang', 'Qianru Sun', 'Yang Tang', 'Bo Li', 'Pan Zhou'], 'affiliations': ['East China University of Science and Technology', 'Singapore Management University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.06593.jpg', 'data': {'categories': ['#dataset', '#cv', '#training', '#graphs', '#open_source', '#architecture', '#synthetic'], 'emoji': '✂️', 'ru': {'title': 'Революция в интерактивном маттинге: от синтетики к реальности', 'desc': 'В статье представлен новый датасет COCO-Matting для задачи интерактивного маттинга изображений, основанный на реальных сложных сценах из COCO. Авторы предлагают архитектуру SEMat, которая улучшает существующие методы на основе SAM, используя feature-aligned transformer и matte-aligned decoder. Введены новые функции потерь для обучения модели, включая регуляризацию и trimap loss. Эксперименты на семи различных датасетах показывают превосходство предложенного метода в задаче интерактивного маттинга естественных изображений.'}, 'en': {'title': 'Revolutionizing Image Matting with Real-World Data and Advanced Architectures', 'desc': 'The paper introduces a new dataset called COCO-Matting, which enhances the generalization of matting models to complex real-world scenes by using real images from the COCO dataset. It addresses the limitations of models trained on synthetic data by converting semantic segmentation masks into matting labels. The authors propose a novel model, SEMat, which includes a feature-aligned transformer and a matte-aligned decoder to improve edge and transparency feature extraction. Their approach, validated through extensive experiments, shows superior performance in interactive image matting, leveraging both new training objectives and architecture improvements.'}, 'zh': {'title': 'SEMat：提升交互式抠图的精度与泛化能力', 'desc': '这篇论文提出了一种新的抠图数据集COCO-Matting，基于COCO数据集构建，旨在解决合成数据训练模型在复杂场景中泛化能力不足的问题。COCO-Matting通过从COCO中选择真实世界的复杂图像，并将语义分割掩码转换为抠图标签，包含了38,251个人物实例级别的alpha mattes。为了充分利用预训练的SAM模型，作者提出了SEMat，通过特征对齐的transformer和抠图对齐的解码器来提取细粒度的边缘和透明度特征，并将粗略的掩码转换为高精度的抠图。实验结果表明，SEMat在七个不同的数据集上表现优异，证明了其在交互式自然图像抠图中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.09745', 'title': 'Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt', 'url': 'https://huggingface.co/papers/2410.09745', 'abstract': "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.", 'score': 2, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '9e1731e2bbd0b081', 'authors': ['Chengguang Gan', 'Tatsunori Mori'], 'affiliations': ['Yokohama National University, Japan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09745.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#optimization', '#interpretability'], 'emoji': '🔄', 'ru': {'title': 'Взаимное усиление классификации слов и текста улучшает понимание языка', 'desc': 'Статья исследует взаимоусиливающий эффект (MRE) между классификацией на уровне слов и текста в задачах классификации текста. Авторы проводят эмпирические эксперименты на 21 наборе данных MRE Mix для подтверждения теории MRE. Результаты сравнительных экспериментов с использованием тонкой настройки подтверждают существование MRE. Применение MRE к обучению с подсказками, используя информацию на уровне слов в качестве вербализатора, значительно улучшило F1-меру на 18 из 21 набора данных.'}, 'en': {'title': 'Boosting Text Understanding: The Power of Word-Level Insights', 'desc': "The paper explores the Mutual Reinforcement Effect (MRE), which suggests that word-level and text-level classifications can improve each other's performance in text classification tasks. Through empirical experiments on 21 datasets, the study demonstrates the presence and impact of MRE, showing that word-level information can enhance text-level predictions. The researchers also apply MRE to prompt learning, using word-level data to improve the model's text-level classification accuracy. The experiments show a significant improvement in F1-scores, confirming that word-level insights can enhance overall text comprehension by the model."}, 'zh': {'title': '词级与文本级分类的相互增强效应', 'desc': '这篇论文研究了词级和文本级分类在文本分类任务中的相互增强效应。通过实验证明了这种效应的存在，并展示了其对模型性能的影响。研究还将这种效应应用于提示学习，利用词级信息来增强文本级分类标签的预测。最终实验结果显示，在21个数据集中的18个，F1分数显著超过基线，验证了词级信息对语言模型理解文本整体的提升作用。'}}}, {'id': 'https://huggingface.co/papers/2410.11619', 'title': 'MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval', 'url': 'https://huggingface.co/papers/2410.11619', 'abstract': 'Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.', 'score': 0, 'issue_id': 130, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'dd603cadf5349b0c', 'authors': ['Reno Kriz', 'Kate Sanders', 'David Etter', 'Kenton Murray', 'Cameron Carpenter', 'Kelly Van Ochten', 'Hannah Recknor', 'Jimena Guallar-Blasco', 'Alexander Martin', 'Ronald Colaianni', 'Nolan King', 'Eugene Yang', 'Benjamin Van Durme'], 'affiliations': ['Human Language Technology Center of Excellence, Johns Hopkins University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11619.jpg', 'data': {'categories': ['#science', '#video', '#survey', '#dataset', '#multilingual', '#graphs', '#benchmark', '#games', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'MultiVENT 2.0: Новый вызов в мире мультимодального поиска видео', 'desc': 'MultiVENT 2.0 - это новый крупномасштабный многоязычный бенчмарк для поиска видео, ориентированный на события. Он включает более 218 000 новостных видео и 3 906 запросов, нацеленных на конкретные мировые события. Бенчмарк требует от систем использования визуального контента, аудио, встроенного текста и текстовых метаданных видео. Предварительные результаты показывают, что современные модели компьютерного зрения и обработки естественного языка значительно уступают в этой задаче, подчеркивая необходимость более надежных мультимодальных систем поиска.'}, 'en': {'title': 'Unlocking the Power of Multimodal Video Retrieval', 'desc': 'The paper introduces MultiVENT 2.0, a new benchmark for video retrieval that focuses on multilingual and event-centric content, addressing the limitations of existing datasets. It includes over 218,000 news videos and 3,906 queries that require systems to utilize visual, audio, and text data effectively. Initial tests reveal that current vision-language models struggle with this complex task, highlighting the need for more advanced multimodal retrieval systems. This research emphasizes the importance of improving video retrieval to enhance multimodal content understanding and generation.'}, 'zh': {'title': '突破多模态视频检索的瓶颈', 'desc': '这篇论文介绍了一个名为MultiVENT 2.0的大规模多语言事件中心视频检索基准，旨在解决现有视频检索数据集范围有限的问题。该基准包含超过218,000个新闻视频和3,906个针对特定世界事件的查询，要求系统利用视频的视觉内容、音频、嵌入文本和文本元数据来完成任务。初步结果显示，最先进的视觉语言模型在这个任务上表现不佳，而其他方法虽然有潜力，但仍不足以解决问题。这表明需要更强大的多模态检索系统，以实现对多模态内容的理解和生成。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (2)', '#alignment (1)', '#architecture (12)', '#audio', '#benchmark (8)', '#cv (4)', '#data (3)', '#dataset (10)', '#diffusion (2)', '#ethics', '#games (3)', '#graphs (2)', '#hallucinations (1)', '#healthcare (2)', '#inference (3)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (4)', '#open_source (7)', '#optimization (9)', '#plp (2)', '#rag (1)', '#reasoning (3)', '#rl (2)', '#rlhf', '#robotics (1)', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (5)', '#training (12)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-16 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-16 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-16 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    