
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. October 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xc2ddb801d2228a78 { background: url("https://hfday.ru/img/20241014/c2ddb801d2228a78.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc2ddb801d2228a78:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc2ddb801d2228a78 { background: url("https://hfday.ru/img/20241014/c2ddb801d2228a78.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc2ddb801d2228a78:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x214a55e4bcf12eb6 { background: url("https://hfday.ru/img/20241012/214a55e4bcf12eb6.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x214a55e4bcf12eb6:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x214a55e4bcf12eb6 { background: url("https://hfday.ru/img/20241012/214a55e4bcf12eb6.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x214a55e4bcf12eb6:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe8c698bf76856366 { background: url("https://hfday.ru/img/20241014/e8c698bf76856366.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe8c698bf76856366:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe8c698bf76856366 { background: url("https://hfday.ru/img/20241014/e8c698bf76856366.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe8c698bf76856366:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfdf872015ec393aa { background: url("https://hfday.ru/img/20240622/fdf872015ec393aa.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfdf872015ec393aa:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfdf872015ec393aa { background: url("https://hfday.ru/img/20240622/fdf872015ec393aa.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfdf872015ec393aa:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x25a9176442c65ae3 { background: url("https://hfday.ru/img/20241015/25a9176442c65ae3.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x25a9176442c65ae3:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x25a9176442c65ae3 { background: url("https://hfday.ru/img/20241015/25a9176442c65ae3.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x25a9176442c65ae3:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xb7f807e5ff9e1614 { background: url("https://hfday.ru/img/20241014/b7f807e5ff9e1614.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xb7f807e5ff9e1614:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xb7f807e5ff9e1614 { background: url("https://hfday.ru/img/20241014/b7f807e5ff9e1614.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xb7f807e5ff9e1614:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x44b55db6ab50c3d4 { background: url("https://hfday.ru/img/20241015/44b55db6ab50c3d4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x44b55db6ab50c3d4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x44b55db6ab50c3d4 { background: url("https://hfday.ru/img/20241015/44b55db6ab50c3d4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x44b55db6ab50c3d4:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1a8ba0c4193aa99 { background: url("https://hfday.ru/img/20241015/e1a8ba0c4193aa99.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1a8ba0c4193aa99:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1a8ba0c4193aa99 { background: url("https://hfday.ru/img/20241015/e1a8ba0c4193aa99.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1a8ba0c4193aa99:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xf1b69c45d56f76d8 { background: url("https://hfday.ru/img/20241014/f1b69c45d56f76d8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xf1b69c45d56f76d8:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xf1b69c45d56f76d8 { background: url("https://hfday.ru/img/20241014/f1b69c45d56f76d8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xf1b69c45d56f76d8:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x5bf065e3ea6b9f7a { background: url("https://hfday.ru/img/20241013/5bf065e3ea6b9f7a.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x5bf065e3ea6b9f7a:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x5bf065e3ea6b9f7a { background: url("https://hfday.ru/img/20241013/5bf065e3ea6b9f7a.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x5bf065e3ea6b9f7a:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x9823a7c3dbc76e44 { background: url("https://hfday.ru/img/20241015/9823a7c3dbc76e44.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x9823a7c3dbc76e44:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x9823a7c3dbc76e44 { background: url("https://hfday.ru/img/20241015/9823a7c3dbc76e44.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x9823a7c3dbc76e44:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa25e89cb7be0bed4 { background: url("https://hfday.ru/img/20241014/a25e89cb7be0bed4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa25e89cb7be0bed4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa25e89cb7be0bed4 { background: url("https://hfday.ru/img/20241014/a25e89cb7be0bed4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa25e89cb7be0bed4:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8aa8e4a555a54086 { background: url("https://hfday.ru/img/20241015/8aa8e4a555a54086.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8aa8e4a555a54086:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8aa8e4a555a54086 { background: url("https://hfday.ru/img/20241015/8aa8e4a555a54086.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8aa8e4a555a54086:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
            .category-option-container {
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 –æ–∫—Ç—è–±—Ä—è</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-15.html">‚¨ÖÔ∏è <span id="prev-date">15.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-17.html">‚û°Ô∏è <span id="next-date">17.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">üìà –¢–æ–ø –∑–∞ –º–µ—Å—è—Ü</a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 16', 'zh': '10Êúà16Êó•'};
        let feedDateNext = {'ru': '17.10', 'en': '10/17', 'zh': '10Êúà17Êó•'};
        let feedDatePrev = {'ru': '15.10', 'en': '10/15', 'zh': '10Êúà15Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '–°—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'Published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.10814', 'title': 'Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free', 'url': 'https://huggingface.co/papers/2410.10814', 'abstract': 'While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.', 'score': 48, 'issue_id': 124, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': 'üß†', 'ru': {'title': 'MoE LLM: –°–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts (MoE) LLM –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –Ω–µ —Ç—Ä–µ–±—É—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ê–Ω–∞–ª–∏–∑ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –≤–µ—Å–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ MoE (RW) –¥–æ–ø–æ–ª–Ω—è—é—Ç —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (HS) LLM, —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ MoEE, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π RW –∏ HS, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —á–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ 6 –∑–∞–¥–∞—á–∞—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å 20 –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Massive Text Embedding Benchmark (MTEB).'}, 'en': {'title': 'Unlocking the Embedding Power of MoE LLMs: No Finetuning Needed!', 'desc': 'The paper investigates the potential of Mixture-of-Experts (MoE) large language models (LLMs) as effective embedding models without additional finetuning. It reveals that the expert routers in MoE LLMs can be used directly for embedding tasks, showing strong performance across various tasks. The study finds that the routing weights (RW) in MoE models complement the hidden states (HS) of LLMs, offering robustness to prompt variations and focusing on high-level semantics. By combining RW and HS, the proposed MoEE model achieves superior results, as demonstrated on multiple datasets from the Massive Text Embedding Benchmark (MTEB).'}, 'zh': {'title': '‰∏ìÂÆ∂Ê∑∑ÂêàÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂµåÂÖ•ËÉΩÂäõÁöÑÊñ∞Ë∑ØÂæÑ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÁîüÊàê‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂‰ªÖËß£Á†ÅÊû∂ÊûÑÈôêÂà∂‰∫ÜÂÖ∂‰Ωú‰∏∫ÂµåÂÖ•Ê®°ÂûãÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâLLMÔºåÂèëÁé∞ÂÖ∂‰∏ìÂÆ∂Ë∑ØÁî±Âô®Êó†ÈúÄÂæÆË∞ÉÂç≥ÂèØÂú®Â§öÁßçÂµåÂÖ•‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•Ω„ÄÇMoEÁöÑË∑ØÁî±ÊùÉÈáçÔºàRWÔºâ‰∏éÈöêËóèÁä∂ÊÄÅÔºàHSÔºâ‰∫íË°•Ôºå‰∏îÂØπÊèêÁ§∫ÈÄâÊã©Êõ¥ÂÖ∑È≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑMoEEÁªìÂêà‰∫ÜRWÂíåHSÔºåÊòæËëóÊèêÂçá‰∫ÜÂµåÂÖ•‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ'}}, 'hash': 'c2ddb801d2228a78', 'pub_date_card': {'ru': '14 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 14', 'zh': '10Êúà14Êó•'}}, {'id': 'https://huggingface.co/papers/2410.09342', 'title': 'LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models', 'url': 'https://huggingface.co/papers/2410.09342', 'abstract': 'Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.', 'score': 37, 'issue_id': 125, 'pub_date': '2024-10-12', 'pub_date_ru': '12 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#long_context', '#rag'], 'emoji': 'üìÑ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LLM√óMapReduce, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∞ –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –ø—Ä–æ—Ç–æ–∫–æ–ª —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM√óMapReduce –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º LLM.'}, 'en': {'title': 'Divide and Conquer: Mastering Long Texts with LLMtimesMapReduce', 'desc': "The paper introduces a new method called LLMtimesMapReduce to help large language models (LLMs) understand very long texts without needing extra training. It uses a divide-and-conquer approach, breaking the text into smaller parts for the LLMs to process, and then combines the results to form a complete understanding. The main challenge is ensuring that important information isn't lost when the text is split, which the authors address with a structured information protocol and a confidence calibration mechanism. Tests show that this method works better than other similar tools and can be used with different LLMs."}, 'zh': {'title': 'ÂàÜËÄåÊ≤ª‰πãÔºöÊó†ËÆ≠ÁªÉÊ°ÜÊû∂Â§ÑÁêÜË∂ÖÈïøÊñáÊú¨', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁß∞‰∏∫LLMtimesMapReduceÔºåÁî®‰∫éÂ§ÑÁêÜË∂ÖÈïøÊñáÊú¨„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜËÄåÊ≤ª‰πãÁöÑÊñπÊ≥ïÔºåÂ∞ÜÊñáÊ°£ÂàÜÊàêÂ§ö‰∏™ÈÉ®ÂàÜ‰æõÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈòÖËØªÔºåÁÑ∂ÂêéÊ±áÊÄª‰∏≠Èó¥Á≠îÊ°à‰ª•ÁîüÊàêÊúÄÁªàËæìÂá∫„ÄÇ‰∏ªË¶ÅÊåëÊàòÂú®‰∫éÂàÜÂâ≤ÊñáÊ°£Êó∂ÂèØËÉΩ‰∏¢Â§±ÈáçË¶ÅÁöÑÈïøË∑ùÁ¶ª‰ø°ÊÅØÔºåÂØºËá¥Ê®°ÂûãÂü∫‰∫éÂàÜÊÆµÊñáÊú¨‰∫ßÁîü‰∏çÂÆåÊï¥ÊàñÈîôËØØÁöÑÁ≠îÊ°à„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLMtimesMapReduceÂú®Â§ÑÁêÜÈïøÊñáÊú¨ÊñπÈù¢‰ºò‰∫éÂÖ∂‰ªñÂºÄÊ∫êÂíåÂïÜ‰∏öÈïø‰∏ä‰∏ãÊñáLLM„ÄÇ'}}, 'hash': '214a55e4bcf12eb6', 'pub_date_card': {'ru': '12 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 12', 'zh': '10Êúà12Êó•'}}, {'id': 'https://huggingface.co/papers/2410.10626', 'title': 'Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts', 'url': 'https://huggingface.co/papers/2410.10626', 'abstract': 'Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.', 'score': 36, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#dataset', '#medicine', '#multilingual'], 'emoji': 'üè•', 'ru': {'title': '–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ LLM: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ü–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª—å–Ω–æ—Å—Ç–∏ Mixture of Experts (MoE). –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ MoE —Å —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Post-MoE, –ø—Ä–∏–º–µ–Ω—è—é—â–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –≤ –ø–æ–∑–¥–Ω–∏—Ö —Å–ª–æ—è—Ö –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Breaking Language Barriers in Medical AI', 'desc': "The paper addresses the challenge of adapting medical Large Language Models (LLMs) to low-resource languages by constructing a high-quality medical dataset and analyzing its quality. It introduces a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing to enhance the model's generalization capabilities. Inspired by circuit theory, the authors propose the Post-MoE architecture, which applies sparse routing in later layers to improve language-specific performance while maintaining interpretability. The approach allows the model to scale efficiently to 50 languages by using language family experts, leveraging linguistic priors without increasing parameters."}, 'zh': {'title': 'Ë∑®ËØ≠Ë®ÄÂåªÁñóÊ®°ÂûãÔºöÁ™ÅÁ†¥ËØ≠Ë®ÄÈöúÁ¢çÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈÄÇÂ∫îÊú¨Âú∞ËØ≠Ë®ÄÔºå‰ª•ÂáèÂ∞ëËé∑ÂèñÂåªÁñóÊúçÂä°ÁöÑÈöúÁ¢ç„ÄÇÁ†îÁ©∂‰∏≠Ôºå‰ΩúËÄÖÈ¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÂåªÁñóÊï∞ÊçÆÈõÜÔºåÂπ∂ËøõË°å‰∫ÜÂàÜÊûê‰ª•Á°Æ‰øùÂÖ∂Ë¥®Èáè„ÄÇÈÄöËøá‰ΩøÁî®‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊ®°ÂùóÂåñÊñπÊ≥ïÔºåÁ†îÁ©∂‰∫ÜÂ§öËØ≠Ë®ÄLLMÁöÑÂÜÖÈÉ®‰ø°ÊÅØÊµÅÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑMoEË∑ØÁî±ÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÊèêÈ´ò‰∫ÜÂ§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËß£ÈáäÊÄß„ÄÇ'}}, 'hash': 'e8c698bf76856366', 'pub_date_card': {'ru': '14 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 14', 'zh': '10Êúà14Êó•'}}, {'id': 'https://huggingface.co/papers/2406.15786', 'title': 'What Matters in Transformers? Not All Attention is Needed', 'url': 'https://huggingface.co/papers/2406.15786', 'abstract': 'While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.', 'score': 27, 'issue_id': 121, 'pub_date': '2024-06-22', 'pub_date_ru': '22 –∏—é–Ω—è', 'data': {'categories': ['#architecture', '#inference'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LLM: –º–µ–Ω—å—à–µ —Å–ª–æ–µ–≤, —Ç–∞ –∂–µ –º–æ—â–Ω–æ—Å—Ç—å', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –±–∞–∑–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention layers) –æ–±–ª–∞–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–∞–ª–µ–Ω–∞ –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª—å Llama-2-70B –¥–æ—Å—Ç–∏–≥–ª–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ 48.4% –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –Ω–∞ 2.4% –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–ª–æ–≤–∏–Ω—ã —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ MLP, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ—â–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–æ–∫—Ä–∞—â–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Prune to Perform: Streamlining Transformers for Efficiency', 'desc': "This paper explores the redundancy in Transformer-based large language models, focusing on the MLP and Attention layers. The authors found that many attention layers are highly similar and can be pruned without significantly affecting performance, as demonstrated by a 48.4% speedup in Llama-2-70B with minimal performance loss. They also propose a method to jointly prune Attention and MLP layers, achieving substantial efficiency gains while maintaining most of the model's performance. This research offers insights into optimizing Transformer architectures for more efficient real-world applications."}, 'zh': {'title': '‰ºòÂåñTransformerÔºöÂáèÂ∞ëÂÜó‰ΩôÔºåÊèêÂçáÊïàÁéá', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂú®TransformerÊ®°Âûã‰∏≠‰∏çÂêåÊ®°ÂùóÁöÑÂÜó‰ΩôÈóÆÈ¢òÔºåÁâπÂà´ÊòØÊ≥®ÊÑèÂäõÂ±ÇÂíåMLPÂ±Ç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Ê≥®ÊÑèÂäõÂ±ÇÂú®Transformer‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÔºå‰ΩÜËÆ∏Â§öÂ±ÇÁöÑÁõ∏‰ººÂ∫¶ËøáÈ´òÔºåÂèØ‰ª•Ë¢´Ââ™ÊûùËÄå‰∏çÂΩ±ÂìçÊÄßËÉΩ„ÄÇÈÄöËøáÂâ™ÊûùÊ≥®ÊÑèÂäõÂ±ÇÔºåÊ®°ÂûãÁöÑÈÄüÂ∫¶ÂèØ‰ª•ÊòæËëóÊèêÈ´òÔºåËÄåÊÄßËÉΩÊçüÂ§±ÂæàÂ∞è„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêåÊó∂Ââ™ÊûùÊ≥®ÊÑèÂäõÂ±ÇÂíåMLPÂ±ÇÁöÑÊñπÊ≥ïÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊïàÁéá„ÄÇ'}}, 'hash': 'fdf872015ec393aa', 'pub_date_card': {'ru': '22 –∏—é–Ω—è', 'en': 'June 22', 'zh': '6Êúà22Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11779', 'title': 'MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation', 'url': 'https://huggingface.co/papers/2410.11779', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.', 'score': 24, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#cv', '#hallucinations', '#multimodal'], 'emoji': 'üîç', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ MLLM —Å–ø–æ—Å–æ–±–Ω—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö, –Ω–æ –≤ –∏—Ç–æ–≥–æ–≤–æ–º –≤—ã–≤–æ–¥–µ –º–æ–≥—É—Ç –∏—Ö –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–∏–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –ø—Ä–∏–æ—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è DeCo, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π –∑–Ω–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π.'}, 'en': {'title': 'DeCo: Correcting Hallucinations in Multimodal Models', 'desc': "This paper investigates why Multimodal Large Language Models (MLLMs) often produce hallucinations, where they generate incorrect outputs. The authors discover that while MLLMs can recognize visual objects in earlier layers, the language model's strong knowledge priors may suppress this visual information, causing hallucinations. To address this, they introduce a new method called dynamic correction decoding (DeCo), which adjusts the integration of knowledge in the final output layer. DeCo is versatile and can be used with different models and decoding strategies, significantly reducing hallucination rates in tests."}, 'zh': {'title': 'Âä®ÊÄÅÊ†°Ê≠£Ëß£Á†ÅÔºöÂáèÂ∞ëÂ§öÊ®°ÊÄÅÊ®°ÂûãÂπªËßâÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂ∏∏Â∏∏‰ºöÂá∫Áé∞ÂπªËßâÁé∞Ë±°Ôºå‰ΩÜÂÖ∂ÂéüÂõ†Â∞ö‰∏çÊòéÁ°Æ„ÄÇÊú¨ÊñáÈÄöËøáÂÆûËØÅÂàÜÊûêÂèëÁé∞ÔºåÂ∞ΩÁÆ°MLLMsÂú®ÊúÄÁªàËæìÂá∫‰∏≠ÈîôËØØÁîüÊàêÂØπË±°Ôºå‰ΩÜÂÆÉ‰ª¨ÂÆûÈôÖ‰∏äËÉΩÂ§üÂú®ÂâçÈù¢ÁöÑÂ±Ç‰∏≠ËØÜÂà´ËßÜËßâÂØπË±°„ÄÇÊàë‰ª¨Êé®ÊµãËøôÂèØËÉΩÊòØÁî±‰∫éËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫Áü•ËØÜÂÖàÈ™åÊäëÂà∂‰∫ÜËßÜËßâ‰ø°ÊÅØÔºåÂØºËá¥ÂπªËßâÁé∞Ë±°„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÊÄÅÊ†°Ê≠£Ëß£Á†ÅÊñπÊ≥ïÔºàDeCoÔºâÔºåÂèØ‰ª•Ëá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÂêàÈÄÇÁöÑÂâçÁΩÆÂ±ÇÔºåÂπ∂Â∞ÜÁü•ËØÜÊåâÊØî‰æãÊï¥ÂêàÂà∞ÊúÄÁªàÂ±Ç‰∏≠Ôºå‰ª•Ë∞ÉÊï¥ËæìÂá∫„ÄÇ'}}, 'hash': '25a9176442c65ae3', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.10816', 'title': 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions', 'url': 'https://huggingface.co/papers/2410.10816', 'abstract': 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.', 'score': 19, 'issue_id': 123, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#multimodal', '#video'], 'emoji': 'üé•', 'ru': {'title': 'LVD-2M: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç LVD-2M –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 2 –º–∏–ª–ª–∏–æ–Ω–∞ –≤–∏–¥–µ–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –±–æ–ª–µ–µ 10 —Å–µ–∫—É–Ω–¥, —Å–Ω—è—Ç—ã—Ö –æ–¥–Ω–∏–º –∫–∞–¥—Ä–æ–º –∏ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º—ã—Ö –ø–ª–æ—Ç–Ω—ã–º–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥–∏–∫—É –æ—Ç–±–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∏ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –ø–æ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': '"Unlocking the Future of Long Video Generation"', 'desc': 'The paper discusses the importance of high-quality datasets for training video generation models, especially for long videos. It introduces a new dataset, LVD-2M, which includes 2 million long-take videos, each over 10 seconds, with diverse content and detailed captions. The authors developed a pipeline to select these videos based on specific quality metrics and to generate temporally dense captions. This dataset aims to advance research in generating long videos with dynamic motions.'}, 'zh': {'title': 'Êé®Âä®ÈïøËßÜÈ¢ëÁîüÊàêÁ†îÁ©∂ÁöÑÊñ∞Êï∞ÊçÆÈõÜ', 'desc': 'ËøôÁØáËÆ∫ÊñáËÆ®ËÆ∫‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜË¥®ÈáèÂØπÊ®°ÂûãÊïàÊûúÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ΩúËÄÖÊåáÂá∫ÔºåÁé∞ÊúâÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂ§ßÂ§öÂü∫‰∫éÁü≠ËßÜÈ¢ëÁâáÊÆµÔºåËÄåÈïøËßÜÈ¢ëÁîüÊàêÁöÑÁ†îÁ©∂ÂèóÂà∞È´òË¥®ÈáèÈïøËßÜÈ¢ëÁº∫‰πèÁöÑÈôêÂà∂„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÈõÜÈÄâÊã©ÊµÅÁ®ãÔºå‰∏ìÊ≥®‰∫éÈïøËßÜÈ¢ëÁöÑË¥®ÈáèËØÑ‰º∞ÂíåÂØÜÈõÜÂ≠óÂπïÁîüÊàê„ÄÇÊúÄÁªàÔºå‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫LVD-2MÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´200‰∏áÊÆµÈïøËßÜÈ¢ëÔºåÂπ∂È™åËØÅ‰∫ÜÂÖ∂Âú®ÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}, 'hash': 'b7f807e5ff9e1614', 'pub_date_card': {'ru': '14 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 14', 'zh': '10Êúà14Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11710', 'title': 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.11710', 'abstract': 'Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.', 'score': 18, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#rlhf'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': '–ú–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–≤—ã–∫–æ–≤ LLM –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MTU-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–≤—ã–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). MTU-Bench –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø—è—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –¥–æ —Å–ª–æ–∂–Ω—ã—Ö –∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API GPT –∏–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MTU-Instruct –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'MTU-Bench: Elevating LLM Tool-Use Evaluation', 'desc': "The paper introduces MTU-Bench, a new benchmark designed to evaluate large language models (LLMs) on their ability to use tools effectively. MTU-Bench addresses the limitations of existing datasets by providing a multi-granularity approach, covering various tool-use scenarios such as single or multiple tools and turns, as well as out-of-distribution tasks. Unlike previous benchmarks, MTU-Bench relies solely on prediction results and ground truth for evaluation, avoiding costly GPT or human assessments. Additionally, the authors present MTU-Instruct, a dataset aimed at enhancing LLMs' tool-use capabilities, demonstrating its effectiveness through comprehensive experiments."}, 'zh': {'title': 'MTU-BenchÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÂíåÂÜ≥Á≠ñËÉΩÂäõ‰∏äÊúâ‰∫ÜÊòæËëóÊèêÂçáÔºåÂπ∂ËÉΩ‰∏éÁî®Êà∑ËøõË°åËá™ÁÑ∂ÂØπËØù„ÄÇÁé∞ÊúâÁöÑÂ∑•ÂÖ∑‰ΩøÁî®Âü∫ÂáÜÊï∞ÊçÆÈõÜÂ≠òÂú®ËØÑ‰º∞Âú∫ÊôØ‰∏çË∂≥ÂíåËØÑ‰º∞ÊàêÊú¨È´òÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÁ≤íÂ∫¶Â∑•ÂÖ∑‰ΩøÁî®Âü∫ÂáÜMTU-BenchÔºåÊ∂µÁõñ‰∫îÁßçÂ∑•ÂÖ∑‰ΩøÁî®Âú∫ÊôØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMTU-BenchÂú®ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÊñπÈù¢ÊïàÊûúÊòæËëó„ÄÇ'}}, 'hash': '44b55db6ab50c3d4', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11795', 'title': 'Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices', 'url': 'https://huggingface.co/papers/2410.11795', 'abstract': 'As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey', 'score': 16, 'issue_id': 124, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#cv', '#diffusion', '#inference', '#multimodal', '#survey', '#video'], 'emoji': 'üåÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–¥–Ω–æ–≥–æ –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–µ—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π –¥–∏–∑–∞–π–Ω. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–µ–π, –º–µ—Ç–æ–¥–∞–º –æ–±—É—á–µ–Ω–∏—è, –±—ã—Å—Ç—Ä–æ–º—É –≤—ã–≤–æ–¥—É –∏ –Ω–∞–¥–µ–∂–Ω–æ–º—É —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é. –°—Ç–∞—Ç—å—è –ø—Ä–∏–∑–≤–∞–Ω–∞ –ø–æ–º–æ—á—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º –±—ã—Å—Ç—Ä–æ –ø–æ–Ω—è—Ç—å –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –Ω–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models: Efficiency in Generative Tasks', 'desc': 'Diffusion models have become a popular choice for generative tasks like image and video creation due to their strong theoretical foundations and practical applications. This paper reviews the design principles and methodologies that make diffusion models efficient and effective. It aims to provide a comprehensive understanding of these models to facilitate their application in new scenarios. The survey focuses on architecture design, model training, fast inference, and reliable deployment to guide future research and development.'}, 'zh': {'title': 'Êâ©Êï£Ê®°ÂûãÔºöÈ´òÊïàÁîüÊàêÁöÑÊú™Êù•', 'desc': 'Êâ©Êï£Ê®°ÂûãÊòØËøëÂπ¥Êù•ÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑÁîüÊàêÊ®°ÂûãÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÂõæÂÉèÂêàÊàê„ÄÅËßÜÈ¢ëÁîüÊàêÁ≠â‰ªªÂä°„ÄÇÂÆÉ‰ª¨ÁöÑÊàêÂäü‰∏ªË¶ÅÂΩíÂäü‰∫éÊ∏êËøõÁöÑËÆæËÆ°ÂéüÂàôÂíåÈ´òÊïàÁöÑÊû∂ÊûÑ„ÄÅËÆ≠ÁªÉ„ÄÅÊé®ÁêÜÂèäÈÉ®ÁΩ≤ÊñπÊ≥ï„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜËøô‰∫õÂéüÂàôÂíåÂÆûË∑µÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰ª•ÊïàÁéá‰∏∫ÂØºÂêëÁöÑÊñ∞ËßÜËßí„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂ∏ÆÂä©ËØªËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ∫îÁî®Êâ©Êï£Ê®°Âûã„ÄÇ'}}, 'hash': 'e1a8ba0c4193aa99', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11096', 'title': 'SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI', 'url': 'https://huggingface.co/papers/2410.11096', 'abstract': "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': 'SecCodePLT: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ AI-–∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SecCodePLT - –Ω–æ–≤—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π AI –¥–ª—è –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —Å–æ—á–µ—Ç–∞—é—â—É—é —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –∞ —Ç–∞–∫–∂–µ –≤–Ω–µ–¥—Ä–∏–ª–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏. SecCodePLT –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥, —Ç–∞–∫ –∏ –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –¥–ª—è –∫–∏–±–µ—Ä–∞—Ç–∞–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ SecCodePLT –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏ –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —Ä–∏—Å–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'SecCodePLT: Elevating Code GenAI Security Evaluation', 'desc': "The paper introduces SecCodePLT, a new platform designed to evaluate the security risks of Code GenAI models more effectively. It addresses the limitations of existing benchmarks by combining expert input with automatic data generation to ensure high-quality, large-scale data for insecure code evaluation. The platform also uses dynamic evaluation metrics, such as test cases, to assess the models' ability to generate executable attacks in a real environment. Extensive experiments demonstrate that SecCodePLT surpasses current benchmarks in identifying security risks, revealing significant vulnerabilities in advanced coding agents like Cursor."}, 'zh': {'title': 'SecCodePLTÔºöÊèêÂçá‰ª£Á†ÅÁîüÊàêAIÂÆâÂÖ®ËØÑ‰º∞ÁöÑÊñ∞Âπ≥Âè∞', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫SecCodePLTÁöÑÊñ∞Âπ≥Âè∞ÔºåÁî®‰∫éËØÑ‰º∞‰ª£Á†ÅÁîüÊàêAIÁöÑÂÆâÂÖ®È£éÈô©„ÄÇSecCodePLTÈÄöËøáÁªìÂêà‰∏ìÂÆ∂ÂíåËá™Âä®ÁîüÊàêÁöÑÊñπÊ≥ïÔºåÂàõÂª∫È´òË¥®Èáè‰∏îÂ§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂‰ΩøÁî®Âä®ÊÄÅËØÑ‰º∞ÊñπÊ≥ïÊù•Ê£ÄÊµã‰∏çÂÆâÂÖ®‰ª£Á†ÅÂíåÁΩëÁªúÊîªÂáªÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÁöÑÂü∫ÂáÜÁõ∏ÊØîÔºåSecCodePLTÂú®ÂÆâÂÖ®Áõ∏ÂÖ≥ÊÄß‰∏äË°®Áé∞Êõ¥Â•ΩÔºåÂπ∂ËÉΩÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´ÊúÄÂÖàËøõÊ®°ÂûãÁöÑÂÆâÂÖ®È£éÈô©„ÄÇÁ†îÁ©∂ËøòÈ¶ñÊ¨°Âú®ÂÖàËøõÁöÑ‰ª£Á†Å‰ª£ÁêÜCursor‰∏≠ÂèëÁé∞‰∫ÜÊòæËëóÁöÑÂÆâÂÖ®È£éÈô©„ÄÇ'}}, 'hash': 'f1b69c45d56f76d8', 'pub_date_card': {'ru': '14 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 14', 'zh': '10Êúà14Êó•'}}, {'id': 'https://huggingface.co/papers/2410.09704', 'title': 'EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation', 'url': 'https://huggingface.co/papers/2410.09704', 'abstract': 'Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-13', 'pub_date_ru': '13 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#cv', '#dataset', '#medicine', '#multimodal', '#rag'], 'emoji': '‚ù§Ô∏è', 'ru': {'title': 'EchoPrime: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞–º–º', 'desc': 'EchoPrime - —ç—Ç–æ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞–º–º, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 12 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –ø–∞—Ä –≤–∏–¥–µ–æ-–æ—Ç—á–µ—Ç–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏–π –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏. EchoPrime –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ü–∏–π –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ–µ–∫—Ü–∏—è–º–∏ –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –≤ 23 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ—Ü–µ–Ω–∫–∏ —Ñ–æ—Ä–º—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–µ—Ä–¥—Ü–∞, —á—Ç–æ –±—ã–ª–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –≤ –¥–≤—É—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.'}, 'en': {'title': 'EchoPrime: Revolutionizing Heart Imaging with Multi-View AI', 'desc': 'EchoPrime is a new AI model designed to improve echocardiography by integrating information from multiple ultrasound views, unlike traditional single-view models. It uses contrastive learning to create a unified representation of echocardiogram data, capturing both common and rare cardiac conditions. The model employs view-classification and anatomic attention to accurately interpret the relationship between different views and heart structures. EchoPrime has demonstrated superior performance across various benchmarks, offering a more comprehensive and precise tool for cardiac assessment.'}, 'zh': {'title': 'EchoPrimeÔºöÂ§öËßÜËßíÂøÉËÑèË∂ÖÂ£∞AIÁöÑÈù©Êñ∞', 'desc': 'EchoPrime ÊòØ‰∏Ä‰∏™Â§öËßÜËßí„ÄÅËßÜÂõæÁü•ÊÉÖÁöÑËßÜÈ¢ëËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÂçïËßÜËßíAIÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÈÄöËøáÂØπÊØîÂ≠¶‰π†ËÆ≠ÁªÉ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂµåÂÖ•Ê®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊ†áÂáÜÂøÉËÑèË∂ÖÂ£∞Ê£ÄÊü•‰∏≠ÁöÑÊâÄÊúâËßÜÂõæ„ÄÇEchoPrime Âà©Áî®ËßÜÂõæÂàÜÁ±ªÂíåËß£ÂâñÊ≥®ÊÑèÊ®°ÂûãÔºåÂáÜÁ°ÆÊò†Â∞ÑË∂ÖÂ£∞ËßÜÂõæ‰∏éËß£ÂâñÁªìÊûÑ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÁªèËøá‰∏•Ê†ºÁöÑ‰∏¥Â∫äËØÑ‰º∞ÔºåEchoPrime Âú®ÂøÉËÑèÂΩ¢ÊÄÅÂíåÂäüËÉΩÁöÑ23‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÂ∏ÆÂä©ÂåªÁîüËøõË°åËá™Âä®ÂåñÁöÑÂàùÊ≠•ËØÑ‰º∞„ÄÇ'}}, 'hash': '5bf065e3ea6b9f7a', 'pub_date_card': {'ru': '13 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 13', 'zh': '10Êúà13Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11805', 'title': 'NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models', 'url': 'https://huggingface.co/papers/2410.11805', 'abstract': 'Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': 'NesTools: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –≤ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç NesTools –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –æ–±—É—á–µ–Ω–∏—é –≤–ª–æ–∂–µ–Ω–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. NesTools –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏. –î–∞—Ç–∞—Å–µ—Ç –ø—Ä–æ—à–µ–ª —Ä—É—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∏ –¥–æ—Ä–∞–±–æ—Ç–∫—É, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–≥–æ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–µ–∞–ª—å–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 22 LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NesTools –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ –µ—â–µ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º.'}, 'en': {'title': "NesTools: Elevating LLMs' Nested Tool Learning Evaluation", 'desc': "The paper introduces NesTools, a new benchmark designed to evaluate the nested tool learning capabilities of large language models (LLMs). NesTools uses an innovative automatic data generation method to create complex nested tool call scenarios, which are then manually reviewed to ensure high quality and real-world relevance. The study highlights that current LLMs struggle with these complex tasks, as demonstrated through extensive experiments on 22 different models. This work aims to fill the gap in existing research by providing a comprehensive dataset for assessing LLMs' ability to handle nested tool learning."}, 'zh': {'title': 'NesToolsÔºöÊèêÂçáÂµåÂ•óÂ∑•ÂÖ∑Â≠¶‰π†ÁöÑÂÖ®Êñ∞Âü∫ÂáÜ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NesToolsÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂµåÂ•óÂ∑•ÂÖ∑Â≠¶‰π†ËÉΩÂäõ„ÄÇNesToolsÈÄöËøáËá™Âä®ÁîüÊàêÂ§ßÈáè‰∏çÂêåÂµåÂ•óÁªìÊûÑÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Êï∞ÊçÆÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÂü∫ÂáÜ‰∏≠Áº∫‰πèÁõ∏ÂÖ≥Êï∞ÊçÆÂÆû‰æãÁöÑÁ©∫ÁôΩ„ÄÇÁªèËøá‰∫∫Â∑•ÂÆ°Ê†∏ÂíåÊîπËøõÔºåÊï∞ÊçÆÈõÜË¥®ÈáèÈ´ò‰∏î‰∏éÁé∞ÂÆûÂú∫ÊôØÁ¥ßÂØÜÁªìÂêà„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂΩìÂâçÁöÑLLMsÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÂµåÂ•óÂ∑•ÂÖ∑Â≠¶‰π†‰ªªÂä°Êó∂‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇ'}}, 'hash': '9823a7c3dbc76e44', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.10934', 'title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'url': 'https://huggingface.co/papers/2410.10934', 'abstract': 'Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.', 'score': 10, 'issue_id': 127, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#agents', '#benchmark', '#rlhf'], 'emoji': 'ü§ñ', 'ru': {'title': '–ê–≥–µ–Ω—Ç —Å—É–¥–∏—Ç –∞–≥–µ–Ω—Ç–∞: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ò–ò-—Å–∏—Å—Ç–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ - Agent-as-a-Judge. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç Agent-as-a-Judge –∫ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ DevAI, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 55 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ò–ò. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Agent-as-a-Judge –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LLM-as-a-Judge –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º —Å –æ—Ü–µ–Ω–∫–æ–π —á–µ–ª–æ–≤–µ–∫–∞.'}, 'en': {'title': '"Agent-as-a-Judge: Revolutionizing Evaluation for Agentic Systems"', 'desc': 'The paper introduces the Agent-as-a-Judge framework, which uses agentic systems to evaluate other agentic systems, providing intermediate feedback throughout the task-solving process. This approach is applied to code generation and is tested using a new benchmark called DevAI, which includes 55 realistic AI development tasks with detailed annotations. The framework is shown to outperform existing evaluation methods like LLM-as-a-Judge and matches human evaluation reliability. Overall, Agent-as-a-Judge offers a significant advancement in providing dynamic and scalable feedback for agentic systems.'}, 'zh': {'title': '‰ª£ÁêÜÂç≥ËØÑÂà§ËÄÖÔºöÁé∞‰ª£‰ª£ÁêÜÁ≥ªÁªüËØÑ‰º∞ÁöÑÊñ∞ÊñπÂêë', 'desc': '‰º†ÁªüÁöÑËØÑ‰º∞ÊñπÊ≥ïÂØπ‰ª£ÁêÜÁ≥ªÁªüÊù•ËØ¥‰∏çÂ§üÂÆåÂñÑÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Ë¶Å‰πàÂè™ÂÖ≥Ê≥®ÊúÄÁªàÁªìÊûúËÄåÂøΩÁï•‰∫Ü‰ª£ÁêÜÁ≥ªÁªüÁöÑÈÄêÊ≠•ÁâπÊÄßÔºåË¶Å‰πàÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•Âä≥Âä®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‚Äú‰ª£ÁêÜÂç≥ËØÑÂà§ËÄÖ‚ÄùÊ°ÜÊû∂ÔºåÂà©Áî®‰ª£ÁêÜÁ≥ªÁªüÊù•ËØÑ‰º∞‰ª£ÁêÜÁ≥ªÁªü„ÄÇËøôÁßçÊñπÊ≥ïÊòØ‚ÄúLLMÂç≥ËØÑÂà§ËÄÖ‚ÄùÊ°ÜÊû∂ÁöÑËá™ÁÑ∂Êâ©Â±ïÔºåÂä†ÂÖ•‰∫Ü‰ª£ÁêÜÁâπÊÄßÔºåËÉΩÂ§üÂú®Êï¥‰∏™‰ªªÂä°Ëß£ÂÜ≥ËøáÁ®ã‰∏≠Êèê‰æõ‰∏≠Èó¥ÂèçÈ¶à„ÄÇÊàë‰ª¨Â∫îÁî®‚Äú‰ª£ÁêÜÂç≥ËØÑÂà§ËÄÖ‚Äù‰∫é‰ª£Á†ÅÁîüÊàê‰ªªÂä°ÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜDevAIÔºåËØÅÊòéÂÖ∂Âú®ËØÑ‰º∞‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ'}}, 'hash': 'a25e89cb7be0bed4', 'pub_date_card': {'ru': '14 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 14', 'zh': '10Êúà14Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11419', 'title': 'GS^3: Efficient Relighting with Triple Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.11419', 'abstract': 'We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.', 'score': 10, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'üé®', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –æ—Å–≤–µ—â–µ–Ω–∏—è –∏ —Ä–∞–∫—É—Ä—Å–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —É–≥–ª–æ–≤—ã—Ö –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ—Ü–µ—Å—Å —Ç—Ä–æ–π–Ω–æ–≥–æ —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞. –î–ª—è –æ–ø–∏—Å–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è, —Å–æ—Å—Ç–æ—è—â–∞—è –∏–∑ –ª–∞–º–±–µ—Ä—Ç–æ–≤—Å–∫–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π –∏ —Å–º–µ—Å–∏ —É–≥–ª–æ–≤—ã—Ö –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ.'}, 'en': {'title': '"Illuminate and Transform: Real-Time View Synthesis with Gaussian Magic"', 'desc': 'This paper introduces a novel method for creating high-quality images with new lighting and viewpoints using a spatial and angular Gaussian representation combined with a triple splatting process. The approach models complex appearances by using a Lambertian and angular Gaussian mixture reflectance function for each spatial Gaussian. To handle self-shadowing, spatial Gaussians are projected towards the light source, with shadow values refined by a small neural network, while another network adjusts for global illumination effects. The method is tested on diverse samples and achieves fast training and rendering times, outperforming existing techniques in quality and performance.'}, 'zh': {'title': 'ÂÆûÊó∂È´òË¥®ÈáèÂÖâÁÖßÂêàÊàêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁ©∫Èó¥ÂíåËßíÂ∫¶È´òÊñØÁöÑË°®Á§∫ÊñπÊ≥ïÔºå‰ª•Âèä‰∏âÈáçÂñ∑Ê∫ÖËøáÁ®ãÔºåÁî®‰∫é‰ªéÂ§öËßÜÁÇπÂÖâÁÖßËæìÂÖ•ÂõæÂÉè‰∏≠ÂÆûÊó∂ÁîüÊàêÈ´òË¥®ÈáèÁöÑÊñ∞ÂÖâÁÖßÂíåËßÜÂõæÂêàÊàê„ÄÇ‰∏∫‰∫ÜÊèèËø∞Â§çÊùÇÁöÑÂ§ñËßÇÔºå‰ΩúËÄÖ‰ΩøÁî®‰∫ÜÊúó‰ºØÂèçÂ∞ÑÂä†‰∏äËßíÂ∫¶È´òÊñØÊ∑∑Âêà‰Ωú‰∏∫ÊØè‰∏™Á©∫Èó¥È´òÊñØÁöÑÊúâÊïàÂèçÂ∞ÑÂáΩÊï∞„ÄÇ‰∏∫‰∫ÜÁîüÊàêËá™Èò¥ÂΩ±ÔºåÊâÄÊúâÁ©∫Èó¥È´òÊñØË¢´Âñ∑Ê∫ÖÂà∞ÂÖâÊ∫êÊñπÂêë‰ª•Ëé∑ÂæóÈò¥ÂΩ±ÂÄºÔºåÂπ∂ÈÄöËøá‰∏Ä‰∏™Â∞èÂûãÂ§öÂ±ÇÊÑüÁü•Âô®Ëøõ‰∏ÄÊ≠•‰ºòÂåñ„ÄÇ‰∏∫‰∫ÜË°•ÂÅøÂÖ®Â±ÄÂÖâÁÖßÁ≠âÂÖ∂‰ªñÊïàÊûúÔºåÂè¶‰∏Ä‰∏™ÁΩëÁªúË¢´ËÆ≠ÁªÉÊù•ËÆ°ÁÆóÂπ∂Ê∑ªÂä†ÊØè‰∏™Á©∫Èó¥È´òÊñØÁöÑRGBÂÄº„ÄÇ'}}, 'hash': '8aa8e4a555a54086', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.09754', 'title': 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.09754', 'abstract': "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.", 'score': 7, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_ru': '13 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#rl'], 'emoji': 'ü§ñ', 'ru': {'title': 'SimBa: –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É SimBa –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). SimBa –≤–∫–ª—é—á–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, —Ä–µ–∑–∏–¥—É–∞–ª—å–Ω—ã–π –±–ª–æ–∫ –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Å–ª–æ–µ–≤ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç–æ—Ç–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SimBa —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL, –≤–∫–ª—é—á–∞—è off-policy, on-policy –∏ unsupervised –º–µ—Ç–æ–¥—ã. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è SimBa –≤ –∞–ª–≥–æ—Ä–∏—Ç–º SAC –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –∏–ª–∏ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã RL –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å –≤—ã—Å–æ–∫–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': '"SimBa: Scaling Simplicity in Deep Reinforcement Learning"', 'desc': 'The paper introduces SimBa, a new architecture for deep reinforcement learning (RL) that scales up network parameters while maintaining simplicity to avoid overfitting. SimBa includes an observation normalization layer, a residual feedforward block, and a layer normalization to enhance model performance. By using SimBa, various deep RL algorithms show improved sample efficiency and computational effectiveness. The architecture proves its versatility by matching or surpassing state-of-the-art methods across different environments and tasks.'}, 'zh': {'title': 'SimBaÔºöÈÄöËøáÁÆÄÂçïÊÄßÂÅèÂ∑ÆÊèêÂçáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†', 'desc': 'ËøëÂπ¥Êù•ÔºåËÆ°ÁÆóÊú∫ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÈÄöËøáÂ¢ûÂä†ÁΩëÁªúÂèÇÊï∞Êï∞ÈáèÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÂ∞ΩÁÆ°‰º†ÁªüÁêÜËÆ∫ËÆ§‰∏∫Êõ¥Â§ßÁöÑÁΩëÁªúÂÆπÊòìËøáÊãüÂêà„ÄÇËøô‰∫õÂ§ßÂûãÁΩëÁªúÈÄöËøáÂºïÂÖ•ÁÆÄÂçïÊÄßÂÅèÂ∑ÆÁöÑÁªÑ‰ª∂Êù•ÈÅøÂÖçËøáÊãüÂêàÔºåÊåáÂØºÊ®°ÂûãÊúùÂêëÁÆÄÂçï‰∏îÂèØÊé®ÂπøÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂú®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÁΩëÁªúËÆæËÆ°ÂíåÊâ©Â±ïÁöÑÁ†îÁ©∂ËæÉÂ∞ë„ÄÇSimBaÊû∂ÊûÑÈÄöËøáÊ≥®ÂÖ•ÁÆÄÂçïÊÄßÂÅèÂ∑ÆÊù•Êâ©Â±ïÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂèÇÊï∞ÔºåÊèêÈ´ò‰∫ÜÂ§öÁßçÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇ'}}, 'hash': '04b27b8dbf829651', 'pub_date_card': {'ru': '13 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 13', 'zh': '10Êúà13Êó•'}}, {'id': 'https://huggingface.co/papers/2410.08001', 'title': 'Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.08001', 'abstract': 'The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/', 'score': 4, 'issue_id': 122, 'pub_date': '2024-10-10', 'pub_date_ru': '10 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#cv', '#multimodal', '#rl', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': 'RoboDual: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω –æ–±–æ–±—â–µ–Ω–Ω–æ–π –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤', 'desc': 'RoboDual - —ç—Ç–æ —Å–∏–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è –¥–≤–æ–π–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±–æ–±—â–µ–Ω–Ω–æ–π –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –û–±–æ–±—â–µ–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ vision-language-action –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–¥–∞—á, –∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è. RoboDual –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å OpenVLA –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 5% –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —á–∞—Å—Ç–æ—Ç—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.'}, 'en': {'title': 'RoboDual: Merging Generalist Flexibility with Specialist Precision', 'desc': 'The paper introduces RoboDual, a dual-system approach that combines the strengths of both generalist and specialist policies for robotic systems. The generalist policy uses a vision-language-action model to provide high-level task understanding, while the specialist policy, based on a diffusion transformer, focuses on efficient multi-step action rollouts. RoboDual significantly improves performance in real-world settings and specific benchmarks by using a specialist policy with a small number of trainable parameters. This system achieves high efficiency and adaptability, even with limited demonstration data, and enhances control frequency in practical applications.'}, 'zh': {'title': 'RoboDualÔºöÈÄöÁî®‰∏é‰∏ì‰∏öÁ≠ñÁï•ÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'RoboDual ÊòØ‰∏Ä‰∏™ÁªìÂêàÈÄöÁî®Á≠ñÁï•Âíå‰∏ì‰∏öÁ≠ñÁï•ÁöÑÂèåÁ≥ªÁªüÔºåÊó®Âú®ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§öÂèòÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÈÄöÁî®Á≠ñÁï•Âà©Áî®Â§ßËßÑÊ®°Ë∑®‰ΩìÊï∞ÊçÆÔºåÊèê‰æõÈ´òÂ±ÇÊ¨°ÁöÑ‰ªªÂä°ÁêÜËß£ÔºåËÄå‰∏ì‰∏öÁ≠ñÁï•Âàô‰∏ìÊ≥®‰∫éÁâπÂÆö‰ªªÂä°ÁöÑÈ´òÊïàÊâßË°å„ÄÇRoboDual ÈÄöËøáÂºïÂÖ•Âü∫‰∫éÊâ©Êï£ÂèòÂéãÂô®ÁöÑ‰∏ì‰∏öÁ≠ñÁï•ÔºåÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËØ•Á≥ªÁªüÂú®‰ªÖ‰ΩøÁî®Â∞ëÈáèÁ§∫ËåÉÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªçËÉΩ‰øùÊåÅÈ´òÊïàÁöÑÊéßÂà∂È¢ëÁéá„ÄÇ'}}, 'hash': 'f9a57a673dfbc6f6', 'pub_date_card': {'ru': '10 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 10', 'zh': '10Êúà10Êó•'}}, {'id': 'https://huggingface.co/papers/2410.06593', 'title': 'Towards Natural Image Matting in the Wild via Real-Scenario Prior', 'url': 'https://huggingface.co/papers/2410.06593', 'abstract': 'Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.', 'score': 2, 'issue_id': 126, 'pub_date': '2024-10-09', 'pub_date_ru': '9 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#architecture', '#cv', '#dataset'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º –º–∞—Ç—Ç–∏–Ω–≥–µ: –æ—Ç —Å–∏–Ω—Ç–µ—Ç–∏–∫–∏ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç COCO-Matting –¥–ª—è –∑–∞–¥–∞—á–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Ç—Ç–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –∏–∑ COCO. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É SEMat, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ SAM, –∏—Å–ø–æ–ª—å–∑—É—è feature-aligned transformer –∏ matte-aligned decoder. –í–≤–µ–¥–µ–Ω—ã –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –∏ trimap loss. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ –∑–∞–¥–∞—á–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Ç—Ç–∏–Ω–≥–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Revolutionizing Image Matting with Real-World Data and Advanced Architectures', 'desc': 'The paper introduces a new dataset called COCO-Matting, which enhances the generalization of matting models to complex real-world scenes by using real images from the COCO dataset. It addresses the limitations of models trained on synthetic data by converting semantic segmentation masks into matting labels. The authors propose a novel model, SEMat, which includes a feature-aligned transformer and a matte-aligned decoder to improve edge and transparency feature extraction. Their approach, validated through extensive experiments, shows superior performance in interactive image matting, leveraging both new training objectives and architecture improvements.'}, 'zh': {'title': 'SEMatÔºöÊèêÂçá‰∫§‰∫íÂºèÊä†ÂõæÁöÑÁ≤æÂ∫¶‰∏éÊ≥õÂåñËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊä†ÂõæÊï∞ÊçÆÈõÜCOCO-MattingÔºåÂü∫‰∫éCOCOÊï∞ÊçÆÈõÜÊûÑÂª∫ÔºåÊó®Âú®Ëß£ÂÜ≥ÂêàÊàêÊï∞ÊçÆËÆ≠ÁªÉÊ®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇCOCO-MattingÈÄöËøá‰ªéCOCO‰∏≠ÈÄâÊã©ÁúüÂÆû‰∏ñÁïåÁöÑÂ§çÊùÇÂõæÂÉèÔºåÂπ∂Â∞ÜËØ≠‰πâÂàÜÂâ≤Êé©Á†ÅËΩ¨Êç¢‰∏∫Êä†ÂõæÊ†áÁ≠æÔºåÂåÖÂê´‰∫Ü38,251‰∏™‰∫∫Áâ©ÂÆû‰æãÁ∫ßÂà´ÁöÑalpha mattes„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑSAMÊ®°ÂûãÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜSEMatÔºåÈÄöËøáÁâπÂæÅÂØπÈΩêÁöÑtransformerÂíåÊä†ÂõæÂØπÈΩêÁöÑËß£Á†ÅÂô®Êù•ÊèêÂèñÁªÜÁ≤íÂ∫¶ÁöÑËæπÁºòÂíåÈÄèÊòéÂ∫¶ÁâπÂæÅÔºåÂπ∂Â∞ÜÁ≤óÁï•ÁöÑÊé©Á†ÅËΩ¨Êç¢‰∏∫È´òÁ≤æÂ∫¶ÁöÑÊä†Âõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSEMatÂú®‰∏É‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®‰∫§‰∫íÂºèËá™ÁÑ∂ÂõæÂÉèÊä†Âõæ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}, 'hash': '656804d454a782ca', 'pub_date_card': {'ru': '9 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 9', 'zh': '10Êúà9Êó•'}}, {'id': 'https://huggingface.co/papers/2410.09745', 'title': 'Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt', 'url': 'https://huggingface.co/papers/2410.09745', 'abstract': "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.", 'score': 2, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_ru': '13 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#rl'], 'emoji': 'üîÑ', 'ru': {'title': '–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–ª–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞ —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∑–∞–∏–º–æ—É—Å–∏–ª–∏–≤–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç (MRE) –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 21 –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MRE Mix –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Ç–µ–æ—Ä–∏–∏ MRE. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ MRE. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ MRE –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ F1-–º–µ—Ä—É –Ω–∞ 18 –∏–∑ 21 –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Boosting Text Understanding: The Power of Word-Level Insights', 'desc': "The paper explores the Mutual Reinforcement Effect (MRE), which suggests that word-level and text-level classifications can improve each other's performance in text classification tasks. Through empirical experiments on 21 datasets, the study demonstrates the presence and impact of MRE, showing that word-level information can enhance text-level predictions. The researchers also apply MRE to prompt learning, using word-level data to improve the model's text-level classification accuracy. The experiments show a significant improvement in F1-scores, confirming that word-level insights can enhance overall text comprehension by the model."}, 'zh': {'title': 'ËØçÁ∫ß‰∏éÊñáÊú¨Á∫ßÂàÜÁ±ªÁöÑÁõ∏‰∫íÂ¢ûÂº∫ÊïàÂ∫î', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜËØçÁ∫ßÂíåÊñáÊú¨Á∫ßÂàÜÁ±ªÂú®ÊñáÊú¨ÂàÜÁ±ª‰ªªÂä°‰∏≠ÁöÑÁõ∏‰∫íÂ¢ûÂº∫ÊïàÂ∫î„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòé‰∫ÜËøôÁßçÊïàÂ∫îÁöÑÂ≠òÂú®ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂ÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ËøòÂ∞ÜËøôÁßçÊïàÂ∫îÂ∫îÁî®‰∫éÊèêÁ§∫Â≠¶‰π†ÔºåÂà©Áî®ËØçÁ∫ß‰ø°ÊÅØÊù•Â¢ûÂº∫ÊñáÊú¨Á∫ßÂàÜÁ±ªÊ†áÁ≠æÁöÑÈ¢ÑÊµã„ÄÇÊúÄÁªàÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®21‰∏™Êï∞ÊçÆÈõÜ‰∏≠ÁöÑ18‰∏™ÔºåF1ÂàÜÊï∞ÊòæËëóË∂ÖËøáÂü∫Á∫øÔºåÈ™åËØÅ‰∫ÜËØçÁ∫ß‰ø°ÊÅØÂØπËØ≠Ë®ÄÊ®°ÂûãÁêÜËß£ÊñáÊú¨Êï¥‰ΩìÁöÑÊèêÂçá‰ΩúÁî®„ÄÇ'}}, 'hash': '9e1731e2bbd0b081', 'pub_date_card': {'ru': '13 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 13', 'zh': '10Êúà13Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11619', 'title': 'MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval', 'url': 'https://huggingface.co/papers/2410.11619', 'abstract': 'Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.', 'score': 0, 'issue_id': 130, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'data': {'categories': ['#benchmark', '#dataset', '#multilingual', '#multimodal', '#video'], 'emoji': 'üé•', 'ru': {'title': 'MultiVENT 2.0: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –≤ –º–∏—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ', 'desc': 'MultiVENT 2.0 - —ç—Ç–æ –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–æ–±—ã—Ç–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 218 000 –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –≤–∏–¥–µ–æ –∏ 3 906 –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–∏—Ä–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è. –ë–µ–Ω—á–º–∞—Ä–∫ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç —Å–∏—Å—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∞—É–¥–∏–æ, –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø–æ–∏—Å–∫–∞.'}, 'en': {'title': 'Unlocking the Power of Multimodal Video Retrieval', 'desc': 'The paper introduces MultiVENT 2.0, a new benchmark for video retrieval that focuses on multilingual and event-centric content, addressing the limitations of existing datasets. It includes over 218,000 news videos and 3,906 queries that require systems to utilize visual, audio, and text data effectively. Initial tests reveal that current vision-language models struggle with this complex task, highlighting the need for more advanced multimodal retrieval systems. This research emphasizes the importance of improving video retrieval to enhance multimodal content understanding and generation.'}, 'zh': {'title': 'Á™ÅÁ†¥Â§öÊ®°ÊÄÅËßÜÈ¢ëÊ£ÄÁ¥¢ÁöÑÁì∂È¢à', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫MultiVENT 2.0ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®Ä‰∫ã‰ª∂‰∏≠ÂøÉËßÜÈ¢ëÊ£ÄÁ¥¢Âü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜÈ¢ëÊ£ÄÁ¥¢Êï∞ÊçÆÈõÜËåÉÂõ¥ÊúâÈôêÁöÑÈóÆÈ¢ò„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´Ë∂ÖËøá218,000‰∏™Êñ∞ÈóªËßÜÈ¢ëÂíå3,906‰∏™ÈíàÂØπÁâπÂÆö‰∏ñÁïå‰∫ã‰ª∂ÁöÑÊü•ËØ¢ÔºåË¶ÅÊ±ÇÁ≥ªÁªüÂà©Áî®ËßÜÈ¢ëÁöÑËßÜËßâÂÜÖÂÆπ„ÄÅÈü≥È¢ë„ÄÅÂµåÂÖ•ÊñáÊú¨ÂíåÊñáÊú¨ÂÖÉÊï∞ÊçÆÊù•ÂÆåÊàê‰ªªÂä°„ÄÇÂàùÊ≠•ÁªìÊûúÊòæÁ§∫ÔºåÊúÄÂÖàËøõÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Ëøô‰∏™‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåËÄåÂÖ∂‰ªñÊñπÊ≥ïËôΩÁÑ∂ÊúâÊΩúÂäõÔºå‰ΩÜ‰ªç‰∏çË∂≥‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇËøôË°®ÊòéÈúÄË¶ÅÊõ¥Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Á≥ªÁªüÔºå‰ª•ÂÆûÁé∞ÂØπÂ§öÊ®°ÊÄÅÂÜÖÂÆπÁöÑÁêÜËß£ÂíåÁîüÊàê„ÄÇ'}}, 'hash': 'dd603cadf5349b0c', 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment (2)', '#architecture (5)', '#audio', '#benchmark (15)', '#cv (7)', '#data (1)', '#dataset (5)', '#diffusion (2)', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#inference (1)', '#interpretability (2)', '#long_context (1)', '#math (1)', '#medicine (1)', '#multilingual (1)', '#multimodal (7)', '#optimization', '#plp', '#rag (2)', '#reasoning', '#rl (2)', '#rlhf (3)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic', '#training', '#transfer_learning (1)', '#translation', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">üìù ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-10-17 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-17 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-17 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    