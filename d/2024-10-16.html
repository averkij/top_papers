
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. October 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-15.html">â¬…ï¸ <span id="prev-date">15.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-17.html">â¡ï¸ <span id="next-date">17.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'};
        let feedDateNext = {'ru': '17.10', 'en': '10/17', 'zh': '10æœˆ17æ—¥'};
        let feedDatePrev = {'ru': '15.10', 'en': '10/15', 'zh': '10æœˆ15æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.10814', 'title': 'Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free', 'url': 'https://huggingface.co/papers/2410.10814', 'abstract': 'While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.', 'score': 48, 'issue_id': 124, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'c2ddb801d2228a78', 'authors': ['Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Department of Computer Science University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10814.jpg', 'data': {'categories': ['#dataset', '#agi', '#interpretability', '#optimization', '#benchmark', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoE LLM: Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts (MoE) LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑĞ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE (RW) Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ (HS) LLM, ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MoEE, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ RW Ğ¸ HS, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 6 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ñ 20 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Massive Text Embedding Benchmark (MTEB).'}, 'en': {'title': 'Unlocking the Embedding Power of MoE LLMs: No Finetuning Needed!', 'desc': 'The paper investigates the potential of Mixture-of-Experts (MoE) large language models (LLMs) as effective embedding models without additional finetuning. It reveals that the expert routers in MoE LLMs can be used directly for embedding tasks, showing strong performance across various tasks. The study finds that the routing weights (RW) in MoE models complement the hidden states (HS) of LLMs, offering robustness to prompt variations and focusing on high-level semantics. By combining RW and HS, the proposed MoEE model achieves superior results, as demonstrated on multiple datasets from the Massive Text Embedding Benchmark (MTEB).'}, 'zh': {'title': 'ä¸“å®¶æ··åˆï¼šæå‡è¯­è¨€æ¨¡å‹åµŒå…¥èƒ½åŠ›çš„æ–°è·¯å¾„', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»…è§£ç æ¶æ„é™åˆ¶äº†å…¶ä½œä¸ºåµŒå…¥æ¨¡å‹çš„æ½œåŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰LLMï¼Œå‘ç°å…¶ä¸“å®¶è·¯ç”±å™¨æ— éœ€å¾®è°ƒå³å¯åœ¨å¤šç§åµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚MoEçš„è·¯ç”±æƒé‡ï¼ˆRWï¼‰ä¸éšè—çŠ¶æ€ï¼ˆHSï¼‰äº’è¡¥ï¼Œä¸”å¯¹æç¤ºé€‰æ‹©æ›´å…·é²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºçš„MoEEç»“åˆäº†RWå’ŒHSï¼Œæ˜¾è‘—æå‡äº†åµŒå…¥ä»»åŠ¡çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09342', 'title': 'LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models', 'url': 'https://huggingface.co/papers/2410.09342', 'abstract': 'Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.', 'score': 37, 'issue_id': 125, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 12', 'zh': '10æœˆ12æ—¥'}, 'hash': '214a55e4bcf12eb6', 'authors': ['Zihan Zhou', 'Chong Li', 'Xinyi Chen', 'Shuo Wang', 'Yu Chao', 'Zhili Li', 'Haoyu Wang', 'Rongqiao An', 'Qi Shi', 'Zhixing Tan', 'Xu Han', 'Xiaodong Shi', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['BUPT', 'Beijing National Research Center for Information Science and Technology', 'Dept. of Comp. Sci. & Tech., Tsinghua University', 'Institute for AI, Tsinghua University', 'Nankai University', 'Peking University', 'Xiamen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09342.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#long_context'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LLMÃ—MapReduce, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLMÃ—MapReduce Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ LLM.'}, 'en': {'title': 'Divide and Conquer: Mastering Long Texts with LLMtimesMapReduce', 'desc': "The paper introduces a new method called LLMtimesMapReduce to help large language models (LLMs) understand very long texts without needing extra training. It uses a divide-and-conquer approach, breaking the text into smaller parts for the LLMs to process, and then combines the results to form a complete understanding. The main challenge is ensuring that important information isn't lost when the text is split, which the authors address with a structured information protocol and a confidence calibration mechanism. Tests show that this method works better than other similar tools and can be used with different LLMs."}, 'zh': {'title': 'åˆ†è€Œæ²»ä¹‹ï¼šæ— è®­ç»ƒæ¡†æ¶å¤„ç†è¶…é•¿æ–‡æœ¬', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºLLMtimesMapReduceï¼Œç”¨äºå¤„ç†è¶…é•¿æ–‡æœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•ï¼Œå°†æ–‡æ¡£åˆ†æˆå¤šä¸ªéƒ¨åˆ†ä¾›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é˜…è¯»ï¼Œç„¶åæ±‡æ€»ä¸­é—´ç­”æ¡ˆä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåˆ†å‰²æ–‡æ¡£æ—¶å¯èƒ½ä¸¢å¤±é‡è¦çš„é•¿è·ç¦»ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åŸºäºåˆ†æ®µæ–‡æœ¬äº§ç”Ÿä¸å®Œæ•´æˆ–é”™è¯¯çš„ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMtimesMapReduceåœ¨å¤„ç†é•¿æ–‡æœ¬æ–¹é¢ä¼˜äºå…¶ä»–å¼€æºå’Œå•†ä¸šé•¿ä¸Šä¸‹æ–‡LLMã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.10626', 'title': 'Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts', 'url': 'https://huggingface.co/papers/2410.10626', 'abstract': 'Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.', 'score': 37, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'e8c698bf76856366', 'authors': ['Guorui Zheng', 'Xidong Wang', 'Juhao Liang', 'Nuo Chen', 'Yuping Zheng', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10626.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#healthcare', '#interpretability', '#optimization', '#transfer_learning', '#architecture', '#low_resource'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ LLM: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Mixture of Experts (MoE). ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Post-MoE, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Breaking Language Barriers in Medical AI', 'desc': "The paper addresses the challenge of adapting medical Large Language Models (LLMs) to low-resource languages by constructing a high-quality medical dataset and analyzing its quality. It introduces a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing to enhance the model's generalization capabilities. Inspired by circuit theory, the authors propose the Post-MoE architecture, which applies sparse routing in later layers to improve language-specific performance while maintaining interpretability. The approach allows the model to scale efficiently to 50 languages by using language family experts, leveraging linguistic priors without increasing parameters."}, 'zh': {'title': 'è·¨è¯­è¨€åŒ»ç–—æ¨¡å‹ï¼šçªç ´è¯­è¨€éšœç¢çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”æœ¬åœ°è¯­è¨€ï¼Œä»¥å‡å°‘è·å–åŒ»ç–—æœåŠ¡çš„éšœç¢ã€‚ç ”ç©¶ä¸­ï¼Œä½œè€…é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒ»ç–—æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†åˆ†æä»¥ç¡®ä¿å…¶è´¨é‡ã€‚é€šè¿‡ä½¿ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å—åŒ–æ–¹æ³•ï¼Œç ”ç©¶äº†å¤šè¯­è¨€LLMçš„å†…éƒ¨ä¿¡æ¯æµï¼Œæå‡ºäº†ä¸€ç§æ–°çš„MoEè·¯ç”±æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æé«˜äº†å¤šè¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2406.15786', 'title': 'What Matters in Transformers? Not All Attention is Needed', 'url': 'https://huggingface.co/papers/2406.15786', 'abstract': 'While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.', 'score': 29, 'issue_id': 121, 'pub_date': '2024-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': 'fdf872015ec393aa', 'authors': ['Shwai He', 'Guoheng Sun', 'Zheyu Shen', 'Ang Li'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2406.15786.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞ»Ğ¾ĞµĞ², Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (attention layers) Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒÑ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Llama-2-70B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 48.4% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2.4% Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ MLP, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµÑ‰Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Prune to Perform: Streamlining Transformers for Efficiency', 'desc': "This paper explores the redundancy in Transformer-based large language models, focusing on the MLP and Attention layers. The authors found that many attention layers are highly similar and can be pruned without significantly affecting performance, as demonstrated by a 48.4% speedup in Llama-2-70B with minimal performance loss. They also propose a method to jointly prune Attention and MLP layers, achieving substantial efficiency gains while maintaining most of the model's performance. This research offers insights into optimizing Transformer architectures for more efficient real-world applications."}, 'zh': {'title': 'ä¼˜åŒ–Transformerï¼šå‡å°‘å†—ä½™ï¼Œæå‡æ•ˆç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨Transformeræ¨¡å‹ä¸­ä¸åŒæ¨¡å—çš„å†—ä½™é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å±‚å’ŒMLPå±‚ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ³¨æ„åŠ›å±‚åœ¨Transformerä¸­éå¸¸é‡è¦ï¼Œä½†è®¸å¤šå±‚çš„ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œå¯ä»¥è¢«å‰ªæè€Œä¸å½±å“æ€§èƒ½ã€‚é€šè¿‡å‰ªææ³¨æ„åŠ›å±‚ï¼Œæ¨¡å‹çš„é€Ÿåº¦å¯ä»¥æ˜¾è‘—æé«˜ï¼Œè€Œæ€§èƒ½æŸå¤±å¾ˆå°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§åŒæ—¶å‰ªææ³¨æ„åŠ›å±‚å’ŒMLPå±‚çš„æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11779', 'title': 'MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation', 'url': 'https://huggingface.co/papers/2410.11779', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.', 'score': 24, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '25a9176442c65ae3', 'authors': ['Chenxi Wang', 'Xiang Chen', 'Ningyu Zhang', 'Bozhong Tian', 'Haoming Xu', 'Shumin Deng', 'Huajun Chen'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11779.jpg', 'data': {'categories': ['#hallucinations', '#training', '#interpretability', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MLLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ½Ğ¾ Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ñ… Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ DeCo, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹.'}, 'en': {'title': 'DeCo: Correcting Hallucinations in Multimodal Models', 'desc': "This paper investigates why Multimodal Large Language Models (MLLMs) often produce hallucinations, where they generate incorrect outputs. The authors discover that while MLLMs can recognize visual objects in earlier layers, the language model's strong knowledge priors may suppress this visual information, causing hallucinations. To address this, they introduce a new method called dynamic correction decoding (DeCo), which adjusts the integration of knowledge in the final output layer. DeCo is versatile and can be used with different models and decoding strategies, significantly reducing hallucination rates in tests."}, 'zh': {'title': 'åŠ¨æ€æ ¡æ­£è§£ç ï¼šå‡å°‘å¤šæ¨¡æ€æ¨¡å‹å¹»è§‰çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¸å¸¸ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œä½†å…¶åŸå› å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡å®è¯åˆ†æå‘ç°ï¼Œå°½ç®¡MLLMsåœ¨æœ€ç»ˆè¾“å‡ºä¸­é”™è¯¯ç”Ÿæˆå¯¹è±¡ï¼Œä½†å®ƒä»¬å®é™…ä¸Šèƒ½å¤Ÿåœ¨å‰é¢çš„å±‚ä¸­è¯†åˆ«è§†è§‰å¯¹è±¡ã€‚æˆ‘ä»¬æ¨æµ‹è¿™å¯èƒ½æ˜¯ç”±äºè¯­è¨€æ¨¡å‹çš„å¼ºçŸ¥è¯†å…ˆéªŒæŠ‘åˆ¶äº†è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€æ ¡æ­£è§£ç æ–¹æ³•ï¼ˆDeCoï¼‰ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„å‰ç½®å±‚ï¼Œå¹¶å°†çŸ¥è¯†æŒ‰æ¯”ä¾‹æ•´åˆåˆ°æœ€ç»ˆå±‚ä¸­ï¼Œä»¥è°ƒæ•´è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.10816', 'title': 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions', 'url': 'https://huggingface.co/papers/2410.10816', 'abstract': 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.', 'score': 19, 'issue_id': 123, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'b7f807e5ff9e1614', 'authors': ['Tianwei Xiong', 'Yuqing Wang', 'Daquan Zhou', 'Zhijie Lin', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10816.jpg', 'data': {'categories': ['#video', '#dataset', '#long_context', '#training', '#data', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'LVD-2M: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LVD-2M Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 10 ÑĞµĞºÑƒĞ½Ğ´, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': '"Unlocking the Future of Long Video Generation"', 'desc': 'The paper discusses the importance of high-quality datasets for training video generation models, especially for long videos. It introduces a new dataset, LVD-2M, which includes 2 million long-take videos, each over 10 seconds, with diverse content and detailed captions. The authors developed a pipeline to select these videos based on specific quality metrics and to generate temporally dense captions. This dataset aims to advance research in generating long videos with dynamic motions.'}, 'zh': {'title': 'æ¨åŠ¨é•¿è§†é¢‘ç”Ÿæˆç ”ç©¶çš„æ–°æ•°æ®é›†', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†è´¨é‡å¯¹æ¨¡å‹æ•ˆæœçš„é‡è¦æ€§ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¤§å¤šåŸºäºçŸ­è§†é¢‘ç‰‡æ®µï¼Œè€Œé•¿è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶å—åˆ°é«˜è´¨é‡é•¿è§†é¢‘ç¼ºä¹çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†é€‰æ‹©æµç¨‹ï¼Œä¸“æ³¨äºé•¿è§†é¢‘çš„è´¨é‡è¯„ä¼°å’Œå¯†é›†å­—å¹•ç”Ÿæˆã€‚æœ€ç»ˆï¼Œä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºLVD-2Mçš„æ•°æ®é›†ï¼ŒåŒ…å«200ä¸‡æ®µé•¿è§†é¢‘ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11710', 'title': 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.11710', 'abstract': 'Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.', 'score': 18, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '44b55db6ab50c3d4', 'authors': ['Pei Wang', 'Yanan Wu', 'Zekun Wang', 'Jiaheng Liu', 'Xiaoshuai Song', 'Zhongyuan Peng', 'Ken Deng', 'Chenchen Zhang', 'Jiakai Wang', 'Junran Peng', 'Ge Zhang', 'Hangyu Guo', 'Zhaoxiang Zhang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'University of Chinese Academy of Sciences', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11710.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#data', '#benchmark', '#open_source'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² LLM Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MTU-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). MTU-Bench Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ API GPT Ğ¸Ğ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MTU-Instruct Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'MTU-Bench: Elevating LLM Tool-Use Evaluation', 'desc': "The paper introduces MTU-Bench, a new benchmark designed to evaluate large language models (LLMs) on their ability to use tools effectively. MTU-Bench addresses the limitations of existing datasets by providing a multi-granularity approach, covering various tool-use scenarios such as single or multiple tools and turns, as well as out-of-distribution tasks. Unlike previous benchmarks, MTU-Bench relies solely on prediction results and ground truth for evaluation, avoiding costly GPT or human assessments. Additionally, the authors present MTU-Instruct, a dataset aimed at enhancing LLMs' tool-use capabilities, demonstrating its effectiveness through comprehensive experiments."}, 'zh': {'title': 'MTU-Benchï¼šæå‡è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œå†³ç­–èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶å¯¹è¯ã€‚ç°æœ‰çš„å·¥å…·ä½¿ç”¨åŸºå‡†æ•°æ®é›†å­˜åœ¨è¯„ä¼°åœºæ™¯ä¸è¶³å’Œè¯„ä¼°æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šç²’åº¦å·¥å…·ä½¿ç”¨åŸºå‡†MTU-Benchï¼Œæ¶µç›–äº”ç§å·¥å…·ä½¿ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTU-Benchåœ¨æå‡è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11795', 'title': 'Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices', 'url': 'https://huggingface.co/papers/2410.11795', 'abstract': 'As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey', 'score': 16, 'issue_id': 124, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'e1a8ba0c4193aa99', 'authors': ['Zhiyuan Ma', 'Yuzhu Zhang', 'Guoli Jia', 'Liangliang Zhao', 'Yichao Ma', 'Mingjie Ma', 'Gaofeng Liu', 'Kaiyan Zhang', 'Jianjun Li', 'Bowen Zhou'], 'affiliations': ['HUST', 'SJTU', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11795.jpg', 'data': {'categories': ['#video', '#survey', '#training', '#inference', '#diffusion', '#architecture', '#multimodal', '#3d'], 'emoji': 'ğŸŒ€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¾ÑÑ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ»ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Unlocking the Power of Diffusion Models: Efficiency in Generative Tasks', 'desc': 'Diffusion models have become a popular choice for generative tasks like image and video creation due to their strong theoretical foundations and practical applications. This paper reviews the design principles and methodologies that make diffusion models efficient and effective. It aims to provide a comprehensive understanding of these models to facilitate their application in new scenarios. The survey focuses on architecture design, model training, fast inference, and reliable deployment to guide future research and development.'}, 'zh': {'title': 'æ‰©æ•£æ¨¡å‹ï¼šé«˜æ•ˆç”Ÿæˆçš„æœªæ¥', 'desc': 'æ‰©æ•£æ¨¡å‹æ˜¯è¿‘å¹´æ¥éå¸¸å—æ¬¢è¿çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆæˆã€è§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ã€‚å®ƒä»¬çš„æˆåŠŸä¸»è¦å½’åŠŸäºæ¸è¿›çš„è®¾è®¡åŸåˆ™å’Œé«˜æ•ˆçš„æ¶æ„ã€è®­ç»ƒã€æ¨ç†åŠéƒ¨ç½²æ–¹æ³•ã€‚æœ¬æ–‡ç»¼è¿°äº†è¿™äº›åŸåˆ™å’Œå®è·µï¼Œæä¾›äº†ä¸€ä¸ªä»¥æ•ˆç‡ä¸ºå¯¼å‘çš„æ–°è§†è§’ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¸®åŠ©è¯»è€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11096', 'title': 'SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI', 'url': 'https://huggingface.co/papers/2410.11096', 'abstract': "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'f1b69c45d56f76d8', 'authors': ['Yu Yang', 'Yuzhou Nie', 'Zhun Wang', 'Yuheng Tang', 'Wenbo Guo', 'Bo Li', 'Dawn Song'], 'affiliations': ['University of California, Berkeley', 'University of California, Los Angeles', 'University of California, Santa Barbara', 'University of Illinois Urbana-Champaign', 'Virtue AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11096.jpg', 'data': {'categories': ['#security', '#data', '#plp', '#agents', '#benchmark', '#optimization', '#synthetic'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'SecCodePLT: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI-ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SecCodePLT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ AI Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SecCodePLT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ°Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SecCodePLT Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'SecCodePLT: Elevating Code GenAI Security Evaluation', 'desc': "The paper introduces SecCodePLT, a new platform designed to evaluate the security risks of Code GenAI models more effectively. It addresses the limitations of existing benchmarks by combining expert input with automatic data generation to ensure high-quality, large-scale data for insecure code evaluation. The platform also uses dynamic evaluation metrics, such as test cases, to assess the models' ability to generate executable attacks in a real environment. Extensive experiments demonstrate that SecCodePLT surpasses current benchmarks in identifying security risks, revealing significant vulnerabilities in advanced coding agents like Cursor."}, 'zh': {'title': 'SecCodePLTï¼šæå‡ä»£ç ç”ŸæˆAIå®‰å…¨è¯„ä¼°çš„æ–°å¹³å°', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSecCodePLTçš„æ–°å¹³å°ï¼Œç”¨äºè¯„ä¼°ä»£ç ç”ŸæˆAIçš„å®‰å…¨é£é™©ã€‚SecCodePLTé€šè¿‡ç»“åˆä¸“å®¶å’Œè‡ªåŠ¨ç”Ÿæˆçš„æ–¹æ³•ï¼Œåˆ›å»ºé«˜è´¨é‡ä¸”å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€è¯„ä¼°æ–¹æ³•æ¥æ£€æµ‹ä¸å®‰å…¨ä»£ç å’Œç½‘ç»œæ”»å‡»çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰çš„åŸºå‡†ç›¸æ¯”ï¼ŒSecCodePLTåœ¨å®‰å…¨ç›¸å…³æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¹¶èƒ½æ›´å‡†ç¡®åœ°è¯†åˆ«æœ€å…ˆè¿›æ¨¡å‹çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶è¿˜é¦–æ¬¡åœ¨å…ˆè¿›çš„ä»£ç ä»£ç†Cursorä¸­å‘ç°äº†æ˜¾è‘—çš„å®‰å…¨é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09704', 'title': 'EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation', 'url': 'https://huggingface.co/papers/2410.09704', 'abstract': 'Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '5bf065e3ea6b9f7a', 'authors': ['Milos Vukadinovic', 'Xiu Tang', 'Neal Yuan', 'Paul Cheng', 'Debiao Li', 'Susan Cheng', 'Bryan He', 'David Ouyang'], 'affiliations': ['Biomedical Imaging Research Institute, Cedars-Sinai Medical Center, Los Angeles, CA', 'Department of Bioengineering, University of California Los Angeles, Los Angeles, CA', 'Department of Cardiology, Smidt Heart Institute, Cedars-Sinai Medical Center, Los Angeles, CA', 'Department of Computer Science, Stanford University, Stanford, CA', 'Department of Medicine, University of California, San Francisco, CA', 'Division of Artificial Intelligence in Medicine, Cedars-Sinai Medical Center, Los Angeles, CA', 'Division of Cardiology, Department of Medicine, Stanford University, Palo Alto, CA', 'Division of Cardiology, San Francisco Veterans Affairs Medical Center, San Francisco, CA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09704.jpg', 'data': {'categories': ['#science', '#video', '#dataset', '#cv', '#healthcare', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#multimodal'], 'emoji': 'â¤ï¸', 'ru': {'title': 'EchoPrime: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼', 'desc': 'EchoPrime - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. EchoPrime Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² 23 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞµÑ€Ğ´Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾ Ğ² Ğ´Ğ²ÑƒÑ… Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'EchoPrime: Revolutionizing Heart Imaging with Multi-View AI', 'desc': 'EchoPrime is a new AI model designed to improve echocardiography by integrating information from multiple ultrasound views, unlike traditional single-view models. It uses contrastive learning to create a unified representation of echocardiogram data, capturing both common and rare cardiac conditions. The model employs view-classification and anatomic attention to accurately interpret the relationship between different views and heart structures. EchoPrime has demonstrated superior performance across various benchmarks, offering a more comprehensive and precise tool for cardiac assessment.'}, 'zh': {'title': 'EchoPrimeï¼šå¤šè§†è§’å¿ƒè„è¶…å£°AIçš„é©æ–°', 'desc': 'EchoPrime æ˜¯ä¸€ä¸ªå¤šè§†è§’ã€è§†å›¾çŸ¥æƒ…çš„è§†é¢‘è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå•è§†è§’AIæ¨¡å‹çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ ‡å‡†å¿ƒè„è¶…å£°æ£€æŸ¥ä¸­çš„æ‰€æœ‰è§†å›¾ã€‚EchoPrime åˆ©ç”¨è§†å›¾åˆ†ç±»å’Œè§£å‰–æ³¨æ„æ¨¡å‹ï¼Œå‡†ç¡®æ˜ å°„è¶…å£°è§†å›¾ä¸è§£å‰–ç»“æ„ä¹‹é—´çš„å…³ç³»ã€‚ç»è¿‡ä¸¥æ ¼çš„ä¸´åºŠè¯„ä¼°ï¼ŒEchoPrime åœ¨å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„23ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿè¿›è¡Œè‡ªåŠ¨åŒ–çš„åˆæ­¥è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11805', 'title': 'NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models', 'url': 'https://huggingface.co/papers/2410.11805', 'abstract': 'Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '9823a7c3dbc76e44', 'authors': ['Han Han', 'Tong Zhu', 'Xiang Zhang', 'Mengsong Wu', 'Hao Xiong', 'Wenliang Chen'], 'affiliations': ['Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11805.jpg', 'data': {'categories': ['#dataset', '#training', '#rag', '#rl', '#optimization', '#benchmark', '#synthetic'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'NesTools: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ NesTools Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². NesTools Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑˆĞµĞ» Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 22 LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NesTools Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼.'}, 'en': {'title': "NesTools: Elevating LLMs' Nested Tool Learning Evaluation", 'desc': "The paper introduces NesTools, a new benchmark designed to evaluate the nested tool learning capabilities of large language models (LLMs). NesTools uses an innovative automatic data generation method to create complex nested tool call scenarios, which are then manually reviewed to ensure high quality and real-world relevance. The study highlights that current LLMs struggle with these complex tasks, as demonstrated through extensive experiments on 22 different models. This work aims to fill the gap in existing research by providing a comprehensive dataset for assessing LLMs' ability to handle nested tool learning."}, 'zh': {'title': 'NesToolsï¼šæå‡åµŒå¥—å·¥å…·å­¦ä¹ çš„å…¨æ–°åŸºå‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNesToolsçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›ã€‚NesToolsé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤§é‡ä¸åŒåµŒå¥—ç»“æ„çš„å·¥å…·è°ƒç”¨æ•°æ®ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†ä¸­ç¼ºä¹ç›¸å…³æ•°æ®å®ä¾‹çš„ç©ºç™½ã€‚ç»è¿‡äººå·¥å®¡æ ¸å’Œæ”¹è¿›ï¼Œæ•°æ®é›†è´¨é‡é«˜ä¸”ä¸ç°å®åœºæ™¯ç´§å¯†ç»“åˆã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨å¤„ç†å¤æ‚çš„åµŒå¥—å·¥å…·å­¦ä¹ ä»»åŠ¡æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.10934', 'title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'url': 'https://huggingface.co/papers/2410.10934', 'abstract': 'Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.', 'score': 10, 'issue_id': 127, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'a25e89cb7be0bed4', 'authors': ['Mingchen Zhuge', 'Changsheng Zhao', 'Dylan Ashley', 'Wenyi Wang', 'Dmitrii Khizbullin', 'Yunyang Xiong', 'Zechun Liu', 'Ernie Chang', 'Raghuraman Krishnamoorthi', 'Yuandong Tian', 'Yangyang Shi', 'Vikas Chandra', 'JÃ¼rgen Schmidhuber'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10934.jpg', 'data': {'categories': ['#interpretability', '#plp', '#agents', '#benchmark', '#optimization', '#alignment', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚ ÑÑƒĞ´Ğ¸Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Agent-as-a-Judge. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Agent-as-a-Judge Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DevAI, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 55 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ˜Ğ˜. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Agent-as-a-Judge Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LLM-as-a-Judge Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': '"Agent-as-a-Judge: Revolutionizing Evaluation for Agentic Systems"', 'desc': 'The paper introduces the Agent-as-a-Judge framework, which uses agentic systems to evaluate other agentic systems, providing intermediate feedback throughout the task-solving process. This approach is applied to code generation and is tested using a new benchmark called DevAI, which includes 55 realistic AI development tasks with detailed annotations. The framework is shown to outperform existing evaluation methods like LLM-as-a-Judge and matches human evaluation reliability. Overall, Agent-as-a-Judge offers a significant advancement in providing dynamic and scalable feedback for agentic systems.'}, 'zh': {'title': 'ä»£ç†å³è¯„åˆ¤è€…ï¼šç°ä»£ä»£ç†ç³»ç»Ÿè¯„ä¼°çš„æ–°æ–¹å‘', 'desc': 'ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¯¹ä»£ç†ç³»ç»Ÿæ¥è¯´ä¸å¤Ÿå®Œå–„ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆåªå…³æ³¨æœ€ç»ˆç»“æœè€Œå¿½ç•¥äº†ä»£ç†ç³»ç»Ÿçš„é€æ­¥ç‰¹æ€§ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥åŠ³åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»£ç†å³è¯„åˆ¤è€…â€æ¡†æ¶ï¼Œåˆ©ç”¨ä»£ç†ç³»ç»Ÿæ¥è¯„ä¼°ä»£ç†ç³»ç»Ÿã€‚è¿™ç§æ–¹æ³•æ˜¯â€œLLMå³è¯„åˆ¤è€…â€æ¡†æ¶çš„è‡ªç„¶æ‰©å±•ï¼ŒåŠ å…¥äº†ä»£ç†ç‰¹æ€§ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªä»»åŠ¡è§£å†³è¿‡ç¨‹ä¸­æä¾›ä¸­é—´åé¦ˆã€‚æˆ‘ä»¬åº”ç”¨â€œä»£ç†å³è¯„åˆ¤è€…â€äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æ–°çš„åŸºå‡†DevAIï¼Œè¯æ˜å…¶åœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11419', 'title': 'GS^3: Efficient Relighting with Triple Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.11419', 'abstract': 'We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.', 'score': 10, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '8aa8e4a555a54086', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#cv', '#training', '#open_source', '#architecture', '#synthetic', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ñ€Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ»Ğ°Ğ¼Ğ±ĞµÑ€Ñ‚Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ.'}, 'en': {'title': '"Illuminate and Transform: Real-Time View Synthesis with Gaussian Magic"', 'desc': 'This paper introduces a novel method for creating high-quality images with new lighting and viewpoints using a spatial and angular Gaussian representation combined with a triple splatting process. The approach models complex appearances by using a Lambertian and angular Gaussian mixture reflectance function for each spatial Gaussian. To handle self-shadowing, spatial Gaussians are projected towards the light source, with shadow values refined by a small neural network, while another network adjusts for global illumination effects. The method is tested on diverse samples and achieves fast training and rendering times, outperforming existing techniques in quality and performance.'}, 'zh': {'title': 'å®æ—¶é«˜è´¨é‡å…‰ç…§åˆæˆçš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç©ºé—´å’Œè§’åº¦é«˜æ–¯çš„è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥åŠä¸‰é‡å–·æº…è¿‡ç¨‹ï¼Œç”¨äºä»å¤šè§†ç‚¹å…‰ç…§è¾“å…¥å›¾åƒä¸­å®æ—¶ç”Ÿæˆé«˜è´¨é‡çš„æ–°å…‰ç…§å’Œè§†å›¾åˆæˆã€‚ä¸ºäº†æè¿°å¤æ‚çš„å¤–è§‚ï¼Œä½œè€…ä½¿ç”¨äº†æœ—ä¼¯åå°„åŠ ä¸Šè§’åº¦é«˜æ–¯æ··åˆä½œä¸ºæ¯ä¸ªç©ºé—´é«˜æ–¯çš„æœ‰æ•ˆåå°„å‡½æ•°ã€‚ä¸ºäº†ç”Ÿæˆè‡ªé˜´å½±ï¼Œæ‰€æœ‰ç©ºé—´é«˜æ–¯è¢«å–·æº…åˆ°å…‰æºæ–¹å‘ä»¥è·å¾—é˜´å½±å€¼ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå°å‹å¤šå±‚æ„ŸçŸ¥å™¨è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ä¸ºäº†è¡¥å¿å…¨å±€å…‰ç…§ç­‰å…¶ä»–æ•ˆæœï¼Œå¦ä¸€ä¸ªç½‘ç»œè¢«è®­ç»ƒæ¥è®¡ç®—å¹¶æ·»åŠ æ¯ä¸ªç©ºé—´é«˜æ–¯çš„RGBå€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09754', 'title': 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.09754', 'abstract': "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.", 'score': 7, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '04b27b8dbf829651', 'authors': ['Hojoon Lee', 'Dongyoon Hwang', 'Donghu Kim', 'Hyunseung Kim', 'Jun Jet Tai', 'Kaushik Subramanian', 'Peter R. Wurman', 'Jaegul Choo', 'Peter Stone', 'Takuma Seno'], 'affiliations': ['Coventry University', 'KAIST', 'Sony AI', 'UT Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09754.jpg', 'data': {'categories': ['#cv', '#training', '#rl', '#optimization', '#games', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SimBa: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SimBa Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). SimBa Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ·Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SimBa ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ off-policy, on-policy Ğ¸ unsupervised Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ SimBa Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ SAC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RL Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': '"SimBa: Scaling Simplicity in Deep Reinforcement Learning"', 'desc': 'The paper introduces SimBa, a new architecture for deep reinforcement learning (RL) that scales up network parameters while maintaining simplicity to avoid overfitting. SimBa includes an observation normalization layer, a residual feedforward block, and a layer normalization to enhance model performance. By using SimBa, various deep RL algorithms show improved sample efficiency and computational effectiveness. The architecture proves its versatility by matching or surpassing state-of-the-art methods across different environments and tasks.'}, 'zh': {'title': 'SimBaï¼šé€šè¿‡ç®€å•æ€§åå·®æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ', 'desc': 'è¿‘å¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸé€šè¿‡å¢åŠ ç½‘ç»œå‚æ•°æ•°é‡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°½ç®¡ä¼ ç»Ÿç†è®ºè®¤ä¸ºæ›´å¤§çš„ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚è¿™äº›å¤§å‹ç½‘ç»œé€šè¿‡å¼•å…¥ç®€å•æ€§åå·®çš„ç»„ä»¶æ¥é¿å…è¿‡æ‹Ÿåˆï¼ŒæŒ‡å¯¼æ¨¡å‹æœå‘ç®€å•ä¸”å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç½‘ç»œè®¾è®¡å’Œæ‰©å±•çš„ç ”ç©¶è¾ƒå°‘ã€‚SimBaæ¶æ„é€šè¿‡æ³¨å…¥ç®€å•æ€§åå·®æ¥æ‰©å±•æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å‚æ•°ï¼Œæé«˜äº†å¤šç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ·æœ¬æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.08001', 'title': 'Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.08001', 'abstract': 'The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/', 'score': 3, 'issue_id': 122, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'f9a57a673dfbc6f6', 'authors': ['Qingwen Bu', 'Hongyang Li', 'Li Chen', 'Jisong Cai', 'Jia Zeng', 'Heming Cui', 'Maoqing Yao', 'Yu Qiao'], 'affiliations': ['AgiBot', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08001.jpg', 'data': {'categories': ['#reasoning', '#training', '#agi', '#inference', '#games', '#open_source', '#diffusion', '#small_models', '#architecture', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboDual: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'RoboDual - ÑÑ‚Ğ¾ ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language-action Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. RoboDual Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ OpenVLA ĞºĞ°Ğº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 5% Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'RoboDual: Merging Generalist Flexibility with Specialist Precision', 'desc': 'The paper introduces RoboDual, a dual-system approach that combines the strengths of both generalist and specialist policies for robotic systems. The generalist policy uses a vision-language-action model to provide high-level task understanding, while the specialist policy, based on a diffusion transformer, focuses on efficient multi-step action rollouts. RoboDual significantly improves performance in real-world settings and specific benchmarks by using a specialist policy with a small number of trainable parameters. This system achieves high efficiency and adaptability, even with limited demonstration data, and enhances control frequency in practical applications.'}, 'zh': {'title': 'RoboDualï¼šé€šç”¨ä¸ä¸“ä¸šç­–ç•¥çš„å®Œç¾ç»“åˆ', 'desc': 'RoboDual æ˜¯ä¸€ä¸ªç»“åˆé€šç”¨ç­–ç•¥å’Œä¸“ä¸šç­–ç•¥çš„åŒç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æœºå™¨äººåœ¨å¤šå˜ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚é€šç”¨ç­–ç•¥åˆ©ç”¨å¤§è§„æ¨¡è·¨ä½“æ•°æ®ï¼Œæä¾›é«˜å±‚æ¬¡çš„ä»»åŠ¡ç†è§£ï¼Œè€Œä¸“ä¸šç­–ç•¥åˆ™ä¸“æ³¨äºç‰¹å®šä»»åŠ¡çš„é«˜æ•ˆæ‰§è¡Œã€‚RoboDual é€šè¿‡å¼•å…¥åŸºäºæ‰©æ•£å˜å‹å™¨çš„ä¸“ä¸šç­–ç•¥ï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç³»ç»Ÿåœ¨ä»…ä½¿ç”¨å°‘é‡ç¤ºèŒƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒé«˜æ•ˆçš„æ§åˆ¶é¢‘ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.06593', 'title': 'Towards Natural Image Matting in the Wild via Real-Scenario Prior', 'url': 'https://huggingface.co/papers/2410.06593', 'abstract': 'Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.', 'score': 2, 'issue_id': 126, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '656804d454a782ca', 'authors': ['Ruihao Xia', 'Yu Liang', 'Peng-Tao Jiang', 'Hao Zhang', 'Qianru Sun', 'Yang Tang', 'Bo Li', 'Pan Zhou'], 'affiliations': ['East China University of Science and Technology', 'Singapore Management University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.06593.jpg', 'data': {'categories': ['#dataset', '#cv', '#training', '#graphs', '#open_source', '#architecture', '#synthetic'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğµ: Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ COCO-Matting Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸Ğ· COCO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SEMat, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SAM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ feature-aligned transformer Ğ¸ matte-aligned decoder. Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ trimap loss. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Matting with Real-World Data and Advanced Architectures', 'desc': 'The paper introduces a new dataset called COCO-Matting, which enhances the generalization of matting models to complex real-world scenes by using real images from the COCO dataset. It addresses the limitations of models trained on synthetic data by converting semantic segmentation masks into matting labels. The authors propose a novel model, SEMat, which includes a feature-aligned transformer and a matte-aligned decoder to improve edge and transparency feature extraction. Their approach, validated through extensive experiments, shows superior performance in interactive image matting, leveraging both new training objectives and architecture improvements.'}, 'zh': {'title': 'SEMatï¼šæå‡äº¤äº’å¼æŠ å›¾çš„ç²¾åº¦ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ å›¾æ•°æ®é›†COCO-Mattingï¼ŒåŸºäºCOCOæ•°æ®é›†æ„å»ºï¼Œæ—¨åœ¨è§£å†³åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚COCO-Mattingé€šè¿‡ä»COCOä¸­é€‰æ‹©çœŸå®ä¸–ç•Œçš„å¤æ‚å›¾åƒï¼Œå¹¶å°†è¯­ä¹‰åˆ†å‰²æ©ç è½¬æ¢ä¸ºæŠ å›¾æ ‡ç­¾ï¼ŒåŒ…å«äº†38,251ä¸ªäººç‰©å®ä¾‹çº§åˆ«çš„alpha mattesã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„SAMæ¨¡å‹ï¼Œä½œè€…æå‡ºäº†SEMatï¼Œé€šè¿‡ç‰¹å¾å¯¹é½çš„transformerå’ŒæŠ å›¾å¯¹é½çš„è§£ç å™¨æ¥æå–ç»†ç²’åº¦çš„è¾¹ç¼˜å’Œé€æ˜åº¦ç‰¹å¾ï¼Œå¹¶å°†ç²—ç•¥çš„æ©ç è½¬æ¢ä¸ºé«˜ç²¾åº¦çš„æŠ å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMatåœ¨ä¸ƒä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨äº¤äº’å¼è‡ªç„¶å›¾åƒæŠ å›¾ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09745', 'title': 'Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt', 'url': 'https://huggingface.co/papers/2410.09745', 'abstract': "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.", 'score': 2, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '9e1731e2bbd0b081', 'authors': ['Chengguang Gan', 'Tatsunori Mori'], 'affiliations': ['Yokohama National University, Japan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09745.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#optimization', '#interpretability'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ (MRE) Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 21 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MRE Mix Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ MRE. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MRE. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MRE Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ F1-Ğ¼ĞµÑ€Ñƒ Ğ½Ğ° 18 Ğ¸Ğ· 21 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Boosting Text Understanding: The Power of Word-Level Insights', 'desc': "The paper explores the Mutual Reinforcement Effect (MRE), which suggests that word-level and text-level classifications can improve each other's performance in text classification tasks. Through empirical experiments on 21 datasets, the study demonstrates the presence and impact of MRE, showing that word-level information can enhance text-level predictions. The researchers also apply MRE to prompt learning, using word-level data to improve the model's text-level classification accuracy. The experiments show a significant improvement in F1-scores, confirming that word-level insights can enhance overall text comprehension by the model."}, 'zh': {'title': 'è¯çº§ä¸æ–‡æœ¬çº§åˆ†ç±»çš„ç›¸äº’å¢å¼ºæ•ˆåº”', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è¯çº§å’Œæ–‡æœ¬çº§åˆ†ç±»åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„ç›¸äº’å¢å¼ºæ•ˆåº”ã€‚é€šè¿‡å®éªŒè¯æ˜äº†è¿™ç§æ•ˆåº”çš„å­˜åœ¨ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶è¿˜å°†è¿™ç§æ•ˆåº”åº”ç”¨äºæç¤ºå­¦ä¹ ï¼Œåˆ©ç”¨è¯çº§ä¿¡æ¯æ¥å¢å¼ºæ–‡æœ¬çº§åˆ†ç±»æ ‡ç­¾çš„é¢„æµ‹ã€‚æœ€ç»ˆå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨21ä¸ªæ•°æ®é›†ä¸­çš„18ä¸ªï¼ŒF1åˆ†æ•°æ˜¾è‘—è¶…è¿‡åŸºçº¿ï¼ŒéªŒè¯äº†è¯çº§ä¿¡æ¯å¯¹è¯­è¨€æ¨¡å‹ç†è§£æ–‡æœ¬æ•´ä½“çš„æå‡ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11619', 'title': 'MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval', 'url': 'https://huggingface.co/papers/2410.11619', 'abstract': 'Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.', 'score': 0, 'issue_id': 130, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'dd603cadf5349b0c', 'authors': ['Reno Kriz', 'Kate Sanders', 'David Etter', 'Kenton Murray', 'Cameron Carpenter', 'Kelly Van Ochten', 'Hannah Recknor', 'Jimena Guallar-Blasco', 'Alexander Martin', 'Ronald Colaianni', 'Nolan King', 'Eugene Yang', 'Benjamin Van Durme'], 'affiliations': ['Human Language Technology Center of Excellence, Johns Hopkins University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11619.jpg', 'data': {'categories': ['#science', '#video', '#survey', '#dataset', '#multilingual', '#graphs', '#benchmark', '#games', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MultiVENT 2.0: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'MultiVENT 2.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 218 000 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3 906 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Unlocking the Power of Multimodal Video Retrieval', 'desc': 'The paper introduces MultiVENT 2.0, a new benchmark for video retrieval that focuses on multilingual and event-centric content, addressing the limitations of existing datasets. It includes over 218,000 news videos and 3,906 queries that require systems to utilize visual, audio, and text data effectively. Initial tests reveal that current vision-language models struggle with this complex task, highlighting the need for more advanced multimodal retrieval systems. This research emphasizes the importance of improving video retrieval to enhance multimodal content understanding and generation.'}, 'zh': {'title': 'çªç ´å¤šæ¨¡æ€è§†é¢‘æ£€ç´¢çš„ç“¶é¢ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMultiVENT 2.0çš„å¤§è§„æ¨¡å¤šè¯­è¨€äº‹ä»¶ä¸­å¿ƒè§†é¢‘æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘æ£€ç´¢æ•°æ®é›†èŒƒå›´æœ‰é™çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡218,000ä¸ªæ–°é—»è§†é¢‘å’Œ3,906ä¸ªé’ˆå¯¹ç‰¹å®šä¸–ç•Œäº‹ä»¶çš„æŸ¥è¯¢ï¼Œè¦æ±‚ç³»ç»Ÿåˆ©ç”¨è§†é¢‘çš„è§†è§‰å†…å®¹ã€éŸ³é¢‘ã€åµŒå…¥æ–‡æœ¬å’Œæ–‡æœ¬å…ƒæ•°æ®æ¥å®Œæˆä»»åŠ¡ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œå…¶ä»–æ–¹æ³•è™½ç„¶æœ‰æ½œåŠ›ï¼Œä½†ä»ä¸è¶³ä»¥è§£å†³é—®é¢˜ã€‚è¿™è¡¨æ˜éœ€è¦æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿï¼Œä»¥å®ç°å¯¹å¤šæ¨¡æ€å†…å®¹çš„ç†è§£å’Œç”Ÿæˆã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (2)', '#alignment (1)', '#architecture (12)', '#audio', '#benchmark (8)', '#cv (4)', '#data (3)', '#dataset (10)', '#diffusion (2)', '#ethics', '#games (3)', '#graphs (2)', '#hallucinations (1)', '#healthcare (2)', '#inference (3)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (4)', '#open_source (7)', '#optimization (9)', '#plp (2)', '#rag (1)', '#reasoning (3)', '#rl (2)', '#rlhf', '#robotics (1)', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (5)', '#training (12)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-16 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-16 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-16 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    