{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2026-01-22 06:38",
    "weekday": 3,
    "issue_id": 708,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.12538",
            "title": "Agentic Reasoning for Large Language Models",
            "url": "https://huggingface.co/papers/2601.12538",
            "abstract": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
            "score": 76,
            "issue_id": 705,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "889c44cfdc0d7221",
            "authors": [
                "Tianxin Wei",
                "Ting-Wei Li",
                "Zhining Liu",
                "Xuying Ning",
                "Ze Yang",
                "Jiaru Zou",
                "Zhichen Zeng",
                "Ruizhong Qiu",
                "Xiao Lin",
                "Dongqi Fu",
                "Zihao Li",
                "Mengting Ai",
                "Duo Zhou",
                "Wenxuan Bao",
                "Yunzhe Li",
                "Gaotang Li",
                "Cheng Qian",
                "Yu Wang",
                "Xiangru Tang",
                "Yin Xiao",
                "Liri Fang",
                "Hui Liu",
                "Xianfeng Tang",
                "Yuji Zhang",
                "Chi Wang",
                "Jiaxuan You",
                "Heng Ji",
                "Hanghang Tong",
                "Jingrui He"
            ],
            "affiliations": [
                "Amazon",
                "Google Deepmind",
                "Meta",
                "UCSD",
                "University of Illinois Urbana-Champaign",
                "Yale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12538.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#healthcare",
                    "#training",
                    "#survey",
                    "#math",
                    "#benchmark",
                    "#science",
                    "#rl",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: LLM ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ), ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ (Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ) Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ (Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ) Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒĞºĞ¸, Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs as Autonomous Agents through Agentic Reasoning",
                    "desc": "This paper introduces agentic reasoning, which transforms large language models (LLMs) into autonomous agents that can plan, act, and learn in changing environments. It categorizes agentic reasoning into three levels: foundational capabilities for single agents, self-evolving abilities that adapt through experience, and collective reasoning for multi-agent collaboration. The authors differentiate between in-context reasoning, which enhances interaction during use, and post-training reasoning, which improves performance through learning techniques. The survey also highlights practical applications and future challenges in deploying these advanced reasoning frameworks in real-world scenarios."
                },
                "zh": {
                    "title": "ä»£ç†æ¨ç†ï¼šè®©è¯­è¨€æ¨¡å‹æˆä¸ºè‡ªä¸»æ™ºèƒ½ä½“",
                    "desc": "ä»£ç†æ¨ç†é‡æ–°å®šä¹‰äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œå­¦ä¹ çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­ä¸–ç•Œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¼€æ”¾å’ŒåŠ¨æ€ç¯å¢ƒä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚ä»£ç†æ¨ç†é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œæ ‡å¿—ç€ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼Œå¼ºè°ƒæŒç»­äº’åŠ¨çš„é‡è¦æ€§ã€‚æœ¬æ–‡ç»„ç»‡äº†ä»£ç†æ¨ç†çš„ä¸‰ä¸ªäº’è¡¥ç»´åº¦ï¼ŒåŒ…æ‹¬åŸºç¡€ä»£ç†æ¨ç†ã€è‡ªæˆ‘æ¼”åŒ–ä»£ç†æ¨ç†å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ€è·¯ä¸è¡ŒåŠ¨çš„è·¯çº¿å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15282",
            "title": "Rethinking Video Generation Model for the Embodied World",
            "url": "https://huggingface.co/papers/2601.15282",
            "abstract": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and embodiments, revealing deficiencies in physical realism and introducing a large-scale dataset to address training data shortages.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
            "score": 20,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "cb5ee149b1d3627f",
            "authors": [
                "Yufan Deng",
                "Zilin Pan",
                "Hongyu Zhang",
                "Xiaojie Li",
                "Ruoqing Hu",
                "Yufei Ding",
                "Yiming Zou",
                "Yan Zeng",
                "Daquan Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15282.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ»Ğ°ÑƒÑĞ¸Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ RoVid-X â€” Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ­Ñ‚Ğ° ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "RBench: Elevating Video Generation for Robotics with Comprehensive Evaluation and Data",
                    "desc": "This paper presents RBench, a comprehensive benchmark for evaluating video generation models in robotics across various tasks and embodiments. It identifies significant shortcomings in the physical realism of generated videos and proposes a refined data pipeline to create RoVid-X, a large-scale dataset with 4 million annotated video clips. The benchmark assesses models based on task correctness and visual fidelity, achieving a high correlation with human evaluations. By combining rigorous evaluation with a rich dataset, the study aims to enhance the training of video generation models and advance embodied AI capabilities."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°æœºå™¨äººè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„æœºå™¨äººåŸºå‡†æµ‹è¯•RBenchï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡é¢†åŸŸå’Œä¸åŒä½“ç°ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆç‰©ç†çœŸå®æ„Ÿæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¹¶ä¸”ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†é™åˆ¶äº†å…¬å¹³æ¯”è¾ƒå’Œè¿›å±•ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®çŸ­ç¼ºçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†RoVid-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«400ä¸‡ä¸ªæ ‡æ³¨è§†é¢‘ç‰‡æ®µçš„å¤§å‹å¼€æºæ•°æ®é›†ï¼Œæ¶µç›–äº†æ•°åƒä¸ªä»»åŠ¡ï¼Œå¹¶é™„æœ‰å…¨é¢çš„ç‰©ç†å±æ€§æ³¨é‡Šã€‚é€šè¿‡è¯„ä¼°å’Œæ•°æ®çš„ååŒä½œç”¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œä»¥ä¿ƒè¿›è§†é¢‘æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°å’Œå¯æ‰©å±•è®­ç»ƒï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½å‘é€šç”¨æ™ºèƒ½çš„æ¼”è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14171",
            "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
            "url": "https://huggingface.co/papers/2601.14171",
            "abstract": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.  \t\t\t\t\tAI-generated summary \t\t\t\t Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
            "score": 19,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "a4a9f4fe5907584a",
            "authors": [
                "Qianli Ma",
                "Chang Guo",
                "Zhiheng Tian",
                "Siyu Wang",
                "Jipeng Xiao",
                "Yuanhao Yue",
                "Zhipeng Zhang"
            ],
            "affiliations": [
                "AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14171.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#hallucinations",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "RebuttalAgent â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞŸĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "RebuttalAgent: Transforming Rebuttal Generation with Evidence-Centric Planning",
                    "desc": "RebuttalAgent is a novel multi-agent framework designed to enhance the process of generating rebuttals in academic peer reviews. It shifts the focus from simple text generation to an evidence-centric planning approach, which helps in addressing critiques more effectively. By breaking down complex feedback into manageable parts and utilizing external literature for support, it improves the accuracy and reliability of the rebuttals. The framework has been validated against existing methods, showing superior performance in terms of coverage, faithfulness, and strategic coherence."
                },
                "zh": {
                    "title": "ä»¥è¯æ®ä¸ºä¸­å¿ƒçš„åé©³ç”Ÿæˆæ¡†æ¶",
                    "desc": "RebuttalAgentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†åé©³ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»¥è¯æ®ä¸ºä¸­å¿ƒçš„è§„åˆ’ä»»åŠ¡ï¼Œä»è€Œæé«˜å­¦æœ¯åŒè¡Œè¯„å®¡ä¸­çš„è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§ã€‚è¯¥ç³»ç»Ÿå°†å¤æ‚çš„åé¦ˆåˆ†è§£ä¸ºåŸºæœ¬é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆæˆå‹ç¼©æ‘˜è¦ä¸é«˜ä¿çœŸæ–‡æœ¬åŠ¨æ€æ„å»ºæ··åˆä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶é›†æˆä¸€ä¸ªè‡ªä¸»çš„å¤–éƒ¨æœç´¢æ¨¡å—ï¼Œä»¥è§£å†³éœ€è¦å¤–éƒ¨æ–‡çŒ®çš„å…³æ³¨ç‚¹ã€‚é€šè¿‡åœ¨èµ·è‰ä¹‹å‰ç”Ÿæˆå¯æ£€æŸ¥çš„å“åº”è®¡åˆ’ï¼ŒRebuttalAgentç¡®ä¿æ¯ä¸ªè®ºç‚¹éƒ½æ˜ç¡®åŸºäºå†…éƒ¨æˆ–å¤–éƒ¨è¯æ®ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨RebuttalBenchä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå¼ºåŸºçº¿ï¼Œæä¾›äº†ä¸€ä¸ªé€æ˜ä¸”å¯æ§çš„åŒè¡Œè¯„å®¡åŠ©æ‰‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12346",
            "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
            "url": "https://huggingface.co/papers/2601.12346",
            "abstract": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.",
            "score": 14,
            "issue_id": 706,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "31732de0e1390005",
            "authors": [
                "Peizhou Huang",
                "Zixuan Zhong",
                "Zhongwei Wan",
                "Donghao Zhou",
                "Samiul Alam",
                "Xin Wang",
                "Zexin Li",
                "Zhihao Dou",
                "Li Zhu",
                "Jing Xiong",
                "Chaofan Tao",
                "Yan Xu",
                "Dimitrios Dimitriadis",
                "Tuo Zhang",
                "Mi Zhang"
            ],
            "affiliations": [
                "Amazon",
                "CUHK",
                "CWRU",
                "HKU",
                "OSU",
                "UCL",
                "UCR",
                "UMich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12346.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#science",
                    "#interpretability",
                    "#multimodal",
                    "#rag"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMDeepResearch-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: FLAE Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°, TRACE Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼, Ğ¸ MOSAIC Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ·Ñ‹ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Evaluating Multimodal Research Agents for Better Report Generation",
                    "desc": "The paper introduces MMDeepResearch-Bench (MMDR-Bench), a new benchmark designed to evaluate multimodal research agents in generating reports that incorporate both text and visual evidence. It consists of 140 tasks across various domains, focusing on the ability of models to synthesize information while accurately citing sources and grounding their claims in visual data. The authors propose a comprehensive evaluation framework that includes multiple metrics to assess report quality, citation accuracy, and the integrity of visual references. The findings indicate that there are significant trade-offs between the quality of generated text, the accuracy of citations, and the effective use of multimodal evidence, suggesting that improving one aspect may compromise another."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ç ”ç©¶ä»£ç†çš„æŠ¥å‘Šç”Ÿæˆæ–°åŸºå‡†",
                    "desc": "MMDeepResearch-Benchï¼ˆMMDR-Benchï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€ç ”ç©¶ä»£ç†åœ¨æŠ¥å‘Šç”Ÿæˆä¸­ä½¿ç”¨è§†è§‰è¯æ®çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«140ä¸ªä¸“å®¶è®¾è®¡çš„ä»»åŠ¡ï¼Œæ¶µç›–21ä¸ªé¢†åŸŸï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€ç†è§£å’ŒåŸºäºå¼•ç”¨çš„æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ã€‚ä¸ä»¥å¾€çš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒMMDR-Benchå¼ºè°ƒæŠ¥å‘Šé£æ ¼çš„åˆæˆï¼Œè¦æ±‚æ¨¡å‹å°†è§†è§‰ææ–™ä¸å¼•ç”¨çš„ä¸»å¼ ç›¸è¿æ¥ï¼Œå¹¶ä¿æŒå™è¿°ã€å¼•ç”¨å’Œè§†è§‰å‚è€ƒçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆè´¨é‡ã€å¼•ç”¨å‡†ç¡®æ€§å’Œå¤šæ¨¡æ€åŸºç¡€ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§çš„æƒè¡¡ï¼Œå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆå¹¶ä¸ä¸€å®šä¿è¯è¯æ®çš„å¯é ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07853",
            "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
            "url": "https://huggingface.co/papers/2601.07853",
            "abstract": "FinVault presents the first execution-grounded security benchmark for financial agents, revealing significant vulnerabilities in current defense mechanisms when applied to real-world financial workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
            "score": 7,
            "issue_id": 706,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "fa449db65899ca42",
            "authors": [
                "Zhi Yang",
                "Runguo Li",
                "Qiqi Qiang",
                "Jiashun Wang",
                "Fangqi Lou",
                "Mengping Li",
                "Dongpo Cheng",
                "Rui Xu",
                "Heng Lian",
                "Shuo Zhang",
                "Xiaolong Liang",
                "Xiaoming Huang",
                "Zheng Wei",
                "Zhaowei Liu",
                "Xin Guo",
                "Huacan Wang",
                "Ronghao Chen",
                "Liwen Zhang"
            ],
            "affiliations": [
                "BUPT",
                "CUHKSZ",
                "FDU",
                "PKU",
                "QuantaAlpha",
                "SUFE",
                "SUIBE",
                "Tencent",
                "UCAS",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07853.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "FinVault Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 31 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸ 963 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ´Ğ¾ 50%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‰Ğ¸Ñ‚ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "FinVault: Strengthening Security for Financial Agents in Real-World Scenarios",
                    "desc": "FinVault introduces a new security benchmark specifically designed for financial agents that utilize large language models (LLMs). This benchmark addresses the vulnerabilities that arise when these agents operate in real-world financial environments, where they perform tasks like investment analysis and risk assessment. The study reveals that current defense mechanisms are inadequate, with high attack success rates even against advanced models. By providing a comprehensive set of scenarios and vulnerabilities, FinVault aims to enhance the security of financial agents and promote the development of more effective defenses."
                },
                "zh": {
                    "title": "é‡‘èä»£ç†çš„å®‰å…¨æ–°åŸºå‡†ï¼šFinVault",
                    "desc": "FinVaultæ˜¯é¦–ä¸ªé’ˆå¯¹é‡‘èä»£ç†çš„æ‰§è¡ŒåŸºç¡€å®‰å…¨åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰é˜²å¾¡æœºåˆ¶åœ¨å®é™…é‡‘èå·¥ä½œæµç¨‹ä¸­å­˜åœ¨çš„é‡å¤§æ¼æ´ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŠ•èµ„åˆ†æã€é£é™©è¯„ä¼°å’Œè‡ªåŠ¨å†³ç­–ä¸­çš„åº”ç”¨ï¼Œæ–°çš„å®‰å…¨é£é™©ä¸æ–­å‡ºç°ã€‚ç°æœ‰çš„å®‰å…¨è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¯­è¨€æ¨¡å‹çš„å†…å®¹åˆè§„æ€§æˆ–æŠ½è±¡ä»£ç†è®¾ç½®ä¸Šï¼Œæœªèƒ½æ•æ‰åˆ°å®é™…æ“ä½œå·¥ä½œæµç¨‹ä¸­äº§ç”Ÿçš„æ‰§è¡ŒåŸºç¡€é£é™©ã€‚FinVaulté€šè¿‡31ä¸ªç›‘ç®¡æ¡ˆä¾‹é©±åŠ¨çš„æ²™ç›’åœºæ™¯å’Œ107ä¸ªçœŸå®ä¸–ç•Œæ¼æ´ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å½“å‰é˜²å¾¡æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰é˜²å¾¡æªæ–½åœ¨ç°å®é‡‘èç¯å¢ƒä¸­ä»ç„¶æ— æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14750",
            "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
            "url": "https://huggingface.co/papers/2601.14750",
            "abstract": "Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
            "score": 4,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "fa7ddd84d3506d5c",
            "authors": [
                "Yifan Wang",
                "Shiyu Li",
                "Peiming Li",
                "Xiaochen Yang",
                "Yang Tang",
                "Zheng Wei"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "School of Mathematics and Statistics, University of Glasgow",
                "Tencent BAC",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14750.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Render-of-Thought (RoT) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°Ğ»Ğ¾Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 3-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ CoT, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Visualizing Reasoning for Enhanced Clarity and Efficiency",
                    "desc": "The Render-of-Thought (RoT) framework transforms reasoning steps from text into images using vision-language models, enhancing the clarity and efficiency of reasoning processes. This approach addresses the limitations of Chain-of-Thought (CoT) prompting, which, while effective, can be verbose and computationally expensive. By visualizing the reasoning chain, RoT makes the underlying rationale more explicit and easier to analyze. Experimental results show that RoT significantly reduces token usage and speeds up inference while maintaining strong performance on reasoning tasks."
                },
                "zh": {
                    "title": "å°†æ¨ç†æ­¥éª¤å¯è§†åŒ–ï¼Œæé«˜æ•ˆç‡ä¸å¯è¿½æº¯æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºRender-of-Thoughtï¼ˆRoTï¼‰ï¼Œæ—¨åœ¨å°†æ–‡æœ¬æ¨ç†æ­¥éª¤è½¬æ¢ä¸ºå›¾åƒï¼Œä»¥æé«˜æ¨ç†çš„å¯è¿½æº¯æ€§å’Œæ•ˆç‡ã€‚RoTåˆ©ç”¨ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰ç¼–ç å™¨ï¼Œå°†è§†è§‰åµŒå…¥ä¸æ–‡æœ¬ç©ºé—´å¯¹é½ï¼Œä»è€Œä½¿æ½œåœ¨çš„æ¨ç†é“¾æ›´åŠ æ˜ç¡®ã€‚ä¸ä¼ ç»Ÿçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç›¸æ¯”ï¼ŒRoTåœ¨ä¿æŒç«äº‰æ€§æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†3-4å€çš„ä»¤ç‰Œå‹ç¼©å’Œæ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚è¯¥æ–¹æ³•çš„å®ç°æ— éœ€é¢å¤–çš„é¢„è®­ç»ƒå¼€é”€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14417",
            "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
            "url": "https://huggingface.co/papers/2601.14417",
            "abstract": "Research investigates the relationship between speaker embeddings and phonological rules in accent control for text-to-speech systems, introducing a metric to measure rule preservation versus embedding influence.  \t\t\t\t\tAI-generated summary \t\t\t\t Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.",
            "score": 2,
            "issue_id": 708,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "91f0d508ead68a91",
            "authors": [
                "Thanathai Lertpetchpun",
                "Yoonjeong Lee",
                "Thanapat Trachu",
                "Jihwan Lee",
                "Tiantian Feng",
                "Dani Byrd",
                "Shrikanth Narayanan"
            ],
            "affiliations": [
                "Department of Linguistics, University of Southern California",
                "Signal Analysis and Interpretation Lab, University of Southern California",
                "Thomas Lord Department of Computer Science, University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14417.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ phoneme shift rate (PSR) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Accent Control in TTS through Phonological Rules and Speaker Embeddings",
                    "desc": "This research explores how speaker embeddings, which are used in text-to-speech (TTS) systems to generate accents, interact with phonological rules that govern speech sounds. The study introduces a new metric called phoneme shift rate (PSR) to measure how well these embeddings maintain or override linguistic rules in accent synthesis. By examining American and British English, the authors demonstrate that combining phonological rules with speaker embeddings can produce more authentic accents. The findings suggest that understanding this relationship can improve accent control and help disentangle speaker identity from accent features in TTS models."
                },
                "zh": {
                    "title": "å£éŸ³æ§åˆ¶ï¼šè§„åˆ™ä¸åµŒå…¥çš„ç»“åˆ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯´è¯è€…åµŒå…¥ä¸è¯­éŸ³è§„åˆ™åœ¨å£éŸ³æ§åˆ¶ä¸­çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºéŸ³ç´ è½¬ç§»ç‡ï¼ˆPSRï¼‰ï¼Œç”¨äºè¡¡é‡åµŒå…¥å¯¹è§„åˆ™ä¿æŒä¸å½±å“çš„ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç¾å¼å’Œè‹±å¼è‹±è¯­çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å®ç°äº†ä¸å£éŸ³ç›¸å…³çš„è¯­éŸ³è§„åˆ™ï¼Œå¹¶å‘ç°å°†è§„åˆ™ä¸åµŒå…¥ç»“åˆå¯ä»¥ç”Ÿæˆæ›´çœŸå®çš„å£éŸ³ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†è¯­éŸ³è§„åˆ™åœ¨å£éŸ³æ§åˆ¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè¯­éŸ³ç”Ÿæˆä¸­çš„è§£è€¦è¯„ä¼°æä¾›äº†æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14352",
            "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
            "url": "https://huggingface.co/papers/2601.14352",
            "abstract": "RoboBrain 2.5 enhances embodied AI through improved 3D spatial reasoning and temporal value estimation for more precise manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io",
            "score": 2,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "a4c2212b43fc1018",
            "authors": [
                "Huajie Tan",
                "Enshen Zhou",
                "Zhiyu Li",
                "Yijie Xu",
                "Yuheng Ji",
                "Xiansheng Chen",
                "Cheng Chi",
                "Pengwei Wang",
                "Huizhu Jia",
                "Yulong Ao",
                "Mingyu Cao",
                "Sixiang Chen",
                "Zhe Li",
                "Mengzhen Liu",
                "Zixiao Wang",
                "Shanyu Rong",
                "Yaoxu Lyu",
                "Zhongxia Zhao",
                "Peterson Co",
                "Yibo Li",
                "Yi Han",
                "Shaoxuan Xie",
                "Guocai Yao",
                "Songjing Wang",
                "Leiduo Zhang",
                "Xi Yang",
                "Yance Jiao",
                "Donghai Shi",
                "Kunchang Xie",
                "Shaokai Nie",
                "Chunlei Men",
                "Yonghua Lin",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Shanghang Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.14352.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "RoboBrain 2.5 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğº Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ² Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Empowering AI with Enhanced 3D Reasoning and Temporal Awareness",
                    "desc": "RoboBrain 2.5 is an advanced embodied AI model that improves how machines understand and interact with 3D environments. It enhances spatial reasoning by using depth-aware predictions instead of just 2D images, allowing for more accurate manipulation of objects. Additionally, it introduces a method for estimating the value of actions over time, which helps the AI predict and understand its progress during tasks. These improvements make RoboBrain 2.5 better at performing complex tasks that require precise movements and awareness of its surroundings."
                },
                "zh": {
                    "title": "æå‡å…·èº«æ™ºèƒ½çš„ç²¾ç¡®æ“ä½œèƒ½åŠ›",
                    "desc": "RoboBrain 2.5 æ˜¯ä¸€ç§å…ˆè¿›çš„å…·èº«äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹ï¼Œæå‡äº†å…¶åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†å’Œæ—¶é—´ä»·å€¼ä¼°è®¡æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡é«˜è´¨é‡çš„æ—¶ç©ºç›‘ç£è®­ç»ƒï¼Œè¯¥æ¨¡å‹å®ç°äº†ç²¾ç¡®çš„ä¸‰ç»´ç©ºé—´æ¨ç†ï¼Œèƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„ä¸‰ç»´æ“ä½œè½¨è¿¹ã€‚å®ƒè¿˜å»ºç«‹äº†å¯†é›†çš„æ—¶é—´ä»·å€¼ä¼°è®¡ï¼Œæä¾›äº†å¯¹æ‰§è¡ŒçŠ¶æ€çš„ç†è§£å’Œè¿›åº¦é¢„æµ‹ã€‚è¿™äº›å‡çº§ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚çš„ç²¾ç»†æ“ä½œä¸­æ›´åŠ ç¨³å®šå’Œæ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14152",
            "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
            "url": "https://huggingface.co/papers/2601.14152",
            "abstract": "Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
            "score": 2,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "ebb86e7319481dfb",
            "authors": [
                "Hyunjong Ok",
                "Jaeho Lee"
            ],
            "affiliations": [
                "HJ AILAB",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14152.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ QOC. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ (CQO) Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 14 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Causal Attention: The Key to Unlocking Prompt Performance",
                    "desc": "This paper investigates how the order of prompts affects the performance of large language models in multiple-choice question answering. It finds that placing context before questions and options (CQO) significantly improves performance compared to the reverse order (QOC), with a drop of over 14 percentage points in the latter. The authors attribute this performance gap to causal attention mechanisms, which restrict option tokens from accessing context in QOC prompts, leading to an information bottleneck. By analyzing various models and datasets, the study highlights the importance of prompt structure in optimizing model performance."
                },
                "zh": {
                    "title": "å› æœæ³¨æ„åŠ›å½±å“å¤šé€‰é¢˜è¡¨ç°",
                    "desc": "æœ¬ç ”ç©¶æ­ç¤ºäº†è¯­è¨€æ¨¡å‹ä¸­çš„å› æœæ³¨æ„åŠ›åœ¨å¤šé€‰é¢˜å›ç­”ä¸­é€ æˆä¿¡æ¯ç“¶é¢ˆçš„é—®é¢˜ã€‚å½“ä¸Šä¸‹æ–‡åœ¨é—®é¢˜å’Œé€‰é¡¹ä¹‹å‰æ—¶ï¼ˆCQOï¼‰ï¼Œæ¨¡å‹çš„è¡¨ç°æ¯”åå‘é¡ºåºï¼ˆQOCï¼‰é«˜å‡ºè¶…è¿‡14ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶æ„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å› æœæ³¨æ„åŠ›æ˜¯æ ¸å¿ƒæœºåˆ¶ï¼šåœ¨QOCæç¤ºä¸­ï¼Œå› æœæ©ç é˜»æ­¢é€‰é¡¹æ ‡è®°å…³æ³¨ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯¼è‡´ä¸Šä¸‹æ–‡å¯¹é€‰é¡¹ä¸å¯è§ã€‚è¿™ä¸ªå‘ç°ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿæ€§æä¾›äº†æ–°çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13918",
            "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
            "url": "https://huggingface.co/papers/2601.13918",
            "abstract": "AgentEHR presents a benchmark for autonomous EHR navigation requiring complex decision-making, while RetroSum framework improves performance through retrospective summarization and evolving experience strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
            "score": 2,
            "issue_id": 707,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "83af4d3596d5e4f2",
            "authors": [
                "Yusheng Liao",
                "Chuan Xuan",
                "Yutong Cai",
                "Lina Yang",
                "Zhe Chen",
                "Yanfeng Wang",
                "Yu Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13918.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#healthcare",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AgentEHR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RetroSum, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 29.16% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° 92.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing EHR Navigation with RetroSum for Better Decision-Making",
                    "desc": "The paper introduces AgentEHR, a benchmark designed for autonomous navigation of Electronic Health Records (EHRs) that involves complex decision-making tasks like diagnosis and treatment planning. It highlights the limitations of current summarization methods, which often lead to loss of critical information and disrupted reasoning. To overcome these challenges, the authors propose RetroSum, a framework that combines retrospective summarization with an evolving experience strategy to maintain logical coherence and prevent information loss. Empirical results show that RetroSum significantly improves performance and reduces interaction errors compared to existing methods."
                },
                "zh": {
                    "title": "æå‡è‡ªä¸»EHRå¯¼èˆªçš„æ™ºèƒ½å†³ç­–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†AgentEHRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªä¸»ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰å¯¼èˆªçš„åŸºå‡†ï¼Œè¦æ±‚å¤æ‚çš„å†³ç­–åˆ¶å®šã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæå‡ºäº†RetroSumæ¡†æ¶ï¼Œé€šè¿‡å›é¡¾æ€§æ€»ç»“å’Œä¸æ–­æ¼”å˜çš„ç»éªŒç­–ç•¥æ¥è§£å†³ä¿¡æ¯ä¸¢å¤±å’Œæ¨ç†ä¸è¿è´¯çš„é—®é¢˜ã€‚AgentEHRæŒ‘æˆ˜æ™ºèƒ½ä½“åœ¨é«˜å™ªå£°æ•°æ®åº“ä¸­è¿›è¡Œé•¿ç¨‹äº¤äº’æ¨ç†ï¼Œæ‰§è¡Œè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroSumåœ¨æ€§èƒ½ä¸Šæ¯”ç«äº‰åŸºçº¿æé«˜äº†29.16%ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†92.3%çš„äº¤äº’é”™è¯¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15059",
            "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
            "url": "https://huggingface.co/papers/2601.15059",
            "abstract": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.   We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.   We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.   We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.   We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
            "score": 1,
            "issue_id": 706,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "7dd0ebf904712ae9",
            "authors": [
                "Oleg Romanchuk",
                "Roman Bondar"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.15059.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² CI/CD",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Â«Ğ²Ğ°ĞºÑƒÑƒĞ¼Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ² CI/CD ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ Ğ»ÑĞ´Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ½Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°, Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ñ€Ğ¸Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ», Ğ·Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾ ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ±ĞµĞ· Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Gap: Addressing Responsibility in Automated Decision-Making",
                    "desc": "This paper discusses a problem in modern Continuous Integration/Continuous Deployment (CI/CD) systems where decisions made by automated agents lack clear responsibility attribution. It introduces the term 'responsibility vacuum' to describe a situation where decisions are made, but no one can be held accountable because the authority to approve and the understanding of the decision's basis do not align. The authors argue that this issue arises not from technical flaws but from a structural limitation when the speed of decision-making outpaces human verification capabilities. They suggest that without redesigning how responsibilities are assigned, organizations will continue to face this challenge as automation increases."
                },
                "zh": {
                    "title": "è´£ä»»çœŸç©ºï¼šå†³ç­–ä¸è´£ä»»çš„å¤±è¡¡",
                    "desc": "ç°ä»£æŒç»­é›†æˆ/æŒç»­äº¤ä»˜ï¼ˆCI/CDï¼‰ç®¡é“ä¸­ï¼Œä»£ç†ç”Ÿæˆçš„ä»£ç å­˜åœ¨è´£ä»»å½’å±çš„ç»“æ„æ€§å¤±è´¥ã€‚å°½ç®¡å†³ç­–é€šè¿‡æ­£å¼çš„æ‰¹å‡†æµç¨‹æ‰§è¡Œï¼Œä½†æ²¡æœ‰ä»»ä½•å®ä½“åŒæ—¶å…·å¤‡æ‰¹å‡†è¿™äº›å†³ç­–çš„æƒå¨å’Œç†è§£å…¶åŸºç¡€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™ç§æƒ…å†µå®šä¹‰ä¸ºè´£ä»»çœŸç©ºï¼šä¸€ç§å†³ç­–å‘ç”Ÿä½†æ— æ³•å½’å±è´£ä»»çš„çŠ¶æ€ï¼Œå› ä¸ºæƒå¨å’ŒéªŒè¯èƒ½åŠ›å¹¶ä¸é‡åˆã€‚æˆ‘ä»¬æŒ‡å‡ºï¼Œè¿™ä¸æ˜¯è¿‡ç¨‹åå·®æˆ–æŠ€æœ¯ç¼ºé™·ï¼Œè€Œæ˜¯å†³ç­–ç”Ÿæˆé€Ÿåº¦è¶…è¿‡äººç±»éªŒè¯èƒ½åŠ›çš„ç»“æ„æ€§ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14245",
            "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2601.14245",
            "abstract": "A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.",
            "score": 1,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "e41f2cd4d64c71b2",
            "authors": [
                "Zhongyu Yang",
                "Wei Pang",
                "Yingfang Yuan"
            ],
            "affiliations": [
                "BCML, Heriot-Watt University Edinburgh, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14245.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº XR Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… FashionIQ, CIRR Ğ¸ CIRCO Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Retrieval with Multi-Agent Coordination",
                    "desc": "This paper presents a multi-agent framework called XR for compositional image retrieval, which enhances the process of matching images with textual descriptions. It utilizes three types of specialized agents: imagination agents that generate target representations, similarity agents that filter results based on hybrid matching, and question agents that verify the accuracy of the results. By coordinating these agents, XR improves the retrieval process to better meet both semantic and visual requirements. The framework shows significant performance improvements over existing methods, demonstrating the importance of each agent in achieving effective retrieval outcomes."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡å›¾åƒæ£€ç´¢æ•ˆæœ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç»„åˆå›¾åƒæ£€ç´¢ï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰å’Œè§†è§‰æŸ¥è¯¢åŒ¹é…çš„æ•ˆæœã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸‰ç§ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼šæƒ³è±¡æ™ºèƒ½ä½“ç”Ÿæˆç›®æ ‡è¡¨ç¤ºï¼Œç›¸ä¼¼æ€§æ™ºèƒ½ä½“è¿›è¡Œç²—ç•¥è¿‡æ»¤ï¼Œé—®é¢˜æ™ºèƒ½ä½“é€šè¿‡é’ˆå¯¹æ€§æ¨ç†éªŒè¯äº‹å®ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„ç›¸ä¼¼æ€§åŸºç¡€æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è·¨æ¨¡æ€çš„ç†è§£å’Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¯ç§æ™ºèƒ½ä½“åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15100",
            "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
            "url": "https://huggingface.co/papers/2601.15100",
            "abstract": "WebSeek is a mixed-initiative browser extension that enables interactive web data extraction and analysis with AI-assisted guidance and automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.",
            "score": 0,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "8a15e6703617633f",
            "authors": [
                "Yanwei Huang",
                "Arpit Narechania"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15100.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ•·ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ĞµĞ±-ÑĞºÑ€ĞµĞ¿Ğ¸Ğ½Ğ³ Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ AI",
                    "desc": "WebSeek â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‘ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ (Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑĞ¿Ğ¸ÑĞºĞ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸) Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²ÑƒÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ…: Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ñ†ĞµĞ½ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ˜Ğ˜ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Empowering Web Data Analysis with AI Collaboration",
                    "desc": "WebSeek is a browser extension that combines human and AI efforts to help users extract and analyze data from the web. It allows users to create and modify data artifacts like tables and visualizations in an interactive workspace. The AI component not only provides guidance based on user actions but also automates certain tasks to enhance the analysis process. A user study showed that participants appreciated having control and clarity while working alongside the AI."
                },
                "zh": {
                    "title": "WebSeekï¼šæ™ºèƒ½åŒ–çš„ç½‘é¡µæ•°æ®æå–ä¸åˆ†æå·¥å…·",
                    "desc": "WebSeek æ˜¯ä¸€ä¸ªæ··åˆä¸»åŠ¨çš„æµè§ˆå™¨æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡ AI è¾…åŠ©çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–æ¥å®ç°äº’åŠ¨å¼ç½‘é¡µæ•°æ®æå–å’Œåˆ†æã€‚ç”¨æˆ·å¯ä»¥åœ¨ä¸€ä¸ªäº’åŠ¨ç”»å¸ƒä¸­å‘ç°å’Œæå–ç½‘é¡µä¿¡æ¯ï¼Œçµæ´»æ„å»ºã€è½¬æ¢å’Œå®Œå–„æ•°æ®å·¥ä»¶ï¼Œå¦‚è¡¨æ ¼ã€åˆ—è¡¨å’Œå¯è§†åŒ–ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–ï¼Œè¿˜èƒ½æ ¹æ®ç”¨æˆ·çš„æ˜ç¡®è¯·æ±‚åšå‡ºååº”ã€‚ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨åˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤šæ ·çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†åœ¨ä¸ AI åä½œæ—¶å¯¹é€æ˜åº¦å’Œæ§åˆ¶çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14681",
            "title": "FARE: Fast-Slow Agentic Robotic Exploration",
            "url": "https://huggingface.co/papers/2601.14681",
            "abstract": "FARE is a hierarchical exploration framework that combines large language model reasoning with reinforcement learning control to enable efficient autonomous robot navigation in complex environments.  \t\t\t\t\tAI-generated summary \t\t\t\t This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.",
            "score": 0,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "b87cdcf418e36496",
            "authors": [
                "Shuhao Liao",
                "Xuxin Lv",
                "Jeric Lew",
                "Shizhe Zhang",
                "Jingsong Liang",
                "Peizhuo Li",
                "Yuhong Cao",
                "Wenjun Wu",
                "Guillaume Sartoretti"
            ],
            "affiliations": [
                "Beihang University, China",
                "Department of Mechanical Engineering, National University of Singapore, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14681.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#graphs",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: LLM Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, RL Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "FARE â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ° Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ RL-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒÑ 200Ã—130 Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FARE: Smart Navigation for Robots with Language and Learning",
                    "desc": "FARE is a novel framework that enhances robot navigation by combining large language model (LLM) reasoning with reinforcement learning (RL) control. It operates on a fast-slow thinking paradigm, where the LLM interprets environmental descriptions to create exploration strategies, while the RL module executes these strategies based on real-time observations. The framework uses a modularity-based pruning mechanism to streamline the exploration graph, improving reasoning efficiency. Experimental results demonstrate that FARE significantly outperforms existing methods in both simulated and real-world environments, showcasing its effectiveness in complex navigation tasks."
                },
                "zh": {
                    "title": "FAREï¼šé«˜æ•ˆè‡ªä¸»æ¢ç´¢çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "FAREæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æ¢ç´¢æ¡†æ¶ï¼Œç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶æ–¹æ³•ï¼Œä»¥å®ç°å¤æ‚ç¯å¢ƒä¸­çš„é«˜æ•ˆè‡ªä¸»æœºå™¨äººå¯¼èˆªã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¿«æ…¢æ€ç»´æ¨¡å¼ï¼Œæ…¢æ€ç»´çš„LLMæ¨¡å—è§£ææœªçŸ¥ç¯å¢ƒçš„æ–‡æœ¬æè¿°ï¼Œå¹¶ç”Ÿæˆæ¢ç´¢ç­–ç•¥ï¼Œéšåé€šè¿‡æ‹“æ‰‘å›¾è½¬åŒ–ä¸ºå…¨å±€è·¯å¾„ç‚¹ã€‚å¿«é€Ÿæ€ç»´çš„RLæ¨¡å—æ ¹æ®å±€éƒ¨è§‚å¯Ÿæ‰§è¡Œæ¢ç´¢ï¼ŒåŒæ—¶å—åˆ°LLMç”Ÿæˆçš„å…¨å±€è·¯å¾„ç‚¹çš„æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAREåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨å®é™…ç¡¬ä»¶ä¸ŠéªŒè¯äº†å…¶åœ¨å¤æ‚å¤§è§„æ¨¡ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14027",
            "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
            "url": "https://huggingface.co/papers/2601.14027",
            "abstract": "A general coding agent paradigm enables flexible formal theorem proving by directly interfacing with proof assistants and retrieving relevant theorems without task-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
            "score": 0,
            "issue_id": 708,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "e189838ba34436ca",
            "authors": [
                "Junqi Liu",
                "Zihao Zhou",
                "Zekai Zhu",
                "Marco Dos Santos",
                "Weikun He",
                "Jiawei Liu",
                "Ran Wang",
                "Yunzhou Xie",
                "Junqiao Zhao",
                "Qiufeng Wang",
                "Lihong Zhi",
                "Jia Li",
                "Wenda Li"
            ],
            "affiliations": [
                "Academy of Mathematics and Systems Science, University of Chinese Academy of Sciences",
                "Imperial College London",
                "Project Numina",
                "Tongji University",
                "University of Cambridge",
                "University of Edinburgh",
                "University of Liverpool",
                "Xian Jiaotong-Liverpool University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14027.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#agents",
                    "#benchmark",
                    "#plp",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ°Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Lean Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Numina-Lean-Agent Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Claude Code Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» MCP, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ€ĞµÑˆĞ¸Ğ² Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ° Putnam 2025 Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ² ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñƒ Brascamp-Lieb ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°."
                },
                "en": {
                    "title": "Empowering Theorem Proving with General Coding Agents",
                    "desc": "This paper introduces a new approach to formal theorem proving using a general coding agent, which allows for flexible reasoning without needing specific training for each task. The proposed system, called Numina-Lean-Agent, can autonomously interact with proof assistants and retrieve relevant theorems, enhancing its versatility. By leveraging a base model like Claude Opus 4.5, the agent can improve performance simply by updating the model rather than retraining. The authors demonstrate the effectiveness of this approach by solving all problems in the Putnam 2025 competition and successfully formalizing a complex mathematical theorem."
                },
                "zh": {
                    "title": "é€šç”¨ç¼–ç ä»£ç†ï¼šçµæ´»çš„å½¢å¼å®šç†è¯æ˜æ–°èŒƒå¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ç¼–ç ä»£ç†èŒƒå¼ï¼Œç”¨äºçµæ´»çš„å½¢å¼å®šç†è¯æ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥ä¸è¯æ˜åŠ©æ‰‹æ¥å£ï¼Œæ£€ç´¢ç›¸å…³å®šç†ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚æˆ‘ä»¬ä»‹ç»çš„Numina-Lean-Agentç»“åˆäº†Claude Codeå’ŒNumina-Lean-MCPï¼Œèƒ½å¤Ÿè‡ªä¸»ä¸Leanè¿›è¡Œäº¤äº’ï¼Œè¿›è¡Œéæ­£å¼è¯æ˜å’Œè¾…åŠ©æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨è§£å†³Putnam 2025ä¸­çš„æ‰€æœ‰é—®é¢˜æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12029",
            "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
            "url": "https://huggingface.co/papers/2601.12029",
            "abstract": "The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.",
            "score": 0,
            "issue_id": 708,
            "pub_date": "2026-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "498493ce44f57bbd",
            "authors": [
                "Sandy H. S. Herho",
                "Faruq Khadami",
                "Iwan P. Anwar",
                "Dasapta E. Irawan"
            ],
            "affiliations": [
                "Applied Geology Research Group, Bandung Institute of Technology",
                "Applied and Environmental Oceanography Research Group, Bandung Institute of Technology",
                "Department of Earth and Planetary Sciences, University of California, Riverside",
                "Ronin Institute for Independent Scholarship 2.0",
                "School of Systems Science and Industrial Engineering, State University of New York, Binghamton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12029.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ sangkuriang â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ° Python Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ĞšĞ¾Ñ€Ñ‚ĞµĞ²ĞµĞ³Ğ°-Ğ´Ğµ Ğ¤Ñ€Ğ¸Ğ·Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ»Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ñ‹. Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ JIT-ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… â€” Ğ¾Ñ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ° Ğ´Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Sangkuriang: Efficient Solving of Nonlinear Wave Dynamics",
                    "desc": "This paper presents 'sangkuriang', a Python library designed to solve the Korteweg-de Vries (KdV) equation, which models nonlinear wave behavior. The library utilizes Fourier pseudo-spectral methods for spatial discretization and adaptive high-order time integration, ensuring efficient computation through just-in-time (JIT) compilation. It validates its performance through various scenarios, including soliton propagation and wave interactions, while monitoring the conservation of classical invariants. The results align closely with theoretical predictions, making it a valuable tool for both educational and research purposes in nonlinear wave physics."
                },
                "zh": {
                    "title": "sangkuriangï¼šé«˜æ•ˆæ±‚è§£KdVæ–¹ç¨‹çš„å¼€æºå·¥å…·",
                    "desc": "Korteweg-de Vries (KdV) æ–¹ç¨‹æ˜¯éçº¿æ€§æ³¢åŠ¨ç‰©ç†ä¸­çš„åŸºç¡€æ¨¡å‹ï¼Œæè¿°äº†è‰²æ•£æ‰©æ•£ä¸éçº¿æ€§é™¡å³­ä¹‹é—´çš„å¹³è¡¡ï¼Œä»è€Œäº§ç”Ÿå­¤ç«‹æ³¢ã€‚æœ¬æ–‡ä»‹ç»äº†sangkuriangï¼Œä¸€ä¸ªå¼€æºçš„Pythonåº“ï¼Œç”¨äºé€šè¿‡å‚…é‡Œå¶ä¼ªè°±ç©ºé—´ç¦»æ•£åŒ–å’Œè‡ªé€‚åº”é«˜é˜¶æ—¶é—´ç§¯åˆ†æ¥æ±‚è§£è¯¥æ–¹ç¨‹ã€‚è¯¥å®ç°åˆ©ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰æé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ•™å­¦çš„å¯åŠæ€§ã€‚é€šè¿‡å¯¹å­¤ç«‹å­¤ç«‹æ³¢ä¼ æ’­ã€å¯¹ç§°åŒæ³¢é…ç½®ã€ä¸åŒå¹…åº¦æ³¢çš„è¶…è¶Šç¢°æ’å’Œä¸‰ä½“ç›¸äº’ä½œç”¨ç­‰å¤æ‚åœºæ™¯çš„éªŒè¯ï¼Œç¡®ä¿äº†ç»å…¸ä¸å˜é‡çš„å®ˆæ’ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-21.html",
    "link_next": "2026-01-23.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 9,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 2,
        "#training": 3,
        "#robotics": 4,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 4,
        "#low_resource": 0
    }
}