{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2026-01-22 20:30",
    "weekday": 3,
    "issue_id": 722,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.12538",
            "title": "Agentic Reasoning for Large Language Models",
            "url": "https://huggingface.co/papers/2601.12538",
            "abstract": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
            "score": 118,
            "issue_id": 705,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "889c44cfdc0d7221",
            "authors": [
                "Tianxin Wei",
                "Ting-Wei Li",
                "Zhining Liu",
                "Xuying Ning",
                "Ze Yang",
                "Jiaru Zou",
                "Zhichen Zeng",
                "Ruizhong Qiu",
                "Xiao Lin",
                "Dongqi Fu",
                "Zihao Li",
                "Mengting Ai",
                "Duo Zhou",
                "Wenxuan Bao",
                "Yunzhe Li",
                "Gaotang Li",
                "Cheng Qian",
                "Yu Wang",
                "Xiangru Tang",
                "Yin Xiao",
                "Liri Fang",
                "Hui Liu",
                "Xianfeng Tang",
                "Yuji Zhang",
                "Chi Wang",
                "Jiaxuan You",
                "Heng Ji",
                "Hanghang Tong",
                "Jingrui He"
            ],
            "affiliations": [
                "Amazon",
                "Google Deepmind",
                "Meta",
                "UCSD",
                "University of Illinois Urbana-Champaign",
                "Yale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12538.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#healthcare",
                    "#training",
                    "#survey",
                    "#math",
                    "#benchmark",
                    "#science",
                    "#rl",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: LLM ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ), ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ (Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ) Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ (Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ) Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒĞºĞ¸, Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs as Autonomous Agents through Agentic Reasoning",
                    "desc": "This paper introduces agentic reasoning, which transforms large language models (LLMs) into autonomous agents that can plan, act, and learn in changing environments. It categorizes agentic reasoning into three levels: foundational capabilities for single agents, self-evolving abilities that adapt through experience, and collective reasoning for multi-agent collaboration. The authors differentiate between in-context reasoning, which enhances interaction during use, and post-training reasoning, which improves performance through learning techniques. The survey also highlights practical applications and future challenges in deploying these advanced reasoning frameworks in real-world scenarios."
                },
                "zh": {
                    "title": "ä»£ç†æ¨ç†ï¼šè®©è¯­è¨€æ¨¡å‹æˆä¸ºè‡ªä¸»æ™ºèƒ½ä½“",
                    "desc": "ä»£ç†æ¨ç†é‡æ–°å®šä¹‰äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œå­¦ä¹ çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­ä¸–ç•Œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¼€æ”¾å’ŒåŠ¨æ€ç¯å¢ƒä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚ä»£ç†æ¨ç†é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œæ ‡å¿—ç€ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼Œå¼ºè°ƒæŒç»­äº’åŠ¨çš„é‡è¦æ€§ã€‚æœ¬æ–‡ç»„ç»‡äº†ä»£ç†æ¨ç†çš„ä¸‰ä¸ªäº’è¡¥ç»´åº¦ï¼ŒåŒ…æ‹¬åŸºç¡€ä»£ç†æ¨ç†ã€è‡ªæˆ‘æ¼”åŒ–ä»£ç†æ¨ç†å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ€è·¯ä¸è¡ŒåŠ¨çš„è·¯çº¿å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12346",
            "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
            "url": "https://huggingface.co/papers/2601.12346",
            "abstract": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.",
            "score": 40,
            "issue_id": 706,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "31732de0e1390005",
            "authors": [
                "Peizhou Huang",
                "Zixuan Zhong",
                "Zhongwei Wan",
                "Donghao Zhou",
                "Samiul Alam",
                "Xin Wang",
                "Zexin Li",
                "Zhihao Dou",
                "Li Zhu",
                "Jing Xiong",
                "Chaofan Tao",
                "Yan Xu",
                "Dimitrios Dimitriadis",
                "Tuo Zhang",
                "Mi Zhang"
            ],
            "affiliations": [
                "Amazon",
                "CUHK",
                "CWRU",
                "HKU",
                "OSU",
                "UCL",
                "UCR",
                "UMich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12346.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#science",
                    "#interpretability",
                    "#multimodal",
                    "#rag"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMDeepResearch-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: FLAE Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°, TRACE Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼, Ğ¸ MOSAIC Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ·Ñ‹ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Evaluating Multimodal Research Agents for Better Report Generation",
                    "desc": "The paper introduces MMDeepResearch-Bench (MMDR-Bench), a new benchmark designed to evaluate multimodal research agents in generating reports that incorporate both text and visual evidence. It consists of 140 tasks across various domains, focusing on the ability of models to synthesize information while accurately citing sources and grounding their claims in visual data. The authors propose a comprehensive evaluation framework that includes multiple metrics to assess report quality, citation accuracy, and the integrity of visual references. The findings indicate that there are significant trade-offs between the quality of generated text, the accuracy of citations, and the effective use of multimodal evidence, suggesting that improving one aspect may compromise another."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ç ”ç©¶ä»£ç†çš„æŠ¥å‘Šç”Ÿæˆæ–°åŸºå‡†",
                    "desc": "MMDeepResearch-Benchï¼ˆMMDR-Benchï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€ç ”ç©¶ä»£ç†åœ¨æŠ¥å‘Šç”Ÿæˆä¸­ä½¿ç”¨è§†è§‰è¯æ®çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«140ä¸ªä¸“å®¶è®¾è®¡çš„ä»»åŠ¡ï¼Œæ¶µç›–21ä¸ªé¢†åŸŸï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€ç†è§£å’ŒåŸºäºå¼•ç”¨çš„æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ã€‚ä¸ä»¥å¾€çš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒMMDR-Benchå¼ºè°ƒæŠ¥å‘Šé£æ ¼çš„åˆæˆï¼Œè¦æ±‚æ¨¡å‹å°†è§†è§‰ææ–™ä¸å¼•ç”¨çš„ä¸»å¼ ç›¸è¿æ¥ï¼Œå¹¶ä¿æŒå™è¿°ã€å¼•ç”¨å’Œè§†è§‰å‚è€ƒçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆè´¨é‡ã€å¼•ç”¨å‡†ç¡®æ€§å’Œå¤šæ¨¡æ€åŸºç¡€ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§çš„æƒè¡¡ï¼Œå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆå¹¶ä¸ä¸€å®šä¿è¯è¯æ®çš„å¯é ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15282",
            "title": "Rethinking Video Generation Model for the Embodied World",
            "url": "https://huggingface.co/papers/2601.15282",
            "abstract": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and embodiments, revealing deficiencies in physical realism and introducing a large-scale dataset to address training data shortages.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
            "score": 35,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "cb5ee149b1d3627f",
            "authors": [
                "Yufan Deng",
                "Zilin Pan",
                "Hongyu Zhang",
                "Xiaojie Li",
                "Ruoqing Hu",
                "Yufei Ding",
                "Yiming Zou",
                "Yan Zeng",
                "Daquan Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15282.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ»Ğ°ÑƒÑĞ¸Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ RoVid-X â€” Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ­Ñ‚Ğ° ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "RBench: Elevating Video Generation for Robotics with Comprehensive Evaluation and Data",
                    "desc": "This paper presents RBench, a comprehensive benchmark for evaluating video generation models in robotics across various tasks and embodiments. It identifies significant shortcomings in the physical realism of generated videos and proposes a refined data pipeline to create RoVid-X, a large-scale dataset with 4 million annotated video clips. The benchmark assesses models based on task correctness and visual fidelity, achieving a high correlation with human evaluations. By combining rigorous evaluation with a rich dataset, the study aims to enhance the training of video generation models and advance embodied AI capabilities."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°æœºå™¨äººè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„æœºå™¨äººåŸºå‡†æµ‹è¯•RBenchï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡é¢†åŸŸå’Œä¸åŒä½“ç°ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆç‰©ç†çœŸå®æ„Ÿæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¹¶ä¸”ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†é™åˆ¶äº†å…¬å¹³æ¯”è¾ƒå’Œè¿›å±•ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®çŸ­ç¼ºçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†RoVid-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«400ä¸‡ä¸ªæ ‡æ³¨è§†é¢‘ç‰‡æ®µçš„å¤§å‹å¼€æºæ•°æ®é›†ï¼Œæ¶µç›–äº†æ•°åƒä¸ªä»»åŠ¡ï¼Œå¹¶é™„æœ‰å…¨é¢çš„ç‰©ç†å±æ€§æ³¨é‡Šã€‚é€šè¿‡è¯„ä¼°å’Œæ•°æ®çš„ååŒä½œç”¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œä»¥ä¿ƒè¿›è§†é¢‘æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°å’Œå¯æ‰©å±•è®­ç»ƒï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½å‘é€šç”¨æ™ºèƒ½çš„æ¼”è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14171",
            "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
            "url": "https://huggingface.co/papers/2601.14171",
            "abstract": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.  \t\t\t\t\tAI-generated summary \t\t\t\t Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
            "score": 35,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "a4a9f4fe5907584a",
            "authors": [
                "Qianli Ma",
                "Chang Guo",
                "Zhiheng Tian",
                "Siyu Wang",
                "Jipeng Xiao",
                "Yuanhao Yue",
                "Zhipeng Zhang"
            ],
            "affiliations": [
                "AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14171.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#hallucinations",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "RebuttalAgent â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞŸĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "RebuttalAgent: Transforming Rebuttal Generation with Evidence-Centric Planning",
                    "desc": "RebuttalAgent is a novel multi-agent framework designed to enhance the process of generating rebuttals in academic peer reviews. It shifts the focus from simple text generation to an evidence-centric planning approach, which helps in addressing critiques more effectively. By breaking down complex feedback into manageable parts and utilizing external literature for support, it improves the accuracy and reliability of the rebuttals. The framework has been validated against existing methods, showing superior performance in terms of coverage, faithfulness, and strategic coherence."
                },
                "zh": {
                    "title": "ä»¥è¯æ®ä¸ºä¸­å¿ƒçš„åé©³ç”Ÿæˆæ¡†æ¶",
                    "desc": "RebuttalAgentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†åé©³ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»¥è¯æ®ä¸ºä¸­å¿ƒçš„è§„åˆ’ä»»åŠ¡ï¼Œä»è€Œæé«˜å­¦æœ¯åŒè¡Œè¯„å®¡ä¸­çš„è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§ã€‚è¯¥ç³»ç»Ÿå°†å¤æ‚çš„åé¦ˆåˆ†è§£ä¸ºåŸºæœ¬é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆæˆå‹ç¼©æ‘˜è¦ä¸é«˜ä¿çœŸæ–‡æœ¬åŠ¨æ€æ„å»ºæ··åˆä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶é›†æˆä¸€ä¸ªè‡ªä¸»çš„å¤–éƒ¨æœç´¢æ¨¡å—ï¼Œä»¥è§£å†³éœ€è¦å¤–éƒ¨æ–‡çŒ®çš„å…³æ³¨ç‚¹ã€‚é€šè¿‡åœ¨èµ·è‰ä¹‹å‰ç”Ÿæˆå¯æ£€æŸ¥çš„å“åº”è®¡åˆ’ï¼ŒRebuttalAgentç¡®ä¿æ¯ä¸ªè®ºç‚¹éƒ½æ˜ç¡®åŸºäºå†…éƒ¨æˆ–å¤–éƒ¨è¯æ®ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨RebuttalBenchä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå¼ºåŸºçº¿ï¼Œæä¾›äº†ä¸€ä¸ªé€æ˜ä¸”å¯æ§çš„åŒè¡Œè¯„å®¡åŠ©æ‰‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14750",
            "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
            "url": "https://huggingface.co/papers/2601.14750",
            "abstract": "Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
            "score": 13,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "fa7ddd84d3506d5c",
            "authors": [
                "Yifan Wang",
                "Shiyu Li",
                "Peiming Li",
                "Xiaochen Yang",
                "Yang Tang",
                "Zheng Wei"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "School of Mathematics and Statistics, University of Glasgow",
                "Tencent BAC",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14750.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Render-of-Thought (RoT) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°Ğ»Ğ¾Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 3-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ CoT, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Visualizing Reasoning for Enhanced Clarity and Efficiency",
                    "desc": "The Render-of-Thought (RoT) framework transforms reasoning steps from text into images using vision-language models, enhancing the clarity and efficiency of reasoning processes. This approach addresses the limitations of Chain-of-Thought (CoT) prompting, which, while effective, can be verbose and computationally expensive. By visualizing the reasoning chain, RoT makes the underlying rationale more explicit and easier to analyze. Experimental results show that RoT significantly reduces token usage and speeds up inference while maintaining strong performance on reasoning tasks."
                },
                "zh": {
                    "title": "å°†æ¨ç†æ­¥éª¤å¯è§†åŒ–ï¼Œæé«˜æ•ˆç‡ä¸å¯è¿½æº¯æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºRender-of-Thoughtï¼ˆRoTï¼‰ï¼Œæ—¨åœ¨å°†æ–‡æœ¬æ¨ç†æ­¥éª¤è½¬æ¢ä¸ºå›¾åƒï¼Œä»¥æé«˜æ¨ç†çš„å¯è¿½æº¯æ€§å’Œæ•ˆç‡ã€‚RoTåˆ©ç”¨ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰ç¼–ç å™¨ï¼Œå°†è§†è§‰åµŒå…¥ä¸æ–‡æœ¬ç©ºé—´å¯¹é½ï¼Œä»è€Œä½¿æ½œåœ¨çš„æ¨ç†é“¾æ›´åŠ æ˜ç¡®ã€‚ä¸ä¼ ç»Ÿçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç›¸æ¯”ï¼ŒRoTåœ¨ä¿æŒç«äº‰æ€§æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†3-4å€çš„ä»¤ç‰Œå‹ç¼©å’Œæ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚è¯¥æ–¹æ³•çš„å®ç°æ— éœ€é¢å¤–çš„é¢„è®­ç»ƒå¼€é”€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13572",
            "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
            "url": "https://huggingface.co/papers/2601.13572",
            "abstract": "Reinforced Agent Merging (RAM) addresses the limitations of traditional merging methods for reinforcement learning-trained agents by distinguishing shared and task-specific parameters to preserve critical behaviors during model integration.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.",
            "score": 13,
            "issue_id": 716,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "b749a16158710c52",
            "authors": [
                "Xiangchi Yuan",
                "Dachuan Shi",
                "Chunhui Zhang",
                "Zheyuan Liu",
                "Shenglong Yao",
                "Soroush Vosoughi",
                "Wenke Lee"
            ],
            "affiliations": [
                "Dartmouth College",
                "Georgia Institute of Technology",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13572.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ: Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ğ² RL-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforced Agent Merging (RAM) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ RL-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ SFT. RAM ÑĞ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑƒÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ RAM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Merging Agents, Preserving Skills: The RAM Approach",
                    "desc": "Reinforced Agent Merging (RAM) is a new method for combining reinforcement learning (RL) agents that focuses on maintaining important behaviors during the merging process. Traditional merging techniques often fail because they treat all parameters equally, which can dilute the unique skills of each agent. RAM solves this by separating shared parameters from those that are specific to each task, allowing for better preservation of critical behaviors. Experiments show that RAM not only improves performance compared to existing methods but also enables combined agents to perform better than individual specialized agents."
                },
                "zh": {
                    "title": "å¼ºåŒ–æ™ºèƒ½ä½“åˆå¹¶ï¼šæå‡RLæ¨¡å‹åˆå¹¶æ•ˆæœçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åæœŸè®­ç»ƒä¸­éå¸¸é‡è¦ï¼Œå°¤å…¶æ˜¯å¯¹äºéœ€è¦ç‰¹å®šæ¨ç†è¡Œä¸ºçš„æ™ºèƒ½ä½“æ¨¡å‹ã€‚ä¼ ç»Ÿçš„æ¨¡å‹åˆå¹¶æ–¹æ³•æ— æ³•æœ‰æ•ˆä¿ç•™RLè®­ç»ƒæ™ºèƒ½ä½“çš„ä»»åŠ¡ç‰¹å®šèƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦é’ˆå¯¹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¼ºåŒ–æ™ºèƒ½ä½“åˆå¹¶ï¼ˆRAMï¼‰ï¼Œå®ƒèƒ½å¤ŸåŒºåˆ†å…±äº«å‚æ•°å’Œä»»åŠ¡ç‰¹å®šå‚æ•°ï¼Œä»è€Œåœ¨åˆå¹¶è¿‡ç¨‹ä¸­ä¿ç•™å…³é”®è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼ŒRAMåœ¨å¤šä¸ªæ™ºèƒ½ä½“é¢†åŸŸå’Œæ¨¡å‹æ¶æ„ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿåˆå¹¶æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ååŒæ•ˆåº”ï¼Œè¶…è¶Šä¸“é—¨åŒ–æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14722",
            "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
            "url": "https://huggingface.co/papers/2601.14722",
            "abstract": "Thai-focused vision-language model for document extraction combining OCR, layout reconstruction, and structural consistency with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.",
            "score": 12,
            "issue_id": 709,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "62bfe412601c5b48",
            "authors": [
                "Surapon Nonesung",
                "Natapong Nitarach",
                "Teetouch Jaknamon",
                "Pittawat Taveekitworachai",
                "Kunat Pipatanakul"
            ],
            "affiliations": [
                "SCB 10X",
                "Typhoon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14722.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#open_source",
                    "#synthetic",
                    "#low_resource",
                    "#cv",
                    "#multilingual",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Typhoon OCR â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ fine-tuning Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ vision-language Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ñ€ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· VLM Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Typhoon OCR V1.5 Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ…, Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Typhoon OCR: Efficient Document Extraction for Thai Language",
                    "desc": "This paper introduces Typhoon OCR, a vision-language model specifically designed for document extraction in Thai and English. It addresses the unique challenges posed by the Thai language, such as complex scripts and unstructured documents, by utilizing a specialized training dataset and a multi-stage data construction pipeline. Typhoon OCR offers a unified framework that performs text transcription, layout reconstruction, and maintains structural consistency, all while being computationally efficient. The model's performance is evaluated across various document types, showing that it can match or exceed the capabilities of larger proprietary models with significantly lower resource requirements."
                },
                "zh": {
                    "title": "æ³°è¯­æ–‡æ¡£æå–çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ³°è¯­çš„è§†è§‰è¯­è¨€æ¨¡å‹Typhoon OCRï¼Œæ—¨åœ¨æé«˜æ–‡æ¡£æå–çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€å¸ƒå±€é‡å»ºå’Œç»“æ„ä¸€è‡´æ€§ï¼Œä¸“é—¨ä¸ºæ³°è¯­å’Œè‹±è¯­æ–‡æ¡£è®¾è®¡ã€‚é€šè¿‡å¤šé˜¶æ®µæ•°æ®æ„å»ºæµç¨‹ï¼ŒTyphoon OCRä½¿ç”¨äº†ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„è®­ç»ƒæ•°æ®é›†ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒç±»å‹çš„æ³°è¯­æ–‡æ¡£ä¸­å®ç°é«˜æ•ˆçš„æ–‡æœ¬è½¬å½•å’Œå¸ƒå±€é‡å»ºã€‚æœ€æ–°ç‰ˆæœ¬Typhoon OCR V1.5åœ¨è®¡ç®—èµ„æºè¦æ±‚ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14490",
            "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
            "url": "https://huggingface.co/papers/2601.14490",
            "abstract": "GutenOCR enhances vision-language models for document understanding by enabling unified reading, detection, and grounding through prompt-based interfaces trained on diverse document types.  \t\t\t\t\tAI-generated summary \t\t\t\t GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
            "score": 11,
            "issue_id": 718,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "8ef07ab88d11d531",
            "authors": [
                "Hunter Heidenreich",
                "Ben Elliott",
                "Olivia Dinica",
                "Yosheb Getachew"
            ],
            "affiliations": [
                "Roots.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14490.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#science",
                    "#open_source",
                    "#training",
                    "#synthetic",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "GutenOCR â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ fine-tuning Qwen2.5-VL Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ prompt'Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ´ĞµĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ bounding box'Ğ¾Ğ². GutenOCR-7B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-VL Ğ¿Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ grounded OCR Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 10.5K Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (0.40 â†’ 0.82), Ñ…Ğ¾Ñ‚Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ trade-off'Ñ‹ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…."
                },
                "en": {
                    "title": "Unified Document Understanding with GutenOCR",
                    "desc": "GutenOCR is a new approach that improves how machines understand documents by combining reading, detecting, and grounding information in a single system. It uses advanced vision-language models that have been fine-tuned on various types of documents, allowing for detailed analysis at both full-page and localized levels. The models can answer specific queries about document content, enhancing their usability in real-world applications. Evaluation results show that GutenOCR significantly outperforms previous models in recognizing and processing text in business and scientific documents, although some challenges remain in handling complex layouts."
                },
                "zh": {
                    "title": "GutenOCRï¼šç»Ÿä¸€æ–‡æ¡£ç†è§£çš„OCRè§£å†³æ–¹æ¡ˆ",
                    "desc": "GutenOCR æ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºæç¤ºçš„æ¥å£å®ç°æ–‡æ¡£ç†è§£ä¸­çš„ç»Ÿä¸€é˜…è¯»ã€æ£€æµ‹å’Œå®šä½ã€‚å®ƒé€šè¿‡å¾®è°ƒ Qwen2.5-VL-3B å’Œ Qwen2.5-VL-7B æ¨¡å‹ï¼Œå½¢æˆäº†ä¸€ç³»åˆ—åŸºç¡€çš„ OCR å‰ç«¯ã€‚è¯¥æ¨¡å‹ç»è¿‡å¤šç§æ–‡æ¡£ç±»å‹çš„è®­ç»ƒï¼Œèƒ½å¤Ÿæ”¯æŒæ•´é¡µå’Œå±€éƒ¨é˜…è¯»ï¼Œå¹¶æä¾›è¡Œå’Œæ®µè½çº§çš„è¾¹ç•Œæ¡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGutenOCR-7B åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—æå‡äº† OCR çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å•†ä¸šå’Œç§‘å­¦æ–‡æ¡£çš„å¤„ç†ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13044",
            "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
            "url": "https://huggingface.co/papers/2601.13044",
            "abstract": "A 115M-parameter FastConformer-Transducer model achieves low-latency Thai speech recognition with reduced computational cost through text normalization and curriculum learning, accompanied by a benchmark dataset for standardized evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.",
            "score": 11,
            "issue_id": 709,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "5cb62ffe8e4a672d",
            "authors": [
                "Warit Sirichotedumrong",
                "Adisai Na-Thalang",
                "Potsawee Manakul",
                "Pittawat Taveekitworachai",
                "Sittipong Sripaisarnmongkol",
                "Kunat Pipatanakul"
            ],
            "affiliations": [
                "SCB 10X",
                "Typhoon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13044.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#audio",
                    "#low_resource",
                    "#benchmark",
                    "#multilingual",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Typhoon ASR Real-time â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞ´ÑŒÑÑĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ FastConformer Ñ 115 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 45-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Whisper Large-v3 Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞµĞ²ĞµÑ€Ğ¾-Ğ²Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ñƒ Ğ¸ÑÑĞ°Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ curriculum learning, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Typhoon ASR Benchmark â€” ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Efficient Thai Speech Recognition with Typhoon ASR",
                    "desc": "The paper presents the Typhoon ASR Real-time model, a 115M-parameter FastConformer-Transducer designed for efficient Thai speech recognition with low latency. It highlights the importance of text normalization and a two-stage curriculum learning approach to improve model performance while reducing computational costs significantly. The model achieves a 45x reduction in computational cost compared to larger models like Whisper, while maintaining comparable accuracy. Additionally, the authors introduce the Typhoon ASR Benchmark, a standardized dataset for evaluating Thai ASR systems, addressing reproducibility issues in the field."
                },
                "zh": {
                    "title": "ä½å»¶è¿Ÿæ³°è¯­è¯­éŸ³è¯†åˆ«çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTyphoon ASR Real-timeçš„115Må‚æ•°FastConformer-Transduceræ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ä½å»¶è¿Ÿçš„æ³°è¯­è¯­éŸ³è¯†åˆ«ã€‚é€šè¿‡æ–‡æœ¬è§„èŒƒåŒ–å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œè¯¥æ¨¡å‹åœ¨è®¡ç®—æˆæœ¬ä¸Šå‡å°‘äº†45å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä»¥é€‚åº”ä¼Šæ¡‘æ–¹è¨€ï¼ŒåŒæ—¶ä¿æŒä¸­å¤®æ³°è¯­çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³æ³°è¯­ASRçš„å¯é‡å¤æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Typhoon ASRåŸºå‡†æ•°æ®é›†ï¼Œæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14027",
            "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
            "url": "https://huggingface.co/papers/2601.14027",
            "abstract": "A general coding agent paradigm enables flexible formal theorem proving by directly interfacing with proof assistants and retrieving relevant theorems without task-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
            "score": 8,
            "issue_id": 708,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "e189838ba34436ca",
            "authors": [
                "Junqi Liu",
                "Zihao Zhou",
                "Zekai Zhu",
                "Marco Dos Santos",
                "Weikun He",
                "Jiawei Liu",
                "Ran Wang",
                "Yunzhou Xie",
                "Junqiao Zhao",
                "Qiufeng Wang",
                "Lihong Zhi",
                "Jia Li",
                "Wenda Li"
            ],
            "affiliations": [
                "Academy of Mathematics and Systems Science, University of Chinese Academy of Sciences",
                "Imperial College London",
                "Project Numina",
                "Tongji University",
                "University of Cambridge",
                "University of Edinburgh",
                "University of Liverpool",
                "Xian Jiaotong-Liverpool University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14027.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#agents",
                    "#benchmark",
                    "#plp",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ°Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Lean Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Numina-Lean-Agent Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Claude Code Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» MCP, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ€ĞµÑˆĞ¸Ğ² Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ° Putnam 2025 Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ² ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñƒ Brascamp-Lieb ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°."
                },
                "en": {
                    "title": "Empowering Theorem Proving with General Coding Agents",
                    "desc": "This paper introduces a new approach to formal theorem proving using a general coding agent, which allows for flexible reasoning without needing specific training for each task. The proposed system, called Numina-Lean-Agent, can autonomously interact with proof assistants and retrieve relevant theorems, enhancing its versatility. By leveraging a base model like Claude Opus 4.5, the agent can improve performance simply by updating the model rather than retraining. The authors demonstrate the effectiveness of this approach by solving all problems in the Putnam 2025 competition and successfully formalizing a complex mathematical theorem."
                },
                "zh": {
                    "title": "é€šç”¨ç¼–ç ä»£ç†ï¼šçµæ´»çš„å½¢å¼å®šç†è¯æ˜æ–°èŒƒå¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ç¼–ç ä»£ç†èŒƒå¼ï¼Œç”¨äºçµæ´»çš„å½¢å¼å®šç†è¯æ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥ä¸è¯æ˜åŠ©æ‰‹æ¥å£ï¼Œæ£€ç´¢ç›¸å…³å®šç†ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚æˆ‘ä»¬ä»‹ç»çš„Numina-Lean-Agentç»“åˆäº†Claude Codeå’ŒNumina-Lean-MCPï¼Œèƒ½å¤Ÿè‡ªä¸»ä¸Leanè¿›è¡Œäº¤äº’ï¼Œè¿›è¡Œéæ­£å¼è¯æ˜å’Œè¾…åŠ©æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨è§£å†³Putnam 2025ä¸­çš„æ‰€æœ‰é—®é¢˜æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07853",
            "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
            "url": "https://huggingface.co/papers/2601.07853",
            "abstract": "FinVault presents the first execution-grounded security benchmark for financial agents, revealing significant vulnerabilities in current defense mechanisms when applied to real-world financial workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
            "score": 8,
            "issue_id": 706,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "fa449db65899ca42",
            "authors": [
                "Zhi Yang",
                "Runguo Li",
                "Qiqi Qiang",
                "Jiashun Wang",
                "Fangqi Lou",
                "Mengping Li",
                "Dongpo Cheng",
                "Rui Xu",
                "Heng Lian",
                "Shuo Zhang",
                "Xiaolong Liang",
                "Xiaoming Huang",
                "Zheng Wei",
                "Zhaowei Liu",
                "Xin Guo",
                "Huacan Wang",
                "Ronghao Chen",
                "Liwen Zhang"
            ],
            "affiliations": [
                "BUPT",
                "CUHKSZ",
                "FDU",
                "PKU",
                "QuantaAlpha",
                "SUFE",
                "SUIBE",
                "Tencent",
                "UCAS",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07853.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "FinVault Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 31 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸ 963 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ´Ğ¾ 50%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‰Ğ¸Ñ‚ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "FinVault: Strengthening Security for Financial Agents in Real-World Scenarios",
                    "desc": "FinVault introduces a new security benchmark specifically designed for financial agents that utilize large language models (LLMs). This benchmark addresses the vulnerabilities that arise when these agents operate in real-world financial environments, where they perform tasks like investment analysis and risk assessment. The study reveals that current defense mechanisms are inadequate, with high attack success rates even against advanced models. By providing a comprehensive set of scenarios and vulnerabilities, FinVault aims to enhance the security of financial agents and promote the development of more effective defenses."
                },
                "zh": {
                    "title": "é‡‘èä»£ç†çš„å®‰å…¨æ–°åŸºå‡†ï¼šFinVault",
                    "desc": "FinVaultæ˜¯é¦–ä¸ªé’ˆå¯¹é‡‘èä»£ç†çš„æ‰§è¡ŒåŸºç¡€å®‰å…¨åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰é˜²å¾¡æœºåˆ¶åœ¨å®é™…é‡‘èå·¥ä½œæµç¨‹ä¸­å­˜åœ¨çš„é‡å¤§æ¼æ´ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŠ•èµ„åˆ†æã€é£é™©è¯„ä¼°å’Œè‡ªåŠ¨å†³ç­–ä¸­çš„åº”ç”¨ï¼Œæ–°çš„å®‰å…¨é£é™©ä¸æ–­å‡ºç°ã€‚ç°æœ‰çš„å®‰å…¨è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¯­è¨€æ¨¡å‹çš„å†…å®¹åˆè§„æ€§æˆ–æŠ½è±¡ä»£ç†è®¾ç½®ä¸Šï¼Œæœªèƒ½æ•æ‰åˆ°å®é™…æ“ä½œå·¥ä½œæµç¨‹ä¸­äº§ç”Ÿçš„æ‰§è¡ŒåŸºç¡€é£é™©ã€‚FinVaulté€šè¿‡31ä¸ªç›‘ç®¡æ¡ˆä¾‹é©±åŠ¨çš„æ²™ç›’åœºæ™¯å’Œ107ä¸ªçœŸå®ä¸–ç•Œæ¼æ´ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å½“å‰é˜²å¾¡æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰é˜²å¾¡æªæ–½åœ¨ç°å®é‡‘èç¯å¢ƒä¸­ä»ç„¶æ— æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14245",
            "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2601.14245",
            "abstract": "A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.",
            "score": 7,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "e41f2cd4d64c71b2",
            "authors": [
                "Zhongyu Yang",
                "Wei Pang",
                "Yingfang Yuan"
            ],
            "affiliations": [
                "BCML, Heriot-Watt University Edinburgh, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14245.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº XR Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… FashionIQ, CIRR Ğ¸ CIRCO Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Retrieval with Multi-Agent Coordination",
                    "desc": "This paper presents a multi-agent framework called XR for compositional image retrieval, which enhances the process of matching images with textual descriptions. It utilizes three types of specialized agents: imagination agents that generate target representations, similarity agents that filter results based on hybrid matching, and question agents that verify the accuracy of the results. By coordinating these agents, XR improves the retrieval process to better meet both semantic and visual requirements. The framework shows significant performance improvements over existing methods, demonstrating the importance of each agent in achieving effective retrieval outcomes."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡å›¾åƒæ£€ç´¢æ•ˆæœ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç»„åˆå›¾åƒæ£€ç´¢ï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰å’Œè§†è§‰æŸ¥è¯¢åŒ¹é…çš„æ•ˆæœã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸‰ç§ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼šæƒ³è±¡æ™ºèƒ½ä½“ç”Ÿæˆç›®æ ‡è¡¨ç¤ºï¼Œç›¸ä¼¼æ€§æ™ºèƒ½ä½“è¿›è¡Œç²—ç•¥è¿‡æ»¤ï¼Œé—®é¢˜æ™ºèƒ½ä½“é€šè¿‡é’ˆå¯¹æ€§æ¨ç†éªŒè¯äº‹å®ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„ç›¸ä¼¼æ€§åŸºç¡€æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è·¨æ¨¡æ€çš„ç†è§£å’Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¯ç§æ™ºèƒ½ä½“åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11141",
            "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
            "url": "https://huggingface.co/papers/2601.11141",
            "abstract": "Chroma 1.0 enables real-time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text-audio token scheduling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .",
            "score": 7,
            "issue_id": 718,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "447c118466c0f950",
            "authors": [
                "Tanyu Chen",
                "Tairan Chen",
                "Kai Shen",
                "Zhenghua Bao",
                "Zhihui Zhang",
                "Man Yuan",
                "Yi Shi"
            ],
            "affiliations": [
                "FlashLabs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11141.jpg",
            "data": {
                "categories": [
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ“Ğ¾Ğ»Ğ¾Ñ Ñ Ğ´ÑƒÑˆĞ¾Ğ¹ â€” Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Chroma 1.0 â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¼ĞµĞ½ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ 1:2, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ½Ğ° 10.96% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ 0.43 Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´."
                },
                "en": {
                    "title": "Chroma 1.0: Real-Time Dialogue with Personalized Voice Cloning",
                    "desc": "Chroma 1.0 is a groundbreaking spoken dialogue system that allows for real-time conversations with personalized voice cloning. It utilizes discrete speech representations and an innovative interleaved text-audio token scheduling to achieve low-latency interactions. This model significantly improves speaker identity preservation, making voice interactions more personal and engaging. Experimental results show that Chroma outperforms human baselines in speaker similarity while maintaining strong dialogue capabilities and fast response times."
                },
                "zh": {
                    "title": "å®æ—¶ä¸ªæ€§åŒ–è¯­éŸ³å¯¹è¯çš„æœªæ¥",
                    "desc": "Chroma 1.0 æ˜¯ä¸€ä¸ªå¼€æºçš„å®æ—¶å¯¹è¯æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°ä¸ªæ€§åŒ–çš„è¯­éŸ³å…‹éš†ã€‚å®ƒé€šè¿‡ç¦»æ•£è¯­éŸ³è¡¨ç¤ºå’Œäº¤é”™çš„æ–‡æœ¬-éŸ³é¢‘ä»¤ç‰Œè°ƒåº¦ï¼Œæ”¯æŒä½å»¶è¿Ÿçš„è¯­éŸ³äº¤äº’ã€‚è¯¥æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­ä¿æŒé«˜è´¨é‡çš„ä¸ªæ€§åŒ–è¯­éŸ³åˆæˆï¼Œå¹¶ä¸”åœ¨è¯´è¯è€…ç›¸ä¼¼æ€§ä¸Šç›¸è¾ƒäºäººç±»åŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChroma 1.0 çš„å®æ—¶å› å­ä¸º 0.43ï¼Œå±•ç°äº†å¼ºå¤§çš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15220",
            "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
            "url": "https://huggingface.co/papers/2601.15220",
            "abstract": "Benign fine-tuning of language models can cause privacy collapse, where models lose contextual privacy reasoning abilities despite maintaining high performance on standard benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.",
            "score": 6,
            "issue_id": 717,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "bcbfde11fe0bcf06",
            "authors": [
                "Anmol Goel",
                "Cornelius Emde",
                "Sangdoo Yun",
                "Seong Joon Oh",
                "Martin Gubri"
            ],
            "affiliations": [
                "NAVER AI Lab",
                "Parameter Lab",
                "TU Darmstadt",
                "University of Oxford",
                "University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15220.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#ethics",
                    "#alignment",
                    "#security",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ñ‡Ğ°Ğ»Ğ¸Ğ²Ñ‹Ğ¹ Ğ²Ñ€Ğ°Ğ³: ĞºĞ°Ğº fine-tuning ĞºÑ€ÑƒÑˆĞ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (fine-tuning) Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ â€” Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº ĞºÑ€Ğ°Ñ…Ñƒ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ¼ĞµĞ½Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸ fine-tuning Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Beware the Silent Privacy Collapse in Fine-Tuned Language Models!",
                    "desc": "This paper discusses a new issue called 'privacy collapse' in language models that occurs during benign fine-tuning. Despite performing well on standard tests, these models lose their ability to understand and maintain contextual privacy. The authors identify various factors, such as exposure to user data and emotional dialogue, that contribute to this degradation of privacy reasoning. Their findings highlight a significant oversight in current safety evaluations, especially for specialized AI agents, as they can still exhibit serious privacy risks while appearing effective."
                },
                "zh": {
                    "title": "è‰¯æ€§å¾®è°ƒå¯èƒ½å¯¼è‡´éšç§å´©æºƒ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œè‰¯æ€§å¾®è°ƒæ—¶å¯èƒ½å‡ºç°çš„éšç§å´©æºƒç°è±¡ã€‚å°½ç®¡æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬çš„ä¸Šä¸‹æ–‡éšç§æ¨ç†èƒ½åŠ›å´æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬å‘ç°ï¼Œè®­ç»ƒæ•°æ®ä¸­çš„å¤šæ ·åŒ–å’Œå¾®å¦™æ¨¡å¼ä¼šå‰Šå¼±ä¸Šä¸‹æ–‡éšç§ï¼ŒåŒ…æ‹¬å¯¹æœ‰ç”¨æ€§çš„ä¼˜åŒ–ã€ç”¨æˆ·ä¿¡æ¯çš„æš´éœ²ä»¥åŠæƒ…æ„Ÿå¯¹è¯ç­‰ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œéšç§å´©æºƒåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šæ™®éå­˜åœ¨ï¼Œæ­ç¤ºäº†å½“å‰å®‰å…¨è¯„ä¼°ä¸­çš„é‡è¦ç¼ºå£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14417",
            "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
            "url": "https://huggingface.co/papers/2601.14417",
            "abstract": "Research investigates the relationship between speaker embeddings and phonological rules in accent control for text-to-speech systems, introducing a metric to measure rule preservation versus embedding influence.  \t\t\t\t\tAI-generated summary \t\t\t\t Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.",
            "score": 5,
            "issue_id": 708,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "91f0d508ead68a91",
            "authors": [
                "Thanathai Lertpetchpun",
                "Yoonjeong Lee",
                "Thanapat Trachu",
                "Jihwan Lee",
                "Tiantian Feng",
                "Dani Byrd",
                "Shrikanth Narayanan"
            ],
            "affiliations": [
                "Department of Linguistics, University of Southern California",
                "Signal Analysis and Interpretation Lab, University of Southern California",
                "Thomas Lord Department of Computer Science, University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14417.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ phoneme shift rate (PSR) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ñ„Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Accent Control in TTS through Phonological Rules and Speaker Embeddings",
                    "desc": "This research explores how speaker embeddings, which are used in text-to-speech (TTS) systems to generate accents, interact with phonological rules that govern speech sounds. The study introduces a new metric called phoneme shift rate (PSR) to measure how well these embeddings maintain or override linguistic rules in accent synthesis. By examining American and British English, the authors demonstrate that combining phonological rules with speaker embeddings can produce more authentic accents. The findings suggest that understanding this relationship can improve accent control and help disentangle speaker identity from accent features in TTS models."
                },
                "zh": {
                    "title": "å£éŸ³æ§åˆ¶ï¼šè§„åˆ™ä¸åµŒå…¥çš„ç»“åˆ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯´è¯è€…åµŒå…¥ä¸è¯­éŸ³è§„åˆ™åœ¨å£éŸ³æ§åˆ¶ä¸­çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºéŸ³ç´ è½¬ç§»ç‡ï¼ˆPSRï¼‰ï¼Œç”¨äºè¡¡é‡åµŒå…¥å¯¹è§„åˆ™ä¿æŒä¸å½±å“çš„ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç¾å¼å’Œè‹±å¼è‹±è¯­çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å®ç°äº†ä¸å£éŸ³ç›¸å…³çš„è¯­éŸ³è§„åˆ™ï¼Œå¹¶å‘ç°å°†è§„åˆ™ä¸åµŒå…¥ç»“åˆå¯ä»¥ç”Ÿæˆæ›´çœŸå®çš„å£éŸ³ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†è¯­éŸ³è§„åˆ™åœ¨å£éŸ³æ§åˆ¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè¯­éŸ³ç”Ÿæˆä¸­çš„è§£è€¦è¯„ä¼°æä¾›äº†æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14352",
            "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
            "url": "https://huggingface.co/papers/2601.14352",
            "abstract": "RoboBrain 2.5 enhances embodied AI through improved 3D spatial reasoning and temporal value estimation for more precise manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io",
            "score": 5,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "a4c2212b43fc1018",
            "authors": [
                "Huajie Tan",
                "Enshen Zhou",
                "Zhiyu Li",
                "Yijie Xu",
                "Yuheng Ji",
                "Xiansheng Chen",
                "Cheng Chi",
                "Pengwei Wang",
                "Huizhu Jia",
                "Yulong Ao",
                "Mingyu Cao",
                "Sixiang Chen",
                "Zhe Li",
                "Mengzhen Liu",
                "Zixiao Wang",
                "Shanyu Rong",
                "Yaoxu Lyu",
                "Zhongxia Zhao",
                "Peterson Co",
                "Yibo Li",
                "Yi Han",
                "Shaoxuan Xie",
                "Guocai Yao",
                "Songjing Wang",
                "Leiduo Zhang",
                "Xi Yang",
                "Yance Jiao",
                "Donghai Shi",
                "Kunchang Xie",
                "Shaokai Nie",
                "Chunlei Men",
                "Yonghua Lin",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Shanghang Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.14352.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "RoboBrain 2.5 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğº Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ² Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Empowering AI with Enhanced 3D Reasoning and Temporal Awareness",
                    "desc": "RoboBrain 2.5 is an advanced embodied AI model that improves how machines understand and interact with 3D environments. It enhances spatial reasoning by using depth-aware predictions instead of just 2D images, allowing for more accurate manipulation of objects. Additionally, it introduces a method for estimating the value of actions over time, which helps the AI predict and understand its progress during tasks. These improvements make RoboBrain 2.5 better at performing complex tasks that require precise movements and awareness of its surroundings."
                },
                "zh": {
                    "title": "æå‡å…·èº«æ™ºèƒ½çš„ç²¾ç¡®æ“ä½œèƒ½åŠ›",
                    "desc": "RoboBrain 2.5 æ˜¯ä¸€ç§å…ˆè¿›çš„å…·èº«äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹ï¼Œæå‡äº†å…¶åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†å’Œæ—¶é—´ä»·å€¼ä¼°è®¡æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡é«˜è´¨é‡çš„æ—¶ç©ºç›‘ç£è®­ç»ƒï¼Œè¯¥æ¨¡å‹å®ç°äº†ç²¾ç¡®çš„ä¸‰ç»´ç©ºé—´æ¨ç†ï¼Œèƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„ä¸‰ç»´æ“ä½œè½¨è¿¹ã€‚å®ƒè¿˜å»ºç«‹äº†å¯†é›†çš„æ—¶é—´ä»·å€¼ä¼°è®¡ï¼Œæä¾›äº†å¯¹æ‰§è¡ŒçŠ¶æ€çš„ç†è§£å’Œè¿›åº¦é¢„æµ‹ã€‚è¿™äº›å‡çº§ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚çš„ç²¾ç»†æ“ä½œä¸­æ›´åŠ ç¨³å®šå’Œæ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13918",
            "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
            "url": "https://huggingface.co/papers/2601.13918",
            "abstract": "AgentEHR presents a benchmark for autonomous EHR navigation requiring complex decision-making, while RetroSum framework improves performance through retrospective summarization and evolving experience strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
            "score": 5,
            "issue_id": 707,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "83af4d3596d5e4f2",
            "authors": [
                "Yusheng Liao",
                "Chuan Xuan",
                "Yutong Cai",
                "Lina Yang",
                "Zhe Chen",
                "Yanfeng Wang",
                "Yu Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13918.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#healthcare",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AgentEHR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RetroSum, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 29.16% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° 92.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing EHR Navigation with RetroSum for Better Decision-Making",
                    "desc": "The paper introduces AgentEHR, a benchmark designed for autonomous navigation of Electronic Health Records (EHRs) that involves complex decision-making tasks like diagnosis and treatment planning. It highlights the limitations of current summarization methods, which often lead to loss of critical information and disrupted reasoning. To overcome these challenges, the authors propose RetroSum, a framework that combines retrospective summarization with an evolving experience strategy to maintain logical coherence and prevent information loss. Empirical results show that RetroSum significantly improves performance and reduces interaction errors compared to existing methods."
                },
                "zh": {
                    "title": "æå‡è‡ªä¸»EHRå¯¼èˆªçš„æ™ºèƒ½å†³ç­–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†AgentEHRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªä¸»ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰å¯¼èˆªçš„åŸºå‡†ï¼Œè¦æ±‚å¤æ‚çš„å†³ç­–åˆ¶å®šã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæå‡ºäº†RetroSumæ¡†æ¶ï¼Œé€šè¿‡å›é¡¾æ€§æ€»ç»“å’Œä¸æ–­æ¼”å˜çš„ç»éªŒç­–ç•¥æ¥è§£å†³ä¿¡æ¯ä¸¢å¤±å’Œæ¨ç†ä¸è¿è´¯çš„é—®é¢˜ã€‚AgentEHRæŒ‘æˆ˜æ™ºèƒ½ä½“åœ¨é«˜å™ªå£°æ•°æ®åº“ä¸­è¿›è¡Œé•¿ç¨‹äº¤äº’æ¨ç†ï¼Œæ‰§è¡Œè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroSumåœ¨æ€§èƒ½ä¸Šæ¯”ç«äº‰åŸºçº¿æé«˜äº†29.16%ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†92.3%çš„äº¤äº’é”™è¯¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14152",
            "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
            "url": "https://huggingface.co/papers/2601.14152",
            "abstract": "Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
            "score": 4,
            "issue_id": 705,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "ebb86e7319481dfb",
            "authors": [
                "Hyunjong Ok",
                "Jaeho Lee"
            ],
            "affiliations": [
                "HJ AILAB",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14152.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ QOC. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ (CQO) Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 14 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Causal Attention: The Key to Unlocking Prompt Performance",
                    "desc": "This paper investigates how the order of prompts affects the performance of large language models in multiple-choice question answering. It finds that placing context before questions and options (CQO) significantly improves performance compared to the reverse order (QOC), with a drop of over 14 percentage points in the latter. The authors attribute this performance gap to causal attention mechanisms, which restrict option tokens from accessing context in QOC prompts, leading to an information bottleneck. By analyzing various models and datasets, the study highlights the importance of prompt structure in optimizing model performance."
                },
                "zh": {
                    "title": "å› æœæ³¨æ„åŠ›å½±å“å¤šé€‰é¢˜è¡¨ç°",
                    "desc": "æœ¬ç ”ç©¶æ­ç¤ºäº†è¯­è¨€æ¨¡å‹ä¸­çš„å› æœæ³¨æ„åŠ›åœ¨å¤šé€‰é¢˜å›ç­”ä¸­é€ æˆä¿¡æ¯ç“¶é¢ˆçš„é—®é¢˜ã€‚å½“ä¸Šä¸‹æ–‡åœ¨é—®é¢˜å’Œé€‰é¡¹ä¹‹å‰æ—¶ï¼ˆCQOï¼‰ï¼Œæ¨¡å‹çš„è¡¨ç°æ¯”åå‘é¡ºåºï¼ˆQOCï¼‰é«˜å‡ºè¶…è¿‡14ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶æ„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å› æœæ³¨æ„åŠ›æ˜¯æ ¸å¿ƒæœºåˆ¶ï¼šåœ¨QOCæç¤ºä¸­ï¼Œå› æœæ©ç é˜»æ­¢é€‰é¡¹æ ‡è®°å…³æ³¨ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯¼è‡´ä¸Šä¸‹æ–‡å¯¹é€‰é¡¹ä¸å¯è§ã€‚è¿™ä¸ªå‘ç°ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿæ€§æä¾›äº†æ–°çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14681",
            "title": "FARE: Fast-Slow Agentic Robotic Exploration",
            "url": "https://huggingface.co/papers/2601.14681",
            "abstract": "FARE is a hierarchical exploration framework that combines large language model reasoning with reinforcement learning control to enable efficient autonomous robot navigation in complex environments.  \t\t\t\t\tAI-generated summary \t\t\t\t This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.",
            "score": 3,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "b87cdcf418e36496",
            "authors": [
                "Shuhao Liao",
                "Xuxin Lv",
                "Jeric Lew",
                "Shizhe Zhang",
                "Jingsong Liang",
                "Peizhuo Li",
                "Yuhong Cao",
                "Wenjun Wu",
                "Guillaume Sartoretti"
            ],
            "affiliations": [
                "Beihang University, China",
                "Department of Mechanical Engineering, National University of Singapore, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14681.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#graphs",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: LLM Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, RL Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "FARE â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ° Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ RL-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒÑ 200Ã—130 Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FARE: Smart Navigation for Robots with Language and Learning",
                    "desc": "FARE is a novel framework that enhances robot navigation by combining large language model (LLM) reasoning with reinforcement learning (RL) control. It operates on a fast-slow thinking paradigm, where the LLM interprets environmental descriptions to create exploration strategies, while the RL module executes these strategies based on real-time observations. The framework uses a modularity-based pruning mechanism to streamline the exploration graph, improving reasoning efficiency. Experimental results demonstrate that FARE significantly outperforms existing methods in both simulated and real-world environments, showcasing its effectiveness in complex navigation tasks."
                },
                "zh": {
                    "title": "FAREï¼šé«˜æ•ˆè‡ªä¸»æ¢ç´¢çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "FAREæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æ¢ç´¢æ¡†æ¶ï¼Œç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶æ–¹æ³•ï¼Œä»¥å®ç°å¤æ‚ç¯å¢ƒä¸­çš„é«˜æ•ˆè‡ªä¸»æœºå™¨äººå¯¼èˆªã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¿«æ…¢æ€ç»´æ¨¡å¼ï¼Œæ…¢æ€ç»´çš„LLMæ¨¡å—è§£ææœªçŸ¥ç¯å¢ƒçš„æ–‡æœ¬æè¿°ï¼Œå¹¶ç”Ÿæˆæ¢ç´¢ç­–ç•¥ï¼Œéšåé€šè¿‡æ‹“æ‰‘å›¾è½¬åŒ–ä¸ºå…¨å±€è·¯å¾„ç‚¹ã€‚å¿«é€Ÿæ€ç»´çš„RLæ¨¡å—æ ¹æ®å±€éƒ¨è§‚å¯Ÿæ‰§è¡Œæ¢ç´¢ï¼ŒåŒæ—¶å—åˆ°LLMç”Ÿæˆçš„å…¨å±€è·¯å¾„ç‚¹çš„æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAREåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨å®é™…ç¡¬ä»¶ä¸ŠéªŒè¯äº†å…¶åœ¨å¤æ‚å¤§è§„æ¨¡ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14256",
            "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
            "url": "https://huggingface.co/papers/2601.14256",
            "abstract": "A unified model learns image representations useful for both recognition and generation by using a hyper-network for implicit neural representation with knowledge distillation, achieving state-of-the-art results while enabling generative capabilities through compressed embeddings.  \t\t\t\t\tAI-generated summary \t\t\t\t Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
            "score": 3,
            "issue_id": 720,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "cb930f31309587e5",
            "authors": [
                "Matthew Gwilliam",
                "Xiao Wang",
                "Xuefeng Hu",
                "Zhenheng Yang"
            ],
            "affiliations": [
                "TikTok"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14256.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (INR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unifying Recognition and Generation in Image Representation Learning",
                    "desc": "This paper presents a novel unified model that simultaneously learns image representations for both recognition and generation tasks. It employs a hyper-network architecture for implicit neural representation, which efficiently maps images to model weights, facilitating quick and accurate image reconstruction. The model incorporates knowledge distillation to enhance its generalization capabilities and overall performance. By achieving a compressed embedding space, the model demonstrates state-of-the-art results across various visual tasks while maintaining generative abilities."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¨¡å‹ï¼šå›¾åƒè¯†åˆ«ä¸ç”Ÿæˆçš„å®Œç¾ç»“åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ å›¾åƒçš„è¯†åˆ«å’Œç”Ÿæˆè¡¨ç¤ºã€‚é€šè¿‡ä½¿ç”¨è¶…ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ï¼Œè¯¥æ¨¡å‹å®ç°äº†éšå¼ç¥ç»è¡¨ç¤ºï¼Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°é‡å»ºå›¾åƒã€‚æ¨¡å‹å­¦ä¹ åˆ°çš„å‹ç¼©åµŒå…¥ç©ºé—´åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§å›¾åƒå¤„ç†ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15059",
            "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
            "url": "https://huggingface.co/papers/2601.15059",
            "abstract": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.   We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.   We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.   We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.   We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
            "score": 2,
            "issue_id": 706,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "7dd0ebf904712ae9",
            "authors": [
                "Oleg Romanchuk",
                "Roman Bondar"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.15059.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² CI/CD",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Â«Ğ²Ğ°ĞºÑƒÑƒĞ¼Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ² CI/CD ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ Ğ»ÑĞ´Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ½Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°, Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ñ€Ğ¸Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ», Ğ·Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾ ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ±ĞµĞ· Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Gap: Addressing Responsibility in Automated Decision-Making",
                    "desc": "This paper discusses a problem in modern Continuous Integration/Continuous Deployment (CI/CD) systems where decisions made by automated agents lack clear responsibility attribution. It introduces the term 'responsibility vacuum' to describe a situation where decisions are made, but no one can be held accountable because the authority to approve and the understanding of the decision's basis do not align. The authors argue that this issue arises not from technical flaws but from a structural limitation when the speed of decision-making outpaces human verification capabilities. They suggest that without redesigning how responsibilities are assigned, organizations will continue to face this challenge as automation increases."
                },
                "zh": {
                    "title": "è´£ä»»çœŸç©ºï¼šå†³ç­–ä¸è´£ä»»çš„å¤±è¡¡",
                    "desc": "ç°ä»£æŒç»­é›†æˆ/æŒç»­äº¤ä»˜ï¼ˆCI/CDï¼‰ç®¡é“ä¸­ï¼Œä»£ç†ç”Ÿæˆçš„ä»£ç å­˜åœ¨è´£ä»»å½’å±çš„ç»“æ„æ€§å¤±è´¥ã€‚å°½ç®¡å†³ç­–é€šè¿‡æ­£å¼çš„æ‰¹å‡†æµç¨‹æ‰§è¡Œï¼Œä½†æ²¡æœ‰ä»»ä½•å®ä½“åŒæ—¶å…·å¤‡æ‰¹å‡†è¿™äº›å†³ç­–çš„æƒå¨å’Œç†è§£å…¶åŸºç¡€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™ç§æƒ…å†µå®šä¹‰ä¸ºè´£ä»»çœŸç©ºï¼šä¸€ç§å†³ç­–å‘ç”Ÿä½†æ— æ³•å½’å±è´£ä»»çš„çŠ¶æ€ï¼Œå› ä¸ºæƒå¨å’ŒéªŒè¯èƒ½åŠ›å¹¶ä¸é‡åˆã€‚æˆ‘ä»¬æŒ‡å‡ºï¼Œè¿™ä¸æ˜¯è¿‡ç¨‹åå·®æˆ–æŠ€æœ¯ç¼ºé™·ï¼Œè€Œæ˜¯å†³ç­–ç”Ÿæˆé€Ÿåº¦è¶…è¿‡äººç±»éªŒè¯èƒ½åŠ›çš„ç»“æ„æ€§ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15100",
            "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
            "url": "https://huggingface.co/papers/2601.15100",
            "abstract": "WebSeek is a mixed-initiative browser extension that enables interactive web data extraction and analysis with AI-assisted guidance and automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.",
            "score": 1,
            "issue_id": 705,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "8a15e6703617633f",
            "authors": [
                "Yanwei Huang",
                "Arpit Narechania"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15100.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ•·ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ĞµĞ±-ÑĞºÑ€ĞµĞ¿Ğ¸Ğ½Ğ³ Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ AI",
                    "desc": "WebSeek â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‘ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ (Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑĞ¿Ğ¸ÑĞºĞ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸) Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²ÑƒÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ…: Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ñ†ĞµĞ½ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ˜Ğ˜ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Empowering Web Data Analysis with AI Collaboration",
                    "desc": "WebSeek is a browser extension that combines human and AI efforts to help users extract and analyze data from the web. It allows users to create and modify data artifacts like tables and visualizations in an interactive workspace. The AI component not only provides guidance based on user actions but also automates certain tasks to enhance the analysis process. A user study showed that participants appreciated having control and clarity while working alongside the AI."
                },
                "zh": {
                    "title": "WebSeekï¼šæ™ºèƒ½åŒ–çš„ç½‘é¡µæ•°æ®æå–ä¸åˆ†æå·¥å…·",
                    "desc": "WebSeek æ˜¯ä¸€ä¸ªæ··åˆä¸»åŠ¨çš„æµè§ˆå™¨æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡ AI è¾…åŠ©çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–æ¥å®ç°äº’åŠ¨å¼ç½‘é¡µæ•°æ®æå–å’Œåˆ†æã€‚ç”¨æˆ·å¯ä»¥åœ¨ä¸€ä¸ªäº’åŠ¨ç”»å¸ƒä¸­å‘ç°å’Œæå–ç½‘é¡µä¿¡æ¯ï¼Œçµæ´»æ„å»ºã€è½¬æ¢å’Œå®Œå–„æ•°æ®å·¥ä»¶ï¼Œå¦‚è¡¨æ ¼ã€åˆ—è¡¨å’Œå¯è§†åŒ–ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–ï¼Œè¿˜èƒ½æ ¹æ®ç”¨æˆ·çš„æ˜ç¡®è¯·æ±‚åšå‡ºååº”ã€‚ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨åˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤šæ ·çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†åœ¨ä¸ AI åä½œæ—¶å¯¹é€æ˜åº¦å’Œæ§åˆ¶çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12029",
            "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
            "url": "https://huggingface.co/papers/2601.12029",
            "abstract": "The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.",
            "score": 1,
            "issue_id": 708,
            "pub_date": "2026-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "498493ce44f57bbd",
            "authors": [
                "Sandy H. S. Herho",
                "Faruq Khadami",
                "Iwan P. Anwar",
                "Dasapta E. Irawan"
            ],
            "affiliations": [
                "Applied Geology Research Group, Bandung Institute of Technology",
                "Applied and Environmental Oceanography Research Group, Bandung Institute of Technology",
                "Department of Earth and Planetary Sciences, University of California, Riverside",
                "Ronin Institute for Independent Scholarship 2.0",
                "School of Systems Science and Industrial Engineering, State University of New York, Binghamton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12029.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ sangkuriang â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ° Python Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ĞšĞ¾Ñ€Ñ‚ĞµĞ²ĞµĞ³Ğ°-Ğ´Ğµ Ğ¤Ñ€Ğ¸Ğ·Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ»Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ñ‹. Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ JIT-ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… â€” Ğ¾Ñ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ° Ğ´Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Sangkuriang: Efficient Solving of Nonlinear Wave Dynamics",
                    "desc": "This paper presents 'sangkuriang', a Python library designed to solve the Korteweg-de Vries (KdV) equation, which models nonlinear wave behavior. The library utilizes Fourier pseudo-spectral methods for spatial discretization and adaptive high-order time integration, ensuring efficient computation through just-in-time (JIT) compilation. It validates its performance through various scenarios, including soliton propagation and wave interactions, while monitoring the conservation of classical invariants. The results align closely with theoretical predictions, making it a valuable tool for both educational and research purposes in nonlinear wave physics."
                },
                "zh": {
                    "title": "sangkuriangï¼šé«˜æ•ˆæ±‚è§£KdVæ–¹ç¨‹çš„å¼€æºå·¥å…·",
                    "desc": "Korteweg-de Vries (KdV) æ–¹ç¨‹æ˜¯éçº¿æ€§æ³¢åŠ¨ç‰©ç†ä¸­çš„åŸºç¡€æ¨¡å‹ï¼Œæè¿°äº†è‰²æ•£æ‰©æ•£ä¸éçº¿æ€§é™¡å³­ä¹‹é—´çš„å¹³è¡¡ï¼Œä»è€Œäº§ç”Ÿå­¤ç«‹æ³¢ã€‚æœ¬æ–‡ä»‹ç»äº†sangkuriangï¼Œä¸€ä¸ªå¼€æºçš„Pythonåº“ï¼Œç”¨äºé€šè¿‡å‚…é‡Œå¶ä¼ªè°±ç©ºé—´ç¦»æ•£åŒ–å’Œè‡ªé€‚åº”é«˜é˜¶æ—¶é—´ç§¯åˆ†æ¥æ±‚è§£è¯¥æ–¹ç¨‹ã€‚è¯¥å®ç°åˆ©ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰æé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ•™å­¦çš„å¯åŠæ€§ã€‚é€šè¿‡å¯¹å­¤ç«‹å­¤ç«‹æ³¢ä¼ æ’­ã€å¯¹ç§°åŒæ³¢é…ç½®ã€ä¸åŒå¹…åº¦æ³¢çš„è¶…è¶Šç¢°æ’å’Œä¸‰ä½“ç›¸äº’ä½œç”¨ç­‰å¤æ‚åœºæ™¯çš„éªŒè¯ï¼Œç¡®ä¿äº†ç»å…¸ä¸å˜é‡çš„å®ˆæ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11387",
            "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
            "url": "https://huggingface.co/papers/2601.11387",
            "abstract": "Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.",
            "score": 1,
            "issue_id": 718,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "d7f12c7c0a97fde1",
            "authors": [
                "Greta Warren",
                "Jingyi Sun",
                "Irina Shklovski",
                "Isabelle Augenstein"
            ],
            "affiliations": [
                "LinkÃ¶ping University",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11387.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ¾Ğ»ÑŒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑ Ñ‚Ğ¸Ğ¿ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ AI Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹ AI, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ğ»Ğ¸ÑÑŒ Ğº Ğ½Ğ¸Ğ¼ Ñ€ĞµĞ¶Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° ÑĞ²Ğ»ÑÑÑ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Evidence Matters: Enhancing Trust in AI with Reliable Information",
                    "desc": "This paper investigates how people use evidence when evaluating AI-generated claims in decision-making tasks. The study varied the type of explanations provided by the AI, the certainty of its predictions, and the correctness of its advice. Results showed that participants often relied on evidence to assess the validity of AI claims, especially when explanations were unclear or inadequate. The findings highlight the importance of evidence in enhancing trust and decision-making in AI systems, suggesting a need for further research on effective evidence presentation."
                },
                "zh": {
                    "title": "è¯æ®æ˜¯è¯„ä¼°AIå¯é æ€§çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯æ®åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è§£é‡Šä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ”¹å˜äº†è§£é‡Šç±»å‹ã€AIé¢„æµ‹çš„ç¡®å®šæ€§å’ŒAIç³»ç»Ÿå»ºè®®çš„æ­£ç¡®æ€§ï¼Œå‚ä¸è€…è¯„ä¼°äº†å£°æ˜çš„çœŸå®æ€§å’ŒAIç³»ç»Ÿçš„é¢„æµ‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨æ‰€æœ‰å®éªŒæ¡ä»¶ä¸‹éƒ½ä¾èµ–è¯æ®æ¥éªŒè¯AIçš„å£°æ˜ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€è§£é‡Šä¸è¶³æ—¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯æ®æ˜¯è¯„ä¼°AIç³»ç»Ÿä¿¡æ¯å¯é æ€§çš„å…³é”®å› ç´ ï¼Œæœªæ¥éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•æœ‰æ•ˆå‘ˆç°è¯æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14253",
            "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
            "url": "https://huggingface.co/papers/2601.14253",
            "abstract": "Motion 3-to-4 synthesizes high-quality 4D dynamic objects from monocular video and 3D mesh by decomposing into static shape generation and motion reconstruction with canonical mesh and transformer-based frame processing.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
            "score": 0,
            "issue_id": 720,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "aec03822d9e2ac31",
            "authors": [
                "Hongyuan Chen",
                "Xingyu Chen",
                "Youjia Zhang",
                "Zexiang Xu",
                "Anpei Chen"
            ],
            "affiliations": [
                "HUST",
                "Hillbot",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14253.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#video",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ñ‡ĞµÑ‚Ğ²Ñ‘Ñ€Ñ‚Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "Motion 3-to-4 â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ 3D ÑĞµÑ‚ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ 3D Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Transforming Monocular Videos into 4D Dynamic Objects",
                    "desc": "Motion 3-to-4 is a novel framework that generates high-quality 4D dynamic objects from a single monocular video and an optional 3D mesh. It tackles the challenges of 4D synthesis by separating the process into two main tasks: generating a static 3D shape and reconstructing motion. The model utilizes a canonical mesh to create a compact representation of motion and predicts the movement of vertices frame by frame, ensuring temporal coherence. By employing a transformer architecture, it effectively handles varying sequence lengths, resulting in improved fidelity and spatial consistency over previous methods."
                },
                "zh": {
                    "title": "é«˜è´¨é‡4DåŠ¨æ€ç‰©ä½“åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "Motion 3-to-4 æ˜¯ä¸€ä¸ªå‰é¦ˆæ¡†æ¶ï¼Œå¯ä»¥ä»å•ä¸ªå•ç›®è§†é¢‘å’Œå¯é€‰çš„ 3D å‚è€ƒç½‘æ ¼åˆæˆé«˜è´¨é‡çš„ 4D åŠ¨æ€ç‰©ä½“ã€‚è¯¥æ–¹æ³•é€šè¿‡å°† 4D åˆæˆåˆ†è§£ä¸ºé™æ€ 3D å½¢çŠ¶ç”Ÿæˆå’Œè¿åŠ¨é‡å»ºæ¥è§£å†³ä»å•ç›®è§†è§’æ¢å¤å‡ ä½•å½¢çŠ¶å’Œè¿åŠ¨çš„æŒ‘æˆ˜ã€‚æ¨¡å‹ä½¿ç”¨è§„èŒƒå‚è€ƒç½‘æ ¼å­¦ä¹ ç´§å‡‘çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹æ¯å¸§çš„é¡¶ç‚¹è½¨è¿¹ï¼Œä»¥æ¢å¤å®Œæ•´ä¸”æ—¶é—´ä¸€è‡´çš„å‡ ä½•å½¢çŠ¶ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMotion 3-to-4 åœ¨ä¿çœŸåº¦å’Œç©ºé—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä¹‹å‰çš„å·¥ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13262",
            "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
            "url": "https://huggingface.co/papers/2601.13262",
            "abstract": "A multilingual medical reasoning framework using curriculum-informed reinforcement learning achieves high language consistency and logical correctness across thirteen languages including underrepresented ones.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
            "score": 0,
            "issue_id": 719,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "48abfc2808f83954",
            "authors": [
                "Eric Onyame",
                "Akash Ghosh",
                "Subhadip Baidya",
                "Sriparna Saha",
                "Xiuying Chen",
                "Chirag Agarwal"
            ],
            "affiliations": [
                "IIT-Patna",
                "MBZUAI",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13262.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#rlhf",
                    "#multilingual",
                    "#rl",
                    "#science",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#healthcare",
                    "#open_source",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CURE-MED â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ CUREMED-BENCH â€” Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´ĞºĞ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ°Ğ¼Ñ…Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑƒĞ°Ñ…Ğ¸Ğ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ fine-tuning Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ code-switching Ğ¸ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ° 85.21% Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ 54.35% Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ 7 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Multilingual Medical Reasoning with CURE-MED",
                    "desc": "This paper presents a new framework called CURE-MED that enhances multilingual medical reasoning using reinforcement learning. It introduces a dataset named CUREMED-BENCH, which includes reasoning queries in thirteen languages, focusing on underrepresented languages. The framework employs curriculum-informed techniques to improve both logical correctness and language consistency in AI models. The results show significant improvements in reasoning accuracy across multiple languages, making it a valuable tool for multilingual healthcare applications."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€åŒ»ç–—æ¨ç†çš„å¯é æ€§ä¸å…¬å¹³æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šè¯­è¨€åŒ»ç–—æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨è¯¾ç¨‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šè¯­è¨€åŒ»ç–—æ¨ç†çš„è¯­è¨€ä¸€è‡´æ€§å’Œé€»è¾‘æ­£ç¡®æ€§ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†CUREMED-BENCHæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªåä¸‰ç§è¯­è¨€çš„å¼€æ”¾å¼æ¨ç†æŸ¥è¯¢ï¼Œæ¶µç›–äº†åŒ…æ‹¬é˜¿å§†å“ˆæ‹‰è¯­ã€çº¦é²å·´è¯­å’Œæ–¯ç“¦å¸Œé‡Œè¯­ç­‰å°‘æ•°è¯­è¨€ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†CURE-MEDæ¡†æ¶ï¼Œç»“åˆäº†ä»£ç åˆ‡æ¢æ„ŸçŸ¥çš„ç›‘ç£å¾®è°ƒå’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä»¥å…±åŒæå‡é€»è¾‘æ­£ç¡®æ€§å’Œè¯­è¨€ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¯­è¨€ä¸Šå‡ä¼˜äºå¼ºåŸºçº¿ï¼Œæ”¯æŒåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°å¯é å’Œå…¬å¹³çš„å¤šè¯­è¨€åŒ»ç–—æ¨ç†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-21.html",
    "link_next": "2026-01-23.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 0,
        "#benchmark": 10,
        "#agents": 11,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 2,
        "#healthcare": 3,
        "#training": 9,
        "#robotics": 4,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 2,
        "#science": 6,
        "#low_resource": 3
    }
}