{
    "date": {
        "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 19",
        "zh": "12æœˆ19æ—¥"
    },
    "time_utc": "2024-12-19 03:23",
    "weekday": 3,
    "issue_id": 1204,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.14161",
            "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
            "url": "https://huggingface.co/papers/2412.14161",
            "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.",
            "score": 12,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "4efc2187cb10b78e",
            "authors": [
                "Frank F. Xu",
                "Yufan Song",
                "Boxuan Li",
                "Yuxuan Tang",
                "Kritanjali Jain",
                "Mengxue Bao",
                "Zora Z. Wang",
                "Xuhui Zhou",
                "Zhitong Guo",
                "Murong Cao",
                "Mingyang Yang",
                "Hao Yang Lu",
                "Amaad Martin",
                "Zhe Su",
                "Leander Maben",
                "Raj Mehta",
                "Wayne Chi",
                "Lawrence Jang",
                "Yiqing Xie",
                "Shuyan Zhou",
                "Graham Neubig"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Duke University",
                "Independent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14161.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agi",
                    "#agents",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¾Ñ„Ğ¸ÑĞµ: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TheAgentCompany Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¯Ğœ) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ĞµĞ±-ÑĞµÑ€Ñ„Ğ¸Ğ½Ğ³, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ 24% Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Evaluating AI Agents: The Future of Work Automation",
                    "desc": "This paper introduces TheAgentCompany, a benchmark designed to evaluate the performance of AI agents in completing work-related tasks similar to those of a digital worker. The study assesses how well these agents, powered by large language models (LLMs), can autonomously perform tasks such as web browsing, coding, and communication within a simulated software company environment. The results indicate that while 24% of tasks can be completed autonomously by the most effective agent, more complex tasks remain challenging for current AI systems. This research highlights the potential and limitations of AI in automating workplace functions, providing insights for industries considering AI integration."
                },
                "zh": {
                    "title": "AIä»£ç†åŠ©åŠ›å·¥ä½œä»»åŠ¡è‡ªåŠ¨åŒ–çš„æ¢ç´¢",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTheAgentCompanyçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨æ‰§è¡ŒçœŸå®å·¥ä½œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿå°å‹è½¯ä»¶å…¬å¸çš„ç¯å¢ƒï¼Œè®¾è®¡äº†å¤šç§ä»»åŠ¡ï¼Œä»£ç†å¯ä»¥é€šè¿‡æµè§ˆç½‘é¡µã€ç¼–å†™ä»£ç å’Œä¸åŒäº‹æ²Ÿé€šæ¥å®Œæˆè¿™äº›ä»»åŠ¡ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»å®Œæˆ24%çš„ä»»åŠ¡ï¼Œè¿™è¡¨æ˜åœ¨ç®€å•ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ–¹é¢ï¼Œå½“å‰çš„è¯­è¨€æ¨¡å‹ä»£ç†è¡¨ç°è‰¯å¥½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¯¹äºæ›´å¤æ‚çš„é•¿æœŸä»»åŠ¡ï¼Œç°æœ‰ç³»ç»Ÿä»ç„¶æ— æ³•èƒœä»»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12571",
            "title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2412.12571",
            "abstract": "Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "8fa92ec2d65420c8",
            "authors": [
                "Lianghua Huang",
                "Wei Wang",
                "Zhi-Fan Wu",
                "Yupeng Shi",
                "Chen Liang",
                "Tong Shen",
                "Han Zhang",
                "Huanzhang Dou",
                "Yu Liu",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Inc.",
                "Institute of Automation, Chinese Academy of Sciences",
                "Shanghai Jiao Tong University",
                "Taobao",
                "Tongyi Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12571.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#cv",
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ChatDiT: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ChatDiT - Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ChatDiT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ChatDiT Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° IDEA-Bench, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ChatDiT: Interactive Visual Generation with Zero-Tuning Diffusion Transformers",
                    "desc": "This paper introduces ChatDiT, a novel framework for visual generation that utilizes pretrained diffusion transformers (DiTs) without requiring any additional tuning or modifications. ChatDiT allows users to create and edit images, design characters, and generate text-image articles through natural language interactions. It operates using a multi-agent system that includes agents for instruction parsing, strategy planning, and execution of generation tasks. The framework has been evaluated on a diverse set of design tasks and has shown superior performance compared to other specialized models, highlighting the potential of DiTs in zero-shot learning scenarios."
                },
                "zh": {
                    "title": "ChatDiTï¼šé›¶-shotäº’åŠ¨è§†è§‰ç”Ÿæˆçš„æœªæ¥",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰å…·æœ‰å†…åœ¨çš„ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸­æ— ç¼é€‚åº”ï¼Œå‡ ä¹ä¸éœ€è¦æ¶æ„ä¿®æ”¹ã€‚è¿™äº›èƒ½åŠ›é€šè¿‡åœ¨å¤šä¸ªè¾“å…¥å’Œç›®æ ‡å›¾åƒä¹‹é—´è¿æ¥è‡ªæ³¨æ„åŠ›æ ‡è®°ï¼Œä»¥åŠç»“åˆåˆ†ç»„å’Œæ©è”½ç”Ÿæˆç®¡é“æ¥å®ç°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ChatDiTï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶-shotã€é€šç”¨ä¸”äº’åŠ¨çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸ChatDiTäº’åŠ¨ï¼Œåˆ›å»ºæ–‡æœ¬-å›¾åƒæ–‡ç« ã€ç¼–è¾‘å›¾åƒç­‰ã€‚ChatDiTçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¤šä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬æŒ‡ä»¤è§£æä»£ç†ã€ç­–ç•¥è§„åˆ’ä»£ç†å’Œæ‰§è¡Œä»£ç†ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å®Œæˆç”¨æˆ·çš„ç”Ÿæˆä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13746",
            "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
            "url": "https://huggingface.co/papers/2412.13746",
            "abstract": "Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training.We release our benchmark and code publicly at https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9159fbad2530d02c",
            "authors": [
                "Zhuoran Jin",
                "Hongbang Yuan",
                "Tianyi Men",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "affiliations": [
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
                "The Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13746.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rag",
                    "#open_source",
                    "#rlhf",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "RAG-RewardBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² RAG",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RAG-RewardBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ retrieval-augmented generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 18 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RAG, 6 Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ² Ğ¸ 24 Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RAG Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… RAG Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ RAG."
                },
                "en": {
                    "title": "Enhancing Human Preference Alignment in Retrieval Augmented Language Models",
                    "desc": "This paper introduces RAG-RewardBench, a benchmark designed to evaluate reward models (RMs) used in retrieval augmented language models (RALMs). The authors identify the challenge of aligning RMs with human preferences and propose four specific scenarios to test their effectiveness. They incorporate a diverse set of RAG subsets, retrievers, and RALMs to ensure comprehensive evaluation. The findings reveal significant limitations in current RMs and emphasize the necessity for training methods that prioritize preference alignment."
                },
                "zh": {
                    "title": "æå‡äººç±»åå¥½çš„æ£€ç´¢å¢å¼ºæ¨¡å‹å¯¹é½",
                    "desc": "å°½ç®¡ç°æœ‰çš„æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆRALMsï¼‰åœ¨æä¾›å¯ä¿¡å“åº”å’Œå¯é æ¥æºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ä¸äººç±»åå¥½çš„æœ‰æ•ˆå¯¹é½ä¸Šä»å­˜åœ¨ä¸è¶³ã€‚åœ¨å¯¹é½è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ä½œä¸ºäººç±»ä»·å€¼è§‚çš„é‡è¦ä»£ç†ï¼ŒæŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå¦‚ä½•è¯„ä¼°å’Œé€‰æ‹©å¯é çš„RMä»¥å®ç°RALMsä¸­çš„åå¥½å¯¹é½ä»ä¸æ˜ç¡®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RAG-RewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°RAGç¯å¢ƒä¸­RMçš„åŸºå‡†ï¼Œè®¾è®¡äº†å››ä¸ªå…³é”®çš„RAGç‰¹å®šåœºæ™¯ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13501",
            "title": "GUI Agents: A Survey",
            "url": "https://huggingface.co/papers/2412.13501",
            "abstract": "Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "93d87411c70f693d",
            "authors": [
                "Dang Nguyen",
                "Jian Chen",
                "Yu Wang",
                "Gang Wu",
                "Namyong Park",
                "Zhengmian Hu",
                "Hanjia Lyu",
                "Junda Wu",
                "Ryan Aponte",
                "Yu Xia",
                "Xintong Li",
                "Jing Shi",
                "Hongjie Chen",
                "Viet Dac Lai",
                "Zhouhang Xie",
                "Sungchul Kim",
                "Ruiyi Zhang",
                "Tong Yu",
                "Mehrab Tanjim",
                "Nesreen K. Ahmed",
                "Puneet Mathur",
                "Seunghyun Yoon",
                "Lina Yao",
                "Branislav Kveton",
                "Thien Huu Nguyen",
                "Trung Bui",
                "Tianyi Zhou",
                "Ryan A. Rossi",
                "Franck Dernoncourt"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "Dolby Labs",
                "Intel AI Research",
                "Meta AI",
                "State University of New York at Buffalo",
                "University of California, San Diego",
                "University of Maryland",
                "University of New South Wales",
                "University of Oregon",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13501.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#survey"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ (GUI), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Automation: The Rise of GUI Agents with Large Models",
                    "desc": "This paper surveys the development of GUI agents that utilize Large Foundation Models to automate interactions with software applications. It categorizes various aspects of these agents, including their benchmarks, evaluation metrics, architectures, and training methods. The authors propose a unified framework that outlines the key capabilities of perception, reasoning, planning, and acting in GUI agents. Additionally, the paper highlights open challenges and future directions for research in this area, providing a foundational understanding for both practitioners and researchers."
                },
                "zh": {
                    "title": "GUIä»£ç†ï¼šäººæœºäº¤äº’çš„æœªæ¥",
                    "desc": "å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ˜¯ç”±å¤§å‹åŸºç¡€æ¨¡å‹é©±åŠ¨çš„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–äººæœºäº¤äº’ã€‚è¿™äº›ä»£ç†å¯ä»¥è‡ªä¸»ä¸æ•°å­—ç³»ç»Ÿæˆ–è½¯ä»¶åº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ï¼Œæ¨¡æ‹Ÿäººç±»çš„ç‚¹å‡»ã€è¾“å…¥å’Œå¯¼èˆªç­‰æ“ä½œã€‚æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è°ƒæŸ¥ï¼Œåˆ†ç±»äº†GUIä»£ç†çš„åŸºå‡†ã€è¯„ä¼°æŒ‡æ ‡ã€æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæè¿°äº†å®ƒä»¬çš„æ„ŸçŸ¥ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¯†åˆ«äº†é‡è¦çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†æœªæ¥çš„å…³é”®æ–¹å‘ï¼Œä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†å¯¹å½“å‰è¿›å±•ã€æŠ€æœ¯ã€åŸºå‡†å’Œå¾…è§£å†³çš„å…³é”®é—®é¢˜çš„ç›´è§‚ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13061",
            "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
            "url": "https://huggingface.co/papers/2412.13061",
            "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "488c580621c13ba2",
            "authors": [
                "Anni Tang",
                "Tianyu He",
                "Junliang Guo",
                "Xinle Cheng",
                "Li Song",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13061.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VidTok: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "VidTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ ÑĞºĞ°Ğ»ÑÑ€Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (FSQ) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. VidTok Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ PSNR, SSIM, LPIPS Ğ¸ FVD, Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "VidTok: Revolutionizing Video Tokenization for Enhanced Performance",
                    "desc": "This paper presents VidTok, a new video tokenizer designed to efficiently convert video content into compact latent tokens, which helps reduce redundancy in pixel data. VidTok stands out by utilizing advanced model architecture, including convolutional layers and up/downsampling modules, to enhance performance. It also addresses common issues in traditional Vector Quantization by implementing Finite Scalar Quantization, which stabilizes training and prevents codebook collapse. Overall, VidTok shows significant improvements in video tokenization performance, as evidenced by better scores in metrics like PSNR, SSIM, LPIPS, and FVD compared to existing methods."
                },
                "zh": {
                    "title": "VidTokï¼šè§†é¢‘æ ‡è®°åŒ–çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVidTokçš„è§†é¢‘ç¼–ç å™¨ï¼Œå®ƒèƒ½å¤Ÿå°†è§†é¢‘å†…å®¹å‹ç¼©ä¸ºç´§å‡‘çš„æ½œåœ¨æ ‡è®°ã€‚VidTokåœ¨è¿ç»­å’Œç¦»æ•£æ ‡è®°åŒ–æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè§£å†³äº†ä¼ ç»Ÿå‘é‡é‡åŒ–æ–¹æ³•ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§å’Œä»£ç æœ¬å´©æºƒé—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨å·ç§¯å±‚ã€ä¸Šä¸‹é‡‡æ ·æ¨¡å—ä»¥åŠæœ‰é™æ ‡é‡é‡åŒ–ï¼ˆFSQï¼‰ï¼ŒVidTokæ˜¾è‘—æé«˜äº†è§†é¢‘æ ‡è®°åŒ–çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œå¦‚PSNRã€SSIMã€LPIPSå’ŒFVDï¼Œå‡è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14015",
            "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation",
            "url": "https://huggingface.co/papers/2412.14015",
            "abstract": "Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.",
            "score": 1,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "1d55ea1f2eb90ac0",
            "authors": [
                "Haotong Lin",
                "Sida Peng",
                "Jingxiao Chen",
                "Songyou Peng",
                "Jiaming Sun",
                "Minghuan Liu",
                "Hujun Bao",
                "Jiashi Feng",
                "Xiaowei Zhou",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "ETH Zurich",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14015.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ LiDAR Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Prompt Depth Anything. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¹ LiDAR Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Depth Anything, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 4K. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ LiDAR Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LiDAR Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-GT Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Depth Estimation with LiDAR Prompts",
                    "desc": "This paper introduces a novel method called Prompt Depth Anything, which integrates prompting techniques into depth estimation models. By using low-cost LiDAR data as a guiding prompt, the model can produce accurate metric depth outputs at high resolutions, up to 4K. The authors present a unique prompt fusion design that effectively incorporates LiDAR information at various scales within the depth decoder. To overcome the challenges of limited training data, they propose a scalable pipeline that combines synthetic LiDAR simulations with real data to generate pseudo ground truth depth, achieving state-of-the-art results on benchmark datasets."
                },
                "zh": {
                    "title": "åˆ©ç”¨æç¤ºæŠ€æœ¯æå‡æ·±åº¦ä¼°è®¡ç²¾åº¦",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œç§°ä¸ºPrompt Depth Anythingï¼Œé¦–æ¬¡å°†æç¤ºæŠ€æœ¯åº”ç”¨äºæ·±åº¦åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä½æˆæœ¬çš„LiDARä½œä¸ºæç¤ºï¼ŒæŒ‡å¯¼Depth Anythingæ¨¡å‹è¾“å‡ºå‡†ç¡®çš„åº¦é‡æ·±åº¦ï¼Œåˆ†è¾¨ç‡é«˜è¾¾4Kã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ·±åº¦è§£ç å™¨ä¸­å¤šå°ºåº¦èåˆLiDARæç¤ºï¼Œè§£å†³äº†æœ‰é™æ•°æ®é›†å¸¦æ¥çš„è®­ç»ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨ARKitSceneså’ŒScanNet++æ•°æ®é›†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶å¯¹3Dé‡å»ºå’Œé€šç”¨æœºå™¨äººæŠ“å–ç­‰ä¸‹æ¸¸åº”ç”¨äº§ç”Ÿäº†ç§¯æå½±å“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-18.html",
    "link_next": "2024-12-20.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.12",
        "en": "12/20",
        "zh": "12æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºå‡†æµ‹è¯•æ€§èƒ½ä¸å®é™…åº”ç”¨ä¹‹é—´ä»å­˜åœ¨å·®è·ã€‚ä¸»è¦åŸå› æ˜¯å½“å‰çš„è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡æ— æ³•å……åˆ†æ•æ‰LLMsçš„å…¨éƒ¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†G-Pass@kï¼Œä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¬¡é‡‡æ ·ä¸­è¿ç»­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé‡åŒ–æ¨¡å‹çš„æœ€é«˜æ€§èƒ½æ½œåŠ›å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LiveMathBenchï¼Œä¸€ä¸ªåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§ã€å½“ä»£æ•°å­¦é—®é¢˜çš„åŠ¨æ€åŸºå‡†ï¼Œæ—¨åœ¨å‡å°‘è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ•°æ®æ³„éœ²é£é™©ã€‚è¯¦ç»†ç»“æœå’ŒåŸºå‡†å¯è®¿é—®ï¼šhttps://github.com/open-compass/GPassKã€‚",
        "title": "Are Your LLMs Capable of Stable Reasoning?",
        "pinyin": "DÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i fÃ¹zÃ¡ xÄ«nglÇ rÃ¨nwÃ¹ zhÅng qÇ”dÃ©le xiÇnzhÃ¹ jÃ¬nbÃ¹. RÃ¡n'Ã©r, jÄ«zhÇ”n cÃ¨shÇ xÃ¬ngnÃ©ng yÇ” shÃ­jÃ¬ yÃ¬ngyÃ²ng zhÄ«jiÄn rÃ©ng cÃºnzÃ i chÄjÃ¹. ZhÇ”yÃ o yuÃ¡nyÄ«n shÃ¬ dÄngqiÃ¡n de pÃ­nggÇ” fÄngfÇ hÃ© zhÇbiÄo wÃºfÇ chÅngfÄ“n bÇngqÇ” LLMs de quÃ¡nbÃ¹ nÃ©nglÃ¬, tÃ¨biÃ© shÃ¬ zÃ i xÅ«yÃ o zhÇ”nquÃ¨xÃ¬ng hÃ© yÄ«zhÃ¬xÃ¬ng de fÃ¹zÃ¡ xÄ«nglÇ rÃ¨nwÃ¹ zhÅng. BÄ›nwÃ©n tÃ­chÅ«le G-Pass@k, yÄ«gÃ¨ xÄ«n de pÃ­nggÇ” zhÇbiÄo, nÃ©nggÃ²u zÃ i duÅcÃ¬ cÇiyÃ ng zhÅng liÃ¡nxÃ¹ pÃ­nggÇ” mÃ³xÃ­ng xÃ¬ngnÃ©ng, liÃ ngzhÃ¬ mÃ³xÃ­ng de zuÃ¬gÄo xÃ¬ngnÃ©ng qiÃ¡nlÃ¬ hÃ© wÄ›ndÃ­ngxÃ¬ng. TÃ³ngshÃ­, wÇ’men tuÄ«chÅ«le LiveMathBench, yÄ«gÃ¨ bÄohÃ¡n jÃ¹yÇ’u tiÇozhÃ nxÃ¬ng, dÄngdÃ i shÃ¹xuÃ© wÃ¨ntÃ­ de dÃ²ngtÃ i jÄ«zhÇ”n, zhÇyÃº jiÇnshÇo pÃ­nggÇ” guÃ²chÃ©ng zhÅng de shÃ¹jÃ¹ lÃ²ushÃ¬ fÄ“ngxiÇn. XiÃ¡ngxÃ¬ jiÃ©guÇ’ hÃ© jÄ«zhÇ”n kÄ› fÄngwÃ¨n: https://github.com/open-compass/GPassK.",
        "vocab": "[{'word': 'å¤§å‹', 'pinyin': 'dÃ  xÃ­ng', 'trans': 'large-scale'},\n{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'language model'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'},\n{'word': 'å–å¾—', 'pinyin': 'qÇ” dÃ©', 'trans': 'achieve'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'å®é™…', 'pinyin': 'shÃ­ jÃ¬', 'trans': 'actual'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'application'},\n{'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'},\n{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æŒ‡æ ‡', 'pinyin': 'zhÇ biÄo', 'trans': 'metric'},\n{'word': 'æ•æ‰', 'pinyin': 'bÇ” zhuÅ', 'trans': 'capture'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'capability'},\n{'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'},\n{'word': 'è¿ç»­', 'pinyin': 'liÃ¡n xÃ¹', 'trans': 'continuous'},\n{'word': 'é‡åŒ–', 'pinyin': 'liÃ ng huÃ ', 'trans': 'quantify'},\n{'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'},\n{'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›n dÃ¬ng xÃ¬ng', 'trans': 'stability'},\n{'word': 'æ¨å‡º', 'pinyin': 'tuÄ« chÅ«', 'trans': 'launch'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'æŒ‘æˆ˜æ€§', 'pinyin': 'tiÇo zhÃ n xÃ¬ng', 'trans': 'challenging'},\n{'word': 'å½“ä»£', 'pinyin': 'dÄng dÃ i', 'trans': 'contemporary'},\n{'word': 'æ•°å­¦', 'pinyin': 'shÃ¹ xuÃ©', 'trans': 'mathematics'},\n{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'},\n{'word': 'æ³„éœ²', 'pinyin': 'xiÃ¨ lÃ²u', 'trans': 'leak'},\n{'word': 'é£é™©', 'pinyin': 'fÄ“ng xiÇn', 'trans': 'risk'},\n{'word': 'è¯¦ç»†', 'pinyin': 'xiÃ¡ng xÃ¬', 'trans': 'detailed'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'è®¿é—®', 'pinyin': 'fÇng wÃ¨n', 'trans': 'access'}]",
        "trans": "Large language models (LLMs) have made significant strides in complex reasoning tasks. However, there remains a gap between benchmark test performance and real-world applications. The primary reason is that current evaluation methods and metrics fail to fully capture the capabilities of LLMs, especially in complex reasoning tasks that require accuracy and consistency. This paper introduces G-Pass@k, a new evaluation metric that can continuously assess model performance across multiple samples, quantifying the model's peak performance potential and stability. Additionally, we present LiveMathBench, a dynamic benchmark containing challenging, contemporary mathematical problems aimed at reducing the risk of data leakage during the evaluation process. Detailed results and benchmarks are available at: https://github.com/open-compass/GPassK.",
        "update_ts": "2024-12-18 09:11"
    }
}