
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. September 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 сентября</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-11.html">⬅️ <span id="prev-date">11.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-15.html">➡️ <span id="next-date">15.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'};
        let feedDateNext = {'ru': '15.09', 'en': '09/15', 'zh': '9月15日'};
        let feedDatePrev = {'ru': '11.09', 'en': '09/11', 'zh': '9月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.09372', 'title': 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model', 'url': 'https://huggingface.co/papers/2509.09372', 'abstract': 'VLA-Adapter reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving state-of-the-art performance and fast inference speed with minimal computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.', 'score': 113, 'issue_id': 5858, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '518f1161098c39e1', 'authors': ['Yihao Wang', 'Pengxiang Ding', 'Lingxiao Li', 'Can Cui', 'Zirui Ge', 'Xinyang Tong', 'Wenxuan Song', 'Han Zhao', 'Wei Zhao', 'Pengxu Hou', 'Siteng Huang', 'Yifan Tang', 'Wenhui Wang', 'Ru Zhang', 'Jianyi Liu', 'Donglin Wang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'OpenHelix Team', 'State Key Laboratory of Networking and Switching Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09372.jpg', 'data': {'categories': ['#architecture', '#agents', '#rl', '#robotics', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Эффективное связывание восприятия и действия без громоздких моделей', 'desc': 'VLA-Adapter - это новая парадигма для моделей зрения-языка-действия (VLA), которая уменьшает зависимость от крупномасштабных моделей зрения-языка (VLM) и длительного предобучения. Метод использует легковесный модуль Policy с Bridge Attention для эффективного связывания пространств восприятия и действия. VLA-Adapter достигает современного уровня производительности на симулированных и реальных робототехнических тестах, используя только 0.5B-параметровую базовую модель без предобучения на робототехнических данных. Благодаря улучшенной парадигме связывания, VLA-Adapter позволяет обучить мощную VLA модель всего за 8 часов на одном потребительском GPU.'}, 'en': {'title': 'Efficient Vision-Language-Action with VLA-Adapter', 'desc': 'The paper introduces VLA-Adapter, a new approach that minimizes the need for large-scale Vision-Language Models (VLMs) and extensive pre-training in Vision-Language-Action (VLA) tasks. It employs a lightweight Policy module with Bridge Attention to effectively connect vision-language representations to action spaces. This method achieves high performance with a compact 0.5B-parameter backbone and eliminates the need for robotic data pre-training. The results show that VLA-Adapter not only reaches state-of-the-art performance but also allows for rapid training and inference on standard hardware.'}, 'zh': {'title': 'VLA-Adapter：高效的视觉语言行动模型', 'desc': 'VLA-Adapter是一种新颖的模型，旨在减少对大型视觉语言模型（VLM）和广泛预训练的依赖。它通过引入轻量级的策略模块和桥接注意力机制，能够在仅使用0.5亿参数的情况下实现高性能。该方法在模拟和真实世界的机器人基准测试中表现出色，且推理速度非常快。VLA-Adapter的设计使得在普通消费级GPU上仅需8小时即可训练出强大的视觉语言行动模型，显著降低了部署的门槛。'}}}, {'id': 'https://huggingface.co/papers/2509.08519', 'title': 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning', 'url': 'https://huggingface.co/papers/2509.08519', 'abstract': 'HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  \t\t\t\t\tAI-generated summary \t\t\t\t Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.', 'score': 86, 'issue_id': 5855, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '2db390fc41f9f85b', 'authors': ['Liyang Chen', 'Tianxiang Ma', 'Jiawei Liu', 'Bingchuan Li', 'Zhuowei Chen', 'Lijie Liu', 'Xu He', 'Gen Li', 'Qian He', 'Zhiyong Wu'], 'affiliations': ['Intelligent Creation Lab, ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.08519.jpg', 'data': {'categories': ['#dataset', '#video', '#multimodal', '#training'], 'emoji': '🎭', 'ru': {'title': 'HuMo: Революция в генерации видео с людьми через мультимодальное управление', 'desc': 'HuMo - это унифицированная система для генерации видео с людьми, учитывающая несколько модальностей входных данных. Она использует двухэтапную парадигму обучения и новые стратегии для сохранения характеристик субъекта и синхронизации аудио с видео. Система решает проблемы нехватки обучающих данных с парными условиями и сложности координации подзадач с мультимодальными входами. HuMo превосходит специализированные современные методы в подзадачах, создавая единую систему для совместной мультимодальной генерации видео с людьми.'}, 'en': {'title': 'HuMo: Unifying Multimodal Control for Human-Centric Video Generation', 'desc': 'HuMo is a novel framework designed for generating human-centric videos by effectively integrating multiple input modalities such as text, images, and audio. It tackles the challenges of limited training data and the need for precise coordination between subject preservation and audio-visual synchronization. The framework employs a two-stage training approach, utilizing a high-quality dataset and innovative strategies like minimal-invasive image injection and focus-by-predicting for improved task performance. HuMo demonstrates superior capabilities in multimodal control, outperforming existing methods in generating coherent and synchronized human videos.'}, 'zh': {'title': 'HuMo：人本视频生成的统一框架', 'desc': 'HuMo是一个统一的人本视频生成框架，旨在通过两阶段训练范式和新颖的策略解决多模态控制中的挑战。该框架能够从文本、图像和音频等多种输入中合成视频，克服了训练数据稀缺和多模态输入下的任务协作难题。为了解决这些问题，HuMo构建了一个高质量的数据集，并提出了渐进式的多模态训练方法。实验结果表明，HuMo在子任务上超越了现有的最先进方法，建立了一个协作的多模态条件视频生成框架。'}}}, {'id': 'https://huggingface.co/papers/2509.09674', 'title': 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.09674', 'abstract': "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL", 'score': 56, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '36851aee36c7e5a0', 'authors': ['Haozhan Li', 'Yuxin Zuo', 'Jiale Yu', 'Yuhao Zhang', 'Zhaohui Yang', 'Kaiyan Zhang', 'Xuekai Zhu', 'Yuchen Zhang', 'Tianxing Chen', 'Ganqu Cui', 'Dehui Wang', 'Dingxiang Luo', 'Yuchen Fan', 'Youbang Sun', 'Jia Zeng', 'Jiangmiao Pang', 'Shanghang Zhang', 'Yu Wang', 'Yao Mu', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09674.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#robotics', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты для роботов', 'desc': 'SimpleVLA-RL - это фреймворк для обучения с подкреплением, разработанный для моделей типа Vision-Language-Action (VLA). Он улучшает планирование действий на длительных горизонтах и достигает наилучших результатов в ряде задач робототехники. SimpleVLA-RL снижает зависимость от больших объемов данных и обеспечивает надежную генерализацию. Во время обучения с подкреплением модель обнаруживает новые паттерны поведения, не встречавшиеся в предыдущем процессе обучения.'}, 'en': {'title': 'Revolutionizing Robotic Action Planning with SimpleVLA-RL', 'desc': 'SimpleVLA-RL is a reinforcement learning framework designed to improve Vision-Language-Action (VLA) models for robotic manipulation. It addresses challenges like the need for extensive human-operated robotic trajectories and the difficulty in generalizing to new tasks. By implementing techniques such as VLA-specific trajectory sampling and multi-environment rendering, SimpleVLA-RL enhances long-horizon action planning and achieves state-of-the-art performance. Additionally, it uncovers new patterns during training, demonstrating its ability to go beyond traditional supervised fine-tuning methods.'}, 'zh': {'title': 'SimpleVLA-RL：提升视觉-语言-动作模型的长时间规划能力', 'desc': 'SimpleVLA-RL是一个针对视觉-语言-动作（VLA）模型的强化学习框架，旨在增强长时间跨度的动作规划能力。该框架通过引入特定的轨迹采样、可扩展的并行处理和多环境渲染等技术，显著提高了模型的性能。SimpleVLA-RL在LIBERO数据集上达到了最先进的表现，并在RoboTwin 1.0和2.0上超越了现有的基准。更重要的是，该框架在训练过程中发现了一种新现象“pushcut”，揭示了模型在学习过程中能够识别出新的模式。'}}}, {'id': 'https://huggingface.co/papers/2509.09174', 'title': 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs', 'url': 'https://huggingface.co/papers/2509.09174', 'abstract': 'EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.', 'score': 52, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'b6e2cc4088bce9ac', 'authors': ['Yuhao Zhang', 'Yuhao Du', 'Zhanchen Dai', 'Xiangnan Ma', 'Kaiqi Kou', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2509.09174.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#audio', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Преодоление разрыва между звуком и смыслом в речевых ИИ-моделях', 'desc': 'EchoX - это речевая большая языковая модель, которая решает проблему акустико-семантического разрыва. Она интегрирует семантические представления и сохраняет способности к рассуждению. EchoX использует динамическую генерацию речевых целей обучения. Модель достигает продвинутых результатов на тестах, основанных на знаниях, используя всего около 6000 часов обучающих данных.'}, 'en': {'title': 'Bridging the Acoustic-Semantic Gap with EchoX', 'desc': 'EchoX is a speech-to-speech large language model (SLLM) designed to overcome the challenges of the acoustic-semantic gap in speech processing. By integrating semantic representations into its training, EchoX maintains strong reasoning capabilities that are often lost in traditional SLLMs. This model dynamically generates speech training targets, allowing it to effectively learn from both acoustic and semantic features. As a result, EchoX demonstrates superior performance on various knowledge-based benchmarks, showcasing its advanced capabilities in understanding and generating speech.'}, 'zh': {'title': 'EchoX：打破声学与语义的壁垒', 'desc': 'EchoX是一种语音到语音的大型语言模型，旨在解决声学与语义之间的差距。它通过整合语义表示，保持推理能力，从而在知识基础的基准测试中取得了优异的表现。当前的语音到语音模型在知识和推理能力上常常表现不佳，EchoX通过动态生成语音训练目标来克服这一限制。实验结果表明，EchoX在约六千小时的训练数据下，在多个知识问答基准上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.09595', 'title': 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis', 'url': 'https://huggingface.co/papers/2509.09595', 'abstract': 'Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.', 'score': 33, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '9128c939612e1d3e', 'authors': ['Yikang Ding', 'Jiwen Liu', 'Wenyuan Zhang', 'Zekun Wang', 'Wentao Hu', 'Liyuan Cui', 'Mingming Lao', 'Yingchao Shao', 'Hui Liu', 'Xiaohan Li', 'Ming Chen', 'Xiaoqiang Liu', 'Yu-Shen Liu', 'Pengfei Wan'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.09595.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#story_generation', '#video', '#games'], 'emoji': '🎭', 'ru': {'title': 'Семантически обоснованные аватары с высокой детализацией', 'desc': 'Kling-Avatar - это новая каскадная архитектура для генерации видео с аватарами на основе аудио. Она объединяет понимание мультимодальных инструкций с фотореалистичной генерацией портретов. На первом этапе мультимодальная языковая модель создает видео-макет, управляющий семантикой высокого уровня. На втором этапе генерируются отдельные фрагменты видео с сохранением мелких деталей и общего замысла.'}, 'en': {'title': 'Kling-Avatar: Bridging Audio and Visual Realism in Avatar Generation', 'desc': 'Kling-Avatar is a new framework designed to improve the generation of avatar videos driven by audio instructions. It combines understanding of multimodal instructions with the creation of realistic portraits, resulting in videos that are both visually appealing and semantically meaningful. The framework operates in two stages: first, it uses a large language model to create a blueprint video that captures high-level semantics like character emotions and movements. Then, it generates detailed sub-clips based on this blueprint, allowing for fast and stable production of long videos while maintaining high fidelity and expressiveness.'}, 'zh': {'title': 'Kling-Avatar：音频驱动虚拟形象生成的新标杆', 'desc': 'Kling-Avatar是一个级联框架，旨在提升音频驱动的虚拟形象视频生成。它通过整合多模态指令理解与逼真的肖像生成，生成高保真且语义明确的视频。该方法采用两阶段流程，首先利用多模态大语言模型生成蓝图视频，然后根据蓝图关键帧并行生成多个子片段。实验表明，Kling-Avatar在视频生成的清晰度、情感表达和指令控制等方面表现优异，适用于数字人直播和视频博客等实际应用。'}}}, {'id': 'https://huggingface.co/papers/2509.09265', 'title': 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents', 'url': 'https://huggingface.co/papers/2509.09265', 'abstract': 'Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/', 'score': 32, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '7850d32271ef8349', 'authors': ['Jiawei Wang', 'Jiacai Liu', 'Yuqian Fu', 'Yingru Li', 'Xintao Wang', 'Yuan Lin', 'Yu Yue', 'Lin Zhang', 'Yang Wang', 'Ke Wang'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2509.09265.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Улучшение обучения языковых моделей через энтропийную модуляцию градиентов', 'desc': 'Метод EMPG (Entropy-Modulated Policy Gradients) решает проблемы динамики обучения в больших языковых моделях (LLM) при выполнении долгосрочных задач. Он перекалибрует градиенты политики на основе неопределенности и результатов задачи, что приводит к улучшению производительности. EMPG усиливает обновления для уверенных правильных действий, штрафует уверенные ошибки и ослабляет обновления от неопределенных шагов для стабилизации исследования. Эксперименты на трех сложных задачах показали, что EMPG значительно превосходит базовые методы градиента политики.'}, 'en': {'title': 'Boosting Learning with Entropy Awareness', 'desc': 'Entropy-Modulated Policy Gradients (EMPG) is a new approach that improves learning in Large Language Models (LLMs) by adjusting policy gradients based on uncertainty and task results. In long-horizon tasks, LLMs struggle with sparse rewards, making it hard to credit intermediate actions. EMPG addresses this by recalibrating the learning signal, enhancing updates for confident actions while reducing the impact of uncertain ones. This method leads to better performance in complex tasks, as shown in experiments with various challenging agent environments.'}, 'zh': {'title': '熵调制策略梯度：提升长时间任务学习效率的关键', 'desc': '本文提出了一种名为熵调制策略梯度（EMPG）的方法，旨在解决大型语言模型（LLMs）在长时间任务中的学习动态问题。通过根据不确定性和任务结果重新校准策略梯度，EMPG能够提高在稀疏奖励环境中的学习效率。该方法放大了对正确自信动作的更新，惩罚自信错误，并减弱来自不确定步骤的更新，从而稳定探索过程。实验结果表明，EMPG在多个复杂任务中显著提升了性能，超越了传统的策略梯度基线。'}}}, {'id': 'https://huggingface.co/papers/2509.09680', 'title': 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark', 'url': 'https://huggingface.co/papers/2509.09680', 'abstract': 'FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .', 'score': 27, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '60acc7b8f0e01329', 'authors': ['Rongyao Fang', 'Aldrich Yu', 'Chengqi Duan', 'Linjiang Huang', 'Shuai Bai', 'Yuxuan Cai', 'Kun Wang', 'Si Liu', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['Alibaba', 'BUAA', 'CUHK', 'HKU'], 'pdf_title_img': 'assets/pdf/title_img/2509.09680.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#open_source', '#long_context', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Революция в обучении и оценке моделей текст-изображение', 'desc': 'FLUX-Reason-6M и PRISM-Bench решают проблему отсутствия наборов данных и эталонов для оценки моделей преобразования текста в изображение, ориентированных на рассуждения. FLUX-Reason-6M представляет собой массивный датасет из 6 миллионов высококачественных изображений с 20 миллионами двуязычных описаний, специально разработанных для обучения сложным рассуждениям. PRISM-Bench предлагает новый стандарт оценки с семью различными направлениями, включая сложную задачу Long Text с использованием Generation Chain-of-Thought (GCoT). Обширная оценка 19 ведущих моделей на PRISM-Bench выявляет критические пробелы в производительности и подчеркивает конкретные области, требующие улучшения.'}, 'en': {'title': 'Empowering Text-to-Image Models with Reasoning Datasets and Benchmarks', 'desc': 'FLUX-Reason-6M and PRISM-Bench are initiatives aimed at enhancing text-to-image (T2I) models by providing a large-scale dataset and a robust evaluation framework. FLUX-Reason-6M includes 6 million images and 20 million bilingual descriptions that focus on teaching complex reasoning through structured characteristics. The dataset is meticulously curated using extensive computational resources, making it a valuable asset for researchers. PRISM-Bench introduces a new evaluation standard with multiple tracks to assess model performance, revealing significant gaps and guiding future improvements in T2I generation.'}, 'zh': {'title': '推动推理导向的文本到图像生成', 'desc': 'FLUX-Reason-6M和PRISM-Bench旨在解决文本到图像模型缺乏以推理为重点的数据集和基准的问题。FLUX-Reason-6M是一个包含600万张高质量图像和2000万条双语描述的大型数据集，专门设计用于教授复杂的推理能力。PRISM-Bench提供了一个新的评估标准，包含七个不同的评估轨道，特别是一个使用生成链思维（GCoT）的长文本挑战。通过这些资源，我们希望推动推理导向的文本到图像生成的下一波发展。'}}}, {'id': 'https://huggingface.co/papers/2509.09666', 'title': 'Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?', 'url': 'https://huggingface.co/papers/2509.09666', 'abstract': 'A novel framework UAE uses reinforcement learning to unify image-to-text and text-to-image processes, enhancing mutual understanding and generation fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder\'s reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising "aha moment" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.', 'score': 24, 'issue_id': 5856, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'd213e626e6faeaa5', 'authors': ['Zhiyuan Yan', 'Kaiqing Lin', 'Zongjian Li', 'Junyan Ye', 'Hui Han', 'Zhendong Wang', 'Hao Liu', 'Bin Lin', 'Hao Li', 'Xue Xu', 'Xinyan Xiao', 'Jingdong Wang', 'Haifeng Wang', 'Li Yuan'], 'affiliations': ['Baidu ERNIE', 'PKU', 'Rabbitpre AI', 'SYSU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2509.09666.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#games', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Объединение понимания и генерации изображений через обучение с подкреплением', 'desc': 'Статья представляет новый фреймворк UAE, использующий обучение с подкреплением для объединения процессов преобразования изображения в текст и текста в изображение. Авторы рассматривают понимание как энкодер (I2T), сжимающий изображения в текст, а генерацию как декодер (T2I), восстанавливающий изображения из текста. Используя качество реконструкции как единую цель обучения, они обеспечивают согласованный двунаправленный поток информации между процессами понимания и генерации. Предложенный подход включает предобучение декодера на масштабных наборах данных и трехэтапный алгоритм Unified-GRPO для улучшения взаимного понимания и качества генерации.'}, 'en': {'title': 'Unifying Image and Text with Reinforcement Learning', 'desc': 'This paper presents a new framework called UAE that uses reinforcement learning to connect image-to-text (I2T) and text-to-image (T2I) processes. It employs an Auto-Encoder approach where the encoder compresses images into text and the decoder reconstructs images from that text. The framework focuses on improving the mutual understanding between these two processes by using reconstruction fidelity as a training goal. The authors also introduce a benchmark called Unified-Bench to evaluate the effectiveness of this unified multimodal learning approach.'}, 'zh': {'title': '统一多模态学习的新框架UAE', 'desc': '本文提出了一种新颖的框架UAE，利用强化学习统一图像到文本和文本到图像的过程，增强了相互理解和生成的准确性。我们通过自编码器的视角，将理解过程视为编码器（I2T），将生成过程视为解码器（T2I），并以重建精度作为统一训练目标。UAE框架通过三个阶段的强化学习实现：冷启动阶段、生成理解阶段和理解生成阶段，确保信息在理解和生成过程中的双向流动。最终，我们引入了Unified-Bench基准，评估统一多模态学习的程度，发现随着强化学习的进展，编码器能够生成更具描述性的文本，而解码器则能更好地理解这些复杂描述。'}}}, {'id': 'https://huggingface.co/papers/2509.06806', 'title': 'MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML', 'url': 'https://huggingface.co/papers/2509.06806', 'abstract': 'MachineLearningLM enhances a general-purpose LLM with robust in-context machine learning capabilities through continued pretraining with synthesized ML tasks, achieving high performance across various domains without task-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.', 'score': 22, 'issue_id': 5873, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '949af9972afa4814', 'authors': ['Haoyu Dong', 'Pengkun Zhang', 'Mingzhe Lu', 'Yanzhen Shen', 'Guolin Ke'], 'affiliations': ['SCUT', 'Stanford', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2509.06806.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#training', '#multimodal', '#transfer_learning', '#agi', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Универсальная языковая модель с встроенными возможностями машинного обучения', 'desc': 'MachineLearningLM - это инновационный подход к улучшению общих языковых моделей (LLM) путем предварительного обучения на синтезированных задачах машинного обучения. Модель демонстрирует высокую производительность в различных предметных областях без специфического обучения под конкретные задачи. Ключевая особенность - способность эффективно обучаться на большом количестве примеров в контексте, что позволяет достичь точности на уровне случайного леса при сохранении общих возможностей чат-бота. MachineLearningLM превосходит базовые LLM на 15% в задачах табличной классификации вне распределения обучающей выборки.'}, 'en': {'title': 'Empowering LLMs with In-Context Machine Learning Skills', 'desc': "MachineLearningLM is a framework that enhances a general-purpose large language model (LLM) by enabling it to perform machine learning tasks effectively through continued pretraining. It synthesizes a variety of machine learning tasks from structural causal models, allowing the model to learn from many examples without needing specific training for each task. The approach improves the model's performance in various domains, achieving significant accuracy in tabular classification tasks while maintaining its general knowledge and reasoning abilities. This method demonstrates that the model's accuracy improves as the number of in-context examples increases, showcasing its robust in-context learning capabilities."}, 'zh': {'title': '增强大语言模型的上下文学习能力', 'desc': 'MachineLearningLM 是一种增强通用大语言模型（LLM）的方法，通过继续预训练合成的机器学习任务，赋予其强大的上下文学习能力。该模型在多个领域中表现出色，无需特定任务的训练。预训练过程中，利用数百万个结构因果模型（SCM）合成机器学习任务，支持多达1024个示例的上下文学习。尽管设置较为简单，MachineLearningLM 在金融、物理、生物和医疗等领域的表格分类任务中，平均超越了强大的基线模型，展现出显著的多示例扩展规律。'}}}, {'id': 'https://huggingface.co/papers/2509.08031', 'title': 'AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs', 'url': 'https://huggingface.co/papers/2509.08031', 'abstract': 'AU-Harness is an efficient and comprehensive evaluation framework for Large Audio Language Models (LALMs) that addresses issues of speed, reproducibility, and task coverage, revealing gaps in temporal understanding and spoken language reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce AU-Harness, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. AU-Harness provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.', 'score': 18, 'issue_id': 5868, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': 'bb86095d23b97040', 'authors': ['Sidharth Surapaneni', 'Hoang Nguyen', 'Jash Mehta', 'Aman Tiwari', 'Oluwanifemi Bamgbose', 'Akshay Kalkunte', 'Sai Rajeswar', 'Sathwik Tejaswi Madhusudhan'], 'affiliations': ['ServiceNow', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2509.08031.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#optimization', '#audio', '#reasoning'], 'emoji': '🎧', 'ru': {'title': 'AU-Harness: революция в оценке аудиоязыковых моделей', 'desc': 'AU-Harness - это эффективный фреймворк для оценки больших аудиоязыковых моделей (LALM). Он решает проблемы скорости, воспроизводимости и охвата задач при тестировании LALM. Фреймворк использует оптимизированную пакетную обработку и параллельное выполнение, что позволяет ускорить оценку до 127% по сравнению с существующими инструментами. AU-Harness также вводит новые категории оценки: LLM-адаптивную диаризацию для понимания временной структуры аудио и рассуждения на основе разговорной речи для сложных когнитивных задач.'}, 'en': {'title': 'AU-Harness: Revolutionizing Evaluation for Large Audio Language Models', 'desc': "AU-Harness is a new evaluation framework designed specifically for Large Audio Language Models (LALMs). It addresses key challenges such as slow processing speeds, inconsistent evaluation methods, and limited task coverage, which hinder effective model assessment. By optimizing batch processing and enabling parallel execution, AU-Harness improves evaluation speed by up to 127%, allowing for more extensive and fair comparisons of models. Additionally, it introduces new evaluation categories to assess temporal understanding and spoken language reasoning, revealing significant gaps in current LALMs' capabilities."}, 'zh': {'title': 'AU-Harness：提升音频语言模型评估效率的利器', 'desc': 'AU-Harness是一个高效且全面的评估框架，专门用于大型音频语言模型（LALMs）。它解决了评估速度慢、可重复性差和任务覆盖面窄的问题，揭示了当前模型在时间理解和口语推理方面的不足。通过优化批处理和并行执行，AU-Harness的处理速度比现有工具提高了127%。此外，它还提供了标准化的提示协议和灵活的配置，支持在多种场景下进行公平的模型比较。'}}}, {'id': 'https://huggingface.co/papers/2509.09676', 'title': 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations', 'url': 'https://huggingface.co/papers/2509.09676', 'abstract': "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  \t\t\t\t\tAI-generated summary \t\t\t\t Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.", 'score': 15, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'b2d981674edaecf5', 'authors': ['Jiahao Wang', 'Yufeng Yuan', 'Rujie Zheng', 'Youtian Lin', 'Jian Gao', 'Lin-Zhuo Chen', 'Yajie Bao', 'Yi Zhang', 'Chang Zeng', 'Yanxi Zhou', 'Xiaoxiao Long', 'Hao Zhu', 'Zhaoxiang Zhang', 'Xun Cao', 'Yao Yao'], 'affiliations': ['Institute of Automation, Chinese Academy of Science', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09676.jpg', 'data': {'categories': ['#3d', '#dataset', '#video'], 'emoji': '🎥', 'ru': {'title': 'SpatialVID: Большие данные для прорыва в пространственном интеллекте', 'desc': 'SpatialVID - это масштабный набор данных, содержащий разнообразные видео с плотными 3D-аннотациями. Он включает более 21 000 часов необработанного видео, обработанного в 2,7 миллиона клипов общей продолжительностью 7 089 часов динамического контента. Датасет обогащен детальной пространственной и семантической информацией, включая позы камеры, карты глубины, динамические маски, структурированные подписи и сериализованные инструкции по движению. SpatialVID способствует улучшению обобщения и производительности моделей в исследованиях компьютерного зрения и 3D-реконструкции.'}, 'en': {'title': 'Unlocking 3D Vision with SpatialVID: A New Era of Video Datasets', 'desc': 'SpatialVID is a comprehensive dataset designed to improve machine learning models in video and 3D vision tasks. It contains over 21,000 hours of diverse, real-world video footage, which has been meticulously processed into 2.7 million clips. Each clip is enriched with dense 3D annotations, including camera poses, depth maps, and motion instructions, providing a rich source of training data. This dataset addresses the limitations of existing datasets by offering high-quality, large-scale data that enhances model generalization and performance in spatial intelligence applications.'}, 'zh': {'title': 'SpatialVID：提升视频与3D视觉研究的关键数据集', 'desc': 'SpatialVID是一个大规模的数据集，包含多样化的视频和密集的3D注释，旨在提升视频和3D视觉研究中的模型泛化能力和性能。该数据集收集了超过21,000小时的原始视频，并通过分层过滤管道处理成270万段视频片段，提供了丰富的动态内容。每个片段都附有详细的空间和语义信息，包括相机位姿、深度图、动态掩码、结构化标题和序列化运动指令。SpatialVID的数据统计分析显示出其丰富性和多样性，直接促进了模型的泛化和性能提升，成为视频和3D视觉研究领域的重要资产。'}}}, {'id': 'https://huggingface.co/papers/2509.09286', 'title': 'Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding', 'url': 'https://huggingface.co/papers/2509.09286', 'abstract': 'VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task.', 'score': 7, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '73de225642b07635', 'authors': ['Bohao Tang', 'Yan Ma', 'Fei Zhang', 'Jiadi Su', 'Ethan Chern', 'Zhulin Hu', 'Zhixin Wang', 'Pengfei Liu', 'Ya Zhang'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09286.jpg', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#multimodal', '#training', '#hallucinations'], 'emoji': '📊', 'ru': {'title': 'Адаптивное рассуждение VLM для улучшенного понимания графиков', 'desc': 'Статья представляет адаптивный фреймворк для улучшения понимания графиков визуально-языковыми моделями (VLM). Предложен подход Code-as-Thought (CaT), который представляет визуальную информацию графика в символьном формате. Введено понятие визуальной программируемости - обучаемого свойства, определяющего оптимальный метод решения для пары график-вопрос. Фреймворк обучается выбирать между CaT и прямым визуальным анализом с помощью обучения с подкреплением, используя двойную систему вознаграждений.'}, 'en': {'title': 'Adaptive Reasoning for Enhanced Chart Understanding in VLMs', 'desc': 'This paper introduces an adaptive framework for Vision-Language Models (VLMs) that enhances their ability to understand charts by selecting between code-based reasoning and direct visual analysis. The authors highlight the limitations of previous methods, which either rely on rigid external tools or single reasoning strategies that are hard to verify. They propose a novel approach called Code-as-Thought (CaT) that represents visual information in a symbolic format, allowing for better verification and accuracy. By implementing Visual Programmability, the model learns to dynamically choose the most effective reasoning pathway for each chart-question pair, leading to improved performance across various benchmarks.'}, 'zh': {'title': '动态选择最佳推理路径的视觉语言模型', 'desc': '本文提出了一种增强视觉语言模型（VLMs）的方法，通过自适应框架在代码基础推理和直接视觉推理之间进行选择，以提高图表理解的性能和鲁棒性。以往的方法存在局限性，依赖外部工具或单一推理策略，导致在复杂图表上表现不佳。我们引入了代码作为思维（CaT）的方法，将图表的视觉信息以可验证的符号格式表示，并提出视觉可编程性，允许模型根据图表和问题的特性选择最佳的推理方式。通过强化学习训练模型的选择策略，结合数据准确性奖励和决策奖励，确保模型在不同任务中动态选择最优推理路径。'}}}, {'id': 'https://huggingface.co/papers/2509.06888', 'title': 'mmBERT: A Modern Multilingual Encoder with Annealed Language Learning', 'url': 'https://huggingface.co/papers/2509.06888', 'abstract': "mmBERT, an encoder-only language model pretrained on multilingual text, achieves high performance on classification and retrieval tasks using an inverse mask ratio schedule and inverse temperature sampling ratio, particularly benefiting from the inclusion of low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.", 'score': 7, 'issue_id': 5866, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '22f0ea59278ee621', 'authors': ['Marc Marone', 'Orion Weller', 'William Fleshman', 'Eugene Yang', 'Dawn Lawrie', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University Center for Language and Speech Processing (CLSP)'], 'pdf_title_img': 'assets/pdf/title_img/2509.06888.jpg', 'data': {'categories': ['#training', '#multilingual', '#low_resource', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'mmBERT: Прорыв в многоязычном машинном обучении', 'desc': 'Представлена модель mmBERT - многоязычная языковая модель типа encoder-only, предобученная на 3T токенов текста на более чем 1800 языках. В модели использованы новые техники, такие как обратное расписание коэффициента маскирования и обратное соотношение температуры сэмплирования. Включение низкоресурсных языков только на этапе затухания обучения значительно повысило производительность модели. mmBERT превосходит предыдущее поколение моделей в задачах классификации и поиска как для распространенных, так и для редких языков.'}, 'en': {'title': 'Empowering Multilingual Understanding with mmBERT', 'desc': "mmBERT is a new encoder-only language model that has been pretrained on a vast dataset of multilingual text, covering over 1800 languages. It introduces innovative techniques such as an inverse mask ratio schedule and inverse temperature sampling ratio, which enhance its performance on classification and retrieval tasks. Notably, mmBERT incorporates low-resource languages during a specific training phase, leading to significant improvements in model accuracy. As a result, mmBERT demonstrates competitive performance compared to leading models like OpenAI's o3 and Google's Gemini 2.5 Pro, especially in handling both high and low-resource languages."}, 'zh': {'title': 'mmBERT：多语言模型的新突破', 'desc': 'mmBERT是一种仅使用编码器的语言模型，经过多语言文本的预训练，能够在分类和检索任务中表现出色。该模型采用了逆掩码比率调度和逆温度采样比率等新颖技术，特别是在低资源语言的引入上取得了显著效果。通过在衰减阶段仅加入1700多种低资源语言，mmBERT在训练数据较少的情况下显著提升了性能。最终，mmBERT在分类和检索任务上超越了以往的模型，表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.09118', 'title': 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval', 'url': 'https://huggingface.co/papers/2509.09118', 'abstract': 'GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.', 'score': 5, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '7693d45e6980cf3f', 'authors': ['Tianlu Zheng', 'Yifan Zhang', 'Xiang An', 'Ziyong Feng', 'Kaicheng Yang', 'Qichuan Ding'], 'affiliations': ['DeepGlint', 'Northeastern University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.09118.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#dataset', '#architecture'], 'emoji': '👤', 'ru': {'title': 'Улучшение CLIP для точного распознавания людей', 'desc': 'Статья представляет GA-DMS фреймворк для улучшения CLIP в задаче обучения представлений людей. Авторы разработали конвейер для создания высококачественного датасета WebPerson из 5 миллионов пар изображение-текст. GA-DMS использует адаптивное маскирование шумных текстовых токенов на основе оценки градиентно-аттенционного сходства. Дополнительно вводятся цели предсказания замаскированных токенов для улучшения обучения семантических представлений.'}, 'en': {'title': 'Enhancing CLIP for Superior Person Representation Learning', 'desc': "The GA-DMS framework enhances the CLIP model for person representation learning by addressing data quality and model architecture challenges. It introduces a new data construction pipeline that uses MLLMs to create a large dataset of 5 million high-quality person-centric image-text pairs, called WebPerson. The framework also employs a dual-masking technique that adapts to noisy text tokens, improving the model's ability to align visual and textual information. As a result, GA-DMS achieves state-of-the-art performance in various benchmarks for person representation tasks."}, 'zh': {'title': 'GA-DMS：提升CLIP的人物表示学习', 'desc': 'GA-DMS框架通过改进数据质量和模型架构，增强了CLIP在人物表示学习中的表现。该研究解决了人物中心图像的标注数据稀缺和全局对比学习的局限性。我们开发了一种抗噪声的数据构建流程，生成了一个包含500万高质量人物图像-文本对的大型数据集WebPerson。GA-DMS框架通过基于梯度注意力相似度分数自适应地屏蔽噪声文本标记，显著提高了跨模态对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2509.06266', 'title': 'Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View\n  Scenes', 'url': 'https://huggingface.co/papers/2509.06266', 'abstract': 'Ego3D-Bench evaluates VLMs on ego-centric, multi-view outdoor data, revealing performance gaps, and Ego3D-VLM enhances 3D spatial reasoning through cognitive map generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.', 'score': 5, 'issue_id': 5868, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '421233f9095a7b13', 'authors': ['Mohsen Gholami', 'Ahmad Rezaei', 'Zhou Weimin', 'Yong Zhang', 'Mohammad Akbari'], 'affiliations': ['Huawei Cloud', 'Huawei Technologies Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.06266.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#3d', '#reasoning', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'Преодоление пространственных ограничений VLM с помощью Ego3D', 'desc': 'Ego3D-Bench - это новый эталонный тест для оценки пространственного мышления моделей компьютерного зрения и обработки естественного языка (VLM) на основе эгоцентричных многоракурсных данных из внешней среды. Тест включает более 8600 пар вопросов и ответов, созданных с участием людей-аннотаторов. Результаты показывают значительный разрыв между производительностью современных VLM и уровнем человека в пространственном понимании. Для улучшения пространственного мышления VLM предложен метод Ego3D-VLM, который генерирует когнитивную карту на основе оценки глобальных 3D-координат.'}, 'en': {'title': 'Bridging the Gap in 3D Spatial Reasoning for VLMs', 'desc': "This paper introduces Ego3D-Bench, a benchmark for evaluating Vision-Language Models (VLMs) on ego-centric, multi-view outdoor data, addressing the challenge of understanding 3D spatial relationships. It highlights the performance gaps between human-level spatial reasoning and that of current VLMs, based on a comprehensive evaluation of 16 state-of-the-art models. To improve VLMs' spatial reasoning, the authors propose Ego3D-VLM, a framework that generates cognitive maps from 3D coordinates, leading to significant performance enhancements in spatial question-answering tasks. Together, these contributions aim to advance VLMs towards achieving human-like spatial understanding in real-world scenarios."}, 'zh': {'title': '提升VLM的三维空间推理能力', 'desc': 'Ego3D-Bench是一个新的基准，用于评估视觉语言模型（VLM）在以自我为中心的多视角户外数据上的空间推理能力。该基准包含超过8600对问答对，确保了数据的质量和多样性。研究结果显示，当前的VLM在空间理解方面与人类水平存在显著差距。为了解决这个问题，提出了Ego3D-VLM框架，通过生成认知地图来增强VLM的三维空间推理能力。'}}}, {'id': 'https://huggingface.co/papers/2509.01964', 'title': '2D Gaussian Splatting with Semantic Alignment for Image Inpainting', 'url': 'https://huggingface.co/papers/2509.01964', 'abstract': "A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.", 'score': 5, 'issue_id': 5855, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '60a42770e646820d', 'authors': ['Hongyu Li', 'Chaofeng Chen', 'Xiaoming Li', 'Guangming Lu'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Nanyang Technological University', 'School of Artificial Intelligence, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01964.jpg', 'data': {'categories': ['#inference', '#cv', '#benchmark'], 'emoji': '🖌️', 'ru': {'title': 'Дорисовка изображений с помощью 2D Gaussian Splatting и семантических признаков', 'desc': 'Новый подход к дорисовке изображений использует технологию 2D Gaussian Splatting, которая преобразует дискретные точки в непрерывные пространственные представления. Метод комбинирует непрерывное представление поля с предобученными признаками модели DINO для обеспечения глобальной семантической согласованности. Предложенный фреймворк кодирует неполные изображения в непрерывное поле коэффициентов 2D гауссовых сплатов и реконструирует финальное изображение через дифференцируемый процесс растеризации. Эксперименты показывают, что метод достигает конкурентоспособных результатов как по количественным метрикам, так и по визуальному качеству.'}, 'en': {'title': 'Revolutionizing Image Inpainting with 2D Gaussian Splatting', 'desc': 'This paper presents a new framework for image inpainting using 2D Gaussian Splatting, which transforms incomplete images into continuous representations. By leveraging pretrained DINO model features, the framework ensures that the inpainted areas maintain global semantic consistency with the surrounding content. The method employs a differentiable rasterization process to reconstruct images, promoting pixel-level coherence in the results. Additionally, a patch-wise rasterization strategy is introduced to enhance efficiency and reduce memory usage, leading to competitive performance in both quantitative and perceptual evaluations.'}, 'zh': {'title': '高效图像修复的新方向：二维高斯点云技术', 'desc': '本文提出了一种新颖的图像修复框架，利用二维高斯点云（2D Gaussian Splatting）技术，结合预训练的DINO模型特征，以实现全局语义一致性。该框架将不完整的图像编码为二维高斯点系数的连续场，并通过可微分光栅化过程重建最终图像。高斯点云的连续渲染方式自然促进了修复结果的像素级一致性。通过引入基于补丁的光栅化策略，本文在提高效率和可扩展性的同时，确保修复内容与周围场景保持一致。'}}}, {'id': 'https://huggingface.co/papers/2509.09332', 'title': 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning', 'url': 'https://huggingface.co/papers/2509.09332', 'abstract': 'OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io', 'score': 3, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '65192793a84b5158', 'authors': ['Yuecheng Liu', 'Dafeng Chi', 'Shiguang Wu', 'Zhanguang Zhang', 'Yuzheng Zhuang', 'Bowen Yang', 'He Zhu', 'Lingfeng Zhang', 'Pengwei Xie', 'David Gamaliel Arcos Bravo', 'Yingxue Zhang', 'Jianye Hao', 'Xingyue Quan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.09332.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#multimodal', '#games', '#3d'], 'emoji': '🤖', 'ru': {'title': 'OmniEVA: Универсальный планировщик для воплощенного ИИ', 'desc': 'OmniEVA - это новая система для воплощенного искусственного интеллекта, которая решает проблемы пространственной адаптации и учета физических ограничений роботов. Она использует механизм адаптивной 3D-привязки к задаче и фреймворк рассуждений с учетом воплощения. OmniEVA демонстрирует передовую производительность в различных задачах воплощенного интеллекта. Система успешно применяется как для простых, так и для составных задач робототехники.'}, 'en': {'title': 'Bridging Gaps for Smarter Embodied Intelligence', 'desc': 'OmniEVA is a new approach that improves how large language models understand and interact with the physical world. It tackles two main problems: the Geometric Adaptability Gap, which limits models trained on 2D data from effectively handling 3D tasks, and the Embodiment Constraint Gap, where models fail to consider the real-world limitations of robots. The paper introduces a Task-Adaptive 3D Grounding mechanism that allows the model to adjust its understanding of 3D space based on the task at hand. Additionally, it presents an Embodiment-Aware Reasoning framework that ensures planning decisions are practical and achievable, leading to superior performance in various embodied tasks.'}, 'zh': {'title': 'OmniEVA：提升具身智能的多模态推理能力', 'desc': 'OmniEVA 是一种新型的多模态大语言模型，旨在解决在具身智能中的空间和具身性差距。它通过任务自适应的三维定位机制和具身感知推理框架，提升了模型在多样化任务中的表现。该模型能够根据上下文需求进行三维信息的选择性融合，从而实现更好的空间理解和决策。实验结果表明，OmniEVA 在多种下游任务中展现了卓越的推理能力和灵活的规划能力。'}}}, {'id': 'https://huggingface.co/papers/2509.09614', 'title': 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering', 'url': 'https://huggingface.co/papers/2509.09614', 'abstract': 'LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.', 'score': 2, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '15a23d38c535fc1c', 'authors': ['Jielin Qiu', 'Zuxin Liu', 'Zhiwei Liu', 'Rithesh Murthy', 'Jianguo Zhang', 'Haolin Chen', 'Shiyu Wang', 'Ming Zhu', 'Liangwei Yang', 'Juntao Tan', 'Zhepeng Cen', 'Cheng Qian', 'Shelby Heinecke', 'Weiran Yao', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.09614.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'LoCoBench: новый рубеж в оценке ИИ для разработки ПО', 'desc': 'LoCoBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом в сложных сценариях разработки программного обеспечения. Он включает 8000 сценариев оценки на 10 языках программирования с контекстом от 10 тыс. до 1 млн токенов. LoCoBench вводит 8 категорий задач, охватывающих ключевые возможности работы с длинным контекстом, такие как понимание архитектуры, рефакторинг между файлами и анализ безопасности. Оценка современных моделей с длинным контекстом выявила значительные пробелы в производительности, показывая, что понимание длинного контекста в сложной разработке ПО остается нерешенной проблемой.'}, 'en': {'title': 'Evaluating Long-Context LLMs for Complex Software Development', 'desc': 'LoCoBench is a new benchmark designed to evaluate long-context language models (LLMs) in complex software development tasks. It focuses on understanding entire codebases and maintaining architectural consistency, which is crucial for large-scale systems. The benchmark includes 8,000 scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens, allowing for a thorough assessment of LLM performance. By introducing 8 task categories and a comprehensive evaluation framework, LoCoBench highlights significant performance gaps in current models, emphasizing the need for improved long-context understanding in software development.'}, 'zh': {'title': '评估长上下文模型的全新基准', 'desc': 'LoCoBench是一个专门评估长上下文语言模型在复杂软件开发场景中的基准测试工具。它填补了对整个代码库理解和在大规模系统中保持架构一致性的评估空白。该基准提供了8000个评估场景，涵盖10种编程语言，能够精确评估长上下文性能的下降。通过引入多种任务类别和评估指标，LoCoBench为长上下文理解在软件开发中的挑战提供了全面的评估框架。'}}}, {'id': 'https://huggingface.co/papers/2509.09594', 'title': 'ObjectReact: Learning Object-Relative Control for Visual Navigation', 'url': 'https://huggingface.co/papers/2509.09594', 'abstract': 'A new object-relative control paradigm using a topometric map representation and a local controller achieves better invariance and generalization in visual navigation tasks compared to image-relative methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent\'s pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/', 'score': 2, 'issue_id': 5864, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'ebc4a50525683497', 'authors': ['Sourav Garg', 'Dustin Craggs', 'Vineeth Bhat', 'Lachlan Mares', 'Stefan Podgorski', 'Madhava Krishna', 'Feras Dayoub', 'Ian Reid'], 'affiliations': ['IIIT Hyderabad, India', 'MBZUAI, UAE', 'University of Adelaide, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2509.09594.jpg', 'data': {'categories': ['#3d', '#video', '#games', '#agents', '#optimization', '#graphs'], 'emoji': '🗺️', 'ru': {'title': 'Объектно-относительное управление: новый взгляд на визуальную навигацию', 'desc': 'В статье представлена новая парадигма управления в задачах визуальной навигации, основанная на объектно-относительном подходе. Вместо использования изображений, которые зависят от положения агента, предлагается использовать топометрическую карту с объектами, что обеспечивает большую инвариантность. Это позволяет агенту прокладывать новые маршруты без необходимости повторять предыдущий опыт и улучшает обобщение на реальные условия. Метод продемонстрировал высокую эффективность в различных задачах навигации, включая движение в обратном направлении по карте.'}, 'en': {'title': 'Navigating with Objects: A New Control Paradigm for Better Generalization', 'desc': "This paper introduces a new approach to visual navigation called object-relative control, which uses a topometric map representation instead of relying solely on images. Unlike traditional image-relative methods, this approach allows for better generalization and invariance, meaning it can adapt to new routes without needing to replicate past experiences. The proposed method utilizes a 3D scene graph to enhance path planning and employs a local controller named 'ObjectReact' that operates independently of direct image inputs. The results show that this method outperforms image-based techniques in various navigation tasks and can effectively transfer learned policies to real-world scenarios."}, 'zh': {'title': '对象相对控制：超越图像的导航新方法', 'desc': '本文提出了一种新的对象相对控制范式，利用拓扑地图表示和局部控制器，在视觉导航任务中比图像相对方法表现更好。与传统的图像相对方法不同，该方法通过对象的特性提供了一种与代理的姿态和轨迹无关的世界表示。我们提出的“ObjectReact”局部控制器直接基于高层次的“WayObject Costmap”表示进行训练，消除了对显式RGB输入的需求。实验表明，该方法在不同传感器高度和多种导航任务中具有更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.07430', 'title': 'The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward', 'url': 'https://huggingface.co/papers/2509.07430', 'abstract': 'A new framework, DPH-RL, uses mass-covering f-divergences to address Pass@k degradation and catastrophic forgetting in fine-tuning LLMs with RLVR, improving both Pass@1 and Pass@k.  \t\t\t\t\tAI-generated summary \t\t\t\t A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.', 'score': 2, 'issue_id': 5854, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '1d2012ade6917cee', 'authors': ['Long Li', 'Jiaran Hao', 'Jason Klein Liu', 'Zhijian Zhou', 'Xiaoyu Tan', 'Wei Chu', 'Zhe Wang', 'Shirui Pan', 'Chao Qu', 'Yuan Qi'], 'affiliations': ['Fudan University', 'Griffith University', 'INFLY TECH'], 'pdf_title_img': 'assets/pdf/title_img/2509.07430.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Сохранение разнообразия знаний при обучении языковых моделей', 'desc': 'Статья представляет новый фреймворк DPH-RL для улучшения обучения больших языковых моделей с помощью обучения с подкреплением. DPH-RL использует f-дивергенции для сохранения разнообразия знаний модели и предотвращения катастрофической забывчивости. Эксперименты показывают, что DPH-RL улучшает как Pass@1, так и Pass@k метрики для задач генерации математических выражений и SQL-запросов. Предложенный подход более эффективен в обучении, так как требует только выборки из начальной политики без использования онлайн-модели сравнения.'}, 'en': {'title': 'Preserving Knowledge in LLMs with DPH-RL', 'desc': 'The paper introduces a new framework called Diversity-Preserving Hybrid Reinforcement Learning (DPH-RL) to tackle issues in fine-tuning Large Language Models (LLMs) using Reinforcement Learning with Verifiable Reward (RLVR). It addresses the problem of Pass@k degradation and catastrophic forgetting by utilizing mass-covering f-divergences, which help maintain a diverse knowledge base during training. The authors argue that traditional divergence measures, like reverse KL-divergence, can lead to knowledge loss, while DPH-RL promotes knowledge retention by continuously referencing the initial policy. Experimental results show that DPH-RL improves both single-attempt accuracy (Pass@1) and multi-attempt performance (Pass@k), making it a more efficient and effective approach for training LLMs.'}, 'zh': {'title': '利用f-散度提升大型语言模型的微调效果', 'desc': '本文提出了一种新的框架DPH-RL，利用质量覆盖的f-散度来解决在使用可验证奖励的强化学习（RLVR）微调大型语言模型（LLMs）时出现的Pass@k性能下降和灾难性遗忘问题。传统的RLVR目标往往忽视了散度项的选择和功能，导致模型在保持知识方面缺乏有效机制。DPH-RL通过使用前向KL散度和JS散度等质量覆盖的f-散度，作为一种排练机制，帮助模型保持广泛的解决方案覆盖。实验结果表明，DPH-RL不仅解决了Pass@k的下降问题，还在领域内外同时提高了Pass@1和Pass@k的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.09313', 'title': 'Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on\n  Open & Industry Data', 'url': 'https://huggingface.co/papers/2509.09313', 'abstract': "A recommender system integrated into CI/CD pipelines uses fine-tuned CodeBERT to detect and localize vulnerabilities in code without disrupting workflows, showing improved performance with appropriate undersampling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO(Automating vulnerability detection Integration for Developers' Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tool's perceived usefulness through a survey with the company's IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while a deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities.", 'score': 1, 'issue_id': 5867, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'a5c0da74ce905048', 'authors': ['Moritz Mock', 'Thomas Forrer', 'Barbara Russo'], 'affiliations': ['Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy', 'R&D Department, Würth Phoenix, Bolzano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.09313.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#architecture', '#security', '#survey', '#open_source', '#dataset', '#training'], 'emoji': '🛡️', 'ru': {'title': 'AI-DO: Интеграция глубокого обучения в CI/CD для автоматического обнаружения уязвимостей', 'desc': 'Эта статья представляет AI-DO - рекомендательную систему, интегрированную в CI/CD для обнаружения уязвимостей в коде. Система использует дообученную модель CodeBERT и показывает улучшенную производительность при правильном применении техник undersampling. Авторы оценивают эффективность CodeBERT для детекции уязвимых функций в промышленном и открытом ПО, анализируя обобщение между доменами. Исследование также включает оценку полезности инструмента через опрос ИТ-специалистов компании.'}, 'en': {'title': 'Seamless Vulnerability Detection in CI/CD with CodeBERT', 'desc': "This paper presents a recommender system that integrates a fine-tuned version of CodeBERT into Continuous Integration and Continuous Deployment (CI/CD) pipelines to identify and locate vulnerabilities in code. The study evaluates CodeBERT's performance in detecting vulnerable functions across both industrial and open-source software, highlighting the challenges of transferring deep learning solutions from academia to industry. It also addresses issues like class imbalance and the need for seamless integration into existing workflows. The findings indicate that while models trained on industrial data excel in their domain, those fine-tuned on open-source data, when combined with undersampling techniques, enhance vulnerability detection capabilities."}, 'zh': {'title': '智能推荐，安全无忧！', 'desc': '本论文提出了一种集成在持续集成/持续部署（CI/CD）管道中的推荐系统，利用微调后的CodeBERT来检测和定位代码中的漏洞，而不会干扰工作流程。研究表明，使用适当的欠采样技术可以提高漏洞检测的性能。我们评估了CodeBERT在工业和开源软件中检测脆弱函数的表现，并分析了其跨领域的泛化能力。最终，我们开发了AI-DO工具，通过调查评估其在IT专业人员中的实用性。'}}}, {'id': 'https://huggingface.co/papers/2509.09254', 'title': 'Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset\n  for Panoramic X-ray Analysis', 'url': 'https://huggingface.co/papers/2509.09254', 'abstract': 'A new dataset and benchmark, MMOral, and a fine-tuned model, OralGPT, address the challenges of interpreting panoramic X-rays in dentistry, showing significant performance improvements over existing large vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT.', 'score': 1, 'issue_id': 5867, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '4897a9ba7cdb2683', 'authors': ['Jing Hao', 'Yuxuan Fan', 'Yanpeng Sun', 'Kaixin Guo', 'Lizhuo Lin', 'Jinrong Yang', 'Qi Yong H. Ai', 'Lun M. Wong', 'Hao Tang', 'Kuo Feng Hung'], 'affiliations': ['CVTE', 'Department of Diagnostic Radiology, The University of Hong Kong', 'Faculty of Dentistry, The University of Hong Kong', 'Imaging and Interventional Radiology, Faculty of Medicine, The Chinese University of Hong Kong', 'National University of Singapore', 'School of Computer Science, Peking University', 'Sun Yat-sen University', 'The Hong Kong University of Science and Technology (GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2509.09254.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#optimization', '#dataset', '#interpretability', '#training'], 'emoji': '🦷', 'ru': {'title': 'OralGPT: Прорыв в ИИ для стоматологической диагностики', 'desc': 'Представлен новый набор данных и эталон MMOral, а также дообученная модель OralGPT для интерпретации панорамных рентгеновских снимков в стоматологии. MMOral включает 20 563 аннотированных изображения с 1,3 миллионами инструкций для различных задач, таких как извлечение атрибутов, генерация отчетов и визуальные вопросы-ответы. Оценка 64 мультимодальных языковых моделей на MMOral-Bench показала значительные ограничения существующих моделей в этой области. OralGPT, полученная путем дообучения Qwen2.5-VL-7B на наборе данных MMOral, демонстрирует существенное улучшение производительности на 24,73%.'}, 'en': {'title': 'Revolutionizing Dental Imaging with MMOral and OralGPT', 'desc': 'This paper introduces MMOral, a new dataset and benchmark specifically designed for interpreting panoramic X-rays in dentistry, addressing the limitations of existing large vision-language models (LVLMs) in this specialized field. The dataset includes over 20,000 annotated images and 1.3 million instruction-following instances for various tasks like attribute extraction and report generation. The authors also present OralGPT, a fine-tuned model that significantly improves performance on these tasks, achieving a 24.73% increase in accuracy after supervised fine-tuning. Overall, MMOral and OralGPT aim to enhance the capabilities of AI in dental diagnostics, paving the way for more effective multimodal AI systems in oral healthcare.'}, 'zh': {'title': '智能牙科的新突破：MMOral与OralGPT', 'desc': '本文介绍了一个新的数据集MMOral和一个微调模型OralGPT，旨在解决牙科全景X光片解读中的挑战。MMOral是第一个针对全景X光解读的大规模多模态指令数据集，包含20563张标注图像和130万条指令实例。研究表明，现有的大型视觉语言模型在这一领域的表现有限，最佳模型的准确率仅为41.45%。通过对Qwen2.5-VL-7B进行监督微调，OralGPT在性能上实现了24.73%的显著提升，为智能牙科和多模态AI系统的发展奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2509.09114', 'title': 'Modality Alignment with Multi-scale Bilateral Attention for Multimodal\n  Recommendation', 'url': 'https://huggingface.co/papers/2509.09114', 'abstract': "MambaRec enhances multimodal recommendation systems by integrating local feature alignment and global distribution regularization to improve cross-modal fusion and reduce representational bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec.", 'score': 1, 'issue_id': 5862, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '7b4a6bd1f51ddfc9', 'authors': ['Kelin Ren', 'Chan-Yang Ju', 'Dong-Ho Lee'], 'affiliations': ['Department of Applied Artificial Intelligence Hanyang University Ansan, Republic of Korea', 'Department of Computer Science and Engineering Hanyang University Ansan, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2509.09114.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'MambaRec: Точное мультимодальное слияние для улучшенных рекомендаций', 'desc': 'MambaRec - это новая система мультимодальных рекомендаций, которая улучшает кросс-модальное слияние и уменьшает смещение представлений. Она использует модуль внимания с разреженными свертками (DREAM) для выравнивания локальных признаков между визуальными и текстовыми модальностями. MambaRec также применяет регуляризацию максимального среднего расхождения (MMD) и контрастные функции потерь для глобального выравнивания модальностей. Эксперименты показывают превосходство MambaRec над существующими методами в качестве слияния, обобщении и эффективности.'}, 'en': {'title': 'MambaRec: Bridging Modalities for Better Recommendations', 'desc': 'MambaRec is a new framework designed to enhance multimodal recommendation systems by improving how different types of data, like images and text, work together. It addresses two main issues: the inability to effectively model detailed relationships between different data types and the inconsistency in overall data representation. The framework uses a special module called DREAM, which employs advanced techniques to align and refine the features from different modalities. By applying regularization methods, MambaRec ensures that the recommendations are more accurate and reliable, leading to better performance in real-world applications.'}, 'zh': {'title': 'MambaRec：提升多模态推荐的智能融合', 'desc': 'MambaRec 是一种增强多模态推荐系统的新框架，通过整合局部特征对齐和全局分布正则化来改善跨模态融合，减少表示偏差。该方法引入了扩张精炼注意力模块（DREAM），利用多尺度扩张卷积和通道、空间注意力对视觉和文本模态之间的细粒度语义模式进行对齐。通过最大均值差异（MMD）和对比损失函数，MambaRec 还增强了全局模态对齐的一致性，提高了语义一致性。实验结果表明，MambaRec 在融合质量、泛化能力和效率方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2509.07225', 'title': 'All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated\n  Vulnerability Detection and Patching', 'url': 'https://huggingface.co/papers/2509.07225', 'abstract': "A Cyber Reasoning System using LLMs autonomously discovered and patched security vulnerabilities in open-source projects, with a public leaderboard for benchmarking LLMs on these tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed a Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilities - including six previously unknown zero-days - in real-world open-source C and Java projects, and successfully patched 14 of them. The complete CRS is open source at https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper provides a detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.", 'score': 1, 'issue_id': 5867, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '1cf7418f34a5be45', 'authors': ['Ze Sheng', 'Qingxiao Xu', 'Jianwei Huang', 'Matthew Woodcock', 'Heqing Huang', 'Alastair F. Donaldson', 'Guofei Gu', 'Jeff Huang'], 'affiliations': ['City University of Hong Kong Hong Kong, China', 'Imperial College London London, UK', 'Texas A&M University College Station, US'], 'pdf_title_img': 'assets/pdf/title_img/2509.07225.jpg', 'data': {'categories': ['#security', '#benchmark', '#open_source', '#dataset', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'ИИ на страже кода: автономное обнаружение и исправление уязвимостей', 'desc': 'Исследователи разработали Систему Кибернетического Рассуждения (СКР), использующую Большие Языковые Модели (LLM) для автономного обнаружения и исправления уязвимостей в проектах с открытым исходным кодом. СКР успешно обнаружила 28 уязвимостей, включая 6 неизвестных ранее, в реальных проектах на C и Java, и исправила 14 из них. Система заняла 4-е место в соревновании DARPA по искусственному интеллекту в кибербезопасности. Авторы также представили публичный лидерборд для сравнения современных LLM в задачах обнаружения и исправления уязвимостей.'}, 'en': {'title': 'Empowering Cybersecurity with Autonomous LLMs', 'desc': 'This paper presents a Cyber Reasoning System (CRS) that utilizes large language models (LLMs) to autonomously identify and fix security vulnerabilities in open-source software. The CRS successfully discovered 28 vulnerabilities, including six zero-day exploits, and patched 14 of them during the DARPA AIxCC competition. The authors also introduce a public leaderboard for evaluating the performance of various LLMs on tasks related to vulnerability detection and patching. This work highlights the potential of LLMs in enhancing software security through automated reasoning and patching capabilities.'}, 'zh': {'title': '自主发现与修复安全漏洞的智能系统', 'desc': '本论文介绍了一种基于大型语言模型（LLM）的网络推理系统（CRS），该系统能够自主发现和修复开源项目中的安全漏洞。在DARPA的人工智能网络挑战赛中，我们的团队成功发现了28个安全漏洞，其中包括6个之前未知的零日漏洞，并成功修复了14个漏洞。我们还建立了一个公共排行榜，用于评估最新的LLM在漏洞检测和修复任务中的表现。该系统的完整代码是开源的，供研究人员和开发者使用。'}}}, {'id': 'https://huggingface.co/papers/2509.05739', 'title': 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More\n  Complicated', 'url': 'https://huggingface.co/papers/2509.05739', 'abstract': "Data poisoning attacks on Large Language Models can target the reasoning process by decomposing triggers, but the models' reasoning capabilities and architectural design provide a form of backdoor robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.   Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.", 'score': 0, 'issue_id': 5859, 'pub_date': '2025-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': 'bc4e67f715e40467', 'authors': ['Hanna Foerster', 'Ilia Shumailov', 'Yiren Zhao', 'Harsh Chaudhari', 'Jamie Hayes', 'Robert Mullins', 'Yarin Gal'], 'affiliations': ['Anthropic', 'DeepMind', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.05739.jpg', 'data': {'categories': ['#data', '#security', '#reasoning', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'Устойчивость языковых моделей к атакам на процесс рассуждения', 'desc': "Статья исследует уязвимости больших языковых моделей (LLM) к атакам отравления данных, нацеленным на процесс рассуждения. Авторы вводят понятие 'разложенного отравления рассуждений', где атакующий модифицирует только путь рассуждения, оставляя промпты и финальные ответы чистыми. Несмотря на возможность внедрения таких отравлений, их надежная активация для изменения конечных ответов оказывается неожиданно сложной. Исследование показывает, что продвинутые возможности рассуждения LLM и архитектурное разделение между рассуждением и генерацией ответов обеспечивают некоторую устойчивость к бэкдорам."}, 'en': {'title': 'Enhancing Backdoor Robustness in LLMs through Reasoning', 'desc': "This paper discusses data poisoning attacks on Large Language Models (LLMs) that specifically target their reasoning processes. It introduces a novel method called 'decomposed reasoning poison', where attackers modify the reasoning steps without altering the prompts or final outputs. Despite the potential for these stealthy attacks, the paper finds that activating such backdoors to influence final answers is challenging. This is due to the models' inherent robustness, which stems from their advanced reasoning capabilities and the structural separation between reasoning and answer generation."}, 'zh': {'title': '推理能力提升后门鲁棒性', 'desc': '本文探讨了针对大型语言模型（LLMs）的数据中毒攻击，特别是如何通过分解触发器来影响推理过程。研究表明，尽管可以注入这些分解的毒素，但要可靠地激活它们以改变最终答案却非常困难。这种困难源于模型在其思维过程中能够从后门中恢复。最终，研究发现，先进的LLMs的推理能力和推理与最终答案生成之间的架构分离，形成了一种新兴的后门鲁棒性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (6)', '#agi (1)', '#alignment', '#architecture (4)', '#audio (2)', '#benchmark (13)', '#cv (1)', '#data (3)', '#dataset (11)', '#diffusion', '#ethics', '#games (5)', '#graphs (1)', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (9)', '#open_source (4)', '#optimization (11)', '#plp', '#rag', '#reasoning (9)', '#rl (6)', '#rlhf (1)', '#robotics (2)', '#science (1)', '#security (3)', '#small_models', '#story_generation (1)', '#survey (2)', '#synthetic', '#training (10)', '#transfer_learning (3)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-13 01:41',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-13 01:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-13 01:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    