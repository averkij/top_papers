{
    "date": {
        "ru": "1 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 1",
        "zh": "12Êúà1Êó•"
    },
    "time_utc": "2025-12-01 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-01",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.22699",
            "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
            "url": "https://huggingface.co/papers/2511.22699",
            "abstract": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
            "score": 183,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "f95b1b395872e6e6",
            "authors": [
                "Z-Image Team",
                "Huanqia Cai",
                "Sihan Cao",
                "Ruoyi Du",
                "Peng Gao",
                "Steven Hoi",
                "Zhaohui Hou",
                "Shijie Huang",
                "Dengyang Jiang",
                "Xin Jin",
                "Liangchen Li",
                "Zhen Li",
                "Zhong-Yu Li",
                "David Liu",
                "Dongyang Liu",
                "Junhan Shi",
                "Qilong Wu",
                "Feng Yu",
                "Chi Zhang",
                "Shifeng Zhang",
                "Shilin Zhou"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22699.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#small_models",
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–ö–∞—á–µ—Å—Ç–≤–æ –±–µ–∑ –∏–∑–±—ã—Ç–∫–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —ç–ø–æ—Ö—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                    "desc": "Z-Image ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 6 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (S3-DiT), –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –¥–æ 314 —Ç—ã—Å—è—á GPU-—á–∞—Å–æ–≤. –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏–≤–µ–ª–∞ –∫ —Å–æ–∑–¥–∞–Ω–∏—é Z-Image-Turbo, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥ –º–µ–Ω–µ–µ —á–µ–º –∑–∞ —Å–µ–∫—É–Ω–¥—É –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ —Å –æ–±—ä—ë–º–æ–º –ø–∞–º—è—Ç–∏ –º–µ–Ω–µ–µ 16 –ì–ë. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ —Å –≤–µ–¥—É—â–∏–º–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –¥–≤—É—è–∑—ã—á–Ω–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ —Ç–µ–∫—Å—Ç–∞."
                },
                "en": {
                    "title": "Efficient Image Generation with Z-Image: High Performance, Low Cost!",
                    "desc": "Z-Image is a new image generation model that uses a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture with only 6 billion parameters, making it efficient and suitable for consumer hardware. It achieves fast inference times of under a second while maintaining high-quality output, challenging the trend of requiring massive models for good performance. The model's training process is optimized to reduce costs and time, completing in about 314,000 GPU hours. Z-Image also includes an editing version that excels in following instructions, demonstrating that high-quality generative models can be developed with lower computational resources."
                },
                "zh": {
                    "title": "Z-ImageÔºöÈ´òÊïàÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÈÄâÊã©",
                    "desc": "Z-ImageÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑ6BÂèÇÊï∞ÁîüÊàêÊ®°ÂûãÔºåÂü∫‰∫éÂèØÊâ©Â±ïÁöÑÂçïÊµÅÊâ©Êï£ÂèòÊç¢Âô®(S3-DiT)Êû∂ÊûÑÔºåÊó®Âú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ÊèêÈ´òÂõæÂÉèÁîüÊàêÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂú®‰ºÅ‰∏öÁ∫ßH800 GPU‰∏äÂÆûÁé∞‰∫Ü‰∫öÁßíÁ∫ßÊé®ÁêÜÂª∂ËøüÔºåÂêåÊó∂ÂÖºÂÆπÊ∂àË¥πËÄÖÁ∫ßÁ°¨‰ª∂ÔºåÈÄÇÂêàÂπøÊ≥õÂ∫îÁî®„ÄÇÈÄöËøá‰ºòÂåñÊ®°ÂûãÁîüÂëΩÂë®ÊúüÂíåËÆ≠ÁªÉÊµÅÁ®ãÔºåZ-ImageÂú®ËÆ≠ÁªÉÊïàÁéá‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§ü‰∏éÂ§ßÂûãÊ®°ÂûãÁ´û‰∫â„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåZ-ImageÂú®ÁîüÊàêÈÄºÁúüÂõæÂÉèÂíåÂèåËØ≠ÊñáÊú¨Ê∏≤ÊüìÊñπÈù¢ÁöÑËÉΩÂäõÔºåËÉΩÂ§ü‰∏éÈ°∂Á∫ßÂïÜ‰∏öÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂ±ïÁ§∫‰∫ÜÂú®ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÊúÄÂÖàËøõÁªìÊûúÁöÑÂèØËÉΩÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22570",
            "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2511.22570",
            "abstract": "A self-verifying large language model for theorem proving improves mathematical reasoning by incentivizing rigorous step-by-step derivations and achieves high scores in international competitions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.",
            "score": 70,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "8b1110df3818640d",
            "authors": [
                "Zhihong Shao",
                "Yuxiang Luo",
                "Chengda Lu",
                "Z. Z. Ren",
                "Jiewen Hu",
                "Tian Ye",
                "Zhibin Gou",
                "Shirong Ma",
                "Xiaokang Zhang"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22570.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#math",
                    "#science"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä—è—é—â–∞—è—Å—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–µ–æ—Ä–µ–º. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≥–¥–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª—É–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–µ–π –Ω–∞–≥—Ä–∞–¥—ã, —Å—Ç–∏–º—É–ª–∏—Ä—É—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤—ã—è–≤–ª—è—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏ –≤ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —É–ª—É—á—à–∞–µ—Ç—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DeepSeekMath-V2 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–æ–ª–æ—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –≤–∫–ª—é—á–∞—è IMO 2025 –∏ CMO 2024."
                },
                "en": {
                    "title": "Empowering Theorem Proving with Self-Verification",
                    "desc": "This paper presents a self-verifying large language model (LLM) designed to enhance mathematical reasoning, particularly in theorem proving. The model, named DeepSeekMath-V2, utilizes reinforcement learning to incentivize rigorous step-by-step derivations rather than just focusing on final answers. By implementing a verification mechanism, the model ensures that the reasoning process is both accurate and comprehensive, addressing the limitations of previous approaches. The results show significant improvements in competitive mathematical reasoning tasks, achieving high scores in prestigious competitions."
                },
                "zh": {
                    "title": "Ëá™ÊàëÈ™åËØÅÊ®°ÂûãÔºåÊèêÂçáÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËá™ÊàëÈ™åËØÅÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÁî®‰∫éÂÆöÁêÜËØÅÊòéÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÊøÄÂä±‰∏•Ê†ºÁöÑÈÄêÊ≠•Êé®ÂØºÔºåËØ•Ê®°ÂûãÂú®ÂõΩÈôÖÁ´ûËµõ‰∏≠ÂèñÂæó‰∫ÜÈ´òÂàÜ„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÊñπÊ≥ïÂú®ÊúÄÁªàÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß‰∏äÊúâÊâÄÊèêÂçáÔºå‰ΩÜÂπ∂Êú™Ëß£ÂÜ≥Êé®ÁêÜËøáÁ®ãÁöÑÊ≠£Á°ÆÊÄßÈóÆÈ¢ò„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÈÄöËøáËá™ÊàëÈ™åËØÅÊù•Â¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÂú®Â§ö‰∏™Êï∞Â≠¶Á´ûËµõ‰∏≠ÁöÑ‰ºòÂºÇË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22625",
            "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
            "url": "https://huggingface.co/papers/2511.22625",
            "abstract": "Integrating reasoning mechanisms into image editing models enhances performance by improving instruction understanding and result correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
            "score": 45,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "f2cb78babe98bca3",
            "authors": [
                "Fukun Yin",
                "Shiyu Liu",
                "Yucheng Han",
                "Zhibo Wang",
                "Peng Xing",
                "Rui Wang",
                "Wei Cheng",
                "Yingming Wang",
                "Aojie Li",
                "Zixin Yin",
                "Pengtao Chen",
                "Xiangyu Zhang",
                "Daxin Jiang",
                "Xianfang Zeng",
                "Gang Yu"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22625.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#diffusion",
                    "#interpretability",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ü§î",
                "ru": {
                    "title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Ü–∏–∫–ª–µ: –º—ã—à–ª–µ–Ω–∏–µ –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–æ—Å—Ç–æ—è—â–µ–π –∏–∑ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MLLM) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ MLLM –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é, –≤–Ω–µ–¥—Ä—è—è –¥–≤–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞: thinking –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ reflection –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ReasonEdit —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ü–∏–∫–ª –º—ã—à–ª–µ–Ω–∏–µ-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ-—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã –∏ –∏–∑–±–µ–≥–∞—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å –ø—Ä–∏—Ä–æ—Å—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 8.2%."
                },
                "en": {
                    "title": "Enhancing Image Editing with Reasoning Mechanisms",
                    "desc": "This paper discusses how integrating reasoning mechanisms into image editing models can improve their performance. The authors focus on a framework that combines a multimodal large language model (MLLM) with a diffusion decoder, allowing for better understanding of instructions and correction of results. They introduce two reasoning processes: thinking, which helps interpret instructions, and reflection, which reviews and corrects editing outcomes. The results show significant performance improvements in various image editing tasks, demonstrating the effectiveness of their approach."
                },
                "zh": {
                    "title": "Êé®ÁêÜÊú∫Âà∂ÊèêÂçáÂõæÂÉèÁºñËæëÊ®°ÂûãÁöÑÊÄßËÉΩ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ∞ÜÊé®ÁêÜÊú∫Âà∂Êï¥ÂêàÂà∞ÂõæÂÉèÁºñËæëÊ®°Âûã‰∏≠Ôºå‰ª•ÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊÄùËÄÉÂíåÂèçÊÄù‰∏§ÁßçÊé®ÁêÜÊú∫Âà∂ÔºåÂ¢ûÂº∫‰∫ÜÂØπÊåá‰ª§ÁöÑÁêÜËß£ÂíåÁºñËæëÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊÄùËÄÉÊú∫Âà∂Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ñÁïåÁü•ËØÜÊù•Ëß£ËØªÊäΩË±°Êåá‰ª§ÔºåËÄåÂèçÊÄùÊú∫Âà∂ÂàôÂÆ°Êü•ÁºñËæëÁªìÊûúÔºåËá™Âä®Á∫†Ê≠£ÊÑèÂ§ñÁöÑÊìç‰Ωú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÂõæÂÉèÁºñËæë‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23199",
            "title": "Vision Bridge Transformer at Scale",
            "url": "https://huggingface.co/papers/2511.23199",
            "abstract": "Bridge Models, instantiated as Vision Bridge Transformer (ViBT), efficiently translate data through direct modeling of input-to-output trajectories, achieving robust performance in image and video editing tasks at large scales.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
            "score": 43,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "5b40d868e46d4882",
            "authors": [
                "Zhenxiong Tan",
                "Zeqing Wang",
                "Xingyi Yang",
                "Songhua Liu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23199.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23475",
            "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
            "url": "https://huggingface.co/papers/2511.23475",
            "abstract": "The proposed AnyTalker framework generates high-quality multi-person talking videos by extending Diffusion Transformer with identity-aware attention, leveraging single-person videos for training, and using a specialized dataset for evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
            "score": 41,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "c92624f175f73ab5",
            "authors": [
                "Zhizhou Zhong",
                "Yicheng Ji",
                "Zhe Kong",
                "Yiying Liu",
                "Jiarui Wang",
                "Jiasun Feng",
                "Lupeng Liu",
                "Xiangyi Wang",
                "Yanjia Li",
                "Yuqing She",
                "Ying Qin",
                "Huan Li",
                "Shuiyang Mao",
                "Wei Liu",
                "Wenhan Luo"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Hong Kong University of Science and Technology",
                "Video Rebirth",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23475.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#synthetic",
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#architecture"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –æ–¥–Ω–æ–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ AnyTalker –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º–∏ –ª—é–¥—å–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç Diffusion Transformer —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º identity-aware attention. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–∏–¥–µ–æ —Å –æ–¥–Ω–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º—É–ª—å—Ç–∏–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å-–∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≥—É–± –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "AnyTalker: Revolutionizing Multi-Person Video Generation with Identity-Aware Attention",
                    "desc": "The AnyTalker framework is designed to create high-quality videos of multiple people talking by enhancing the Diffusion Transformer model with a unique identity-aware attention mechanism. This approach allows the model to process audio and identity information together, enabling it to generate videos with various characters while maintaining coherent interactions. Instead of relying on large datasets of multi-person videos, AnyTalker trains on single-person videos and fine-tunes its performance with a limited number of multi-person clips. The framework also introduces a new evaluation metric and dataset to assess the realism and interactivity of the generated videos, achieving impressive results in lip synchronization and visual fidelity."
                },
                "zh": {
                    "title": "AnyTalkerÔºöÈ´òÊïàÁîüÊàêÂ§ö‰∫∫Áâ©ÂØπËØùËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "AnyTalkerÊ°ÜÊû∂ÈÄöËøáÊâ©Â±ïÊâ©Êï£ÂèòÊç¢Âô®ÁöÑË∫´‰ªΩÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÂ§ö‰∫∫Áâ©ÂØπËØùËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Âçï‰∫∫ËßÜÈ¢ëËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰ΩøÁî®‰∏ìÈó®ÁöÑÊï∞ÊçÆÈõÜËøõË°åËØÑ‰º∞Ôºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÂ§ö‰∫∫Áâ©Êï∞ÊçÆÊî∂ÈõÜÁöÑÈ´òÊàêÊú¨ÂíåÂ§öË∫´‰ªΩ‰∫íÂä®ÁöÑÈöæÈ¢ò„ÄÇAnyTalkerÁöÑÂ§öÊµÅÂ§ÑÁêÜÊû∂ÊûÑÂÖÅËÆ∏ÁÅµÊ¥ªÊâ©Â±ïÂèØÈ©±Âä®ÁöÑË∫´‰ªΩÔºåÂπ∂ÈÄöËøáÂ∞ëÈáèÁúüÂÆûÁöÑÂ§ö‰∫∫Áâ©ÁâáÊÆµÊù•‰ºòÂåñ‰∫íÂä®ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAnyTalkerÂú®ÂîáÂêåÊ≠•„ÄÅËßÜËßâË¥®ÈáèÂíåËá™ÁÑ∂‰∫íÂä®ÊÄßÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüÂπ≥Ë°°‰∫ÜÊï∞ÊçÆÊàêÊú¨ÂíåË∫´‰ªΩÂèØÊâ©Â±ïÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22659",
            "title": "Geometrically-Constrained Agent for Spatial Reasoning",
            "url": "https://huggingface.co/papers/2511.22659",
            "abstract": "Geometrically-Constrained Agent (GCA) addresses the semantic-to-geometric gap in vision language models by decoupling semantic analysis and task solving with formal constraints, achieving state-of-the-art performance in spatial reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "43eba9ece990a49c",
            "authors": [
                "Zeren Chen",
                "Xiaoya Lu",
                "Zhijie Zheng",
                "Pengrui Li",
                "Lehan He",
                "Yijin Zhou",
                "Jing Shao",
                "Bohan Zhuang",
                "Lu Sheng"
            ],
            "affiliations": [
                "School of Software, Beihang University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "ZIP Lab, Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22659.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18890",
            "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
            "url": "https://huggingface.co/papers/2511.18890",
            "abstract": "The study identifies key architectural factors and efficient operators to optimize small language models for real-device latency, introducing the Nemotron-Flash family for improved accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.",
            "score": 29,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 –Ω–æ—è–±—Ä—è",
                "en": "November 24",
                "zh": "11Êúà24Êó•"
            },
            "hash": "7cd0a2bfd5155b52",
            "authors": [
                "Yonggan Fu",
                "Xin Dong",
                "Shizhe Diao",
                "Matthijs Van keirsbilck",
                "Hanrong Ye",
                "Wonmin Byeon",
                "Yashaswi Karnati",
                "Lucas Liebenwein",
                "Hannah Zhang",
                "Nikolaus Binder",
                "Maksim Khadkevich",
                "Alexander Keller",
                "Jan Kautz",
                "Yingyan Celine Lin",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18890.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#small_models",
                    "#architecture",
                    "#training"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –∏ —à–∏—Ä–∏–Ω—ã —Å–µ—Ç–∏ –∏ –≤—ã–±–æ—Ä –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –º–∞–ª—ã—Ö –±–∞—Ç—á–∞—Ö, —Ç–∞–∫ –∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Nemotron-Flash, –∫–æ—Ç–æ—Ä–æ–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ñ—Ä–æ–Ω—Ç–∏–µ—Ä —Ç–æ—á–Ω–æ—Å—Ç—å-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Optimizing Small Language Models for Real-World Latency",
                    "desc": "This paper focuses on optimizing small language models (SLMs) for better performance on real devices, particularly under strict latency requirements. It identifies key architectural factors, such as depth-width ratios and operator choices, that significantly influence the latency and throughput of SLMs. The authors propose a new family of models called Nemotron-Flash, which combines efficient operators and improved training techniques to enhance both accuracy and efficiency. The results show that these models outperform existing SLMs, achieving higher accuracy and lower latency, making them suitable for real-world applications."
                },
                "zh": {
                    "title": "‰ºòÂåñÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊèêÂçáÁúüÂÆûËÆæÂ§áÊÄßËÉΩ",
                    "desc": "Êú¨Á†îÁ©∂ËØÜÂà´‰∫Ü‰ºòÂåñÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂú®ÁúüÂÆûËÆæÂ§á‰∏äÂª∂ËøüÁöÑÂÖ≥ÈîÆÊû∂ÊûÑÂõ†Á¥†ÂíåÈ´òÊïàÊìç‰ΩúÁ¨¶ÔºåÊèêÂá∫‰∫ÜNemotron-FlashÁ≥ªÂàó‰ª•ÊèêÈ´òÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ‰ª•ÂæÄÁöÑSLMËÆæËÆ°‰∏ªË¶ÅÂÖ≥Ê≥®ÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºå‰ΩÜÂèÇÊï∞ÊïàÁéáÂπ∂‰∏ç‰∏ÄÂÆöËÉΩÂ∏¶Êù•Áõ∏Â∫îÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇÊàë‰ª¨ÂèëÁé∞Ê∑±Â∫¶-ÂÆΩÂ∫¶ÊØîÂíåÊìç‰ΩúÁ¨¶ÈÄâÊã©ÊòØÂΩ±ÂìçSLMÁúüÂÆûËÆæÂ§áÂª∂ËøüÁöÑ‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÔºåÂπ∂ÊèêÂá∫‰∫ÜÁõ∏Â∫îÁöÑËÆæËÆ°ÂíåËÆ≠ÁªÉÂéüÂàô„ÄÇÈÄöËøáËøô‰∫õÊñπÊ≥ïÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÊ∑∑ÂêàSLMÁ≥ªÂàóÔºåÊòæËëóÊèêÂçá‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22663",
            "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
            "url": "https://huggingface.co/papers/2511.22663",
            "abstract": "The proposed Attention Interaction Alignment (AIA) loss improves cross-modal attention and performance in unified multimodal models for image generation and understanding without decoupling.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
            "score": 28,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "89cc047e45695037",
            "authors": [
                "Dian Zheng",
                "Manyuan Zhang",
                "Hongyu Li",
                "Kai Zou",
                "Hongbo Liu",
                "Ziyu Guo",
                "Kaituo Feng",
                "Yexin Liu",
                "Ying Luo",
                "Yan Feng",
                "Peng Pei",
                "Xunliang Cai",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Meituan",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22663.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18822",
            "title": "DiP: Taming Diffusion Models in Pixel Space",
            "url": "https://huggingface.co/papers/2511.18822",
            "abstract": "DiP, a pixel space diffusion framework, combines a Diffusion Transformer and a Patch Detailer Head to achieve computational efficiency and high-quality image generation without using VAEs.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10times faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256times256.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 –Ω–æ—è–±—Ä—è",
                "en": "November 24",
                "zh": "11Êúà24Êó•"
            },
            "hash": "6e8bc038b6e47959",
            "authors": [
                "Zhennan Chen",
                "Junwei Zhu",
                "Xu Chen",
                "Jiangning Zhang",
                "Xiaobin Hu",
                "Hanzhen Zhao",
                "Chengjie Wang",
                "Jian Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "Nanjing University",
                "National University of Singapore",
                "Tencent Youtu Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18822.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.21025",
            "title": "CaptionQA: Is Your Caption as Useful as the Image Itself?",
            "url": "https://huggingface.co/papers/2511.21025",
            "abstract": "CaptionQA evaluates caption utility by measuring their effectiveness in supporting downstream tasks, revealing significant gaps compared to traditional image-QA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-11-26",
            "pub_date_card": {
                "ru": "26 –Ω–æ—è–±—Ä—è",
                "en": "November 26",
                "zh": "11Êúà26Êó•"
            },
            "hash": "c0f311411875caef",
            "authors": [
                "Shijia Yang",
                "Yunong Liu",
                "Bohan Zhai",
                "Ximeng Sun",
                "Zicheng Liu",
                "Emad Barsoum",
                "Manling Li",
                "Chenfeng Xu"
            ],
            "affiliations": [
                "ADVANCED MICRO DEVICES, INC.",
                "INDEPENDENT RESEARCHER",
                "NORTHWESTERN UNIVERSITY",
                "STANFORD UNIVERSITY",
                "UT AUSTIN"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.21025.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "üì∏",
                "ru": {
                    "title": "–ü–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CaptionQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –±–æ–ª–µ–µ —á–µ–º 33 —Ç—ã—Å—è—á –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —á–µ—Ç—ã—Ä–µ –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ –æ—Ç–≤–µ—Ç—ã —Ç—Ä–µ–±—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –≤—ã—è–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–¥–ø–∏—Å–µ–π –∏ –∏—Ö —Ä–µ–∞–ª—å–Ω–æ–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö."
                },
                "en": {
                    "title": "Evaluating Caption Utility for Real-World Tasks with CaptionQA",
                    "desc": "CaptionQA is a new benchmark designed to assess the effectiveness of image captions in supporting various downstream tasks, such as retrieval and recommendation. It highlights the shortcomings of traditional image-QA benchmarks by showing that captions often do not perform as well as images in practical applications. The benchmark includes a large dataset of 33,027 annotated questions across four domains, allowing for a detailed evaluation of caption quality. Results indicate that state-of-the-art models show a significant drop in performance when relying solely on captions, underscoring the need for improved caption generation methods."
                },
                "zh": {
                    "title": "CaptionQAÔºöËØÑ‰º∞ÂõæÂÉèÊèèËø∞Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊïàÁî®",
                    "desc": "CaptionQA ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ÂõæÂÉèÊèèËø∞ÊúâÊïàÊÄßÁöÑÂü∫ÂáÜÔºå‰∏ªË¶ÅÈÄöËøáÊµãÈáèÂÖ∂Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊîØÊåÅËÉΩÂäõÊù•Êè≠Á§∫‰∏é‰º†ÁªüÂõæÂÉèÈóÆÁ≠îÂü∫ÂáÜ‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ù„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñËá™ÁÑ∂„ÄÅÊñáÊ°£„ÄÅÁîµÂ≠êÂïÜÂä°ÂíåÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁ≠âÂõõ‰∏™È¢ÜÂüüÔºåÊèê‰æõ‰∫ÜÁªÜËá¥ÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÔºå‰ª•ËØÜÂà´ÁâπÂÆö‰ªªÂä°ÊâÄÈúÄÁöÑ‰ø°ÊÅØ„ÄÇÈÄöËøáÊûÑÂª∫ 33,027 ‰∏™ÂØÜÈõÜÊ≥®ÈáäÁöÑÂ§öÈ°πÈÄâÊã©È¢òÔºåCaptionQA Áõ¥Êé•ÊµãÈáèÊèèËø∞ÊòØÂê¶‰øùÁïô‰∫ÜÂõæÂÉèÁ∫ßÂà´ÁöÑÊïàÁî®ÔºåÂπ∂ËÉΩË¢´‰∏ãÊ∏∏Â§ßËØ≠Ë®ÄÊ®°ÂûãÊúâÊïàÂà©Áî®„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Âú®‰º†ÁªüÂõæÂÉèÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞Áõ∏‰ººÔºåÊ®°ÂûãÂú®ÊèèËø∞ÊïàÁî®‰∏äÂç¥‰ΩéËá≥ 32%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22677",
            "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
            "url": "https://huggingface.co/papers/2511.22677",
            "abstract": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "dfbb44ed75494dc0",
            "authors": [
                "Dongyang Liu",
                "Peng Gao",
                "David Liu",
                "Ruoyi Du",
                "Zhen Li",
                "Qilong Wu",
                "Xin Jin",
                "Sihan Cao",
                "Shifeng Zhang",
                "Hongsheng Li",
                "Steven Hoi"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22677.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "CFG Augmentation ‚Äî –∏—Å—Ç–∏–Ω–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è —Ä–æ–ª—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ Distribution Matching Distillation (DMD) –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≤ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂—É—â–µ–π —Å–∏–ª–æ–π —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —è–≤–ª—è–µ—Ç—Å—è CFG Augmentation, –∞ –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ —É—á–µ–Ω–∏–∫–∞ –∏ —É—á–∏—Ç–µ–ª—è. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–∞—é—â–∏–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π —à—É–º–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–∞."
                },
                "en": {
                    "title": "Unlocking the Power of CFG Augmentation in Distillation",
                    "desc": "This paper investigates the mechanisms behind few-step distillation in text-to-image generation, specifically focusing on Distribution Matching Distillation (DMD). It reveals that CFG Augmentation (CA) is the main factor driving performance, rather than the traditional emphasis on distribution matching. The authors show that while distribution matching serves as a stabilizing regularizer, it can be replaced by simpler methods without losing effectiveness. Their findings lead to new strategies for improving distillation processes, which have been successfully implemented in a state-of-the-art image generation model."
                },
                "zh": {
                    "title": "CFGÂ¢ûÂº∫ÔºöÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊ†∏ÂøÉÈ©±Âä®Âäõ",
                    "desc": "Êú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÔºåCFGÂ¢ûÂº∫ÊòØÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºàDMDÔºâ‰∏≠Â∞ëÊ≠•Ëí∏È¶èÁöÑ‰∏ªË¶ÅÈ©±Âä®Âõ†Á¥†ÔºåËÄåÂàÜÂ∏ÉÂåπÈÖçÈ°πÂàôÂÖÖÂΩìÊ≠£ÂàôÂåñÂô®„ÄÇÊàë‰ª¨ÈÄöËøáÂØπDMDËÆ≠ÁªÉÁõÆÊ†áÁöÑ‰∏•Ê†ºÂàÜËß£ÔºåÊåëÊàò‰∫Ü‰º†ÁªüÁêÜËß£ÔºåÊåáÂá∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÔºåCFGÂ¢ûÂº∫ÊòØËí∏È¶èÁöÑÊ†∏ÂøÉÂºïÊìé„ÄÇÊàë‰ª¨È™åËØÅ‰∫ÜËøô‰∏ÄÂàÜÁ¶ªÔºåË°®ÊòéËôΩÁÑ∂ÂàÜÂ∏ÉÂåπÈÖçÈ°πÊòØÊúâÊïàÁöÑÊ≠£ÂàôÂåñÂô®Ôºå‰ΩÜÂπ∂ÈùûÂîØ‰∏ÄÔºåÂÖ∂‰ªñÁÆÄÂçïÁöÑÈùûÂèÇÊï∞Á∫¶ÊùüÊàñÂü∫‰∫éGANÁöÑÁõÆÊ†á‰πüËÉΩÂÆûÁé∞Áõ∏‰ººÁöÑÁ®≥ÂÆöÂäüËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Ëí∏È¶èËøáÁ®ãÁöÑÂéüÂàôÊÄß‰øÆÊîπÊèê‰æõ‰∫ÜÂü∫Á°ÄÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23319",
            "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
            "url": "https://huggingface.co/papers/2511.23319",
            "abstract": "Hierarchical Sparse Attention (HSA) is integrated into Transformers to efficiently handle ultra-long contexts, achieving high accuracy on in-context retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: sparsity, random-access flexibility, and length generalization. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "7303c3ce66732f64",
            "authors": [
                "Xiang Hu",
                "Zhanchao Zhou",
                "Ruiqi Liang",
                "Zehuan Li",
                "Wei Wu",
                "Jianguo Li"
            ],
            "affiliations": [
                "Ant Group",
                "Westlake University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23319.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22475",
            "title": "Adversarial Flow Models",
            "url": "https://huggingface.co/papers/2511.22475",
            "abstract": "Adversarial flow models unify adversarial and flow-based generative models, offering stable training, efficient generation, and high performance on image datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "6dc43b8b78da2d3a",
            "authors": [
                "Shanchuan Lin",
                "Ceyuan Yang",
                "Zhijie Lin",
                "Hao Chen",
                "Haoqi Fan"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22475.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã adversarial flow –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç adversarial –ø–æ–¥—Ö–æ–¥ –∏ flow-based –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–±—É—á–∞–µ—Ç—Å—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–∑ —à—É–º–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, —á—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç adversarial –æ–±—É—á–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—É—é –∏–ª–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏–∑—É—á–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç —ë–º–∫–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–Ω–∏–∂–∞–µ—ÇÁ¥ØÁßØ–æ—à–∏–±–∫—É. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ ImageNet-256px –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤—ã—Ö –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å FID 2.38, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥."
                },
                "en": {
                    "title": "Unifying Adversarial and Flow Models for Efficient Image Generation",
                    "desc": "Adversarial flow models are a new type of generative model that combine the strengths of adversarial and flow-based approaches. They allow for stable training and efficient image generation by learning a direct mapping from noise to data distributions, rather than relying on complex transport plans. This method reduces the need for intermediate steps in generation, which saves resources and minimizes errors. The models achieve high performance on image datasets, demonstrating their effectiveness in generating high-quality images with fewer training iterations."
                },
                "zh": {
                    "title": "ÂØπÊäóÊµÅÊ®°ÂûãÔºöÁ®≥ÂÆöÈ´òÊïàÁöÑÁîüÊàêÊñ∞ÊñπÊ≥ï",
                    "desc": "ÂØπÊäóÊµÅÊ®°ÂûãÊòØ‰∏ÄÁßçÁîüÊàêÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂØπÊäóÊ®°ÂûãÂíåÊµÅÊ®°ÂûãÁöÑ‰ºòÁÇπ„ÄÇËØ•ÊñπÊ≥ïÊîØÊåÅ‰∏ÄÊ≠•ÊàñÂ§öÊ≠•ÁîüÊàêÔºåÂπ∂‰ΩøÁî®ÂØπÊäóÁõÆÊ†áËøõË°åËÆ≠ÁªÉ„ÄÇ‰∏é‰º†ÁªüÁöÑÁîüÊàêÂØπÊäóÁΩëÁªúÔºàGANÔºâ‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÁîüÊàêÂô®Â≠¶‰π†ÁöÑÊòØÁ°ÆÂÆöÊÄßÁöÑÂô™Â£∞Âà∞Êï∞ÊçÆÁöÑÊò†Â∞ÑÔºå‰ªéËÄåÊòæËëóÁ®≥ÂÆö‰∫ÜÂØπÊäóËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÁõ¥Êé•Â≠¶‰π†‰∏ÄÊ≠•ÊàñÂ∞ëÊ≠•ÁîüÊàêÔºåËäÇÁúÅ‰∫ÜÊ®°ÂûãÂÆπÈáèÔºåÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉËø≠‰ª£ÔºåÂπ∂ÈÅøÂÖç‰∫ÜËØØÂ∑ÆÁ¥ØÁßØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22134",
            "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
            "url": "https://huggingface.co/papers/2511.22134",
            "abstract": "DualVLA enhances action performance while preserving reasoning capability in Vision-Language-Action models through dual-layer data pruning and dual-teacher adaptive distillation, achieving high success rates in multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "54f879dcca2db009",
            "authors": [
                "Zhen Fang",
                "Zhuoyang Liu",
                "Jiaming Liu",
                "Hao Chen",
                "Yu Zeng",
                "Shiting Huang",
                "Zehui Chen",
                "Lin Chen",
                "Shanghang Zhang",
                "Feng Zhao"
            ],
            "affiliations": [
                "CUHK",
                "MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, Peking University",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22134.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22173",
            "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
            "url": "https://huggingface.co/papers/2511.22173",
            "abstract": "RefineBench evaluates language models' ability to self-refine and improve their responses through self-reflection and guided refinement across various challenging tasks, highlighting the need for significant improvements in self-refinement capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "b66b5af4c2e8fd57",
            "authors": [
                "Young-Jun Lee",
                "Seungone Kim",
                "Byung-Kwan Lee",
                "Minkyeong Moon",
                "Yechan Hwang",
                "Jong Myoung Kim",
                "Graham Neubig",
                "Sean Welleck",
                "Ho-Jin Choi"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "KAIST",
                "NVIDIA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22173.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22815",
            "title": "Captain Safari: A World Engine",
            "url": "https://huggingface.co/papers/2511.22815",
            "abstract": "Captain Safari, a pose-conditioned world engine using a dynamic local memory and retriever for pose-aligned world tokens, generates high-quality, 3D-consistent long videos with accurate camera maneuvers, outperforming existing methods across quality, consistency, and trajectory following.  \t\t\t\t\tAI-generated summary \t\t\t\t World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "01688d8243558dc1",
            "authors": [
                "Yu-Cheng Chou",
                "Xingrui Wang",
                "Yitong Li",
                "Jiahao Wang",
                "Hanting Liu",
                "Cihang Xie",
                "Alan Yuille",
                "Junfei Xiao"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "Tsinghua University",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22815.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#video",
                    "#3d"
                ],
                "emoji": "üöÅ",
                "ru": {
                    "title": "–ú–∏—Ä–æ–≤–æ–π –¥–≤–∏–∂–æ–∫ —Å –ø–∞–º—è—Ç—å—é –¥–ª—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ª–≥–∏—Ö –≤–∏–¥–µ–æ",
                    "desc": "Captain Safari ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã—Ö –ø–æ –ø–æ–ª–æ–∂–µ–Ω–∏—é –∫–∞–º–µ—Ä—ã. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞–Ω–µ–≤—Ä–æ–≤ –∫–∞–º–µ—Ä—ã –≤ —à–µ—Å—Ç–∏ —Å—Ç–µ–ø–µ–Ω—è—Ö —Å–≤–æ–±–æ–¥—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–∂–µ –Ω–∞ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç OpenSafari —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –≤–∏–¥–µ–æ –¥—Ä–æ–Ω–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∫–∞–º–µ—Ä—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ –¥–∏–∫–æ–π –ø—Ä–∏—Ä–æ–¥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ, 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω–æ–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π –≤ 67.6% –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Revolutionizing 3D Video Generation with Pose-Conditioned Memory",
                    "desc": "Captain Safari is a novel pose-conditioned world engine designed to generate long, 3D-consistent videos that allow for interactive exploration with user-controlled camera movements. It addresses the limitations of existing systems that struggle with complex trajectories and outdoor environments by utilizing a dynamic local memory and a retriever to fetch pose-aligned world tokens. This approach ensures stable 3D structure and accurate camera maneuvers, significantly improving video quality and trajectory adherence. The introduction of the OpenSafari dataset further validates the effectiveness of Captain Safari, showcasing its superior performance in comparison to state-of-the-art methods."
                },
                "zh": {
                    "title": "ÂßøÊÄÅÈ©±Âä®ÁöÑ‰∏ñÁïåÂºïÊìéÔºåÁîüÊàêÈ´òË¥®ÈáèÈïøËßÜÈ¢ë",
                    "desc": "Captain Safari ÊòØ‰∏ÄÁßçÂü∫‰∫éÂßøÊÄÅÁöÑ‰∏ñÁïåÂºïÊìéÔºåÂà©Áî®Âä®ÊÄÅÊú¨Âú∞ËÆ∞ÂøÜÂíåÊ£ÄÁ¥¢Âô®ÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰∏âÁª¥‰∏ÄËá¥ÁöÑÈïøËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ªéÊåÅ‰πÖÁöÑ‰∏ñÁïåËÆ∞ÂøÜ‰∏≠Ê£ÄÁ¥¢‰∏éÂßøÊÄÅÂØπÈΩêÁöÑ‰∏ñÁïåÊ†áËÆ∞ÔºåÁ°Æ‰øùÂú®Â§çÊùÇÁöÑÁõ∏Êú∫Ë∑ØÂæÑ‰∏ã‰øùÊåÅÁ®≥ÂÆöÁöÑ‰∏âÁª¥ÁªìÊûÑ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCaptain Safari Âú®ËßÜÈ¢ëË¥®Èáè„ÄÅ‰∏ÄËá¥ÊÄßÂíåËΩ®ËøπË∑üË∏™ÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêËßÜÈ¢ëÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ËøòÊûÑÂª∫‰∫Ü OpenSafari Êï∞ÊçÆÈõÜÔºå‰∏∫Êú™Êù•ÁöÑ‰∏ñÁïåÂºïÊìéÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊåëÊàòÂü∫ÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22787",
            "title": "World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.22787",
            "abstract": "LVLMs struggle with preserving cultural identities in mixed visual scenes, but supervised fine-tuning with culture mixing datasets improves their performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "875721f95f9deab4",
            "authors": [
                "Eunsu Kim",
                "Junyeong Park",
                "Na Min An",
                "Junseong Kim",
                "Hitesh Laxmichand Patel",
                "Jiho Jin",
                "Julia Kruk",
                "Amit Agarwal",
                "Srikant Panda",
                "Fenal Ashokbhai Ilasariya",
                "Hyunjung Shim",
                "Alice Oh"
            ],
            "affiliations": [
                "KAIST",
                "Meta",
                "Oracle",
                "Stevens Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22787.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#synthetic",
                    "#benchmark",
                    "#ethics",
                    "#training",
                    "#cv"
                ],
                "emoji": "üåç",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ LVLM –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º–æ–¥–µ–ª—è–º–∏ LVLM —Å–º–µ—à–∞–Ω–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –≥–¥–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ CultureMix —Å 23 —Ç—ã—Å—è—á–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–∏—Ç–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω—É—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö. –ß–µ—Ä–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 10 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LVLM –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∫—É–ª—å—Ç—É—Ä–Ω—É—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ñ–æ–Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ supervised fine-tuning –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö —Å –∫—É–ª—å—Ç—É—Ä–Ω—ã–º —Å–º–µ—à–∏–≤–∞–Ω–∏–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π –∏ —Å–Ω–∏–∂–∞–µ—Ç –∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ñ–æ–Ω–∞."
                },
                "en": {
                    "title": "Enhancing LVLMs for Culturally Diverse Visual Understanding",
                    "desc": "This paper addresses the challenges that Large Vision-Language Models (LVLMs) face when interpreting mixed cultural elements in visual scenes. It introduces a benchmark called CultureMix, which consists of 23,000 images designed to test LVLMs on various food-related tasks involving cultural mixing. The study reveals that LVLMs often fail to maintain the distinct identities of different cultures when they are presented together, particularly showing a reliance on background context. To improve performance, the authors propose supervised fine-tuning with diverse culture mixing datasets, which significantly enhances model consistency and reduces sensitivity to background information."
                },
                "zh": {
                    "title": "ÊèêÂçáLVLMsÂú®ÊñáÂåñÊ∑∑ÂêàÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®Ê∑∑ÂêàÊñáÂåñÂú∫ÊôØ‰∏≠‰øùÊåÅÊñáÂåñË∫´‰ªΩÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩì‰∏çÂêåÊñáÂåñÂÖÉÁ¥†ÂêåÊó∂Âá∫Áé∞Âú®ËßÜËßâÂú∫ÊôØ‰∏≠Êó∂ÔºåÁé∞ÊúâÊ®°ÂûãÁöÑË°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÉåÊôØ‰ø°ÊÅØÁöÑÂΩ±Âìç‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫CultureMixÁöÑËßÜËßâÈóÆÁ≠îÂü∫ÂáÜÔºåÂπ∂ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÊñáÂåñÊ∑∑ÂêàÊï∞ÊçÆÈõÜÊù•ÊèêÈ´òÊ®°ÂûãÁöÑ‰∏ÄËá¥ÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ¢ûÂº∫ÂØπÊñáÂåñÊ∑∑ÂêàÂú∫ÊôØÁöÑÂÖ≥Ê≥®ÊòØÊèêÂçáLVLMsÂú®Â§öÂÖÉÊñáÂåñÁéØÂ¢É‰∏≠ÂèØÈù†ÊÄßÁöÑÂÖ≥ÈîÆ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22281",
            "title": "The Collapse of Patches",
            "url": "https://huggingface.co/papers/2511.22281",
            "abstract": "Patch collapse, a phenomenon where observing certain image patches reduces uncertainty in others, improves masked image modeling and promotes vision efficiency through selective patch exposure.  \t\t\t\t\tAI-generated summary \t\t\t\t Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "e5a9638ca080c7bf",
            "authors": [
                "Wei Guo",
                "Shunqi Mao",
                "Zhuonan Liang",
                "Heng Wang",
                "Weidong Cai"
            ],
            "affiliations": [
                "School of Computer Science, The University of Sydney"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22281.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22055",
            "title": "OralGPT-Omni: A Versatile Dental Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2511.22055",
            "abstract": "OralGPT-Omni, a dental-specialized multimodal large language model, enhances dental image analysis through TRACE-CoT reasoning supervision and achieves superior performance on MMOral-Uni benchmark compared to GPT-5.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "e10e022898c2e810",
            "authors": [
                "Jing Hao",
                "Yuci Liang",
                "Lizhuo Lin",
                "Yuxuan Fan",
                "Wenkai Zhou",
                "Kaixin Guo",
                "Zanting Ye",
                "Yanpeng Sun",
                "Xinyu Zhang",
                "Yanqi Yang",
                "Qiankun Li",
                "Hao Tang",
                "James Kit-Hon Tsoi",
                "Linlin Shen",
                "Kuo Feng Hung"
            ],
            "affiliations": [
                "College of Artificial Intelligence, Shenzhen University",
                "College of Computer Science and Software Engineering, Shenzhen University",
                "Faculty of Dentistry, The University of Hong Kong",
                "School of Biomedical Engineering, Southern Medical University",
                "School of Computer Science, Peking University",
                "Singapore University of Technology and Design",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Auckland",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22055.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#healthcare",
                    "#training",
                    "#science"
                ],
                "emoji": "ü¶∑",
                "ru": {
                    "title": "–°—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ò–ò —Å –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –ª–æ–≥–∏–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "OralGPT-Omni ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (MLLM) –¥–ª—è —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TRACE-CoT, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Ç—Ä–∞–∂–∞—é—â–∏–º–∏ –ª–æ–≥–∏–∫—É –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–æ–≤-—Ä–µ–Ω—Ç–≥–µ–Ω–æ–ª–æ–≥–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ MMOral-Uni ‚Äî –ø–µ—Ä–≤—ã–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 2800 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—è—Ç–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º. OralGPT-Omni –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-5 –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ, –ø—Ä–æ–¥–≤–∏–≥–∞—è —Ä–∞–∑–≤–∏—Ç–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏."
                },
                "en": {
                    "title": "Revolutionizing Dental Image Analysis with OralGPT-Omni",
                    "desc": "OralGPT-Omni is a specialized multimodal large language model (MLLM) designed to improve dental image analysis. It utilizes TRACE-CoT reasoning supervision to better mimic the diagnostic processes of dental professionals, enhancing its understanding of dental images. The model was evaluated using the MMOral-Uni benchmark, where it significantly outperformed GPT-5, achieving high scores in various dental tasks. This research aims to advance intelligent dentistry by providing a robust framework for analyzing dental images and making the tools publicly available for further development."
                },
                "zh": {
                    "title": "Êô∫ËÉΩÁâôÁßëÁöÑÊú™Êù•ÔºöOralGPT-OmniÁöÑÂ¥õËµ∑",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜOralGPT-OmniÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÁâôÁßëÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÂçáÁâôÁßëÂõæÂÉèÂàÜÊûêÁöÑËÉΩÂäõ„ÄÇÈÄöËøáTRACE-CoTÊé®ÁêÜÁõëÁù£ÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÁâôÂåªÁöÑËØäÊñ≠Êé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÂØπÁâôÁßëÂõæÂÉèÁöÑÁêÜËß£ÂíåÂàÜÊûêËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜMMOral-UniÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁâôÁßëÂõæÂÉèÂàÜÊûêÂü∫ÂáÜÔºåÂåÖÂê´Â§öÁßçÊ®°ÊÄÅÂíå‰ªªÂä°ÁöÑÂºÄÊîæÂºèÈóÆÁ≠îÂØπ„ÄÇOralGPT-OmniÂú®ËØ•Âü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜGPT-5ÁöÑÊàêÁª©ÔºåÊé®Âä®‰∫ÜÊô∫ËÉΩÁâôÁßëÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.21750",
            "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
            "url": "https://huggingface.co/papers/2511.21750",
            "abstract": "A benchmark evaluates schema-grounded information extraction and reasoning over visual inputs for multimodal large language models, revealing gaps and guiding future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-23",
            "pub_date_card": {
                "ru": "23 –Ω–æ—è–±—Ä—è",
                "en": "November 23",
                "zh": "11Êúà23Êó•"
            },
            "hash": "6368fe988a89fff8",
            "authors": [
                "Di Feng",
                "Kaixin Ma",
                "Feng Nan",
                "Haofeng Chen",
                "Bohan Zhai",
                "David Griffiths",
                "Mingfei Gao",
                "Zhe Gan",
                "Eshan Verma",
                "Yinfei Yang",
                "Zhifeng Chen",
                "Afshin Dehghan"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.21750.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "üìã",
                "ru": {
                    "title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ü–µ–Ω–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SO-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º–∏ —Å—Ö–µ–º–∞–º–∏ JSON. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–∞: UI-—ç–∫—Ä–∞–Ω—ã, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏, –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ 6,5 —Ç—ã—Å—è—á —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ö–µ–º –∏ 1,8 —Ç—ã—Å—è—á–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Å—Ö–µ–º–∞. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ö–µ–º–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç —Å–¥–µ–ª–∞—Ç—å –±–µ–Ω—á–º–∞—Ä–∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞."
                },
                "en": {
                    "title": "Bridging Gaps in Multimodal Schema-Grounded Reasoning",
                    "desc": "This paper introduces a benchmark called SO-Bench, designed to evaluate how well multimodal large language models (MLLMs) can extract information and reason about visual inputs while adhering to specific data schemas. The benchmark includes a diverse set of over 6,500 JSON schemas and 1,800 image-schema pairs, covering various visual domains such as UI screens and natural images. The study reveals significant gaps in the ability of current MLLMs to produce accurate outputs that comply with these schemas, indicating a need for improved structured reasoning capabilities. Additionally, the authors conduct training experiments to enhance these capabilities and plan to share the benchmark with the research community."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõ",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâËæìÂÖ•‰∏äÁöÑ‰ø°ÊÅØÊèêÂèñÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Âú®ÊñáÊú¨ÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜÂú®ËßÜËßâËæìÂÖ•ÁöÑÁªìÊûÑÂåñÁîüÊàê‰∏ä‰ªçÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÈÄöËøáËÆæËÆ°SO-BenchÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÁî®Êà∑ÁïåÈù¢„ÄÅËá™ÁÑ∂ÂõæÂÉè„ÄÅÊñáÊ°£ÂíåÂõæË°®Á≠âÂõõ‰∏™ËßÜËßâÈ¢ÜÂüüÔºå‰ΩøÁî®‰∫ÜË∂ÖËøá6500‰∏™Â§öÊ†∑ÂåñÁöÑJSONÊ®°ÂºèÂíå1800‰∏™ÁªèËøá‰∫∫Â∑•È™åËØÅÁöÑÂõæÂÉè-Ê®°ÂºèÂØπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÊ®°ÂûãÂú®ÁîüÊàêÁ¨¶ÂêàÈ¢ÑÂÆö‰πâÊï∞ÊçÆÊ®°ÂºèÁöÑËæìÂá∫ÊñπÈù¢‰ªçÊúâÂæÖÊîπËøõÔºåÂº∫Ë∞É‰∫ÜÂ§öÊ®°ÊÄÅÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõÁöÑÊèêÂçáÈúÄÊ±Ç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22688",
            "title": "Test-time scaling of diffusions with flow maps",
            "url": "https://huggingface.co/papers/2511.22688",
            "abstract": "The proposed Flow Map Trajectory Tilting (FMTT) algorithm improves diffusion models at test-time by leveraging flow maps to better align with user-specified rewards, enabling more effective sampling and image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "454c4582d6e71d10",
            "authors": [
                "Amirmojtaba Sabour",
                "Michael S. Albergo",
                "Carles Domingo-Enrich",
                "Nicholas M. Boffi",
                "Sanja Fidler",
                "Karsten Kreis",
                "Eric Vanden-Eijnden"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Courant Institute, New York University",
                "Harvard University",
                "IAIFI",
                "Kempner Institute",
                "ML Lab at Capital Fund Management (CFM)",
                "Microsoft Research",
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22688.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –∫–∞—Ä—Ç—É –ø–æ—Ç–æ–∫–∞",
                    "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Flow Map Trajectory Tilting (FMTT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç flow map –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –Ω–∞–≥—Ä–∞–¥—ã, –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É —Ç–æ–≥–æ, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ü–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å flow map –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ —Å–≤—è–∑—å —Å velocity field, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–µ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—è –ø–æ –Ω–∞–≥—Ä–∞–¥–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —Ç–æ—á–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ importance weighting, —Ç–∞–∫ –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ —Å–ª–æ–∂–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º –Ω–∞–≥—Ä–∞–¥—ã, –≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å vision language –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Flow Map Trajectory Tilting",
                    "desc": "The Flow Map Trajectory Tilting (FMTT) algorithm enhances diffusion models during testing by utilizing flow maps to align better with user-defined rewards. This method addresses the challenge of incorporating reward gradients, which are often poorly defined during the diffusion process. By directly manipulating the flow map, FMTT allows for more effective sampling and image editing, outperforming traditional methods that rely on reward gradients. The algorithm also facilitates interaction with complex reward functions, enabling innovative applications in image generation and editing."
                },
                "zh": {
                    "title": "ÊµÅÂõæËΩ®ËøπÂÄæÊñúÔºöÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÊô∫ËÉΩÈááÊ†∑‰∏éÁºñËæë",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊµÅÂõæËΩ®ËøπÂÄæÊñúÔºàFMTTÔºâÁöÑÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂà©Áî®ÊµÅÂõæÂú®ÊµãËØïÊó∂ÊîπËøõÊâ©Êï£Ê®°ÂûãÔºå‰ª•Êõ¥Â•ΩÂú∞‰∏éÁî®Êà∑ÊåáÂÆöÁöÑÂ•ñÂä±ÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÈááÊ†∑ÂíåÂõæÂÉèÁºñËæë„ÄÇ‰º†ÁªüÊñπÊ≥ïÂú®Êâ©Êï£ËøáÁ®ã‰∏≠ÂºïÂÖ•Â•ñÂä±ÁöÑÊ¢ØÂ∫¶Ôºå‰ΩÜÂ∏∏Â∏∏‰ºöÂØºËá¥ÈóÆÈ¢òÔºåÂõ†‰∏∫Áî®Êà∑ÊåáÂÆöÁöÑÂ•ñÂä±ÈÄöÂ∏∏Âè™Âú®ÁîüÊàêÁªìÊùüÊó∂ÊâçÊòéÁ°ÆÂÆö‰πâ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁõ¥Êé•‰ΩøÁî®ÊµÅÂõæÔºåÂà©Áî®ÊµÅÂõæ‰∏éÁû¨Êó∂‰º†ËæìÈÄüÂ∫¶Âú∫‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÊûÑÂª∫‰∫ÜFMTTÁÆóÊ≥ïÔºåËØÅÊòéÂÖ∂Âú®Â•ñÂä±‰∏äÊØîÊ†áÂáÜÊµãËØïÊñπÊ≥ïË°®Áé∞Êõ¥‰Ω≥„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÈÄöËøáÈáçË¶ÅÊÄßÂä†ÊùÉËøõË°åÁ≤æÁ°ÆÈááÊ†∑ÔºåÊàñÈÄöËøáÂéüÂàôÊÄßÊêúÁ¥¢ËØÜÂà´Â•ñÂä±ÂÄæÊñúÂàÜÂ∏ÉÁöÑÂ±ÄÈÉ®ÊûÅÂ§ßÂÄºÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇÂ•ñÂä±ÂáΩÊï∞‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22176",
            "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
            "url": "https://huggingface.co/papers/2511.22176",
            "abstract": "F-CoT, an input-centric approach inspired by cognitive psychology, reduces token usage in large language models by structuring context and focusing reasoning, maintaining accuracy on arithmetic problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "6190970a85fa89ff",
            "authors": [
                "Lukas Struppek",
                "Dominik Hintersdorf",
                "Hannah Struppek",
                "Daniel Neider",
                "Kristian Kersting"
            ],
            "affiliations": [
                "Centre for Cognitive Science",
                "FAR.AI",
                "German Research Center for Artificial Intelligence (DFKI)",
                "Hessian Center for AI (Hessian.AI)",
                "TU Center for Trustworthy Data Science and Security",
                "TU Dortmund University",
                "Technical University of Darmstadt",
                "University Alliance Ruhr",
                "University of Kassel"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22176.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22805",
            "title": "From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images",
            "url": "https://huggingface.co/papers/2511.22805",
            "abstract": "CogIP-Bench evaluates MLLMs on image cognitive properties, revealing a gap that post-training can bridge, enhancing human-like perception and improving creative tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "72dc20b8e577714a",
            "authors": [
                "Yiming Chen",
                "Junlin Han",
                "Tianyi Bai",
                "Shengbang Tong",
                "Filippos Kokkinos",
                "Philip Torr"
            ],
            "affiliations": [
                "HKUST",
                "New York University",
                "Oxford University",
                "University College London"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22805.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#benchmark",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CogIP-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–µ–º–æ—Å—Ç—å, —é–º–æ—Ä –∏ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º–∏ MLLM –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —ç—Ç–∏—Ö —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —ç—Ç–∞–ø –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (post-training) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —ç—Ç–æ—Ç —Ä–∞–∑—Ä—ã–≤ –∏ —É–ª—É—á—à–∞–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Å—É–∂–¥–µ–Ω–∏—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ, —á—Ç–æ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ –∑–∞–¥–∞—á–∞–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∂–µ–ª–∞–µ–º—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI's Understanding of Image Perception",
                    "desc": "The paper introduces CogIP-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of cognitive properties of images. It highlights a significant gap in current MLLMs, which can identify objects in images but struggle with subjective qualities like emotional impact and aesthetic appeal. The authors propose a post-training phase that enhances the models' alignment with human perception, allowing them to better understand and generate images that resonate on a cognitive level. This improved alignment not only aids in image generation but also enhances the models' performance in creative tasks, making AI more human-centric."
                },
                "zh": {
                    "title": "Áº©Â∞è‰∫∫Á±ª‰∏éAIÁöÑËÆ§Áü•Â∑ÆË∑ù",
                    "desc": "CogIP-Bench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂõæÂÉèËÆ§Áü•Â±ûÊÄß‰∏äÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÊ®°ÂûãÂú®ÁêÜËß£ÂõæÂÉèÁöÑ‰∏ªËßÇËÆ§Áü•Â±ûÊÄßÊñπÈù¢‰∏é‰∫∫Á±ªÊÑüÁü•Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÈÄöËøáÂêéÊúüËÆ≠ÁªÉÔºåÂèØ‰ª•ÊúâÊïàÁº©Â∞èËøô‰∏ÄÂ∑ÆË∑ùÔºå‰ªéËÄåÊèêÂçáÊ®°Âûã‰∏é‰∫∫Á±ªÂà§Êñ≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåËøôÁßçËÆ§Áü•ÂØπÈΩê‰∏ç‰ªÖÂÖ∑ÊúâÈ¢ÑÊµãÊÄßÔºåËøòÂèØ‰ª•ËΩ¨ÁßªÂà∞‰∏ãÊ∏∏ÁöÑÂàõÊÑè‰ªªÂä°‰∏≠„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19990",
            "title": "OmniRefiner: Reinforcement-Guided Local Diffusion Refinement",
            "url": "https://huggingface.co/papers/2511.19990",
            "abstract": "A detail-aware refinement framework using single-image diffusion and reinforcement learning enhances reference-guided image generation, improving detail preservation and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce , a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that  significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 –Ω–æ—è–±—Ä—è",
                "en": "November 25",
                "zh": "11Êúà25Êó•"
            },
            "hash": "eadd286d549aa44a",
            "authors": [
                "Yaoli Liu",
                "Ziheng Ouyang",
                "Shengtao Lou",
                "Yiren Song"
            ],
            "affiliations": [
                "Creatly.ai",
                "Nankai University",
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19990.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Ç–µ—Ä–∏ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –∏–∑-–∑–∞ —Å–∂–∞—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ VAE-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤: —Å–Ω–∞—á–∞–ª–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–∫–∞–¥—Ä–æ–≤–æ–≥–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ –∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞, –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "Enhancing Image Generation with Detail-Aware Refinement",
                    "desc": "This paper presents a new framework for improving image generation that uses reference images to guide the process. It addresses the common problem of losing fine details during image refinement by using a two-step approach: first, it refines the image globally while keeping its structure intact, and then it enhances local details through reinforcement learning. The framework specifically targets the preservation of textures and attributes that are often lost in traditional methods. Experiments show that this approach significantly outperforms existing models in maintaining visual consistency and detail accuracy."
                },
                "zh": {
                    "title": "ÁªÜËäÇÊÑüÁü•ÔºåÊèêÂçáÂõæÂÉèÁîüÊàêË¥®Èáè",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªÜËäÇÊÑüÁü•ÁöÑÂõæÂÉè‰øÆÂ§çÊ°ÜÊû∂ÔºåÂà©Áî®ÂçïÂõæÂÉèÊâ©Êï£ÂíåÂº∫ÂåñÂ≠¶‰π†Êù•Â¢ûÂº∫ÂèÇËÄÉÂºïÂØºÁöÑÂõæÂÉèÁîüÊàê„ÄÇÂΩìÂâçÁöÑÊâ©Êï£Ê®°ÂûãÂú®‰ΩøÁî®ÂèÇËÄÉÂõæÂÉè‰øÆÂ§çÁîüÊàêÂõæÂÉèÊó∂ÔºåÈöæ‰ª•‰øùÁïôÁªÜËá¥ÁöÑËßÜËßâÁªÜËäÇ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂØπÂçïÂõæÂÉèÊâ©Êï£ÁºñËæëÂô®ËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜËçâÂõæÂõæÂÉèÂíåÂèÇËÄÉÂõæÂÉèÔºå‰ªéËÄåÂÆûÁé∞ÂÖ®Â±Ä‰∏ÄËá¥ÁöÑ‰øÆÂ§ç„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÂ±ÄÈÉ®ÁºñËæëËÉΩÂäõÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁªÜËäÇÂáÜÁ°ÆÊÄßÂíåËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20809",
            "title": "Layer-Aware Video Composition via Split-then-Merge",
            "url": "https://huggingface.co/papers/2511.20809",
            "abstract": "A novel framework, Split-then-Merge, improves generative video composition through unsupervised learning of foreground-background interactions, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 –Ω–æ—è–±—Ä—è",
                "en": "November 25",
                "zh": "11Êúà25Êó•"
            },
            "hash": "a21d930d8067ba63",
            "authors": [
                "Ozgur Kara",
                "Yujia Chen",
                "Ming-Hsuan Yang",
                "James M. Rehg",
                "Wen-Sheng Chu",
                "Du Tran"
            ],
            "affiliations": [
                "Google",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20809.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13344",
            "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
            "url": "https://huggingface.co/papers/2511.13344",
            "abstract": "A Mixture-of-Experts framework with adaptive routing among multiple YOLOv9-T experts improves object detection performance by achieving higher mAP and AR.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 –Ω–æ—è–±—Ä—è",
                "en": "November 17",
                "zh": "11Êúà17Êó•"
            },
            "hash": "80f50dad111e2d93",
            "authors": [
                "Ori Meiraz",
                "Sharon Shalev",
                "Avishai Weizman"
            ],
            "affiliations": [
                "Ben-Gurion University of the Negev",
                "Technion, Israel Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13344.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22533",
            "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
            "url": "https://huggingface.co/papers/2511.22533",
            "abstract": "Fast3Dcache accelerates 3D diffusion model inference with minimal geometric quality degradation by using a geometry-aware caching framework with dynamic cache quotas and spatiotemporal stability criteria.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "d520e1b6cb263e94",
            "authors": [
                "Mengyu Yang",
                "Yanming Yang",
                "Chenyi Xu",
                "Chenxi Song",
                "Yufan Zuo",
                "Tong Zhao",
                "Ruibo Li",
                "Chi Zhang"
            ],
            "affiliations": [
                "AGI Lab, Westlake University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22533.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#3d",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–ë—ã—Å—Ç—Ä–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è 3D –¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fast3Dcache ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è inference –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä—è–º–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è 2D –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–∞—Ä—É—à–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å 3D –º–æ–¥–µ–ª–µ–π –∏–∑-–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç —ç—Ç—É –∑–∞–¥–∞—á—É —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∏—Å–ø–æ–ª—å–∑—É—é—Ç Predictive Caching Scheduler Constraint –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—ä—ë–º–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤–æ–∫—Å–µ–ª–µ–π –∏ Spatiotemporal Stability Criterion –¥–ª—è –æ—Ç–±–æ—Ä–∞ –Ω–∞–¥—ë–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 27% –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ 55% —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏."
                },
                "en": {
                    "title": "Accelerating 3D Diffusion with Geometry-Aware Caching",
                    "desc": "Fast3Dcache is a novel framework designed to speed up the inference process of 3D diffusion models while maintaining high geometric quality. It utilizes a geometry-aware caching system that dynamically adjusts cache quotas based on the stability of voxel features over time. By implementing a Predictive Caching Scheduler Constraint (PCSC) and a Spatiotemporal Stability Criterion (SSC), the framework effectively reuses stable features, minimizing errors that could lead to geometric inconsistencies. Experimental results demonstrate that Fast3Dcache can significantly enhance inference speed and reduce computational load, achieving notable improvements in performance metrics without compromising the structural integrity of the generated 3D shapes."
                },
                "zh": {
                    "title": "Âä†ÈÄü3DÊé®ÁêÜÔºå‰øùÊåÅÂá†‰ΩïË¥®ÈáèÁöÑÂàõÊñ∞ÊñπÊ°à",
                    "desc": "Fast3Dcache ÊòØ‰∏ÄÁßçÂá†‰ΩïÊÑüÁü•ÁºìÂ≠òÊ°ÜÊû∂ÔºåÊó®Âú®Âä†ÈÄü 3D Êâ©Êï£Ê®°ÂûãÁöÑÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅÂá†‰ΩïË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂä®ÊÄÅÁ°ÆÂÆöÁºìÂ≠òÈÖçÈ¢ùÂíåÁ©∫Èó¥Êó∂Èó¥Á®≥ÂÆöÊÄßÊ†áÂáÜÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂºÄÈîÄ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåFast3Dcache ËÉΩÂ§üÊòæËëóÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶ÔºåÊúÄÈ´òÂèØËææ 27.12% ÁöÑÂä†ÈÄüÂíå 54.8% ÁöÑ FLOPs ÂáèÂ∞ë„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂá†‰ΩïË¥®ÈáèÁöÑÈôçËß£ÈùûÂ∏∏Â∞èÔºåChamfer Ë∑ùÁ¶ªÂíå F-Score ÁöÑÂèòÂåñÂàÜÂà´‰ªÖ‰∏∫ 2.48% Âíå 1.95%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22265",
            "title": "FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning",
            "url": "https://huggingface.co/papers/2511.22265",
            "abstract": "FedRE, a federated learning framework, uses entangled representations and labels to enhance performance, privacy, and reduce communication overhead in model-heterogeneous environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "a84960de574107ae",
            "authors": [
                "Yuan Yao",
                "Lixu Wang",
                "Jiaqi Wu",
                "Jin Song",
                "Simin Chen",
                "Zehua Wang",
                "Zijian Tian",
                "Wei Chen",
                "Huixia Li",
                "Xiaoxiao Li"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "China University of Mining and Technology",
                "China University of Mining and Technology-Beijing",
                "Nanjing University of Posts and Telecommunications",
                "Northwestern University",
                "Teleinfo, CAICT",
                "Tsinghua University",
                "University of British Columbia",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22265.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19496",
            "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM",
            "url": "https://huggingface.co/papers/2511.19496",
            "abstract": "Xmodel-2.5, a 1.3-billion-parameter language model, uses maximal-update parameterization and a modified training curriculum to improve performance and efficiency, making it suitable for edge deployments.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present Xmodel-2.5, a 1.3-billion-parameter small language model designed as a drop-in agent core. Training with maximal-update parameterization (ŒºP) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied tie-word-embedding architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that switching from AdamW to Muon during the decay phase improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints). Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-23",
            "pub_date_card": {
                "ru": "23 –Ω–æ—è–±—Ä—è",
                "en": "November 23",
                "zh": "11Êúà23Êó•"
            },
            "hash": "4c94a2c9e81f5654",
            "authors": [
                "Yang Liu",
                "Xiaolong Zhong",
                "Ling Jiang"
            ],
            "affiliations": [
                "Xiaoduo AI Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19496.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16854",
            "title": "MRI Super-Resolution with Deep Learning: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2511.16854",
            "abstract": "A survey examines deep learning-based super-resolution techniques in MRI, addressing challenges and providing resources for generating high-resolution images from low-resolution scans.  \t\t\t\t\tAI-generated summary \t\t\t\t High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.   IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 –Ω–æ—è–±—Ä—è",
                "en": "November 20",
                "zh": "11Êúà20Êó•"
            },
            "hash": "b16bad51b44ffab8",
            "authors": [
                "Mohammad Khateri",
                "Serge Vasylechko",
                "Morteza Ghahremani",
                "Liam Timms",
                "Deniz Kocanaogullari",
                "Simon K. Warfield",
                "Camilo Jaimes",
                "Davood Karimi",
                "Alejandra Sierra",
                "Jussi Tohka",
                "Sila Kurugol",
                "Onur Afacan"
            ],
            "affiliations": [
                "A. I. Virtanen Institute for Molecular Sciences, Faculty of Health Sciences, University of Eastern Finland",
                "Artificial Intelligence in Medical Imaging group at the Department of Radiology, Technical University of Munich",
                "Department of Radiology, Massachusetts General Hospital",
                "Harvard Medical School and Boston Childrens Hospital"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16854.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#healthcare",
                    "#survey",
                    "#cv",
                    "#science"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –¥–µ—Ç–∞–ª—è–º: –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ú–†–¢",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ú–†–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–Ω–∏–º–∫–æ–≤ –∏–∑ –Ω–∏–∑–∫–æ—Ä–µ–∑–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∫–∞–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–∑–∏—Ü–∏–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–º–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–ª—è –ú–†–¢ —Å —É—á—ë—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ."
                },
                "en": {
                    "title": "Enhancing MRI with Deep Learning Super-Resolution",
                    "desc": "This paper surveys deep learning-based super-resolution (SR) techniques specifically for magnetic resonance imaging (MRI). It highlights the importance of generating high-resolution images from low-resolution scans to enhance diagnostic accuracy while minimizing costs and technical limitations. The authors categorize various deep learning methods, discussing their theoretical foundations, architectures, and performance metrics. Additionally, the paper identifies ongoing challenges in the field and provides valuable resources for researchers and practitioners."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶Â≠¶‰π†Âä©ÂäõMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÁöÑÊú™Êù•",
                    "desc": "ËøôÁØáËÆ∫ÊñáË∞ÉÊü•‰∫ÜÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÔºåÊó®Âú®Ëß£ÂÜ≥‰ªé‰ΩéÂàÜËæ®ÁéáÊâ´ÊèèÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÊåëÊàò„ÄÇË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÊèê‰æõ‰∫Ü‰∏ÄÁßçËÆ°ÁÆóÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†Á°¨‰ª∂ÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇËÆ∫ÊñáÂõûÈ°æ‰∫ÜMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÈáçÁÇπÂÖ≥Ê≥®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÔºåÂπ∂‰ªéËÆ°ÁÆóÊú∫ËßÜËßâ„ÄÅËÆ°ÁÆóÊàêÂÉèÂíåÈÄÜÈóÆÈ¢òÁ≠âËßíÂ∫¶ËøõË°åÂàÜÊûê„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ≥ªÁªüÁöÑÂàÜÁ±ªÊ≥ïÔºåÂπ∂Êèê‰æõ‰∫ÜÂºÄÊîæËé∑ÂèñÁöÑËµÑÊ∫êÂíåÂ∑•ÂÖ∑Ôºå‰ª•ÊîØÊåÅËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13944",
            "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
            "url": "https://huggingface.co/papers/2511.13944",
            "abstract": "A cluster-based frame selection strategy groups visually similar frames to create more representative and balanced dataset partitions, reducing information leakage in video-derived frames datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 –Ω–æ—è–±—Ä—è",
                "en": "November 17",
                "zh": "11Êúà17Êó•"
            },
            "hash": "c9bc2f22b5767e65",
            "authors": [
                "Noam Glazner",
                "Noam Tsfaty",
                "Sharon Shalev",
                "Avishai Weizman"
            ],
            "affiliations": [
                "Afeka College of Engineering",
                "Bar-Ilan University",
                "Ben-Gurion University of the Negev"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13944.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13276",
            "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
            "url": "https://huggingface.co/papers/2511.13276",
            "abstract": "A dual-backbone framework using convolutional and transformer representations with top-k pooling detects anomalies in surveillance videos with 90.7% AUC on UCF-Crime.  \t\t\t\t\tAI-generated summary \t\t\t\t We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 –Ω–æ—è–±—Ä—è",
                "en": "November 17",
                "zh": "11Êúà17Êó•"
            },
            "hash": "da02fc0df7e044a2",
            "authors": [
                "Noam Tsfaty",
                "Avishai Weizman",
                "Liav Cohen",
                "Moshe Tshuva",
                "Yehudit Aperstein"
            ],
            "affiliations": [
                "Afeka College of Engineering",
                "Ben-Gurion University of the Negev"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13276.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        }
    ],
    "link_prev": "2025-11-28.html",
    "link_next": "2025-12-02.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11Êúà28Êó•"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12Êúà2Êó•"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 1,
        "#cv": 6,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 2,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 2,
        "#diffusion": 8,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 3,
        "#low_resource": 0
    }
}