
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. October 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 октября</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-22.html">⬅️ <span id="prev-date">22.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-24.html">➡️ <span id="next-date">24.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'};
        let feedDateNext = {'ru': '24.10', 'en': '10/24', 'zh': '10月24日'};
        let feedDatePrev = {'ru': '22.10', 'en': '10/22', 'zh': '10月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.17247', 'title': 'PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction', 'url': 'https://huggingface.co/papers/2410.17247', 'abstract': 'In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom "A picture is worth a thousand words" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model. To this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs.', 'score': 43, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'b399abce7351d374', 'data': {'categories': ['#architecture', '#cv', '#inference'], 'emoji': '🔺', 'ru': {'title': 'PyramidDrop: Эффективное сокращение визуальной избыточности в LVLM', 'desc': 'Статья представляет новый метод PyramidDrop для повышения эффективности крупных визуально-языковых моделей (LVLM). Авторы обнаружили, что токены изображений становятся избыточными в глубоких слоях модели. PyramidDrop постепенно уменьшает количество токенов изображения на разных этапах обработки, создавая пирамидальную структуру. Эксперименты показывают, что метод позволяет ускорить обучение на 40% и снизить вычислительные затраты при инференсе на 55% без значительной потери производительности.'}, 'en': {'title': '"PyramidDrop: Streamlining Image Processing in Vision-Language Models"', 'desc': 'The paper discusses the challenge of handling large amounts of image data in large vision-language models (LVLMs), which can be computationally expensive. It introduces PyramidDrop, a method that reduces the number of image tokens progressively through the model layers, maintaining essential information while improving efficiency. This approach allows for significant reductions in training time and inference computations without sacrificing model performance. PyramidDrop can also be used as a standalone strategy to speed up inference, offering better performance and lower costs compared to existing methods.'}, 'zh': {'title': 'PyramidDrop：提升视觉语言模型效率的新策略', 'desc': '在大型视觉语言模型中，图像作为输入携带大量信息，但这会导致计算成本高昂。PyramidDrop是一种减少视觉冗余的策略，通过在模型的不同阶段丢弃部分图像标记来提高效率。实验表明，PyramidDrop可以在保持性能的同时显著加速训练和推理过程。此方法不仅能在训练中使用，还能作为推理加速的即插即用策略。'}}}, {'id': 'https://huggingface.co/papers/2410.17249', 'title': 'SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes', 'url': 'https://huggingface.co/papers/2410.17249', 'abstract': 'We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.', 'score': 39, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '17c17b383cd97344', 'data': {'categories': ['#3d', '#cv'], 'emoji': '✨', 'ru': {'title': 'Реалистичный рендеринг динамических зеркальных объектов', 'desc': 'SpectroMotion - это новый подход, объединяющий 3D гауссово сплаттинг (3DGS) с физически корректным рендерингом (PBR) и полями деформации для реконструкции динамических зеркальных сцен. Метод вводит технику остаточной коррекции для точного вычисления нормалей поверхности при деформации, а также деформируемую карту окружения для адаптации к изменяющимся условиям освещения. Используется стратегия обучения от грубого к точному, значительно улучшающая геометрию сцены и предсказание зеркального цвета. SpectroMotion превосходит существующие методы в синтезе фотореалистичных динамических зеркальных сцен реального мира.'}, 'en': {'title': 'SpectroMotion: Shining a New Light on Dynamic 3D Scenes', 'desc': 'SpectroMotion is a new method that improves how we create 3D images of shiny, moving objects by combining advanced techniques like 3D Gaussian Splatting and physically-based rendering. It solves previous problems with accurately showing shiny surfaces by using a special correction method to get the surface details right, even when the object moves. The method also uses a flexible map that changes with the lighting, making the scenes look more realistic. By training the model in stages from simple to complex, it becomes better at predicting both the shape and the shiny colors of the objects, outperforming other methods in creating lifelike images of moving, shiny scenes.'}, 'zh': {'title': 'SpectroMotion：动态镜面场景的真实感重建', 'desc': 'SpectroMotion 是一种新方法，结合了 3D 高斯散射和基于物理的渲染，用于重建动态镜面场景。以前的方法在处理动态场景时难以准确表示镜面表面。我们的方法通过引入残差校正技术来解决这一问题，确保在变形过程中准确计算表面法线，并使用可变形环境图适应时间变化的光照条件。我们采用由粗到细的训练策略，大大提高了场景几何和镜面颜色预测的准确性。'}}}, {'id': 'https://huggingface.co/papers/2410.17131', 'title': 'Aligning Large Language Models via Self-Steering Optimization', 'url': 'https://huggingface.co/papers/2410.17131', 'abstract': "Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization (SSO), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. SSO maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. SSO can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of SSO with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, SSO leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by SSO significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.", 'score': 19, 'issue_id': 223, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '3823bd06ecf4a69a', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Автоматическое выравнивание ИИ без участия человека', 'desc': 'Статья представляет алгоритм Self-Steering Optimization (SSO) для автоматизированного выравнивания языковых моделей. SSO генерирует качественные сигналы предпочтений без ручной разметки, поддерживая постоянный разрыв между выбранными и отвергнутыми ответами. Алгоритм улучшает онлайн и офлайн обучение политики модели, а также обучение моделей вознаграждения. Эксперименты с моделями Qwen2 и Llama3.1 показали значительное повышение производительности на шести бенчмарках.'}, 'en': {'title': 'Automated Alignment: Let Machines Learn Their Own Preferences', 'desc': "The paper introduces Self-Steering Optimization (SSO), an algorithm that autonomously generates preference signals for machine learning models without human annotation. SSO ensures these signals are accurate by maintaining a consistent gap between chosen and rejected responses, aligning with the model's learning capacity. This approach benefits both online and offline training, improving the performance of policy and reward models. The effectiveness of SSO is validated with models like Qwen2 and Llama3.1, showing significant improvements across various benchmarks."}, 'zh': {'title': '自引导优化：实现自动化对齐的新路径', 'desc': '这篇论文介绍了一种名为自引导优化（SSO）的算法，它可以在训练过程中自动生成高质量的偏好信号，而无需人工标注。SSO通过确保选择和拒绝的响应之间保持一致的差距来维持信号的准确性，并使它们都符合当前策略模型的学习能力。实验表明，SSO在不需要人工标注或外部模型的情况下，显著提高了多个基准测试的性能。SSO生成的偏好数据也显著提升了奖励模型在Rewardbench上的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.16198', 'title': 'Improve Vision Language Model Chain-of-thought Reasoning', 'url': 'https://huggingface.co/papers/2410.16198', 'abstract': "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.", 'score': 16, 'issue_id': 234, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'f0a749332feba0df', 'data': {'categories': ['#benchmark', '#interpretability', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений в визуально-языковых моделях: от коротких ответов к глубокому пониманию', 'desc': 'Статья представляет новый подход к улучшению рассуждений в визуально-языковых моделях (VLM). Авторы предлагают обогащать обучающие данные подробными объяснениями, полученными от GPT-4, и применять обучение с подкреплением для калибровки качества рассуждений. Эксперименты показывают значительное улучшение способностей VLM к рассуждениям на эталонных наборах данных. Работа подчеркивает важность включения детальных обоснований в процесс обучения моделей машинного обучения.'}, 'en': {'title': '"Boosting VLMs: From Short Answers to Deep Reasoning"', 'desc': "The paper discusses how current vision language models (VLMs) struggle with reasoning tasks due to a lack of detailed rationale in training data. To improve this, the authors propose enriching training data with rationales distilled from a more advanced model, GPT-4o, and fine-tuning VLMs. They also use reinforcement learning to enhance reasoning by comparing correct and incorrect reasoning chains and applying the Direct Preference Optimization algorithm. The study shows that these methods significantly improve the models' reasoning abilities and their ability to generalize to new tasks."}, 'zh': {'title': '细化推理，提升视觉语言模型的可信度', 'desc': '这篇论文探讨了视觉语言模型中链式推理的重要性，强调其对模型解释性和可信度的提升。研究发现，现有的训练数据多为简短注释，缺乏详细的推理过程，导致模型在复杂推理任务中的表现不佳。为解决这一问题，作者提出了从GPT-4o模型中提取推理过程以丰富训练数据，并通过强化学习进一步优化推理质量。实验结果表明，这种方法显著提高了模型在基准数据集上的推理能力，并改善了直接答案预测的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2410.16267', 'title': 'xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs', 'url': 'https://huggingface.co/papers/2410.16267', 'abstract': "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html", 'score': 14, 'issue_id': 233, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '7e29d42e819fbb96', 'data': {'categories': ['#architecture', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Эффективный анализ видео с меньшим количеством токенов', 'desc': "В статье представлена мультимодальная языковая модель xGen-MM-Vid (BLIP-3-Video) для обработки видео. Модель использует 'временной кодировщик' в дополнение к обычному визуальному токенизатору, что позволяет эффективно захватывать временную информацию из нескольких кадров. BLIP-3-Video требует значительно меньше визуальных токенов по сравнению с конкурирующими моделями. Экспериментально подтверждено, что BLIP-3-Video достигает точности ответов на вопросы по видео, сопоставимой с гораздо более крупными современными моделями, при этом являясь меньше по размеру и эффективнее."}, 'en': {'title': 'Efficient Video Understanding with Fewer Tokens', 'desc': "The paper introduces xGen-MM-Vid (BLIP-3-Video), a multimodal language model designed to efficiently process video data by capturing temporal information across multiple frames. It uses a 'temporal encoder' alongside a visual tokenizer to convert sequences of frames into a compact set of visual tokens, significantly reducing the number of tokens needed compared to other models. The model explores various temporal encoders, including spatio-temporal pooling and Token Turing Machines, to enhance its efficiency. Experimental results show that BLIP-3-Video achieves competitive video question-answering performance with fewer resources, being smaller and more efficient than larger models."}, 'zh': {'title': '高效捕捉视频时间信息的多模态语言模型', 'desc': '这篇论文介绍了一种名为xGen-MM-Vid（BLIP-3-Video）的多模态语言模型，专为视频设计，能够高效捕捉多帧的时间信息。BLIP-3-Video利用了“时间编码器”，结合传统的视觉标记器，将多帧序列映射为紧凑的视觉标记集。与其他模型相比，它使用的视觉标记数量大大减少（例如，32个对比4608个）。实验结果表明，BLIP-3-Video在视频问答准确性上与更大规模的模型相当，但其规模更小且效率更高。'}}}, {'id': 'https://huggingface.co/papers/2410.15926', 'title': 'Mitigating Object Hallucination via Concentric Causal Attention', 'url': 'https://huggingface.co/papers/2410.15926', 'abstract': "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot conversational and reasoning capabilities given multimodal queries. Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs are prone to generate textual responses not factually aligned with image inputs. Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs tend to hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. Additionally, we observe a similar effect when reversing the sequential order of visual tokens during multimodal alignment. Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances. We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens. With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination. Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.", 'score': 14, 'issue_id': 226, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '9dfb7677b0acecf1', 'data': {'categories': ['#alignment', '#cv', '#hallucinations', '#long_context', '#rlhf'], 'emoji': '👁️', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных ИИ-моделях', 'desc': 'Исследователи обнаружили, что большие мультимодальные языковые модели (LVLMs) склонны к галлюцинациям объектов из-за особенностей позиционного кодирования RoPE. Эта проблема усугубляется, когда визуальные подсказки находятся далеко от текстовых инструкций во входной последовательности. Авторы предложили метод Concentric Causal Attention (CCA) для уменьшения относительного расстояния между визуальными и текстовыми токенами. CCA значительно превзошел существующие методы по снижению галлюцинаций на нескольких бенчмарках.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs: Tackling Object Hallucination with CCA', 'desc': "The paper discusses how Large Vision Language Models (LVLMs) can sometimes generate incorrect textual responses that don't match the images they are analyzing, a problem known as object hallucination. This issue is linked to Rotary Position Encoding (RoPE), which struggles with long-term decay, causing LVLMs to hallucinate when visual cues are far from instruction tokens. The authors propose a new method called Concentric Causal Attention (CCA) to address this, which helps align visual and instruction tokens more effectively. By using CCA, the model's ability to perceive and interpret images improves, significantly reducing object hallucination compared to other methods."}, 'zh': {'title': '同心因果注意力：消除对象幻觉的有效策略', 'desc': '这篇论文研究了大型视觉语言模型（LVLMs）在处理多模态查询时出现的对象幻觉问题。研究发现，这种现象与广泛使用的旋转位置编码（RoPE）有关，因为RoPE的长期衰减导致LVLMs在视觉线索与指令标记距离较远时更容易出现幻觉。为了解决这个问题，作者提出了一种新的位置对齐策略，称为同心因果注意力（CCA），可以有效减少视觉标记与指令标记之间的相对距离。通过这种方法，模型的感知能力得到增强，对象幻觉现象得到显著缓解。'}}}, {'id': 'https://huggingface.co/papers/2410.16392', 'title': 'LLM-based Optimization of Compound AI Systems: A Survey', 'url': 'https://huggingface.co/papers/2410.16392', 'abstract': "In a compound AI system, components such as an LLM call, a retriever, a code interpreter, or tools are interconnected. The system's behavior is primarily driven by parameters such as instructions or tool definitions. Recent advancements enable end-to-end optimization of these parameters using an LLM. Notably, leveraging an LLM as an optimizer is particularly efficient because it avoids gradient computation and can generate complex code and instructions. This paper presents a survey of the principles and emerging trends in LLM-based optimization of compound AI systems. It covers archetypes of compound AI systems, approaches to LLM-based end-to-end optimization, and insights into future directions and broader impacts. Importantly, this survey uses concepts from program analysis to provide a unified view of how an LLM optimizer is prompted to optimize a compound AI system. The exhaustive list of paper is provided at https://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.", 'score': 13, 'issue_id': 239, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '45fb4ad9f9ca0b4c', 'data': {'categories': ['#survey', '#optimization', '#plp'], 'emoji': '🧠', 'ru': {'title': 'LLM как универсальный оптимизатор для сложных AI-систем', 'desc': 'Статья представляет обзор принципов и тенденций в оптимизации составных систем искусственного интеллекта с использованием языковых моделей (LLM). Авторы рассматривают архетипы составных AI-систем и подходы к их сквозной оптимизации с помощью LLM. Особое внимание уделяется эффективности LLM как оптимизатора, способного генерировать сложный код и инструкции без вычисления градиентов. В работе используются концепции анализа программ для создания единого представления о том, как LLM-оптимизатор настраивается для улучшения составной AI-системы.'}, 'en': {'title': 'Optimizing Compound AI Systems with LLMs: A New Frontier', 'desc': 'This paper explores how large language models (LLMs) can optimize complex AI systems that consist of various interconnected components like retrievers and code interpreters. It highlights the efficiency of using LLMs for end-to-end optimization, as they can generate intricate code and instructions without needing gradient calculations. The authors provide a comprehensive survey of different types of compound AI systems and the methods for LLM-based optimization. Additionally, they discuss future trends and the broader implications of using LLMs in this context, integrating concepts from program analysis for a cohesive understanding.'}, 'zh': {'title': '利用LLM优化复合AI系统的未来', 'desc': '本文探讨了复合人工智能系统中，如何利用大型语言模型（LLM）进行端到端的优化。复合AI系统的组件如LLM调用、检索器和代码解释器相互连接，其行为由参数驱动。通过使用LLM作为优化器，可以避免梯度计算，并生成复杂的代码和指令，从而提高效率。文章还总结了复合AI系统的原型、LLM优化的方法以及未来的发展方向。'}}}, {'id': 'https://huggingface.co/papers/2410.17250', 'title': 'JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation', 'url': 'https://huggingface.co/papers/2410.17250', 'abstract': 'Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.', 'score': 12, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '245b35927b3cf09b', 'data': {'categories': ['#benchmark', '#multilingual'], 'emoji': '🗾', 'ru': {'title': 'Культурно-ориентированная оценка мультимодальных моделей на японском языке', 'desc': 'Статья представляет JMMMU - первый масштабный японский бенчмарк для оценки больших мультимодальных моделей (LMM) на экспертных задачах в японском культурном контексте. Бенчмарк состоит из двух подмножеств: культурно-агностического (CA) и культурно-специфического (CS). Исследование выявило снижение производительности многих LMM при оценке на японском языке, а также их недостаточное понимание японской культуры. Авторы надеются, что эта работа поможет улучшить производительность LMM на японском языке и послужит руководством для создания качественных, культурно разнообразных бенчмарков для многоязычного развития LMM.'}, 'en': {'title': 'Bridging Language and Culture in AI: The JMMMU Benchmark', 'desc': 'The paper introduces JMMMU, a benchmark for evaluating Large Multimodal Models (LMMs) in Japanese, focusing on both culture-agnostic and culture-specific tasks. It highlights that many LMMs show reduced performance in Japanese due to language differences, and struggle with tasks requiring deep cultural understanding. The study reveals that some models perform well on general tasks but fail on culturally nuanced ones, indicating a lack of cultural depth. This work aims to improve LMMs in Japanese and guide the creation of culturally diverse benchmarks for multilingual models.'}, 'zh': {'title': '推动多语言模型的文化深度理解', 'desc': '这篇论文介绍了JMMMU，这是第一个用于评估大型多模态模型在日本文化背景下表现的基准。JMMMU包括两个子集：文化无关子集和文化特定子集。研究发现，许多模型在日语环境下表现下降，尤其是在文化特定子集上表现不佳，显示出对日本文化理解的不足。通过这项研究，作者希望推动多语言模型在日语中的表现，并为创建高标准的多文化基准提供指导。'}}}, {'id': 'https://huggingface.co/papers/2410.17215', 'title': 'MiniPLM: Knowledge Distillation for Pre-Training Language Models', 'url': 'https://huggingface.co/papers/2410.17215', 'abstract': "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.", 'score': 12, 'issue_id': 221, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '50ff5ac54c5a78a2', 'data': {'categories': ['#dataset'], 'emoji': '🧠', 'ru': {'title': 'MiniPLM: Эффективная дистилляция знаний для предобучения языковых моделей', 'desc': 'Статья представляет MiniPLM - новый фреймворк для дистилляции знаний при предварительном обучении языковых моделей. MiniPLM решает проблемы эффективности, гибкости и эффективности существующих методов, выполняя офлайн-вывод учительской модели и работая только с обучающим корпусом. Этот подход улучшает распределение обучающих данных, повышая их сложность и разнообразие. Эксперименты показывают, что MiniPLM улучшает производительность ученических моделей на различных задачах и поддерживает дистилляцию знаний между разными семействами моделей.'}, 'en': {'title': 'MiniPLM: Efficient and Flexible Knowledge Distillation for Smarter Language Models', 'desc': 'The paper introduces MiniPLM, a knowledge distillation framework designed to improve the pre-training of small language models by using the knowledge from larger models. MiniPLM enhances efficiency by performing offline teacher model inference, which reduces computational costs and allows for training multiple student models simultaneously. It also increases flexibility by working directly with the training corpus, enabling knowledge distillation across different model families. The framework improves the effectiveness of training by refining the data distribution, which helps student models learn more complex and diverse information, leading to better performance on various tasks.'}, 'zh': {'title': 'MiniPLM：高效灵活的知识蒸馏框架', 'desc': '知识蒸馏（KD）常用于通过大型教师语言模型（LMs）训练小型高性能学生语言模型。MiniPLM是一种新的KD框架，通过离线教师推理提高效率，减少训练时间成本。它仅依赖训练语料库，增强了模型间的灵活性，并通过丰富训练数据的难度和多样性提高学生模型的效果。实验表明，MiniPLM在多项任务中提升了学生模型的表现，并减少了预训练计算量。'}}}, {'id': 'https://huggingface.co/papers/2410.14649', 'title': 'EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search', 'url': 'https://huggingface.co/papers/2410.14649', 'abstract': 'The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the "importance" of a given layer towards the loss, based on assumptions such as error monotonicity, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, error monotonicity does not hold for LLMs: compressed models with lower sum of per-layer errors can perform worse than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.', 'score': 7, 'issue_id': 222, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': '8c7a7ff7f03a5de4', 'data': {'categories': ['#inference', '#optimization'], 'emoji': '🗜️', 'ru': {'title': 'EvoPress: оптимальное динамическое сжатие языковых моделей', 'desc': 'Статья представляет новый подход к динамической компрессии больших языковых моделей (LLM) под названием EvoPress. Авторы опровергают предположение о монотонности ошибок в LLM и предлагают эволюционный фреймворк с доказуемой сходимостью и низкой вычислительной сложностью. EvoPress показывает высокую эффективность при сжатии моделей Llama, Mistral и Phi, устанавливая новые стандарты в структурном прунинге, неструктурированном прореживании и квантизации с динамической разрядностью. Код реализации доступен в открытом репозитории GitHub.'}, 'en': {'title': 'EvoPress: Revolutionizing LLM Compression with Dynamic Precision', 'desc': 'The paper addresses the challenge of reducing the computational costs of large language models by introducing a new method called EvoPress for dynamic compression. Unlike traditional methods that rely on assumptions like error monotonicity, EvoPress uses an evolutionary framework to adjust compression levels per block or layer, ensuring minimal accuracy loss. This approach is proven to be optimal within a specific input range and demonstrates superior performance across various compression techniques, including pruning and quantization. The authors provide theoretical guarantees for EvoPress, which translate into practical improvements for models like Llama, Mistral, and Phi.'}, 'zh': {'title': 'EvoPress：大语言模型压缩的新突破', 'desc': '这篇论文探讨了大语言模型的压缩问题，提出了一种新的动态压缩方法。传统方法依赖于假设层的重要性来进行压缩，而这种新方法则不依赖于这些假设。作者提出了一种名为EvoPress的进化框架，能够在给定输入范围内实现最优压缩。实验结果表明，EvoPress在多种压缩方法中都达到了最新的性能水平。'}}}, {'id': 'https://huggingface.co/papers/2410.16930', 'title': "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", 'url': 'https://huggingface.co/papers/2410.16930', 'abstract': "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.", 'score': 5, 'issue_id': 226, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '13a2560869429449', 'data': {'categories': ['#interpretability', '#math'], 'emoji': '🧮', 'ru': {'title': 'Нейрохирургия для ИИ: точечное улучшение математических способностей языковых моделей', 'desc': 'Статья представляет метод MathNeuro для выделения параметров, отвечающих за математические рассуждения в больших языковых моделях (LLM). Этот метод позволяет целенаправленно улучшать математические способности модели, не затрагивая другие языковые навыки. Исследователи показали, что удаление выявленных параметров уничтожает математические способности LLM, в то время как их масштабирование улучшает производительность на 4-17% в тесте GSM8K. MathNeuro эффективен даже при использовании единственного образца данных, что открывает перспективы для дальнейших исследований в области целенаправленного вмешательства в параметры LLM.'}, 'en': {'title': '"Unlocking Math Genius in AI: Isolate, Enhance, Excel!"', 'desc': 'The paper introduces Math Neurosurgery (MathNeuro), a method to isolate math-specific parameters in Large Language Models (LLMs) using only forward passes. MathNeuro identifies and prunes parameters crucial for math reasoning without affecting general language abilities. By scaling these parameters, the method enhances math performance by 4-17% on GSM8K, while keeping non-math behavior unchanged. This approach is data-efficient, requiring only a single sample to identify math-specific parameters, paving the way for targeted improvements in math reasoning.'}, 'zh': {'title': 'MathNeuro：精准提升数学推理能力的利器', 'desc': '这篇论文介绍了一种名为MathNeuro的方法，用于在大型语言模型中隔离数学推理参数。MathNeuro通过前向传播计算参数的重要性，并去除对一般语言任务重要的参数，从而专注于数学推理。通过调整这些参数，模型的数学性能可以提高4-17%，而不影响其语言能力。MathNeuro展示了在不改变非数学行为的情况下，如何有效地提升数学推理能力。'}}}, {'id': 'https://huggingface.co/papers/2410.17241', 'title': 'Frontiers in Intelligent Colonoscopy', 'url': 'https://huggingface.co/papers/2410.17241', 'abstract': 'Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: https://github.com/ai4colonoscopy/IntelliScope.', 'score': 2, 'issue_id': 239, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'ad0212da4f7fb7b9', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#medicine'], 'emoji': '🔬', 'ru': {'title': 'Мультимодальные ИИ-технологии открывают новые горизонты в колоноскопии', 'desc': 'Это исследование посвящено изучению современных интеллектуальных методов колоноскопии и их потенциальному применению в мультимодальных медицинских приложениях. Авторы оценивают текущее состояние методов, основанных на данных и моделях, для четырех задач восприятия колоноскопических сцен: классификации, обнаружения, сегментации и понимания визуально-языковой информации. На основе этой оценки они выявляют специфические для данной области проблемы и показывают, что мультимодальные исследования в колоноскопии остаются открытыми для дальнейшего изучения. Для поддержки развития мультимодальных подходов авторы предлагают три основные инициативы: большой набор данных для мультимодальной настройки инструкций ColonINST, мультимодальную языковую модель ColonGPT, разработанную специально для колоноскопии, и мультимодальный бенчмарк.'}, 'en': {'title': 'Advancing Intelligent Colonoscopy for Better Cancer Detection', 'desc': 'This paper explores advanced techniques in intelligent colonoscopy, which is crucial for detecting colorectal cancer. It evaluates current methods in four key areas: classification, detection, segmentation, and vision-language understanding, highlighting specific challenges in the field. The authors propose three initiatives to support multimodal research, including a new dataset called ColonINST, a specialized language model named ColonGPT, and a benchmark for evaluation. Additionally, they offer a public website to keep the community informed about developments in this area.'}, 'zh': {'title': '智能结肠镜：多模态医学的未来', 'desc': '本研究探讨了智能结肠镜技术的前沿及其在多模态医学应用中的潜在影响。我们通过分类、检测、分割和视觉语言理解四个任务评估了当前的以数据和模型为中心的研究现状。研究发现，结肠镜领域的多模态研究仍有许多挑战和未开发的空间。为迎接多模态时代，我们建立了三个基础性倡议，包括大规模的多模态指令调优数据集ColonINST、专为结肠镜设计的多模态语言模型ColonGPT，以及一个多模态基准。'}}}, {'id': 'https://huggingface.co/papers/2410.16266', 'title': '3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors', 'url': 'https://huggingface.co/papers/2410.16266', 'abstract': 'Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .', 'score': 2, 'issue_id': 237, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'f00040bffceab087', 'data': {'categories': ['#3d', '#benchmark', '#dataset', '#video'], 'emoji': '🎥', 'ru': {'title': 'Улучшение синтеза новых ракурсов с помощью видео-диффузии', 'desc': 'Статья представляет 3DGS-Enhancer - новый метод улучшения качества представления сцен в задаче синтеза новых ракурсов. Авторы используют 2D видео-диффузионные приоры для решения проблемы согласованности ракурсов в 3D, переформулируя ее как достижение временной согласованности в процессе генерации видео. Метод восстанавливает согласованные латентные признаки отрендеренных новых ракурсов и интегрирует их с исходными видами через пространственно-временной декодер. Улучшенные виды затем используются для дообучения исходной 3DGS модели, значительно повышая качество рендеринга.'}, 'en': {'title': 'Enhancing 3D Views with Temporal Consistency', 'desc': 'The paper introduces 3DGS-Enhancer, a new method to improve the quality of 3D Gaussian splatting (3DGS) for novel-view synthesis, especially in challenging scenarios with sparse input views. By using 2D video diffusion priors, the approach addresses the issue of maintaining 3D view consistency by ensuring temporal consistency in video generation. The method enhances the latent features of rendered views and combines them with input views using a spatial-temporal decoder, which is then used to refine the original 3DGS model. Experiments show that 3DGS-Enhancer significantly outperforms existing methods in rendering high-quality, photorealistic scenes.'}, 'zh': {'title': '3D视图一致性的新突破：3DGS-Enhancer', 'desc': '这篇论文介绍了一种名为3DGS-Enhancer的新方法，用于提升3D高斯点云表示的质量。通过利用2D视频扩散先验，该方法解决了3D视图一致性的问题，将其重新定义为视频生成过程中的时间一致性。3DGS-Enhancer通过空间-时间解码器将渲染的新视图与输入视图结合，恢复视图一致的潜在特征。实验表明，该方法在大规模数据集上表现出色，显著提高了渲染性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents', '#agi', '#alignment (2)', '#architecture (2)', '#audio', '#benchmark (5)', '#cv (3)', '#data', '#dataset (3)', '#diffusion', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#inference (2)', '#interpretability (2)', '#long_context (1)', '#math (1)', '#medicine (1)', '#multilingual (1)', '#multimodal (2)', '#optimization (2)', '#plp (1)', '#rag', '#reasoning', '#rl', '#rlhf (3)', '#robotics', '#security', '#story_generation', '#survey (1)', '#synthetic', '#training', '#transfer_learning', '#translation', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-23 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-23 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-23 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    