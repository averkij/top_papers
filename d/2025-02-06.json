{
    "date": {
        "ru": "6 февраля",
        "en": "February 6",
        "zh": "2月6日"
    },
    "time_utc": "2025-02-06 03:14",
    "weekday": 3,
    "issue_id": 2063,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.01506",
            "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
            "url": "https://huggingface.co/papers/2502.01506",
            "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.",
            "score": 4,
            "issue_id": 2063,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 февраля",
                "en": "February 3",
                "zh": "2月3日"
            },
            "hash": "f5ec0450054af574",
            "authors": [
                "Yuzhe Yang",
                "Yifei Zhang",
                "Minghao Wu",
                "Kaidi Zhang",
                "Yunmiao Zhang",
                "Honghai Yu",
                "Yan Hu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01506.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "LLM-агенты раскрывают тайны социально-экономической динамики",
                    "desc": "Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы применяют LLM-агентов для более реалистичного моделирования человеческого поведения, учитывая когнитивные искажения и эмоциональные факторы. В экспериментах на симулированном фондовом рынке демонстрируется, как индивидуальные действия приводят к групповому поведению и эмергентным явлениям. Этот подход позволяет лучше понять взаимосвязь между индивидуальным принятием решений и коллективными социально-экономическими паттернами."
                },
                "en": {
                    "title": "Harnessing LLMs for Realistic Socio-Economic Simulations",
                    "desc": "This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns."
                },
                "zh": {
                    "title": "利用大型语言模型模拟社会经济系统的涌现现象",
                    "desc": "本研究探讨了社会涌现现象，传统的基于规则的代理模型（ABM）难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。我们提出了一种新的多代理框架TwinMarket，利用大型语言模型（LLM）来模拟社会经济系统。通过模拟股票市场环境的实验，我们展示了个体行为如何通过互动和反馈机制引发集体动态，导致金融泡沫和经济衰退等涌现现象。该方法为个体决策与集体社会经济模式之间的复杂关系提供了宝贵的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02339",
            "title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
            "url": "https://huggingface.co/papers/2502.02339",
            "abstract": "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.",
            "score": 2,
            "issue_id": 2063,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "3f3413717efb32f6",
            "authors": [
                "Jinyang Wu",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Ruihan Jin",
                "Feihu Che",
                "Zengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing",
                "Department of Automation, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02339.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AStar: Эффективное структурированное мышление для мультимодальных ИИ",
                    "desc": "Статья представляет новый подход к улучшению визуального рассуждения мультимодальных больших языковых моделей (MLLM). Авторы предлагают метод AStar, использующий автоматизированное структурированное мышление на основе поиска Монте-Карло по дереву (MCTS). AStar автоматически извлекает высокоуровневые паттерны рассуждений из ограниченных данных и интегрирует внутренние способности модели с внешними указаниями. Эксперименты показывают, что AStar достигает точности 54.0% на бенчмарке MathVerse, превосходя GPT-4o при высокой эффективности использования данных и вычислений."
                },
                "en": {
                    "title": "AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking",
                    "desc": "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."
                },
                "zh": {
                    "title": "AStar：高效的多模态推理新范式",
                    "desc": "多模态大型语言模型（MLLMs）在复杂视觉推理方面表现出色，但仍面临挑战。尽管最近的研究尝试通过引入结构化思维和教师指导来增强推理能力，但在性能和效率之间的平衡仍然困难。本文提出了一种名为AStar的自动化结构化思维范式，利用蒙特卡洛树搜索（MCTS）从有限数据中自动推导高层次的认知推理模式。AStar通过统一的推理框架，结合模型的内部推理能力和外部推理指导，实现高效推理，显著提高了准确性和数据利用效率。"
                }
            }
        }
    ],
    "link_prev": "2025-02-05.html",
    "link_next": "2025-02-07.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "05.02",
        "en": "02/05",
        "zh": "2月5日"
    },
    "short_date_next": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2月7日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了扩散桥模型（DBMs）在图像到图像翻译中的应用。DBMs虽然有前景，但推理速度慢。作者提出了一种新的蒸馏技术，可以加速DBMs的推理过程。这种方法可以处理有条件和无条件的DBMs，并且只需要使用受损图像进行训练。实验结果显示，这种技术可以将DBMs的推理速度提高4到100倍，并在某些情况下提供比原始模型更好的生成质量。",
        "title": "Inverse Bridge Matching Distillation",
        "pinyin": "这篇文章讨论了扩散桥模型（DBMs）在图像到图像翻译中的应用。DBMs虽然有前景，但推理速度慢。作者提出了一种新的蒸馏技术，可以加速DBMs的推理过程。这种方法可以处理有条件和无条件的DBMs，并且只需要使用受损图像进行训练。实验结果显示，这种技术可以将DBMs的推理速度提高4到100倍，并在某些情况下提供比原始模型更好的生成质量。\n\nZhè piān wénzhāng tǎolùn le kuòsàn qiáo móxíng (DBMs) zài túxiàng dào túxiàng fānyì zhōng de yìngyòng. DBMs suīrán yǒu qiánjǐng, dàn tuīlǐ sùdù màn. Zuòzhě tíchū le yīzhǒng xīn de zhēngliù jìshù, kěyǐ jiāsù DBMs de tuīlǐ guòchéng. Zhè zhǒng fāngfǎ kěyǐ chǔlǐ yǒu tiáojiàn hé wú tiáojiàn de DBMs, bìngqiě zhǐ xūyào shǐyòng shòusǔn túxiàng jìnxíng xùnliàn. Shíyàn jiéguǒ xiǎnshì, zhè zhǒng jìshù kěyǐ jiāng DBMs de tuīlǐ sùdù tígāo 4 dào 100 bèi, bìng zài mǒuxiē qíngkuàng xià tígōng bǐ yuánshǐ móxíng gèng hǎo de shēngchéng zhìliàng.",
        "vocab": "[{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '桥', 'pinyin': 'qiáo', 'trans': 'bridge'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '翻译', 'pinyin': 'fān yì', 'trans': 'translation'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '蒸馏', 'pinyin': 'zhēng liú', 'trans': 'distillation'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '有条件', 'pinyin': 'yǒu tiáo jiàn', 'trans': 'conditional'}, {'word': '无条件', 'pinyin': 'wú tiáo jiàn', 'trans': 'unconditional'}, {'word': '受损', 'pinyin': 'shòu sǔn', 'trans': 'damaged'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}]",
        "trans": "This article discusses the application of Diffusion Bridge Models (DBMs) in image-to-image translation. While DBMs hold promise, they suffer from slow inference speeds. The authors propose a new distillation technique that can accelerate the inference process of DBMs. This method can handle both conditional and unconditional DBMs and requires only the use of corrupted images for training. Experimental results show that this technique can increase the inference speed of DBMs by 4 to 100 times and, in some cases, provide better generation quality than the original model.",
        "update_ts": "2025-02-05 09:11"
    }
}