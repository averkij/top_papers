{
    "date": {
        "ru": "6 февраля",
        "en": "February 6",
        "zh": "2月6日"
    },
    "time_utc": "2025-02-06 06:14",
    "weekday": 3,
    "issue_id": 2066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.01506",
            "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
            "url": "https://huggingface.co/papers/2502.01506",
            "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.",
            "score": 18,
            "issue_id": 2063,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 февраля",
                "en": "February 3",
                "zh": "2月3日"
            },
            "hash": "f5ec0450054af574",
            "authors": [
                "Yuzhe Yang",
                "Yifei Zhang",
                "Minghao Wu",
                "Kaidi Zhang",
                "Yunmiao Zhang",
                "Honghai Yu",
                "Yan Hu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01506.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "LLM-агенты раскрывают тайны социально-экономической динамики",
                    "desc": "Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы применяют LLM-агентов для более реалистичного моделирования человеческого поведения, учитывая когнитивные искажения и эмоциональные факторы. В экспериментах на симулированном фондовом рынке демонстрируется, как индивидуальные действия приводят к групповому поведению и эмергентным явлениям. Этот подход позволяет лучше понять взаимосвязь между индивидуальным принятием решений и коллективными социально-экономическими паттернами."
                },
                "en": {
                    "title": "Harnessing LLMs for Realistic Socio-Economic Simulations",
                    "desc": "This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns."
                },
                "zh": {
                    "title": "利用大型语言模型模拟社会经济系统的涌现现象",
                    "desc": "本研究探讨了社会涌现现象，传统的基于规则的代理模型（ABM）难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。我们提出了一种新的多代理框架TwinMarket，利用大型语言模型（LLM）来模拟社会经济系统。通过模拟股票市场环境的实验，我们展示了个体行为如何通过互动和反馈机制引发集体动态，导致金融泡沫和经济衰退等涌现现象。该方法为个体决策与集体社会经济模式之间的复杂关系提供了宝贵的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03373",
            "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2502.03373",
            "abstract": "Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.",
            "score": 9,
            "issue_id": 2064,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 февраля",
                "en": "February 5",
                "zh": "2月5日"
            },
            "hash": "a1d00a6c8452131a",
            "authors": [
                "Edward Yeo",
                "Yuxuan Tong",
                "Morry Niu",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "IN.AI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03373.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая секреты длинных цепочек рассуждений в ИИ",
                    "desc": "Статья исследует механизмы длинных цепочек рассуждений (CoT) в больших языковых моделях (LLM). Авторы выявляют ключевые факторы, влияющие на способность моделей генерировать длинные CoT траектории через эксперименты с обучением с подкреплением (RL) и тонкой настройкой. Исследование показывает важность масштабирования вычислительных ресурсов, формирования наград и использования веб-данных для улучшения рассуждений. Результаты предоставляют практические рекомендации по оптимизации стратегий обучения для усиления длинных CoT рассуждений в LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning Power in Large Language Models",
                    "desc": "This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs."
                },
                "zh": {
                    "title": "优化训练策略，提升长推理链能力",
                    "desc": "本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02339",
            "title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
            "url": "https://huggingface.co/papers/2502.02339",
            "abstract": "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.",
            "score": 5,
            "issue_id": 2063,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "3f3413717efb32f6",
            "authors": [
                "Jinyang Wu",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Ruihan Jin",
                "Feihu Che",
                "Zengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing",
                "Department of Automation, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02339.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AStar: Эффективное структурированное мышление для мультимодальных ИИ",
                    "desc": "Статья представляет новый подход к улучшению визуального рассуждения мультимодальных больших языковых моделей (MLLM). Авторы предлагают метод AStar, использующий автоматизированное структурированное мышление на основе поиска Монте-Карло по дереву (MCTS). AStar автоматически извлекает высокоуровневые паттерны рассуждений из ограниченных данных и интегрирует внутренние способности модели с внешними указаниями. Эксперименты показывают, что AStar достигает точности 54.0% на бенчмарке MathVerse, превосходя GPT-4o при высокой эффективности использования данных и вычислений."
                },
                "en": {
                    "title": "AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking",
                    "desc": "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."
                },
                "zh": {
                    "title": "AStar：高效的多模态推理新范式",
                    "desc": "多模态大型语言模型（MLLMs）在复杂视觉推理方面表现出色，但仍面临挑战。尽管最近的研究尝试通过引入结构化思维和教师指导来增强推理能力，但在性能和效率之间的平衡仍然困难。本文提出了一种名为AStar的自动化结构化思维范式，利用蒙特卡洛树搜索（MCTS）从有限数据中自动推导高层次的认知推理模式。AStar通过统一的推理框架，结合模型的内部推理能力和外部推理指导，实现高效推理，显著提高了准确性和数据利用效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03387",
            "title": "LIMO: Less is More for Reasoning",
            "url": "https://huggingface.co/papers/2502.03387",
            "abstract": "We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as \"cognitive templates\" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.",
            "score": 2,
            "issue_id": 2066,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 февраля",
                "en": "February 5",
                "zh": "2月5日"
            },
            "hash": "ad1fa98bc3904527",
            "authors": [
                "Yixin Ye",
                "Zhen Huang",
                "Yang Xiao",
                "Ethan Chern",
                "Shijie Xia",
                "Pengfei Liu"
            ],
            "affiliations": [
                "SJTU, SII, GAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03387.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Меньше значит больше: революция в обучении языковых моделей сложным рассуждениям",
                    "desc": "Исследователи обнаружили, что сложные математические рассуждения в больших языковых моделях можно вызвать с помощью удивительно малого количества примеров. Их модель LIMO достигла впечатляющих результатов на математических тестах, используя всего 817 обучающих образцов, что значительно меньше, чем у предыдущих подходов. LIMO также продемонстрировала исключительную способность к обобщению вне распределения, превзойдя модели, обученные на гораздо большем объеме данных. На основе этих результатов авторы предлагают гипотезу LIMO, согласно которой сложные рассуждения могут возникать через минимальные, но точно организованные демонстрации когнитивных процессов в предварительно обученных моделях."
                },
                "en": {
                    "title": "Less Data, More Reasoning: The LIMO Hypothesis",
                    "desc": "This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples."
                },
                "zh": {
                    "title": "少即是多，推理能力的新发现",
                    "desc": "本文提出了一项重要发现，挑战了我们对大型语言模型中复杂推理能力产生机制的理解。传统观点认为，复杂推理任务需要大量的训练数据，但我们证明只需少量示例即可有效引发复杂的数学推理能力。我们的模型LIMO在数学推理方面表现出前所未有的性能，使用仅817个训练样本，分别在AIME和MATH上达到了57.1%和94.8%的准确率。我们提出的“少即是多推理假设”表明，在基础模型中，经过充分编码的领域知识可以通过精心设计的少量示例来激发复杂推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02737",
            "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model",
            "url": "https://huggingface.co/papers/2502.02737",
            "abstract": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.",
            "score": 1,
            "issue_id": 2066,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "c78fe4c39300443d",
            "authors": [
                "Loubna Ben Allal",
                "Anton Lozhkov",
                "Elie Bakouch",
                "Gabriel Martín Blázquez",
                "Guilherme Penedo",
                "Lewis Tunstall",
                "Andrés Marafioti",
                "Hynek Kydlíček",
                "Agustín Piqueres Lajarín",
                "Vaibhav Srivastav",
                "Joshua Lochner",
                "Caleb Fahlgren",
                "Xuan-Son Nguyen",
                "Clémentine Fourrier",
                "Ben Burtenshaw",
                "Hugo Larcher",
                "Haojun Zhao",
                "Cyril Zakka",
                "Mathieu Morlon",
                "Colin Raffel",
                "Leandro von Werra",
                "Thomas Wolf"
            ],
            "affiliations": [
                "HuggingFaceTB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02737.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#low_resource",
                    "#training",
                    "#small_models"
                ],
                "emoji": "🤏",
                "ru": {
                    "title": "Большие возможности в маленьком пакете: SmolLM2 - компактная языковая модель с впечатляющей производительностью",
                    "desc": "Статья описывает разработку SmolLM2 - современной 'маленькой' языковой модели с 1,7 миллиардами параметров. Модель обучалась на ~11 триллионах токенов данных с использованием многоэтапного процесса, сочетающего веб-тексты со специализированными данными по математике, коду и выполнению инструкций. Авторы также представили новые специализированные наборы данных и провели эксперименты для оптимизации процесса обучения. В результате SmolLM2 превзошла другие современные малые языковые модели, такие как Qwen2.5-1.5B и Llama3.2-1B."
                },
                "en": {
                    "title": "SmolLM2: Efficient Language Modeling for Resource-Constrained Environments",
                    "desc": "This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research."
                },
                "zh": {
                    "title": "小型语言模型的强大突破",
                    "desc": "本论文介绍了SmolLM2的开发，这是一个具有17亿参数的小型语言模型。为了实现强大的性能，我们在约11万亿个数据上进行了过度训练，采用了多阶段训练过程，结合了网络文本、数学、代码和指令跟随数据。我们还引入了新的专用数据集，以解决现有数据集规模小或质量低的问题。最终，我们证明SmolLM2在性能上超越了其他近期的小型语言模型，如Qwen2.5-1.5B和Llama3.2-1B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03275",
            "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning",
            "url": "https://huggingface.co/papers/2502.03275",
            "abstract": "Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.",
            "score": 0,
            "issue_id": 2066,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 февраля",
                "en": "February 5",
                "zh": "2月5日"
            },
            "hash": "f94d674e0f57dcf9",
            "authors": [
                "DiJia Su",
                "Hanlin Zhu",
                "Yingchen Xu",
                "Jiantao Jiao",
                "Yuandong Tian",
                "Qinqing Zheng"
            ],
            "affiliations": [
                "Meta AI",
                "UC Berkeley",
                "UCL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03275.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гибридное представление рассуждений: эффективность через абстракцию",
                    "desc": "Данная статья предлагает гибридный подход к представлению процесса рассуждений в больших языковых моделях (LLM). Авторы используют латентные дискретные токены, генерируемые VQ-VAE, для частичной абстракции начальных шагов рассуждения, что значительно сокращает длину входных данных. Метод применяется как при обучении модели с нуля, так и при дообучении существующих LLM на гибридных данных с расширенным словарем. Предложенный подход превосходит базовые методы в различных тестах на логические и математические рассуждения."
                },
                "en": {
                    "title": "Streamlining Reasoning with Hybrid Token Representations",
                    "desc": "This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "优化推理过程，提升模型效率",
                    "desc": "本文探讨了大型语言模型（LLMs）在推理和规划中的应用，特别是在链式思维（CoT）数据训练时的表现。我们提出了一种混合表示法，通过使用VQ-VAE生成的潜在离散标记，部分抽象化初始推理步骤，从而显著减少推理过程的长度。我们在两个场景中探索了潜在追踪抽象的使用：一是从头开始训练模型解决钥匙寻找迷宫问题，二是对LLMs进行微调以处理逻辑和数学推理问题。我们的训练方法通过随机混合潜在标记和文本标记，促进了对新潜在标记的快速适应，且在多个基准测试中表现优于基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.01618",
            "title": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods",
            "url": "https://huggingface.co/papers/2502.01618",
            "abstract": "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.",
            "score": 0,
            "issue_id": 2065,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 февраля",
                "en": "February 3",
                "zh": "2月3日"
            },
            "hash": "c9971916eb027101",
            "authors": [
                "Isha Puri",
                "Shivchander Sudalairaj",
                "Guangxuan Xu",
                "Kai Xu",
                "Akash Srivastava"
            ],
            "affiliations": [
                "MIT CSAIL",
                "Red Hat AI Innovation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01618.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#math",
                    "#inference"
                ],
                "emoji": "🎲",
                "ru": {
                    "title": "Вероятностный подход к масштабированию вывода LLM",
                    "desc": "Статья представляет новый подход к масштабированию вычислений во время вывода для больших языковых моделей (LLM). Вместо оптимизации с помощью моделей вознаграждения, авторы рассматривают задачу как вероятностный вывод, используя методы Монте-Карло на основе частиц. Эмпирическая оценка показывает, что предложенный метод имеет в 4-16 раз лучшую скорость масштабирования по сравнению с детерминированными аналогами на сложных задачах математических рассуждений. Исследование демонстрирует, как небольшие модели могут достичь точности крупных моделей при меньшем количестве прогонов."
                },
                "en": {
                    "title": "Revolutionizing Inference: Probabilistic Scaling for LLMs",
                    "desc": "This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance."
                },
                "zh": {
                    "title": "推理时间扩展的新方法：概率推理与粒子采样结合",
                    "desc": "大型语言模型（LLMs）通过增加模型规模和数据量取得了显著的性能提升。然而，最近的研究表明，这种方法的收益递减，促使我们考虑在推理时增加计算量。现有的推理时间扩展方法通常将任务视为搜索问题，容易受到奖励模型的近似误差影响而导致奖励操控。本文提出了一种新的推理时间扩展方法，通过适应基于粒子的蒙特卡洛方法，将推理时间扩展视为概率推理任务，从而在各种数学推理任务中实现了更好的扩展率。"
                }
            }
        }
    ],
    "link_prev": "2025-02-05.html",
    "link_next": "2025-02-07.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "05.02",
        "en": "02/05",
        "zh": "2月5日"
    },
    "short_date_next": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2月7日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了扩散桥模型（DBMs）在图像到图像翻译中的应用。DBMs虽然有前景，但推理速度慢。作者提出了一种新的蒸馏技术，可以加速DBMs的推理过程。这种方法可以处理有条件和无条件的DBMs，并且只需要使用受损图像进行训练。实验结果显示，这种技术可以将DBMs的推理速度提高4到100倍，并在某些情况下提供比原始模型更好的生成质量。",
        "title": "Inverse Bridge Matching Distillation",
        "pinyin": "这篇文章讨论了扩散桥模型（DBMs）在图像到图像翻译中的应用。DBMs虽然有前景，但推理速度慢。作者提出了一种新的蒸馏技术，可以加速DBMs的推理过程。这种方法可以处理有条件和无条件的DBMs，并且只需要使用受损图像进行训练。实验结果显示，这种技术可以将DBMs的推理速度提高4到100倍，并在某些情况下提供比原始模型更好的生成质量。\n\nZhè piān wénzhāng tǎolùn le kuòsàn qiáo móxíng (DBMs) zài túxiàng dào túxiàng fānyì zhōng de yìngyòng. DBMs suīrán yǒu qiánjǐng, dàn tuīlǐ sùdù màn. Zuòzhě tíchū le yīzhǒng xīn de zhēngliù jìshù, kěyǐ jiāsù DBMs de tuīlǐ guòchéng. Zhè zhǒng fāngfǎ kěyǐ chǔlǐ yǒu tiáojiàn hé wú tiáojiàn de DBMs, bìngqiě zhǐ xūyào shǐyòng shòusǔn túxiàng jìnxíng xùnliàn. Shíyàn jiéguǒ xiǎnshì, zhè zhǒng jìshù kěyǐ jiāng DBMs de tuīlǐ sùdù tígāo 4 dào 100 bèi, bìng zài mǒuxiē qíngkuàng xià tígōng bǐ yuánshǐ móxíng gèng hǎo de shēngchéng zhìliàng.",
        "vocab": "[{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '桥', 'pinyin': 'qiáo', 'trans': 'bridge'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '翻译', 'pinyin': 'fān yì', 'trans': 'translation'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '蒸馏', 'pinyin': 'zhēng liú', 'trans': 'distillation'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '有条件', 'pinyin': 'yǒu tiáo jiàn', 'trans': 'conditional'}, {'word': '无条件', 'pinyin': 'wú tiáo jiàn', 'trans': 'unconditional'}, {'word': '受损', 'pinyin': 'shòu sǔn', 'trans': 'damaged'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}]",
        "trans": "This article discusses the application of Diffusion Bridge Models (DBMs) in image-to-image translation. While DBMs hold promise, they suffer from slow inference speeds. The authors propose a new distillation technique that can accelerate the inference process of DBMs. This method can handle both conditional and unconditional DBMs and requires only the use of corrupted images for training. Experimental results show that this technique can increase the inference speed of DBMs by 4 to 100 times and, in some cases, provide better generation quality than the original model.",
        "update_ts": "2025-02-05 09:11"
    }
}