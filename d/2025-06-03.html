
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. June 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 Ğ¸ÑĞ½Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-02.html">â¬…ï¸ <span id="prev-date">02.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-04.html">â¡ï¸ <span id="next-date">04.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'};
        let feedDateNext = {'ru': '04.06', 'en': '06/04', 'zh': '6æœˆ4æ—¥'};
        let feedDatePrev = {'ru': '02.06', 'en': '06/02', 'zh': '6æœˆ2æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.01049', 'title': 'Taming LLMs by Scaling Learning Rates with Gradient Grouping', 'url': 'https://huggingface.co/papers/2506.01049', 'abstract': 'SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.', 'score': 12, 'issue_id': 4087, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '350401d748400bad', 'authors': ['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu'], 'affiliations': ['Peking University', 'The Hong Kong University of Science and Technology', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01049.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'SGG: Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SGG (Scaling with Gradient Grouping). SGG Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SGG Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'SGG: Optimizing Learning Rates for Better LLM Training', 'desc': 'This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions.'}, 'zh': {'title': 'SGGï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨', 'desc': 'SGGæ˜¯ä¸€ç§ä¼˜åŒ–å™¨åŒ…è£…å™¨ï¼Œé€šè¿‡å¯¹æ¢¯åº¦è¿›è¡Œåˆ†ç»„å’Œåº”ç”¨ç‰¹å®šäºé›†ç¾¤çš„ç¼©æ”¾ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚å®ƒè§£å†³äº†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚SGGé€šè¿‡åŠ¨æ€åˆ†ç»„å’Œé›†ç¾¤ç‰¹å®šçš„ç¼©æ”¾æ¥æ”¹å–„å­¦ä¹ ç‡ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„å‚æ•°è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGGä¸ç°æœ‰ä¼˜åŒ–å™¨æ— ç¼é›†æˆï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šæä¾›äº†ä¸€è‡´çš„æ€§èƒ½æå‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23001', 'title': 'DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors', 'url': 'https://huggingface.co/papers/2505.23001', 'abstract': 'DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'cd584a75fce48ae2', 'authors': ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2505.23001.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#leakage'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'DyePack: Ğ›Ğ¾Ğ²ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ğ½ĞµÑ‡ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'DyePack - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ‚Ğ¸Ğ¿Ğ° backdoor Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ backdoor Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ½Ğ° Ğ½Ğ¸Ñ…. DyePack Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ²Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'DyePack: Safeguarding Model Integrity with Backdoor Detection', 'desc': 'DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates.'}, 'zh': {'title': 'DyePackï¼šç²¾å‡†è¯†åˆ«è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹', 'desc': 'DyePackæ˜¯ä¸€ä¸ªåˆ©ç”¨åé—¨æ”»å‡»çš„æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«åœ¨è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥è‰¯æ€§åé—¨æ ·æœ¬ï¼Œç¡®ä¿å‡†ç¡®çš„å‡é˜³æ€§ç‡ï¼ŒåŒæ—¶é˜²æ­¢é”™è¯¯æŒ‡æ§ã€‚DyePackçš„è®¾è®¡ç»“åˆäº†å¤šä¸ªå…·æœ‰éšæœºç›®æ ‡çš„åé—¨ï¼Œä½¿å¾—åœ¨æ ‡è®°æ¯ä¸ªæ¨¡å‹æ—¶èƒ½å¤Ÿç²¾ç¡®è®¡ç®—å‡é˜³æ€§ç‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒDyePackæˆåŠŸæ£€æµ‹åˆ°æ‰€æœ‰å—æ±¡æŸ“çš„æ¨¡å‹ï¼Œä¸”å‡é˜³æ€§ç‡æä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00577', 'title': 'Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs', 'url': 'https://huggingface.co/papers/2506.00577', 'abstract': 'Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .', 'score': 4, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '0ed5d0b7064f9962', 'authors': ['Yufa Zhou', 'Shaobo Wang', 'Xingyu Dong', 'Xiangqi Jin', 'Yifang Chen', 'Yue Min', 'Kexin Yang', 'Xingzhang Ren', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, Shanghai Jiao Tong University', 'Qwen Team, Alibaba Group', 'The University of Chicago', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.00577.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#agents', '#open_source', '#reasoning', '#games', '#dataset'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Recon - 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Economic Reasoning in LLMs through Post-Training Techniques', 'desc': "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."}, 'zh': {'title': 'åè®­ç»ƒæŠ€æœ¯æå‡æ™ºèƒ½ä½“æ¨ç†ä¸ç»æµç†æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åè®­ç»ƒæŠ€æœ¯å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›å’Œç»æµç†æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé’ˆå¯¹ç»æµæ¨ç†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¼•å…¥Reconæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨é«˜è´¨é‡ç»æµæ¨ç†é—®é¢˜çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†åè®­ç»ƒï¼Œå¹¶åœ¨ç»æµæ¨ç†åŸºå‡†å’Œå¤šæ™ºèƒ½ä½“æ¸¸æˆä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡åè®­ç»ƒçš„æ¨¡å‹åœ¨ç»“æ„åŒ–æ¨ç†å’Œç»æµç†æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†é¢†åŸŸå¯¹é½çš„åè®­ç»ƒåœ¨å¢å¼ºæ¨ç†å’Œæ™ºèƒ½ä½“å¯¹é½æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'url': 'https://huggingface.co/papers/2505.23977', 'abstract': 'VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'score': 4, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'fef2cab0e56bc9bd', 'authors': ['Yichen Feng', 'Zhangchen Xu', 'Fengqing Jiang', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23977.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#reasoning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'VisualSphinx - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° VisualSphinx ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° VisualSphinx, Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualSphinx', 'desc': 'VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry.'}, 'zh': {'title': 'VisualSphinxï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›', 'desc': 'VisualSphinxæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºé€»è¾‘æ¨ç†ä»»åŠ¡ï¼Œè§£å†³äº†å½“å‰æ¨¡å‹ç¼ºä¹ç»“æ„åŒ–è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é€šè¿‡è§„åˆ™åˆ°å›¾åƒçš„åˆæˆæµç¨‹ï¼ŒVisualSphinxèƒ½å¤Ÿç”Ÿæˆä¸é—®é¢˜ç›¸å…³çš„å›¾åƒï¼Œå¢å¼ºæ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ã€ä»£æ•°æ¨ç†ã€ç®—æœ¯æ¨ç†å’Œå‡ ä½•æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23504', 'title': 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.23504', 'abstract': 'VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.', 'score': 4, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'c243189c9ec32d1f', 'authors': ['Liyun Zhu', 'Qixiang Chen', 'Xi Shen', 'Xiaodong Cun'], 'affiliations': ['Australian National University', 'GVC Lab, Great Bay University', 'Intellindust AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.23504.jpg', 'data': {'categories': ['#rl', '#interpretability', '#multimodal', '#reasoning', '#video', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸', 'desc': 'VAU-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VAU-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° VAU-R1 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench', 'desc': 'The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts.'}, 'zh': {'title': 'æå‡è§†é¢‘å¼‚å¸¸æ¨ç†çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'VAU-R1 æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆReinforcement Fine-Tuningï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº† VAU-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè§†é¢‘å¼‚å¸¸æ¨ç†çš„é“¾å¼æ€ç»´åŸºå‡†ï¼ŒåŒ…å«å¤šé¡¹é€‰æ‹©é—®ç­”ã€è¯¦ç»†æ¨ç†ã€æ—¶é—´æ ‡æ³¨å’Œæè¿°æ€§æ ‡é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAU-R1 åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00643', 'title': 'SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions', 'url': 'https://huggingface.co/papers/2506.00643', 'abstract': "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.", 'score': 3, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': 'f95c367c9eaf00a9', 'authors': ['Weijie Xu', 'Shixian Cui', 'Xi Fang', 'Chi Xue', 'Stephanie Eckman', 'Chandan Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2506.00643.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#interpretability', '#data', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SATA-BENCH - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ»Ğ¸ÑˆÑŒ 41.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Choice Funnel, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼.'}, 'en': {'title': 'Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel', 'desc': 'The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks.'}, 'zh': {'title': 'æå‡å¤šç­”æ¡ˆæ¨ç†çš„å‡†ç¡®æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SATA-BENCHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç­”æ¡ˆé—®é¢˜ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨é€‰æ‹©æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶å­˜åœ¨æ˜¾è‘—çš„é€‰æ‹©åå·®å’Œè®¡æ•°åå·®ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Choice Funnelè§£ç ç­–ç•¥ï¼Œé€šè¿‡å»åå’Œè‡ªé€‚åº”é˜ˆå€¼å¼•å¯¼æ¨¡å‹åšå‡ºæ›´å®Œæ•´å’Œå‡†ç¡®çš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChoice Funnelåœ¨å‡†ç¡®åŒ¹é…ç‡ä¸Šæ¯”ç«äº‰åŸºçº¿æé«˜äº†29%ï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬è¶…è¿‡64%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors', 'url': 'https://huggingface.co/papers/2505.24625', 'abstract': "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'score': 3, 'issue_id': 4087, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8bfa132788ee6990', 'authors': ['Duo Zheng', 'Shijia Huang', 'Yanyang Li', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.24625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#games', '#architecture', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D Geometry Large Language Model (VG LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², VG LLM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ¼ ÑĞ²ĞµÑ€Ñ…Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ 3D-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VG LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding from Video Alone!', 'desc': 'The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data.'}, 'zh': {'title': 'è§†é¢‘é©±åŠ¨çš„3Dç†è§£æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘-3Då‡ ä½•å¤§è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ä»è§†é¢‘åºåˆ—ä¸­æå–3Dä¿¡æ¯ï¼Œä»è€Œå¢å¼º3Dåœºæ™¯ç†è§£ï¼Œè€Œæ— éœ€é¢å¤–çš„3Dæ•°æ®ã€‚è¯¥æ¨¡å‹åˆ©ç”¨3Dè§†è§‰å‡ ä½•ç¼–ç å™¨ï¼Œä»è§†é¢‘ä¸­æå–3Då…ˆéªŒä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æ ‡è®°ç»“åˆï¼Œè¾“å…¥åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„4Bæ¨¡å‹åœ¨ä¸ä¾èµ–æ˜¾å¼3Dæ•°æ®è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾çš„ç»“æœï¼Œç”šè‡³åœ¨VSI-Benchè¯„ä¼°ä¸­è¶…è¶Šäº†Gemini-1.5-Proã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01881', 'title': 'WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue', 'url': 'https://huggingface.co/papers/2506.01881', 'abstract': 'STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.', 'score': 2, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'e82ff37de8341d1a', 'authors': ['Yaoyao Qian', 'Jindan Huang', 'Yuanli Wang', 'Simon Yu', 'Kyrie Zhixuan Zhou', 'Jiayuan Mao', 'Mingfu Liang', 'Hanhan Zhou'], 'affiliations': ['Boston University, Boston, MA', 'George Washington University, Washington, DC', 'Massachusetts Institute of Technology, Cambridge, MA', 'Northeastern University, Boston, MA', 'Northwestern University, Evanston, IL', 'Tufts University, Medford, MA', 'University of Texas at San Antonio, San Antonio, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.01881.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#alignment', '#agents'], 'emoji': 'ğŸŒªï¸', 'ru': {'title': 'STORM: Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº STORM Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. STORM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - UserLLM Ğ¸ AgentLLM - Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Enhancing Dialogue Systems through Collaborative Intent Formation', 'desc': 'The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions.'}, 'zh': {'title': 'STORMï¼šä¿ƒè¿›äººæœºåä½œçš„æ„å›¾å½¢æˆ', 'desc': 'STORMæ¡†æ¶é€šè¿‡å»ºæ¨¡ç”¨æˆ·å’Œä»£ç†ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°åŠ¨æ€ï¼Œä¿ƒè¿›äº†ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„åä½œæ„å›¾å½¢æˆã€‚ç”¨æˆ·çš„è¡¨è¾¾è™½ç„¶åœ¨è¯­è¨€ä¸Šå®Œæ•´ï¼Œä½†å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ‰€éœ€çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´ç³»ç»Ÿæ— æ³•æ­£ç¡®å“åº”ã€‚STORMæ¡†æ¶èƒ½å¤Ÿæ•æ‰è¡¨è¾¾è½¨è¿¹å’Œæ½œåœ¨çš„è®¤çŸ¥è½¬å˜ï¼Œä»è€Œç³»ç»ŸåŒ–åˆ†æåä½œç†è§£çš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé€‚åº¦çš„ä¸ç¡®å®šæ€§ï¼ˆ40-60%ï¼‰å¯ä»¥ä¼˜äºå®Œå…¨é€æ˜çš„ä¿¡æ¯ï¼Œè¿™ä¸ºäººæœºåä½œä¸­çš„ä¿¡æ¯å®Œæ•´æ€§æä¾›äº†æ–°çš„æ€è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00338', 'title': 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning', 'url': 'https://huggingface.co/papers/2506.00338', 'abstract': 'The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.', 'score': 2, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '2f4783eb2db68192', 'authors': ['Yifan Peng', 'Shakeel Muhammad', 'Yui Sudo', 'William Chen', 'Jinchuan Tian', 'Chyi-Jiunn Lin', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University, United States', 'Honda Research Institute Japan, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.00338.jpg', 'data': {'categories': ['#training', '#low_resource', '#open_source', '#multilingual', '#data', '#audio', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ OWSM ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 166 000 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 75 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ OWSM v4, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Whisper Ğ¸ MMS, Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Multilingual Speech Models with Cleaned Web Data', 'desc': 'The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„å¼€åˆ›æ€§è¿›å±•', 'desc': 'OWSMé¡¹ç›®é€šè¿‡æ•´åˆä¸€ä¸ªå¤§å‹æ¸…æ´—è¿‡çš„ç½‘ç»œæ•°æ®é›†YODASï¼Œæå‡äº†å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„æ€§èƒ½ã€‚YODASæ•°æ®é›†åŒ…å«äº†å¤§é‡çš„è¯­éŸ³æ•°æ®ï¼Œä½†ç”±äºå…¶åŸå§‹ç‰¹æ€§ï¼Œå­˜åœ¨è¯­è¨€æ ‡ç­¾é”™è¯¯å’ŒéŸ³é¢‘æ–‡æœ¬ä¸å¯¹é½ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«75ç§è¯­è¨€ã€166,000å°æ—¶è¯­éŸ³çš„æ•°æ®é›†ã€‚æ–°çš„OWSM v4æ¨¡å‹åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨å¤šä¸ªåœºæ™¯ä¸­ä¸é¢†å…ˆçš„å·¥ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01943', 'title': 'Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control', 'url': 'https://huggingface.co/papers/2506.01943', 'abstract': 'Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.', 'score': 1, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '4acb1e4fc9635b8a', 'authors': ['Xiao Fu', 'Xintao Wang', 'Xian Liu', 'Jianhong Bai', 'Runsen Xu', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01943.jpg', 'data': {'categories': ['#robotics', '#video', '#diffusion'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboMaster: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RoboMaster Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´Ğ¾, Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling', 'desc': "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."}, 'zh': {'title': 'RoboMasterï¼šæå‡æœºå™¨äººæ“ä½œçš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRoboMasterçš„æ–°æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡å¤šç‰©ä½“ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œä»¥æ”¹å–„æœºå™¨äººå†³ç­–æ•°æ®çš„ç”Ÿæˆã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒRoboMasterå°†äº¤äº’è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå­é˜¶æ®µï¼šé¢„äº¤äº’ã€äº¤äº’å’Œåäº¤äº’ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸»å¯¼ç‰©ä½“çš„ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRoboMasteræœ‰æ•ˆåœ°è§£å†³äº†å¤šç‰©ä½“ç‰¹å¾èåˆå¸¦æ¥çš„é—®é¢˜ï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'score': 1, 'issue_id': 4088, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'c7587646c2140ddd', 'authors': ['Zafir Stojanovski', 'Oliver Stanley', 'Joe Sharratt', 'Richard Jones', 'Abdulhakeem Adefioye', 'Jean Kaddour', 'Andreas KÃ¶pf'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24760.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#games', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ˜Ğ˜ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Reasoning Gym (RG) - ÑÑ‚Ğ¾ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ°, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ¸Ğ³Ñ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ RG - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RG ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlock Infinite Reasoning with Reasoning Gym!', 'desc': 'The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings.'}, 'zh': {'title': 'æ¨ç†è®­ç»ƒåœºï¼šæ— é™ç”Ÿæˆï¼ŒæŒç»­è¯„ä¼°', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†æ¨ç†è®­ç»ƒåœºï¼ˆReasoning Gymï¼ŒRGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†ç¯å¢ƒåº“ï¼Œå…·æœ‰å¯éªŒè¯çš„å¥–åŠ±ã€‚å®ƒæä¾›äº†è¶…è¿‡100ä¸ªæ•°æ®ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œæ¶µç›–ä»£æ•°ã€ç®—æœ¯ã€è®¡ç®—ã€è®¤çŸ¥ã€å‡ ä½•ã€å›¾è®ºã€é€»è¾‘å’Œå„ç§å¸¸è§æ¸¸æˆç­‰å¤šä¸ªé¢†åŸŸã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºèƒ½å¤Ÿç”Ÿæˆå‡ ä¹æ— é™çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥è°ƒæ•´å¤æ‚æ€§ï¼Œè¿™ä¸å¤§å¤šæ•°å›ºå®šçš„æ¨ç†æ•°æ®é›†ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRGåœ¨è¯„ä¼°å’Œå¼ºåŒ–å­¦ä¹ æ¨ç†æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23907', 'title': 'Cora: Correspondence-aware image editing using few step diffusion', 'url': 'https://huggingface.co/papers/2505.23907', 'abstract': 'Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.', 'score': 1, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '7de1457440a0b449', 'authors': ['Amirhossein Almohammadi', 'Aryan Mikaeili', 'Sauradip Nag', 'Negar Hassanpour', 'Andrea Tagliasacchi', 'Ali Mahdavi-Amiri'], 'affiliations': ['Google Deepmind, Canada', 'Huawei, Canada', 'Simon Fraser University, Canada', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.23907.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€', 'desc': 'Cora - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°. Cora Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾.'}, 'en': {'title': 'Cora: Revolutionizing Image Editing with Precision and Control', 'desc': 'The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods.'}, 'zh': {'title': 'Coraï¼šå›¾åƒç¼–è¾‘çš„æ–°çªç ´', 'desc': 'Coraæ¡†æ¶é€šè¿‡å¼•å…¥å¯¹åº”æ„ŸçŸ¥å™ªå£°æ ¡æ­£å’Œæ’å€¼æ³¨æ„åŠ›å›¾ï¼Œå¢å¼ºäº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚å®ƒèƒ½å¤Ÿåœ¨æºå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å¯¹é½çº¹ç†å’Œç»“æ„ï¼Œä»è€Œå®ç°å‡†ç¡®çš„çº¹ç†è½¬ç§»å’Œå¿…è¦çš„æ–°å†…å®¹ç”Ÿæˆã€‚Coraåœ¨å†…å®¹ç”Ÿæˆå’Œä¿ç•™ä¹‹é—´æä¾›äº†è‰¯å¥½çš„æ§åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å§¿æ€å˜åŒ–ã€ç‰©ä½“æ·»åŠ å’Œçº¹ç†ç»†åŒ–ç­‰å¤šç§ç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoraåœ¨ç»“æ„ã€çº¹ç†å’Œèº«ä»½çš„ä¿æŒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿè¯å®äº†å…¶ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23590', 'title': 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles', 'url': 'https://huggingface.co/papers/2505.23590', 'abstract': 'The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.', 'score': 1, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '583be7c36c626f1e', 'authors': ['Zifu Wang', 'Junyi Zhu', 'Bo Tang', 'Zhiyu Li', 'Feiyu Xiong', 'Jiaqian Yu', 'Matthew B. Blaschko'], 'affiliations': ['ESAT-PSI, KU Leuven', 'Institute for Advanced Algorithms Research, Shanghai', 'Memory Tensor, Shanghai', 'Samsung R&D Institute China, Beijing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23590.jpg', 'data': {'categories': ['#multimodal', '#training', '#transfer_learning', '#rl', '#cv', '#open_source', '#reasoning', '#games'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ğ·Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº-Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ğ·Ğ»Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Learning with Jigsaw Puzzles and RL', 'desc': 'This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks.'}, 'zh': {'title': 'åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ–°å‘ç°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ‹¼å›¾ä½œä¸ºå®éªŒæ¡†æ¶ï¼Œå‘ç°ç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨ç®€å•æ‹¼å›¾ä¸Šçš„è¡¨ç°ä»éšæœºçŒœæµ‹æå‡è‡³æ¥è¿‘å®Œç¾çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°å¤æ‚çš„æœªè§é…ç½®ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ¨¡å‹å¯ä»¥åœ¨æœ‰æ— æ˜¾å¼æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ³›åŒ–ï¼Œå°½ç®¡å¼€æºæ¨¡å‹æ›´å€¾å‘äºç›´æ¥å›ç­”ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”åˆå§‹çš„ç›‘ç£å¾®è°ƒå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šå¦¨ç¢åç»­çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23059', 'title': 'From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval', 'url': 'https://huggingface.co/papers/2505.23059', 'abstract': 'State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.', 'score': 1, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '83af42c01de2e64c', 'authors': ['Dohyeon Lee', 'Yeonseok Jeong', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23059.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SMR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State Machine Reasoning (SMR). SMR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SMR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° 3.4% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 74.4%.'}, 'en': {'title': 'Streamlining Retrieval with State Machine Reasoning', 'desc': 'State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments.'}, 'zh': {'title': 'çŠ¶æ€æœºæ¨ç†ï¼šæå‡æ£€ç´¢æ•ˆç‡ï¼Œå‡å°‘èµ„æºæ¶ˆè€—', 'desc': 'çŠ¶æ€æœºæ¨ç†ï¼ˆSMRï¼‰é€šè¿‡ç¦»æ•£åŠ¨ä½œæ¡†æ¶æ¥æ”¹å–„ä¿¡æ¯æ£€ç´¢æ€§èƒ½ï¼Œå¹¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œä½¿ç”¨ï¼Œè§£å†³äº†è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¯†åˆ«äº†ä¿¡æ¯æ£€ç´¢ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå†—ä½™è½¨è¿¹å’Œè¯¯å¯¼æ€§æ¨ç†ã€‚SMRé‡‡ç”¨åŸºäºè½¬ç§»çš„æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«ç²¾ç»†æ§åˆ¶çš„ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚ç²¾ç‚¼ã€é‡æ–°æ’åºå’Œåœæ­¢ï¼‰ï¼Œæ”¯æŒæ—©æœŸåœæ­¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMRåœ¨BEIRå’ŒBRIGHTåŸºå‡†ä¸Šæé«˜äº†3.4%çš„æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†74.4%çš„ä»¤ç‰Œä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01413', 'title': 'Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.01413', 'abstract': 'Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.', 'score': 0, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '3f0db6c1e3cc1878', 'authors': ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent YouTu Lab', 'The Chinese University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01413.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ ÑƒĞ¼Ğ½ĞµĞµ, Ğ° Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Reasoning to Deep Understanding', 'desc': "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“æŒ‡ä»¤åŒ…å«å¤šä¸ªå¹¶è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„çº¦æŸæ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œé€šè¿‡æ¿€åŠ±æ¨ç†æ¥æå‡LLMså¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯çš„è§„åˆ™ä¸­å¿ƒå¥–åŠ±ä¿¡å·ï¼ŒåŸ¹å…»æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”æ ·æœ¬ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨å¤æ‚æŒ‡ä»¤ä¸‹æ¨ç†çš„æµ…å±‚å’Œéæœ¬è´¨ç‰¹æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (1)', '#audio (1)', '#benchmark (7)', '#cv (3)', '#data (2)', '#dataset (6)', '#diffusion (2)', '#ethics', '#games (4)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability (2)', '#leakage (1)', '#long_context', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (6)', '#open_source (4)', '#optimization (3)', '#plp', '#rag', '#reasoning (10)', '#rl (5)', '#rlhf', '#robotics (1)', '#science', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-06-03 03:41',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-03 03:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-03 03:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    