{
    "date": {
        "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 9",
        "zh": "10æœˆ9æ—¥"
    },
    "time_utc": "2025-10-09 03:28",
    "weekday": 3,
    "issue_id": 6322,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.06917",
            "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
            "url": "https://huggingface.co/papers/2510.06917",
            "abstract": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  \t\t\t\t\tAI-generated summary \t\t\t\t Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
            "score": 20,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "8587217f59423924",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06917.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#inference",
                    "#long_context"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¿Ğ¾ĞºĞ° ÑĞ»ÑƒÑˆĞ°ĞµÑˆÑŒ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SHANKS â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ spoken language models (SLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° Ğ½Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SHANKS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ° 37.1% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ 56.9% Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "SHANKS: Real-Time Reasoning for Smarter Conversations",
                    "desc": "The paper introduces SHANKS, a novel inference framework designed for spoken language models (SLMs) that allows them to engage in unspoken reasoning while listening to user input. This approach addresses the limitation of current large language models, which only process information after the user has finished speaking, leading to delays in interaction. By streaming input in chunks, SHANKS enables real-time reasoning and decision-making, allowing the model to interrupt users when necessary and complete tasks more efficiently. The results show significant improvements in interruption accuracy and tool call completion, demonstrating the potential for more dynamic and responsive conversational AI."
                },
                "zh": {
                    "title": "å®æ—¶æ€è€ƒï¼Œæå‡äº¤äº’æ•ˆç‡",
                    "desc": "SHANKSæ˜¯ä¸€ä¸ªé€šç”¨æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·è¾“å…¥æ—¶çš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨ç”¨æˆ·è¯´è¯æ—¶ç”Ÿæˆæœªè¯´å‡ºçš„æ¨ç†ï¼Œä»è€Œå‡å°‘å“åº”å»¶è¿Ÿã€‚SHANKSé€šè¿‡å°†è¾“å…¥è¯­éŸ³åˆ†æˆå›ºå®šæ—¶é•¿çš„å—è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿåœ¨æ¥æ”¶æ¯ä¸ªå—æ—¶è¿›è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒSHANKSåœ¨ç”¨æˆ·è§£å†³æ•°å­¦é—®é¢˜æ—¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¸­æ–­ç”¨æˆ·ï¼Œå¹¶åœ¨å¯¹è¯ä¸­æå‰å®Œæˆå·¥å…·è°ƒç”¨ï¼Œæ˜¾è‘—æé«˜äº†äº¤äº’æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07315",
            "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
            "url": "https://huggingface.co/papers/2510.07315",
            "abstract": "Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
            "score": 14,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "664f235019a59971",
            "authors": [
                "Ming Zhong",
                "Xiang Zhou",
                "Ting-Yun Chang",
                "Qingze Wang",
                "Nan Xu",
                "Xiance Si",
                "Dan Garrette",
                "Shyam Upadhyay",
                "Jeremiah Liu",
                "Jiawei Han",
                "Benoit Schillings",
                "Jiao Sun"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "University of Illinois Urbana-Champaign (UIUC)",
                "University of Southern California (USC)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07315.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Vibe Check: ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ´ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Vibe Checker â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VeriCode â€” Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸Ğ· 30 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 31 Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´ Â«Ğ¾Ñ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Â» â€” Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¼Ñ‹ÑĞµĞ»."
                },
                "en": {
                    "title": "Aligning AI with Human Coding Preferences",
                    "desc": "This paper introduces Vibe Checker, a new method for evaluating Large Language Models (LLMs) that combines functional correctness with instruction following to align better with human coding preferences. It highlights the importance of not just passing functional tests but also ensuring that code feels right and meets user expectations. The authors present VeriCode, a taxonomy of 30 verifiable code instructions, to measure how well models follow these instructions. Their findings show that a composite score of functional correctness and instruction following is a better predictor of human preference in coding tasks, revealing that instruction following is crucial for improving LLM performance in real-world applications."
                },
                "zh": {
                    "title": "Vibe Checkerï¼šæ›´è´´è¿‘äººç±»ç¼–ç åå¥½çš„è¯„ä¼°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVibe Checkerçš„è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚Vibe Checkerç»“åˆäº†åŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œæ—¨åœ¨æ›´å¥½åœ°ç¬¦åˆäººç±»çš„ç¼–ç åå¥½ã€‚æˆ‘ä»¬å¼•å…¥äº†VeriCodeï¼Œä¸€ä¸ªåŒ…å«30ä¸ªå¯éªŒè¯ä»£ç æŒ‡ä»¤çš„åˆ†ç±»æ³•ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„ç»¼åˆå¾—åˆ†ä¸äººç±»åå¥½é«˜åº¦ç›¸å…³ï¼Œåè€…åœ¨å®é™…ç¼–ç¨‹ä»»åŠ¡ä¸­æˆä¸ºä¸»è¦çš„åŒºåˆ†å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06308",
            "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
            "url": "https://huggingface.co/papers/2510.06308",
            "abstract": "Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
            "score": 14,
            "issue_id": 6322,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "7ea946e09493c01b",
            "authors": [
                "Yi Xin",
                "Qi Qin",
                "Siqi Luo",
                "Kaiwen Zhu",
                "Juncheng Yan",
                "Yan Tai",
                "Jiayi Lei",
                "Yuewen Cao",
                "Keqi Wang",
                "Yibin Wang",
                "Jinbin Bai",
                "Qian Yu",
                "Dengyang Jiang",
                "Yuandong Pu",
                "Haoxing Chen",
                "Le Zhuo",
                "Junjun He",
                "Gen Luo",
                "Tianbin Li",
                "Ming Hu",
                "Jin Ye",
                "Shenglong Ye",
                "Bo Zhang",
                "Chang Xu",
                "Wenhai Wang",
                "Hongsheng Li",
                "Guangtao Zhai",
                "Tianfan Xue",
                "Bin Fu",
                "Xiaohong Liu",
                "Yu Qiao",
                "Yihao Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Sydney",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06308.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Lumina-DiMOO - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ foundational Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… unified Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ autoregressive Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multi-Modal Generation with Lumina-DiMOO",
                    "desc": "Lumina-DiMOO is an open-source foundational model designed for efficient multi-modal generation and understanding. It employs fully discrete diffusion modeling, which distinguishes it from traditional autoregressive and hybrid models. This innovative technique enhances sampling efficiency and supports a wide range of tasks, such as text-to-image generation and image editing. By achieving state-of-the-art results on various benchmarks, Lumina-DiMOO aims to advance research in multi-modal and discrete diffusion models."
                },
                "zh": {
                    "title": "Lumina-DiMOOï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆä¸ç†è§£æ¨¡å‹",
                    "desc": "Lumina-DiMOOæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨å®Œå…¨ç¦»æ•£çš„æ‰©æ•£å»ºæ¨¡æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚ä¸ä¹‹å‰çš„ç»Ÿä¸€æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å¤„ç†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºæ—¶è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ç­‰å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°äº†æ›´é«˜çš„é‡‡æ ·æ•ˆç‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ¨åŠ¨å¤šæ¨¡æ€å’Œç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶è¿›å±•ï¼Œæˆ‘ä»¬å°†ä»£ç å’Œæ£€æŸ¥ç‚¹å‘å¸ƒç»™ç¤¾åŒºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03215",
            "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.03215",
            "abstract": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.",
            "score": 12,
            "issue_id": 6321,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "3453eb2a78f90630",
            "authors": [
                "Tianyu Fu",
                "Zihan Min",
                "Hanling Zhang",
                "Jichao Yan",
                "Guohao Dai",
                "Wanli Ouyang",
                "Yu Wang"
            ],
            "affiliations": [
                "Infinigence AI",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03215.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#agi"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±Ñ‰ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ²: Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑÑˆ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Cache-to-Cache (C2C) Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· KV-Cache Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºÑÑˆĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8.5-10.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ° 3.0-5.0% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ C2C Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Direct Semantic Communication for Enhanced LLM Collaboration",
                    "desc": "Cache-to-Cache (C2C) introduces a novel method for Large Language Models (LLMs) to communicate directly using their internal KV-caches instead of relying on text. This approach enhances the semantic richness of the communication, allowing models to share information more effectively and efficiently. By employing a neural network to project and merge the KV-caches, C2C minimizes the loss of information and reduces latency associated with text-based exchanges. Experimental results demonstrate that C2C improves accuracy by 8.5-10.5% and speeds up communication by approximately 2.0x compared to traditional methods."
                },
                "zh": {
                    "title": "ç›´æ¥è¯­ä¹‰é€šä¿¡ï¼Œæå‡æ¨¡å‹æ•ˆç‡",
                    "desc": "Cache-to-Cache (C2C) æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´ç›´æ¥è¿›è¡Œè¯­ä¹‰é€šä¿¡ã€‚é€šè¿‡ç¥ç»ç½‘ç»œæŠ•å½±ï¼ŒC2C å¯ä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘å»¶è¿Ÿï¼Œé¿å…äº†ä¼ ç»Ÿæ–‡æœ¬é€šä¿¡ä¸­ä¿¡æ¯æŸå¤±å’Œé€å­—ç”Ÿæˆçš„å»¶è¿Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆæºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„KVç¼“å­˜ï¼Œç›´æ¥ä¼ é€’è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹é—´äº¤æµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC2C åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ–‡æœ¬é€šä¿¡æ–¹å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04678",
            "title": "Multi-Agent Tool-Integrated Policy Optimization",
            "url": "https://huggingface.co/papers/2510.04678",
            "abstract": "MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
            "score": 10,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "a8d8251d93e429c4",
            "authors": [
                "Zhanfeng Mo",
                "Xingxuan Li",
                "Yuntao Chen",
                "Lidong Bing"
            ],
            "affiliations": [
                "MiroMind AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04678.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ñ†ĞµĞ»Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MATPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ»Ğ¸Ğ±Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. MATPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18,38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Multi-Agent Roles in LLMs for Enhanced Performance",
                    "desc": "MATPO is a novel reinforcement learning approach that enhances the performance of large language models (LLMs) by integrating multiple agent roles within a single model. It introduces a planner-worker framework, where the planner strategizes and the worker executes tasks, allowing for better management of context and tool responses. This method leverages a credit assignment mechanism to optimize the training of these roles, avoiding the need for multiple LLMs and thus saving memory. Experimental results demonstrate that MATPO significantly improves performance and robustness compared to traditional single-agent systems, making it a promising solution for complex reasoning tasks."
                },
                "zh": {
                    "title": "å¤šä»£ç†å·¥å…·é›†æˆä¼˜åŒ–ï¼Œæå‡LLMæ€§èƒ½ä¸é²æ£’æ€§",
                    "desc": "MATPOæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å•ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤šä»£ç†è§’è‰²ï¼Œæå‡å…¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è§’è‰²ç‰¹å®šçš„æç¤ºï¼Œå…è®¸è§„åˆ’è€…å’Œå·¥ä½œè€…åœ¨åŒä¸€LLMå®ä¾‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†ç°æœ‰å•ä»£ç†ç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡é•¿åº¦å’Œå·¥å…·å“åº”å™ªå£°æ–¹é¢çš„å±€é™ã€‚MATPOé‡‡ç”¨äº†ä¸€ç§åŸåˆ™æ€§çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œé¿å…äº†éƒ¨ç½²å¤šä¸ªLLMæ‰€éœ€çš„é«˜å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸“ä¸šåŒ–çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMATPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºå•ä»£ç†åŸºçº¿å¹³å‡æé«˜äº†18.38%çš„æ€§èƒ½ï¼Œå¹¶å¯¹å™ªå£°å·¥å…·è¾“å‡ºè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06751",
            "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
            "url": "https://huggingface.co/papers/2510.06751",
            "abstract": "OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
            "score": 8,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "719c136fdd131e9e",
            "authors": [
                "Junhan Zhu",
                "Hesong Wang",
                "Mingluo Su",
                "Zefang Wang",
                "Huan Wang"
            ],
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06751.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´",
                    "desc": "OBS-Diff â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (pruning) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Optimal Brain Surgeon Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ñ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ²ĞµÑ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Pruning for Powerful Diffusion Models",
                    "desc": "OBS-Diff is a new framework designed to efficiently prune large-scale text-to-image diffusion models, which are typically expensive to run. Traditional one-shot pruning methods struggle with these models due to their complex iterative denoising processes. This paper introduces a modified version of the Optimal Brain Surgeon technique, allowing for various levels of pruning while maintaining model performance. By focusing on the error accumulation during the diffusion process and implementing a smart pruning strategy, OBS-Diff significantly speeds up inference with little loss in image quality."
                },
                "zh": {
                    "title": "OBS-Diffï¼šé«˜æ•ˆå‹ç¼©æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "OBS-Diffæ˜¯ä¸€ç§æ–°é¢–çš„ä¸€æ¬¡æ€§å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨å‹ç¼©å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„è´¨é‡æŸå¤±å’Œæ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚ç°æœ‰çš„ä¸€æ¬¡æ€§ç½‘ç»œå‰ªææ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å…·æœ‰è¿­ä»£å»å™ªçš„ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒOBS-Diffå¯¹ç»å…¸çš„æœ€ä½³è„‘å¤–ç§‘åŒ»ç”Ÿï¼ˆOBSï¼‰è¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿å…¶é€‚åº”ç°ä»£æ‰©æ•£æ¨¡å‹çš„å¤æ‚æ¶æ„ï¼Œå¹¶æ”¯æŒå¤šç§å‰ªæç²’åº¦ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æ­¥é•¿æ„ŸçŸ¥çš„Hessianæ„é€ å’Œé«˜æ•ˆçš„åˆ†ç»„é¡ºåºå‰ªæç­–ç•¥ï¼ŒOBS-Diffåœ¨è§†è§‰è´¨é‡æŸå¤±æœ€å°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ‰©æ•£æ¨¡å‹çš„æœ€å…ˆè¿›çš„ä¸€æ¬¡æ€§å‰ªæã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07318",
            "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
            "url": "https://huggingface.co/papers/2510.07318",
            "abstract": "A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
            "score": 6,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "95c7f8990db3ab94",
            "authors": [
                "Yunhao Fang",
                "Weihao Yu",
                "Shu Zhong",
                "Qinghao Ye",
                "Xuehan Xiong",
                "Lai Wei"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07318.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ¿Ğ¿Ğ¾ĞºĞ°Ğ¼Ğ¿ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ KV-ĞºĞµÑˆĞ° Transformer) Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Artificial Hippocampus Network). AHN Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RNN-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Mamba2, DeltaNet Ğ¸ Gated DeltaNet. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ AHN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹ Ñ full-attention Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 40.5% Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 74%."
                },
                "en": {
                    "title": "Enhancing Long-Sequence Modeling with Memory Integration",
                    "desc": "This paper presents a new memory framework for neural networks that combines short-term and long-term memory to enhance the modeling of long sequences. The framework uses a sliding window for short-term memory, which retains recent information, while an Artificial Hippocampus Network (AHN) compresses older data into a fixed-size long-term memory. By integrating this approach into existing RNN-like architectures, the models show improved efficiency and performance on long-context tasks. Experiments reveal that these models not only outperform traditional methods but also significantly reduce computational costs and memory usage."
                },
                "zh": {
                    "title": "æå‡é•¿åºåˆ—å»ºæ¨¡æ•ˆç‡çš„è®°å¿†æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„ç¥ç»ç½‘ç»œè®°å¿†æ¡†æ¶ï¼Œä»¥æé«˜é•¿åºåˆ—å»ºæ¨¡çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†è®¤çŸ¥ç§‘å­¦ä¸­çš„å¤šå­˜å‚¨æ¨¡å‹ï¼Œä½¿ç”¨å˜æ¢å™¨çš„KVç¼“å­˜ä½œä¸ºæ— æŸçŸ­æœŸè®°å¿†ï¼ŒåŒæ—¶é€šè¿‡äººå·¥æµ·é©¬ä½“ç½‘ç»œï¼ˆAHNï¼‰å°†è¶…å‡ºçª—å£çš„ä¿¡æ¯å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„é•¿æœŸè®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨AHNçš„æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5-3B-Instructæ¨¡å‹åœ¨å¼•å…¥AHNåï¼Œæ¨ç†è®¡ç®—é‡å‡å°‘äº†40.5%ï¼Œå†…å­˜ç¼“å­˜å‡å°‘äº†74.0%ï¼ŒåŒæ—¶åœ¨LV-Evalä¸Šçš„å¹³å‡å¾—åˆ†ä»4.41æé«˜åˆ°5.88ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04212",
            "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
            "url": "https://huggingface.co/papers/2510.04212",
            "abstract": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
            "score": 6,
            "issue_id": 6321,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "e0a5e1e23247359f",
            "authors": [
                "Haiquan Qiu",
                "Quanming Yao"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04212.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¥",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ²: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¸, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ flash attention Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ²ÑƒÑ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… low-rank Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºÑ€ÑƒĞ³ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºĞ°Ğ¶Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ flash attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Stabilizing Low-Precision Training in Transformers",
                    "desc": "This paper addresses the challenges of training transformer models using low-precision formats, which often lead to significant training instabilities. It identifies the root cause of catastrophic loss explosions during low-precision training with flash attention, linking it to low-rank representations and biased rounding errors. The authors explain how these issues create a cycle of error accumulation that disrupts weight updates and training dynamics. To resolve this, they propose a minimal modification to the flash attention mechanism that reduces rounding bias, stabilizing the training process effectively."
                },
                "zh": {
                    "title": "ä½ç²¾åº¦è®­ç»ƒä¸­çš„é—ªå­˜æ³¨æ„åŠ›ç¨³å®šæ€§è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨ä½ç²¾åº¦è®­ç»ƒå˜æ¢å™¨æ¨¡å‹æ—¶ï¼Œä½¿ç”¨é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶æ‰€é¢ä¸´çš„ç¾éš¾æ€§æŸå¤±çˆ†ç‚¸é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§é—®é¢˜å¹¶éå¶ç„¶ï¼Œè€Œæ˜¯ç”±äºæ³¨æ„åŠ›æœºåˆ¶ä¸­å‡ºç°çš„ç›¸ä¼¼ä½ç§©è¡¨ç¤ºå’Œä½ç²¾åº¦ç®—æœ¯ä¸­å›ºæœ‰çš„åå·®èˆå…¥è¯¯å·®ç›¸äº’ä½œç”¨æ‰€å¯¼è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶çš„æœ€å°ä¿®æ”¹ï¼Œèƒ½å¤Ÿå‡è½»èˆå…¥è¯¯å·®çš„åå·®ï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€ç®€å•çš„æ”¹åŠ¨ï¼Œæˆ‘ä»¬éªŒè¯äº†åˆ†æç»“æœï¼Œå¹¶ä¸ºè¿™ä¸€é•¿æœŸå­˜åœ¨çš„é—®é¢˜æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07313",
            "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
            "url": "https://huggingface.co/papers/2510.07313",
            "abstract": "WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
            "score": 4,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "72337b6fb9bb399b",
            "authors": [
                "Zezhong Qian",
                "Xiaowei Chi",
                "Yuming Li",
                "Shizun Wang",
                "Zhiyuan Qin",
                "Xiaozhu Ju",
                "Sirui Han",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beijing Innovation Center of Humanoid Robotics",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07313.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "WristWorld â€” ÑÑ‚Ğ¾ 4D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ 4D ÑÑ†ĞµĞ½Ñƒ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞºĞ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ´ĞºĞ¾ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLA Ğ½Ğ° 3.81% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Generating Wrist-View Videos from Anchor Views",
                    "desc": "WristWorld is a novel 4D world model designed to generate wrist-view videos from anchor views, addressing the challenge of limited wrist-view data in video generation. It enhances video generation consistency and improves Visual Language Action (VLA) performance by capturing detailed hand-object interactions. The model operates in two stages: first, it reconstructs wrist-view poses and 4D point clouds using a Spatial Projection Consistency (SPC) Loss, and second, it synthesizes coherent wrist-view videos from these reconstructions. Experiments show that WristWorld achieves state-of-the-art results in video generation, significantly improving task completion rates and bridging the gap between anchor and wrist views."
                },
                "zh": {
                    "title": "WristWorldï¼šä»é”šç‚¹è§†å›¾ç”Ÿæˆæ‰‹è…•è§†è§’è§†é¢‘çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "WristWorldæ˜¯ä¸€ä¸ª4Dä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿä»é”šç‚¹è§†å›¾ç”Ÿæˆæ‰‹è…•è§†è§’çš„è§†é¢‘ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§å’ŒVLAæ€§èƒ½ã€‚æ‰‹è…•è§†è§’çš„è§‚å¯Ÿå¯¹äºVLAæ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ•æ‰åˆ°ç»†è‡´çš„æ‰‹ä¸ç‰©ä½“çš„äº¤äº’ï¼Œç›´æ¥å¢å¼ºäº†æ“ä½œæ€§èƒ½ã€‚ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹æ— æ³•å¼¥è¡¥é”šç‚¹è§†å›¾ä¸æ‰‹è…•è§†å›¾ä¹‹é—´çš„å·®è·ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦æ‰‹è…•è§†è§’çš„ç¬¬ä¸€å¸§ã€‚WristWorldé€šè¿‡é‡å»ºå’Œç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨ç©ºé—´æŠ•å½±ä¸€è‡´æ€§æŸå¤±å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒæˆåŠŸåˆæˆäº†æ—¶é—´ä¸Šè¿è´¯çš„æ‰‹è…•è§†è§’è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05862",
            "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
            "url": "https://huggingface.co/papers/2510.05862",
            "abstract": "Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
            "score": 4,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "efa8b7d057b81865",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Juntao Li",
                "Lijun Wu",
                "Haijia Gui",
                "Min Zhang"
            ],
            "affiliations": [
                "LCM Laboratory",
                "Shanghai Artificial Intelligence Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context Denoising Training (CDT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼), Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Integrated Gradient Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ CDT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ GPT-4o."
                },
                "en": {
                    "title": "Enhancing Long-Context Models with Context Denoising Training",
                    "desc": "Context Denoising Training (CDT) is a novel approach designed to enhance the performance of long-context models (LCMs) by reducing the impact of contextual noise. This noise, which consists of irrelevant tokens, can distract the model from focusing on important information needed for accurate predictions. The paper introduces the Integrated Gradient (IG) score as a metric to identify and measure this noise, allowing for targeted mitigation strategies. By implementing CDT, the model's attention on critical tokens is improved, leading to better overall performance in various tasks, even achieving results comparable to advanced models like GPT-4o."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡å»å™ªï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "ä¸Šä¸‹æ–‡å»å™ªè®­ç»ƒï¼ˆCDTï¼‰é€šè¿‡å‡å°‘ä¸Šä¸‹æ–‡å™ªå£°ï¼Œæå‡äº†é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½ã€‚é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆLCMsï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“å—åˆ°æ— å…³æ ‡è®°çš„å¹²æ‰°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åº¦é‡æ ‡å‡†â€”â€”ç§¯åˆ†æ¢¯åº¦ï¼ˆIGï¼‰åˆ†æ•°ï¼Œç”¨äºæ£€æµ‹å’Œé‡åŒ–ä¸Šä¸‹æ–‡ä¸­çš„å™ªå£°ä¿¡æ¯ã€‚é€šè¿‡ç®€å•çš„å™ªå£°ç¼“è§£æ–¹æ³•ï¼ŒCDTæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å…³é”®æ ‡è®°çš„å…³æ³¨ï¼Œä»è€Œæ”¹å–„äº†åç»­çš„é¢„æµ‹æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07310",
            "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
            "url": "https://huggingface.co/papers/2510.07310",
            "abstract": "MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
            "score": 3,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "1dfefa942d4dfe75",
            "authors": [
                "Siyoon Jin",
                "Seongchan Kim",
                "Dahyun Chung",
                "Jaeho Lee",
                "Hyunwook Choi",
                "Jisu Nam",
                "Jiyoung Kim",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07310.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#video",
                    "#interpretability"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¼Ğ°ÑĞºĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MATRIX-11K Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞºĞ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, ĞºĞ°Ğº video DiT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² semantic grounding (ÑĞ²ÑĞ·ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸) Ğ¸ semantic propagation (ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MATRIX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ hallucination Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Generation with MATRIX Regularization",
                    "desc": "This paper introduces the MATRIX-11K dataset, which includes interaction-aware captions and multi-instance mask tracks to improve video generation models known as video DiTs. The authors analyze how these models represent interactions through two main perspectives: semantic grounding and semantic propagation, focusing on how well they capture and maintain relationships between objects over time. They propose a new regularization technique called MATRIX that aligns the attention mechanisms in specific layers of video DiTs with the multi-instance mask tracks from their dataset. The results show that MATRIX enhances interaction fidelity and semantic alignment, leading to better video generation outcomes while minimizing issues like drift and hallucination."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„äº¤äº’ä¿çœŸåº¦ä¸è¯­ä¹‰å¯¹é½",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†MATRIX-11Kæ•°æ®é›†å’ŒMATRIXæ­£åˆ™åŒ–å¦‚ä½•æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆDiTsï¼‰åœ¨äº¤äº’ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šå®ä¾‹æˆ–ä¸»ä½“-å¯¹è±¡äº¤äº’æ—¶å­˜åœ¨å›°éš¾ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«äº¤äº’æ„è¯†å­—å¹•å’Œå¤šå®ä¾‹æ©ç è½¨è¿¹çš„æ•°æ®é›†ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰åŸºç¡€å’Œè¯­ä¹‰ä¼ æ’­ä¸¤ä¸ªè§†è§’ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘åˆ°æ–‡æœ¬å’Œè§†é¢‘åˆ°è§†é¢‘çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æœ€ç»ˆï¼ŒMATRIXæ­£åˆ™åŒ–é€šè¿‡å¯¹é½ç‰¹å®šå±‚çš„æ³¨æ„åŠ›ä¸å¤šå®ä¾‹æ©ç è½¨è¿¹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06783",
            "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
            "url": "https://huggingface.co/papers/2510.06783",
            "abstract": "TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.",
            "score": 3,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "54afe116302f9e7c",
            "authors": [
                "Akshit Singh",
                "Shyam Marjit",
                "Wei Lin",
                "Paul Gavrikov",
                "Serena Yeung-Levy",
                "Hilde Kuehne",
                "Rogerio Feris",
                "Sivan Doveh",
                "James Glass",
                "M. Jehanzeb Mirza"
            ],
            "affiliations": [
                "IISc Bangalore",
                "Independent Researcher",
                "JKU Linz",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Stanford",
                "TÃ¼bingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06783.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ TTRV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğµ GRPO, Ğ³Ğ´Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: Ğ´Ğ¾ 52.4% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ 29.8% Ğ² visual question answering Ğ½Ğ° 16 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ TTRV Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ InternVL 8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 2.3%, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Enhancing Vision Language Understanding with Test-Time Reinforcement Learning",
                    "desc": "The paper introduces TTRV, a novel approach that enhances vision language understanding using test-time reinforcement learning (RL) without requiring labeled data. It improves the Group Relative Policy Optimization (GRPO) framework by creating rewards based on the frequency of outputs from the base model, allowing for dynamic adaptation during inference. The method also encourages diversity in outputs by rewarding lower entropy in the empirical distribution of predictions. TTRV shows significant performance improvements in object recognition and visual question answering (VQA), outperforming existing models like GPT-4o in certain benchmarks, even in scenarios with limited data."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€ç†è§£",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTTRVçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºè§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œè€Œæ— éœ€æ ‡è®°æ•°æ®ã€‚è¯¥æ–¹æ³•æ”¹è¿›äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºé¢‘ç‡è®¾è®¡å¥–åŠ±ä¿¡å·ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚TTRVåœ¨ç‰©ä½“è¯†åˆ«å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«æé«˜äº†52.4%å’Œ29.8%ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨æ•°æ®æå…¶æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒTTRVä¹Ÿèƒ½åœ¨è¯†åˆ«ä»»åŠ¡ä¸­å®ç°é«˜è¾¾5.5%çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04999",
            "title": "Bridging Text and Video Generation: A Survey",
            "url": "https://huggingface.co/papers/2510.04999",
            "abstract": "A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
            "score": 3,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "2ff1fd3dd4354c0a",
            "authors": [
                "Nilay Kumar",
                "Priyansh Bhandari",
                "G. Maragatham"
            ],
            "affiliations": [
                "Department of Computational Intelligence SRM Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#video",
                    "#diffusion",
                    "#survey"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ GAN Ğº Diffusion: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° (text-to-video), Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¾Ñ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… GAN Ğ¸ VAE Ğ´Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Diffusion-Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ’ Ğ·Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Transforming Text into Video: A Journey Through Generative Models",
                    "desc": "This paper surveys the evolution of text-to-video (T2V) generative models, highlighting the transition from Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) to advanced Diffusion-Transformer architectures. It discusses the potential applications of T2V technology in various fields and the improvements in output quality and coherence achieved through these newer models. The paper also addresses ongoing challenges such as alignment, long-range coherence, and computational efficiency, while providing insights into training configurations and evaluation metrics used in the field. Finally, it outlines future research directions and open challenges to guide further advancements in T2V generation."
                },
                "zh": {
                    "title": "æ–‡æœ¬ç”Ÿæˆè§†é¢‘æŠ€æœ¯çš„æœªæ¥æ¢ç´¢",
                    "desc": "æœ¬æ–‡å¯¹æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åˆ°æ··åˆæ‰©æ•£-å˜æ¢å™¨æ¶æ„çš„å‘å±•å†ç¨‹ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¯¹é½ã€é•¿æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è¿™äº›æ¨¡å‹çš„å·¥ä½œåŸç†ã€è§£å†³çš„å±€é™æ€§ä»¥åŠä¸ºä½•éœ€è¦å‘æ–°æ¶æ„èŒƒå¼è½¬å˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å½“å‰çš„å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨T2VæŠ€æœ¯çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06261",
            "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
            "url": "https://huggingface.co/papers/2510.06261",
            "abstract": "AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
            "score": 3,
            "issue_id": 6322,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "d4056c4e12881117",
            "authors": [
                "Zhanke Zhou",
                "Chentao Cao",
                "Xiao Feng",
                "Xuan Li",
                "Zongze Li",
                "Xiangyu Lu",
                "Jiangchao Yao",
                "Weikai Huang",
                "Linrui Xu",
                "Tian Cheng",
                "Guanyu Jiang",
                "Yiming Zheng",
                "Brando Miranda",
                "Tongliang Liu",
                "Sanmi Koyejo",
                "Masashi Sugiyama",
                "Bo Han"
            ],
            "affiliations": [
                "Cooperative Medianet Innovation Center, Shanghai Jiao Tong University",
                "RIKEN AIP",
                "Stanford University",
                "Sydney AI Centre, The University of Sydney",
                "TMLR Group, Department of Computer Science, Hong Kong Baptist University",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06261.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: AlphaApollo Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ğ»Ğ¾Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM",
                    "desc": "AlphaApollo â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ foundation models: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ â€” Python Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ retrieval Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ â€” Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. AlphaApollo Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIME 2024/2025 ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: +5.15% Average@32 Ğ¸ +23.34% Pass@32 Ğ´Ğ»Ñ Qwen2.5-14B-Instruct, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾."
                },
                "en": {
                    "title": "Elevating Foundation Models with Self-Evolving Reasoning",
                    "desc": "AlphaApollo is a self-evolving reasoning system designed to improve the performance of foundation models (FMs) by integrating various tools and refining its processes iteratively. It addresses limitations in the intrinsic reasoning capacity of models and enhances reliability during testing by using a combination of computational and retrieval tools. The system allows for multi-round solution evolution, maintaining a shared state map that tracks candidates and feedback for continuous improvement. In evaluations, AlphaApollo demonstrated significant performance gains across multiple models, showcasing its ability to effectively utilize tools and surpass traditional baselines."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„æ¨ç†ç³»ç»Ÿï¼Œæå‡åŸºç¡€æ¨¡å‹æ€§èƒ½",
                    "desc": "AlphaApollo æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¨ç†ç³»ç»Ÿï¼Œé€šè¿‡å·¥å…·é›†æˆå’Œè¿­ä»£ä¼˜åŒ–æ¥æå‡åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒè§£å†³äº†åŸºç¡€æ¨¡å‹æ¨ç†ä¸­çš„ä¸¤ä¸ªç“¶é¢ˆï¼šæ¨¡å‹å†…åœ¨èƒ½åŠ›æœ‰é™å’Œæµ‹è¯•æ—¶è¿­ä»£ä¸å¯é ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è®¡ç®—å·¥å…·ï¼ˆå¦‚ Python åŠå…¶æ•°å€¼å’Œç¬¦å·åº“ï¼‰å’Œæ£€ç´¢å·¥å…·ï¼ˆç›¸å…³å¤–éƒ¨ä¿¡æ¯ï¼‰ï¼Œä»¥æ‰§è¡Œç²¾ç¡®è®¡ç®—å’Œåšå‡ºåŸºäºè¯æ®çš„å†³ç­–ã€‚é€šè¿‡å…±äº«çŠ¶æ€å›¾è®°å½•å€™é€‰é¡¹ã€å¯æ‰§è¡Œæ£€æŸ¥å’Œåé¦ˆï¼ŒAlphaApollo æ”¯æŒå¤šè½®ã€å¤šæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆæ¼”åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œé€šè¿‡ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07307",
            "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
            "url": "https://huggingface.co/papers/2510.07307",
            "abstract": "MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. We introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. We apply MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality.",
            "score": 2,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "3cffdc5a57565890",
            "authors": [
                "Rushi Qiang",
                "Yuchen Zhuang",
                "Anikait Singh",
                "Percy Liang",
                "Chao Zhang",
                "Sherry Yang",
                "Bo Dai"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07307.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#agents"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ° ML-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MLE-Smith â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ-Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MLE-Smith Ğº 224 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ 606 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Automating Diverse MLE Tasks with MLE-Smith",
                    "desc": "MLE-Smith is a novel automated system designed to create diverse and high-quality machine learning engineering (MLE) tasks from raw datasets. It utilizes a multi-agent pipeline that follows a generate-verify-execute approach, ensuring that the tasks produced are scalable and maintain rigorous quality standards. The system incorporates a hybrid verification mechanism to uphold structural integrity and semantic accuracy, while also confirming the tasks' practical applicability through interactive execution. By applying MLE-Smith to numerous real-world datasets, the study demonstrates its capability to generate a wide variety of tasks that align well with the performance of existing language models."
                },
                "zh": {
                    "title": "MLE-Smithï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æœºå™¨å­¦ä¹ ä»»åŠ¡çš„åˆ©å™¨",
                    "desc": "MLE-Smith æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“ç®¡é“ï¼Œèƒ½å¤Ÿä»åŸå§‹æ•°æ®é›†ä¸­åˆ›å»ºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ï¼ˆMLEï¼‰ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç”Ÿæˆ-éªŒè¯-æ‰§è¡Œçš„é«˜æ•ˆèŒƒå¼ï¼Œæå‡äº†ä»»åŠ¡çš„å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ç¡®ä¿äº†ä»»åŠ¡çš„è´¨é‡å’Œå®é™…åº”ç”¨æ€§ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç»“æ„åŒ–ä»»åŠ¡è®¾è®¡å’Œæ ‡å‡†åŒ–é‡æ„ï¼Œå¹¶ç»“åˆæ··åˆéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡çš„ç»“æ„è§„åˆ™å’Œè¯­ä¹‰åˆç†æ€§ã€‚é€šè¿‡åœ¨224ä¸ªçœŸå®æ•°æ®é›†ä¸Šåº”ç”¨MLE-Smithï¼Œç”Ÿæˆäº†606ä¸ªè·¨å¤šä¸ªç±»åˆ«å’Œç›®æ ‡çš„ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07238",
            "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
            "url": "https://huggingface.co/papers/2510.07238",
            "abstract": "Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.",
            "score": 2,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "1477f170bed5f8aa",
            "authors": [
                "Xunyi Jiang",
                "Dingyi Chang",
                "Julian McAuley",
                "Xin Xu"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07238.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑÑ‚Ğ°Ñ€ĞµÑÑ‚: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°ÑÑ‚ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ¾ÑĞµĞ¼ÑŒ LLM, Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑÑ‚Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ñ… ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Outdated Benchmarks, Unreliable Evaluations: Rethinking LLM Factuality",
                    "desc": "This research paper examines how the aging of factuality benchmarks affects the evaluation of large language models (LLMs). It highlights that many commonly used benchmarks are outdated, which can lead to unreliable assessments of how factual these models are. The authors conducted a systematic study of five popular benchmarks and eight LLMs, using a new fact retrieval pipeline and metrics to measure the impact of benchmark aging. The findings suggest that relying on these old benchmarks can mislead evaluations, emphasizing the need for updated standards in assessing LLM factuality."
                },
                "zh": {
                    "title": "åŸºå‡†è€åŒ–å½±å“LLMäº‹å®æ€§è¯„ä¼°çš„å¯é æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†äº‹å®åŸºå‡†çš„è€åŒ–åŠå…¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº‹å®æ€§è¯„ä¼°çš„å½±å“ï¼Œå‘ç°ç”±äºæ ·æœ¬è¿‡æ—¶ï¼Œè¯„ä¼°ç»“æœå­˜åœ¨æ˜¾è‘—çš„ä¸å¯é æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°å®ä¸–ç•Œçš„å¿«é€Ÿå‘å±•ï¼Œå¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°åŸºå‡†çš„é™æ€ç‰¹æ€§å¼•å‘äº†å¯¹å…¶å¯é æ€§çš„æ‹…å¿§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†äº”ä¸ªæµè¡Œçš„äº‹å®åŸºå‡†å’Œå…«ä¸ªä¸åŒå¹´ä»½å‘å¸ƒçš„LLMï¼Œæå‡ºäº†ä¸€ä¸ªæ›´æ–°çš„äº‹å®æ£€ç´¢ç®¡é“å’Œä¸‰ç§æŒ‡æ ‡æ¥é‡åŒ–åŸºå‡†è€åŒ–åŠå…¶å¯¹LLMäº‹å®æ€§è¯„ä¼°çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¤šæµè¡Œäº‹å®åŸºå‡†ä¸­çš„æ ·æœ¬å·²ç»è¿‡æ—¶ï¼Œå¯¼è‡´å¯¹LLMäº‹å®æ€§çš„è¯„ä¼°ä¸å¯é ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07041",
            "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
            "url": "https://huggingface.co/papers/2510.07041",
            "abstract": "U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "382dc490ec7534fd",
            "authors": [
                "Fenghe Tang",
                "Chengqi Dong",
                "Wenxin Ma",
                "Zikang Xu",
                "Heqin Zhu",
                "Zihang Jiang",
                "Rongsheng Wang",
                "Yuhao Wang",
                "Chenxu Wu",
                "Shaohua Kevin Zhou"
            ],
            "affiliations": [
                "HCNS",
                "MIRACLE Center",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07041.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "U-Bench: Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "U-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 100 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² U-Net Ğ½Ğ° 28 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ 10 Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° U-Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¾Ğ²ĞµÑ‚Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "U-Bench: A New Standard for Evaluating U-Net Models in Medical Imaging",
                    "desc": "U-Bench is a new benchmark designed to evaluate 100 different U-Net models across 28 datasets and 10 types of imaging. It focuses on three main areas: how robust the models are statistically, how well they can generalize to new data without prior training (zero-shot), and how efficient they are in terms of computation. The benchmark introduces a unique metric called U-Score, which helps to balance performance and efficiency, making it easier to choose the right model for specific tasks. By providing comprehensive evaluations and public access to resources, U-Bench aims to improve the way U-Net models are assessed and utilized in medical image segmentation."
                },
                "zh": {
                    "title": "U-Benchï¼šU-Netå˜ä½“çš„å…¨é¢è¯„ä¼°åŸºå‡†",
                    "desc": "U-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°100ç§U-Netå˜ä½“åœ¨28ä¸ªæ•°æ®é›†å’Œ10ç§æˆåƒæ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨ç»Ÿè®¡ç¨³å¥æ€§ã€é›¶æ ·æœ¬æ³›åŒ–å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥åŸºå‡†æµ‹è¯•å¡«è¡¥äº†ä»¥å¾€ç¼ºä¹ç³»ç»Ÿè¯„ä¼°çš„ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†U-Scoreï¼Œå¸®åŠ©ç ”ç©¶è€…ç†è§£æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡ç³»ç»Ÿåˆ†ææ•°æ®é›†ç‰¹å¾å’Œæ¨¡å‹æ¶æ„å¯¹æ€§èƒ½çš„å½±å“ï¼ŒU-Benchä¸ºç ”ç©¶è€…æä¾›äº†æ¨¡å‹é€‰æ‹©çš„æŒ‡å¯¼ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œåè®®å‡å·²å…¬å¼€ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„å†ç°æ€§å’Œæœªæ¥æ–¹æ³•çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05891",
            "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
            "url": "https://huggingface.co/papers/2510.05891",
            "abstract": "A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D^3QE across different AR models, with robustness to real-world perturbations. Code is available at https://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "b5fce1a59c0d659b",
            "authors": [
                "Yanran Zhang",
                "Bingyao Yu",
                "Yu Zheng",
                "Wenzhao Zheng",
                "Yueqi Duan",
                "Lei Chen",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University, China",
                "Department of Electronic Engineering, Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05891.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DÂ³QE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ² codebook Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ codebook Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ARForensics Ğ¸Ğ· 7 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Detecting AI-Generated Images with D^3QE",
                    "desc": "This paper presents a new method called Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for detecting images created by visual autoregressive models. It focuses on analyzing the frequency statistics of codebooks and the quantization errors that arise during image generation. By using a transformer that incorporates these frequency statistics into its attention mechanism, the method effectively distinguishes between real and synthetic images. The proposed approach shows high accuracy and generalization across various autoregressive models, making it robust against real-world image variations."
                },
                "zh": {
                    "title": "åˆ©ç”¨D^3QEæ£€æµ‹è‡ªå›å½’ç”Ÿæˆå›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£åˆ†å¸ƒå·®å¼‚æ„ŸçŸ¥é‡åŒ–è¯¯å·®ï¼ˆD^3QEï¼‰æ¥æ£€æµ‹ç”±è§†è§‰è‡ªå›å½’æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æä»£ç æœ¬é¢‘ç‡ç»Ÿè®¡å’Œé‡åŒ–è¯¯å·®ï¼Œè¯†åˆ«çœŸå®ä¸ä¼ªé€ å›¾åƒä¹‹é—´çš„ç‹¬ç‰¹æ¨¡å¼å’Œé¢‘ç‡åˆ†å¸ƒåå·®ã€‚ä¸ä¼ ç»Ÿçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè‡ªå›å½’æ¨¡å‹é€šè¿‡ç¦»æ•£æ ‡è®°é¢„æµ‹ç”Ÿæˆå›¾åƒï¼Œå±•ç°å‡ºæ›´é«˜çš„åˆæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD^3QEåœ¨ä¸åŒè‡ªå›å½’æ¨¡å‹ä¸­å…·æœ‰ä¼˜è¶Šçš„æ£€æµ‹å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-08.html",
    "link_next": "2025-10-10.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "08.10",
        "en": "10/08",
        "zh": "10æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.10",
        "en": "10/10",
        "zh": "10æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 3,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 11,
        "#survey": 3,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}