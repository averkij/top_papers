{
    "date": {
        "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 9",
        "zh": "10æœˆ9æ—¥"
    },
    "time_utc": "2025-10-09 02:18",
    "weekday": 3,
    "issue_id": 6321,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.07315",
            "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
            "url": "https://huggingface.co/papers/2510.07315",
            "abstract": "Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
            "score": 7,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "664f235019a59971",
            "pdf_title_img": "assets/pdf/title_img/2510.07315.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Vibe Check: ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ´ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Vibe Checker â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VeriCode â€” Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸Ğ· 30 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 31 Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´ Â«Ğ¾Ñ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Â» â€” Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¼Ñ‹ÑĞµĞ»."
                },
                "en": {
                    "title": "Aligning AI with Human Coding Preferences",
                    "desc": "This paper introduces Vibe Checker, a new method for evaluating Large Language Models (LLMs) that combines functional correctness with instruction following to align better with human coding preferences. It highlights the importance of not just passing functional tests but also ensuring that code feels right and meets user expectations. The authors present VeriCode, a taxonomy of 30 verifiable code instructions, to measure how well models follow these instructions. Their findings show that a composite score of functional correctness and instruction following is a better predictor of human preference in coding tasks, revealing that instruction following is crucial for improving LLM performance in real-world applications."
                },
                "zh": {
                    "title": "Vibe Checkerï¼šæ›´è´´è¿‘äººç±»ç¼–ç åå¥½çš„è¯„ä¼°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVibe Checkerçš„è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚Vibe Checkerç»“åˆäº†åŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œæ—¨åœ¨æ›´å¥½åœ°ç¬¦åˆäººç±»çš„ç¼–ç åå¥½ã€‚æˆ‘ä»¬å¼•å…¥äº†VeriCodeï¼Œä¸€ä¸ªåŒ…å«30ä¸ªå¯éªŒè¯ä»£ç æŒ‡ä»¤çš„åˆ†ç±»æ³•ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„ç»¼åˆå¾—åˆ†ä¸äººç±»åå¥½é«˜åº¦ç›¸å…³ï¼Œåè€…åœ¨å®é™…ç¼–ç¨‹ä»»åŠ¡ä¸­æˆä¸ºä¸»è¦çš„åŒºåˆ†å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06917",
            "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
            "url": "https://huggingface.co/papers/2510.06917",
            "abstract": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  \t\t\t\t\tAI-generated summary \t\t\t\t Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
            "score": 7,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "8587217f59423924",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06917.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#inference",
                    "#long_context"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¿Ğ¾ĞºĞ° ÑĞ»ÑƒÑˆĞ°ĞµÑˆÑŒ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SHANKS â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ spoken language models (SLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° Ğ½Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SHANKS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ° 37.1% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ 56.9% Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "SHANKS: Real-Time Reasoning for Smarter Conversations",
                    "desc": "The paper introduces SHANKS, a novel inference framework designed for spoken language models (SLMs) that allows them to engage in unspoken reasoning while listening to user input. This approach addresses the limitation of current large language models, which only process information after the user has finished speaking, leading to delays in interaction. By streaming input in chunks, SHANKS enables real-time reasoning and decision-making, allowing the model to interrupt users when necessary and complete tasks more efficiently. The results show significant improvements in interruption accuracy and tool call completion, demonstrating the potential for more dynamic and responsive conversational AI."
                },
                "zh": {
                    "title": "å®æ—¶æ€è€ƒï¼Œæå‡äº¤äº’æ•ˆç‡",
                    "desc": "SHANKSæ˜¯ä¸€ä¸ªé€šç”¨æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·è¾“å…¥æ—¶çš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨ç”¨æˆ·è¯´è¯æ—¶ç”Ÿæˆæœªè¯´å‡ºçš„æ¨ç†ï¼Œä»è€Œå‡å°‘å“åº”å»¶è¿Ÿã€‚SHANKSé€šè¿‡å°†è¾“å…¥è¯­éŸ³åˆ†æˆå›ºå®šæ—¶é•¿çš„å—è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿåœ¨æ¥æ”¶æ¯ä¸ªå—æ—¶è¿›è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒSHANKSåœ¨ç”¨æˆ·è§£å†³æ•°å­¦é—®é¢˜æ—¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¸­æ–­ç”¨æˆ·ï¼Œå¹¶åœ¨å¯¹è¯ä¸­æå‰å®Œæˆå·¥å…·è°ƒç”¨ï¼Œæ˜¾è‘—æé«˜äº†äº¤äº’æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04678",
            "title": "Multi-Agent Tool-Integrated Policy Optimization",
            "url": "https://huggingface.co/papers/2510.04678",
            "abstract": "MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
            "score": 7,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "a8d8251d93e429c4",
            "authors": [
                "Zhanfeng Mo",
                "Xingxuan Li",
                "Yuntao Chen",
                "Lidong Bing"
            ],
            "affiliations": [
                "MiroMind AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04678.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ñ†ĞµĞ»Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MATPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ»Ğ¸Ğ±Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. MATPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18,38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Multi-Agent Roles in LLMs for Enhanced Performance",
                    "desc": "MATPO is a novel reinforcement learning approach that enhances the performance of large language models (LLMs) by integrating multiple agent roles within a single model. It introduces a planner-worker framework, where the planner strategizes and the worker executes tasks, allowing for better management of context and tool responses. This method leverages a credit assignment mechanism to optimize the training of these roles, avoiding the need for multiple LLMs and thus saving memory. Experimental results demonstrate that MATPO significantly improves performance and robustness compared to traditional single-agent systems, making it a promising solution for complex reasoning tasks."
                },
                "zh": {
                    "title": "å¤šä»£ç†å·¥å…·é›†æˆä¼˜åŒ–ï¼Œæå‡LLMæ€§èƒ½ä¸é²æ£’æ€§",
                    "desc": "MATPOæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å•ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤šä»£ç†è§’è‰²ï¼Œæå‡å…¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è§’è‰²ç‰¹å®šçš„æç¤ºï¼Œå…è®¸è§„åˆ’è€…å’Œå·¥ä½œè€…åœ¨åŒä¸€LLMå®ä¾‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†ç°æœ‰å•ä»£ç†ç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡é•¿åº¦å’Œå·¥å…·å“åº”å™ªå£°æ–¹é¢çš„å±€é™ã€‚MATPOé‡‡ç”¨äº†ä¸€ç§åŸåˆ™æ€§çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œé¿å…äº†éƒ¨ç½²å¤šä¸ªLLMæ‰€éœ€çš„é«˜å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸“ä¸šåŒ–çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMATPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºå•ä»£ç†åŸºçº¿å¹³å‡æé«˜äº†18.38%çš„æ€§èƒ½ï¼Œå¹¶å¯¹å™ªå£°å·¥å…·è¾“å‡ºè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07318",
            "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
            "url": "https://huggingface.co/papers/2510.07318",
            "abstract": "A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
            "score": 3,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "95c7f8990db3ab94",
            "authors": [
                "Yunhao Fang",
                "Weihao Yu",
                "Shu Zhong",
                "Qinghao Ye",
                "Xuehan Xiong",
                "Lai Wei"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07318.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ¿Ğ¿Ğ¾ĞºĞ°Ğ¼Ğ¿ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ KV-ĞºĞµÑˆĞ° Transformer) Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Artificial Hippocampus Network). AHN Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RNN-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Mamba2, DeltaNet Ğ¸ Gated DeltaNet. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ AHN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹ Ñ full-attention Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 40.5% Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 74%."
                },
                "en": {
                    "title": "Enhancing Long-Sequence Modeling with Memory Integration",
                    "desc": "This paper presents a new memory framework for neural networks that combines short-term and long-term memory to enhance the modeling of long sequences. The framework uses a sliding window for short-term memory, which retains recent information, while an Artificial Hippocampus Network (AHN) compresses older data into a fixed-size long-term memory. By integrating this approach into existing RNN-like architectures, the models show improved efficiency and performance on long-context tasks. Experiments reveal that these models not only outperform traditional methods but also significantly reduce computational costs and memory usage."
                },
                "zh": {
                    "title": "æå‡é•¿åºåˆ—å»ºæ¨¡æ•ˆç‡çš„è®°å¿†æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„ç¥ç»ç½‘ç»œè®°å¿†æ¡†æ¶ï¼Œä»¥æé«˜é•¿åºåˆ—å»ºæ¨¡çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†è®¤çŸ¥ç§‘å­¦ä¸­çš„å¤šå­˜å‚¨æ¨¡å‹ï¼Œä½¿ç”¨å˜æ¢å™¨çš„KVç¼“å­˜ä½œä¸ºæ— æŸçŸ­æœŸè®°å¿†ï¼ŒåŒæ—¶é€šè¿‡äººå·¥æµ·é©¬ä½“ç½‘ç»œï¼ˆAHNï¼‰å°†è¶…å‡ºçª—å£çš„ä¿¡æ¯å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„é•¿æœŸè®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨AHNçš„æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5-3B-Instructæ¨¡å‹åœ¨å¼•å…¥AHNåï¼Œæ¨ç†è®¡ç®—é‡å‡å°‘äº†40.5%ï¼Œå†…å­˜ç¼“å­˜å‡å°‘äº†74.0%ï¼ŒåŒæ—¶åœ¨LV-Evalä¸Šçš„å¹³å‡å¾—åˆ†ä»4.41æé«˜åˆ°5.88ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03215",
            "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.03215",
            "abstract": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.",
            "score": 3,
            "issue_id": 6321,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "3453eb2a78f90630",
            "authors": [
                "Tianyu Fu",
                "Zihan Min",
                "Hanling Zhang",
                "Jichao Yan",
                "Guohao Dai",
                "Wanli Ouyang",
                "Yu Wang"
            ],
            "affiliations": [
                "Infinigence AI",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03215.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#agi"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±Ñ‰ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ²: Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑÑˆ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Cache-to-Cache (C2C) Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· KV-Cache Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºÑÑˆĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8.5-10.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ° 3.0-5.0% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ C2C Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Direct Semantic Communication for Enhanced LLM Collaboration",
                    "desc": "Cache-to-Cache (C2C) introduces a novel method for Large Language Models (LLMs) to communicate directly using their internal KV-caches instead of relying on text. This approach enhances the semantic richness of the communication, allowing models to share information more effectively and efficiently. By employing a neural network to project and merge the KV-caches, C2C minimizes the loss of information and reduces latency associated with text-based exchanges. Experimental results demonstrate that C2C improves accuracy by 8.5-10.5% and speeds up communication by approximately 2.0x compared to traditional methods."
                },
                "zh": {
                    "title": "ç›´æ¥è¯­ä¹‰é€šä¿¡ï¼Œæå‡æ¨¡å‹æ•ˆç‡",
                    "desc": "Cache-to-Cache (C2C) æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´ç›´æ¥è¿›è¡Œè¯­ä¹‰é€šä¿¡ã€‚é€šè¿‡ç¥ç»ç½‘ç»œæŠ•å½±ï¼ŒC2C å¯ä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘å»¶è¿Ÿï¼Œé¿å…äº†ä¼ ç»Ÿæ–‡æœ¬é€šä¿¡ä¸­ä¿¡æ¯æŸå¤±å’Œé€å­—ç”Ÿæˆçš„å»¶è¿Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆæºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„KVç¼“å­˜ï¼Œç›´æ¥ä¼ é€’è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹é—´äº¤æµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC2C åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ–‡æœ¬é€šä¿¡æ–¹å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05862",
            "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
            "url": "https://huggingface.co/papers/2510.05862",
            "abstract": "Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
            "score": 2,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "efa8b7d057b81865",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Juntao Li",
                "Lijun Wu",
                "Haijia Gui",
                "Min Zhang"
            ],
            "affiliations": [
                "LCM Laboratory",
                "Shanghai Artificial Intelligence Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context Denoising Training (CDT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼), Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Integrated Gradient Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ CDT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ GPT-4o."
                },
                "en": {
                    "title": "Enhancing Long-Context Models with Context Denoising Training",
                    "desc": "Context Denoising Training (CDT) is a novel approach designed to enhance the performance of long-context models (LCMs) by reducing the impact of contextual noise. This noise, which consists of irrelevant tokens, can distract the model from focusing on important information needed for accurate predictions. The paper introduces the Integrated Gradient (IG) score as a metric to identify and measure this noise, allowing for targeted mitigation strategies. By implementing CDT, the model's attention on critical tokens is improved, leading to better overall performance in various tasks, even achieving results comparable to advanced models like GPT-4o."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡å»å™ªï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "ä¸Šä¸‹æ–‡å»å™ªè®­ç»ƒï¼ˆCDTï¼‰é€šè¿‡å‡å°‘ä¸Šä¸‹æ–‡å™ªå£°ï¼Œæå‡äº†é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½ã€‚é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆLCMsï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“å—åˆ°æ— å…³æ ‡è®°çš„å¹²æ‰°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åº¦é‡æ ‡å‡†â€”â€”ç§¯åˆ†æ¢¯åº¦ï¼ˆIGï¼‰åˆ†æ•°ï¼Œç”¨äºæ£€æµ‹å’Œé‡åŒ–ä¸Šä¸‹æ–‡ä¸­çš„å™ªå£°ä¿¡æ¯ã€‚é€šè¿‡ç®€å•çš„å™ªå£°ç¼“è§£æ–¹æ³•ï¼ŒCDTæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å…³é”®æ ‡è®°çš„å…³æ³¨ï¼Œä»è€Œæ”¹å–„äº†åç»­çš„é¢„æµ‹æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04212",
            "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
            "url": "https://huggingface.co/papers/2510.04212",
            "abstract": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
            "score": 2,
            "issue_id": 6321,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "e0a5e1e23247359f",
            "authors": [
                "Haiquan Qiu",
                "Quanming Yao"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04212.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¥",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ²: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¸, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ flash attention Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ²ÑƒÑ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… low-rank Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºÑ€ÑƒĞ³ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºĞ°Ğ¶Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ flash attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Stabilizing Low-Precision Training in Transformers",
                    "desc": "This paper addresses the challenges of training transformer models using low-precision formats, which often lead to significant training instabilities. It identifies the root cause of catastrophic loss explosions during low-precision training with flash attention, linking it to low-rank representations and biased rounding errors. The authors explain how these issues create a cycle of error accumulation that disrupts weight updates and training dynamics. To resolve this, they propose a minimal modification to the flash attention mechanism that reduces rounding bias, stabilizing the training process effectively."
                },
                "zh": {
                    "title": "ä½ç²¾åº¦è®­ç»ƒä¸­çš„é—ªå­˜æ³¨æ„åŠ›ç¨³å®šæ€§è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨ä½ç²¾åº¦è®­ç»ƒå˜æ¢å™¨æ¨¡å‹æ—¶ï¼Œä½¿ç”¨é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶æ‰€é¢ä¸´çš„ç¾éš¾æ€§æŸå¤±çˆ†ç‚¸é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§é—®é¢˜å¹¶éå¶ç„¶ï¼Œè€Œæ˜¯ç”±äºæ³¨æ„åŠ›æœºåˆ¶ä¸­å‡ºç°çš„ç›¸ä¼¼ä½ç§©è¡¨ç¤ºå’Œä½ç²¾åº¦ç®—æœ¯ä¸­å›ºæœ‰çš„åå·®èˆå…¥è¯¯å·®ç›¸äº’ä½œç”¨æ‰€å¯¼è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶çš„æœ€å°ä¿®æ”¹ï¼Œèƒ½å¤Ÿå‡è½»èˆå…¥è¯¯å·®çš„åå·®ï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€ç®€å•çš„æ”¹åŠ¨ï¼Œæˆ‘ä»¬éªŒè¯äº†åˆ†æç»“æœï¼Œå¹¶ä¸ºè¿™ä¸€é•¿æœŸå­˜åœ¨çš„é—®é¢˜æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07310",
            "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
            "url": "https://huggingface.co/papers/2510.07310",
            "abstract": "MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "1dfefa942d4dfe75",
            "authors": [
                "Siyoon Jin",
                "Seongchan Kim",
                "Dahyun Chung",
                "Jaeho Lee",
                "Hyunwook Choi",
                "Jisu Nam",
                "Jiyoung Kim",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07310.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#video",
                    "#interpretability"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¼Ğ°ÑĞºĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MATRIX-11K Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞºĞ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, ĞºĞ°Ğº video DiT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² semantic grounding (ÑĞ²ÑĞ·ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸) Ğ¸ semantic propagation (ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MATRIX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ hallucination Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Generation with MATRIX Regularization",
                    "desc": "This paper introduces the MATRIX-11K dataset, which includes interaction-aware captions and multi-instance mask tracks to improve video generation models known as video DiTs. The authors analyze how these models represent interactions through two main perspectives: semantic grounding and semantic propagation, focusing on how well they capture and maintain relationships between objects over time. They propose a new regularization technique called MATRIX that aligns the attention mechanisms in specific layers of video DiTs with the multi-instance mask tracks from their dataset. The results show that MATRIX enhances interaction fidelity and semantic alignment, leading to better video generation outcomes while minimizing issues like drift and hallucination."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„äº¤äº’ä¿çœŸåº¦ä¸è¯­ä¹‰å¯¹é½",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†MATRIX-11Kæ•°æ®é›†å’ŒMATRIXæ­£åˆ™åŒ–å¦‚ä½•æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆDiTsï¼‰åœ¨äº¤äº’ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šå®ä¾‹æˆ–ä¸»ä½“-å¯¹è±¡äº¤äº’æ—¶å­˜åœ¨å›°éš¾ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«äº¤äº’æ„è¯†å­—å¹•å’Œå¤šå®ä¾‹æ©ç è½¨è¿¹çš„æ•°æ®é›†ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰åŸºç¡€å’Œè¯­ä¹‰ä¼ æ’­ä¸¤ä¸ªè§†è§’ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘åˆ°æ–‡æœ¬å’Œè§†é¢‘åˆ°è§†é¢‘çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æœ€ç»ˆï¼ŒMATRIXæ­£åˆ™åŒ–é€šè¿‡å¯¹é½ç‰¹å®šå±‚çš„æ³¨æ„åŠ›ä¸å¤šå®ä¾‹æ©ç è½¨è¿¹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07041",
            "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
            "url": "https://huggingface.co/papers/2510.07041",
            "abstract": "U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "382dc490ec7534fd",
            "authors": [
                "Fenghe Tang",
                "Chengqi Dong",
                "Wenxin Ma",
                "Zikang Xu",
                "Heqin Zhu",
                "Zihang Jiang",
                "Rongsheng Wang",
                "Yuhao Wang",
                "Chenxu Wu",
                "Shaohua Kevin Zhou"
            ],
            "affiliations": [
                "HCNS",
                "MIRACLE Center",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07041.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "U-Bench: Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "U-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 100 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² U-Net Ğ½Ğ° 28 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ 10 Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° U-Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¾Ğ²ĞµÑ‚Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "U-Bench: A New Standard for Evaluating U-Net Models in Medical Imaging",
                    "desc": "U-Bench is a new benchmark designed to evaluate 100 different U-Net models across 28 datasets and 10 types of imaging. It focuses on three main areas: how robust the models are statistically, how well they can generalize to new data without prior training (zero-shot), and how efficient they are in terms of computation. The benchmark introduces a unique metric called U-Score, which helps to balance performance and efficiency, making it easier to choose the right model for specific tasks. By providing comprehensive evaluations and public access to resources, U-Bench aims to improve the way U-Net models are assessed and utilized in medical image segmentation."
                },
                "zh": {
                    "title": "U-Benchï¼šU-Netå˜ä½“çš„å…¨é¢è¯„ä¼°åŸºå‡†",
                    "desc": "U-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°100ç§U-Netå˜ä½“åœ¨28ä¸ªæ•°æ®é›†å’Œ10ç§æˆåƒæ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨ç»Ÿè®¡ç¨³å¥æ€§ã€é›¶æ ·æœ¬æ³›åŒ–å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥åŸºå‡†æµ‹è¯•å¡«è¡¥äº†ä»¥å¾€ç¼ºä¹ç³»ç»Ÿè¯„ä¼°çš„ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†U-Scoreï¼Œå¸®åŠ©ç ”ç©¶è€…ç†è§£æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡ç³»ç»Ÿåˆ†ææ•°æ®é›†ç‰¹å¾å’Œæ¨¡å‹æ¶æ„å¯¹æ€§èƒ½çš„å½±å“ï¼ŒU-Benchä¸ºç ”ç©¶è€…æä¾›äº†æ¨¡å‹é€‰æ‹©çš„æŒ‡å¯¼ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œåè®®å‡å·²å…¬å¼€ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„å†ç°æ€§å’Œæœªæ¥æ–¹æ³•çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05891",
            "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
            "url": "https://huggingface.co/papers/2510.05891",
            "abstract": "A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D^3QE across different AR models, with robustness to real-world perturbations. Code is available at https://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "b5fce1a59c0d659b",
            "authors": [
                "Yanran Zhang",
                "Bingyao Yu",
                "Yu Zheng",
                "Wenzhao Zheng",
                "Yueqi Duan",
                "Lei Chen",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University, China",
                "Department of Electronic Engineering, Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05891.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DÂ³QE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ² codebook Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ codebook Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ARForensics Ğ¸Ğ· 7 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Detecting AI-Generated Images with D^3QE",
                    "desc": "This paper presents a new method called Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for detecting images created by visual autoregressive models. It focuses on analyzing the frequency statistics of codebooks and the quantization errors that arise during image generation. By using a transformer that incorporates these frequency statistics into its attention mechanism, the method effectively distinguishes between real and synthetic images. The proposed approach shows high accuracy and generalization across various autoregressive models, making it robust against real-world image variations."
                },
                "zh": {
                    "title": "åˆ©ç”¨D^3QEæ£€æµ‹è‡ªå›å½’ç”Ÿæˆå›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£åˆ†å¸ƒå·®å¼‚æ„ŸçŸ¥é‡åŒ–è¯¯å·®ï¼ˆD^3QEï¼‰æ¥æ£€æµ‹ç”±è§†è§‰è‡ªå›å½’æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æä»£ç æœ¬é¢‘ç‡ç»Ÿè®¡å’Œé‡åŒ–è¯¯å·®ï¼Œè¯†åˆ«çœŸå®ä¸ä¼ªé€ å›¾åƒä¹‹é—´çš„ç‹¬ç‰¹æ¨¡å¼å’Œé¢‘ç‡åˆ†å¸ƒåå·®ã€‚ä¸ä¼ ç»Ÿçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè‡ªå›å½’æ¨¡å‹é€šè¿‡ç¦»æ•£æ ‡è®°é¢„æµ‹ç”Ÿæˆå›¾åƒï¼Œå±•ç°å‡ºæ›´é«˜çš„åˆæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD^3QEåœ¨ä¸åŒè‡ªå›å½’æ¨¡å‹ä¸­å…·æœ‰ä¼˜è¶Šçš„æ£€æµ‹å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04999",
            "title": "Bridging Text and Video Generation: A Survey",
            "url": "https://huggingface.co/papers/2510.04999",
            "abstract": "A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "2ff1fd3dd4354c0a",
            "authors": [
                "Nilay Kumar",
                "Priyansh Bhandari",
                "G. Maragatham"
            ],
            "affiliations": [
                "Department of Computational Intelligence SRM Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#video",
                    "#diffusion",
                    "#survey"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ GAN Ğº Diffusion: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° (text-to-video), Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¾Ñ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… GAN Ğ¸ VAE Ğ´Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Diffusion-Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ’ Ğ·Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Transforming Text into Video: A Journey Through Generative Models",
                    "desc": "This paper surveys the evolution of text-to-video (T2V) generative models, highlighting the transition from Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) to advanced Diffusion-Transformer architectures. It discusses the potential applications of T2V technology in various fields and the improvements in output quality and coherence achieved through these newer models. The paper also addresses ongoing challenges such as alignment, long-range coherence, and computational efficiency, while providing insights into training configurations and evaluation metrics used in the field. Finally, it outlines future research directions and open challenges to guide further advancements in T2V generation."
                },
                "zh": {
                    "title": "æ–‡æœ¬ç”Ÿæˆè§†é¢‘æŠ€æœ¯çš„æœªæ¥æ¢ç´¢",
                    "desc": "æœ¬æ–‡å¯¹æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åˆ°æ··åˆæ‰©æ•£-å˜æ¢å™¨æ¶æ„çš„å‘å±•å†ç¨‹ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¯¹é½ã€é•¿æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è¿™äº›æ¨¡å‹çš„å·¥ä½œåŸç†ã€è§£å†³çš„å±€é™æ€§ä»¥åŠä¸ºä½•éœ€è¦å‘æ–°æ¶æ„èŒƒå¼è½¬å˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å½“å‰çš„å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨T2VæŠ€æœ¯çš„å‘å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-08.html",
    "link_next": "2025-10-10.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "08.10",
        "en": "10/08",
        "zh": "10æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.10",
        "en": "10/10",
        "zh": "10æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}