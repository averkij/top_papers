{
    "date": {
        "ru": "17 Ğ¸ÑĞ»Ñ",
        "en": "July 17",
        "zh": "7æœˆ17æ—¥"
    },
    "time_utc": "2025-07-17 03:54",
    "weekday": 3,
    "issue_id": 4861,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.12465",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "url": "https://huggingface.co/papers/2507.12465",
            "abstract": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
            "score": 9,
            "issue_id": 4861,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "ece62f7e4ecd0487",
            "authors": [
                "Ziang Cao",
                "Zhaoxi Chen",
                "Linag Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "PhysX Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhysXNet Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PhysXGen Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. PhysXGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Bridging 3D Generation with Real-World Physics",
                    "desc": "PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations."
                },
                "zh": {
                    "title": "ç‰©ç†é©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "PhysXæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³3Dç”Ÿæˆæ¨¡å‹ä¸­ç¼ºä¹ç‰©ç†å±æ€§çš„é—®é¢˜ã€‚å®ƒå¼•å…¥äº†PhysXNetï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†æ³¨é‡Šçš„æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°æ ‡æ³¨äº†äº”ä¸ªåŸºç¡€ç»´åº¦ï¼ŒåŒ…æ‹¬ç»å¯¹å°ºåº¦ã€ææ–™ã€å¯ç”¨æ€§ã€è¿åŠ¨å­¦å’ŒåŠŸèƒ½æè¿°ã€‚é€šè¿‡PhysXGenæ¡†æ¶ï¼Œç‰©ç†çŸ¥è¯†è¢«æ•´åˆåˆ°3Dèµ„äº§ç”Ÿæˆä¸­ï¼Œåˆ©ç”¨åŒåˆ†æ”¯æ¶æ„å»ºæ¨¡3Dç»“æ„ä¸ç‰©ç†å±æ€§ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆå…·æœ‰å¯ä¿¡ç‰©ç†é¢„æµ‹çš„3Dèµ„äº§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09025",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "url": "https://huggingface.co/papers/2507.09025",
            "abstract": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
            "score": 1,
            "issue_id": 4861,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "3490901c2a32da3d",
            "authors": [
                "Chien Van Nguyen",
                "Ruiyi Zhang",
                "Hanieh Deilamsalehy",
                "Puneet Mathur",
                "Viet Dac Lai",
                "Haoliang Wang",
                "Jayakumar Subramanian",
                "Ryan A. Rossi",
                "Trung Bui",
                "Nikos Vlassis",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Lizard: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Lizard - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Lizard Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Lizard Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Lizard: Efficient Infinite-Context Generation for Transformers",
                    "desc": "Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks."
                },
                "zh": {
                    "title": "Lizardï¼šé«˜æ•ˆæ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "Lizardæ˜¯ä¸€ä¸ªçº¿æ€§åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬å˜ä¸ºçµæ´»çš„äºšäºŒæ¬¡æ¶æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ç§äºšäºŒæ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿsoftmaxæ³¨æ„åŠ›åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ æ—¶çš„å†…å­˜å’Œè®¡ç®—ç“¶é¢ˆï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚Lizardè¿˜ç»“åˆäº†é—¨æ§æ¨¡å—ï¼Œæ”¯æŒè‡ªé€‚åº”å†…å­˜æ§åˆ¶å’Œå¸¸é‡å†…å­˜æ¨ç†ï¼Œå¢å¼ºäº†æ¨¡å‹è®¾è®¡çš„çµæ´»æ€§ã€‚é€šè¿‡æ··åˆé—¨æ§çº¿æ€§æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ŒLizardèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–å’Œç»†ç²’åº¦çš„å±€éƒ¨äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11527",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "url": "https://huggingface.co/papers/2507.11527",
            "abstract": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
            "score": 0,
            "issue_id": 4861,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "e6f20729b2c748f9",
            "authors": [
                "Yinsheng Li",
                "Zhen Dong",
                "Yi Shao"
            ],
            "affiliations": [
                "Department of Civil Engineering McGill University",
                "NVIDIA",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "DrafterBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "DrafterBench - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶ĞµĞ¹. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 12 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ°Ñ…, Ñ 46 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ²ÑĞµĞ³Ğ¾ 1920 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹. DrafterBench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DrafterBench: Evaluating LLMs for Technical Drawing Mastery",
                    "desc": "DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications."
                },
                "zh": {
                    "title": "DrafterBenchï¼šè¯„ä¼°LLMä»£ç†çš„æŠ€æœ¯å›¾çº¸ä¿®è®¢èƒ½åŠ›",
                    "desc": "DrafterBenchæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æŠ€æœ¯å›¾çº¸ä¿®è®¢ä¸­çš„èƒ½åŠ›ã€‚å®ƒæ¶µç›–äº†ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œã€æŒ‡ä»¤éµå¾ªå’Œæ‰¹åˆ¤æ€§æ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªçœŸå®ç»˜å›¾æ–‡ä»¶çš„åäºŒç§ä»»åŠ¡ï¼Œæä¾›äº†46ç§å®šåˆ¶åŠŸèƒ½å’Œ1920ä¸ªä»»åŠ¡ã€‚DrafterBenchæ—¨åœ¨æ·±å…¥åˆ†æä»£ç†çš„èƒ½åŠ›ï¼Œå¸®åŠ©è¯†åˆ«åœ¨å·¥ç¨‹åº”ç”¨ä¸­æ•´åˆLLMçš„æ”¹è¿›ç›®æ ‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-16.html",
    "link_next": "2025-07-18.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.07",
        "en": "07/18",
        "zh": "7æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}