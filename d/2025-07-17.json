{
    "date": {
        "ru": "17 Ğ¸ÑĞ»Ñ",
        "en": "July 17",
        "zh": "7æœˆ17æ—¥"
    },
    "time_utc": "2025-07-17 04:31",
    "weekday": 3,
    "issue_id": 4862,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.12465",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "url": "https://huggingface.co/papers/2507.12465",
            "abstract": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
            "score": 11,
            "issue_id": 4861,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "ece62f7e4ecd0487",
            "authors": [
                "Ziang Cao",
                "Zhaoxi Chen",
                "Linag Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "PhysX Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhysXNet Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PhysXGen Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. PhysXGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Bridging 3D Generation with Real-World Physics",
                    "desc": "PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations."
                },
                "zh": {
                    "title": "ç‰©ç†é©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "PhysXæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³3Dç”Ÿæˆæ¨¡å‹ä¸­ç¼ºä¹ç‰©ç†å±æ€§çš„é—®é¢˜ã€‚å®ƒå¼•å…¥äº†PhysXNetï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†æ³¨é‡Šçš„æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°æ ‡æ³¨äº†äº”ä¸ªåŸºç¡€ç»´åº¦ï¼ŒåŒ…æ‹¬ç»å¯¹å°ºåº¦ã€ææ–™ã€å¯ç”¨æ€§ã€è¿åŠ¨å­¦å’ŒåŠŸèƒ½æè¿°ã€‚é€šè¿‡PhysXGenæ¡†æ¶ï¼Œç‰©ç†çŸ¥è¯†è¢«æ•´åˆåˆ°3Dèµ„äº§ç”Ÿæˆä¸­ï¼Œåˆ©ç”¨åŒåˆ†æ”¯æ¶æ„å»ºæ¨¡3Dç»“æ„ä¸ç‰©ç†å±æ€§ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆå…·æœ‰å¯ä¿¡ç‰©ç†é¢„æµ‹çš„3Dèµ„äº§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11527",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "url": "https://huggingface.co/papers/2507.11527",
            "abstract": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
            "score": 5,
            "issue_id": 4861,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "e6f20729b2c748f9",
            "authors": [
                "Yinsheng Li",
                "Zhen Dong",
                "Yi Shao"
            ],
            "affiliations": [
                "Department of Civil Engineering McGill University",
                "NVIDIA",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "DrafterBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "DrafterBench - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶ĞµĞ¹. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 12 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ°Ñ…, Ñ 46 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ²ÑĞµĞ³Ğ¾ 1920 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹. DrafterBench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DrafterBench: Evaluating LLMs for Technical Drawing Mastery",
                    "desc": "DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications."
                },
                "zh": {
                    "title": "DrafterBenchï¼šè¯„ä¼°LLMä»£ç†çš„æŠ€æœ¯å›¾çº¸ä¿®è®¢èƒ½åŠ›",
                    "desc": "DrafterBenchæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æŠ€æœ¯å›¾çº¸ä¿®è®¢ä¸­çš„èƒ½åŠ›ã€‚å®ƒæ¶µç›–äº†ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œã€æŒ‡ä»¤éµå¾ªå’Œæ‰¹åˆ¤æ€§æ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªçœŸå®ç»˜å›¾æ–‡ä»¶çš„åäºŒç§ä»»åŠ¡ï¼Œæä¾›äº†46ç§å®šåˆ¶åŠŸèƒ½å’Œ1920ä¸ªä»»åŠ¡ã€‚DrafterBenchæ—¨åœ¨æ·±å…¥åˆ†æä»£ç†çš„èƒ½åŠ›ï¼Œå¸®åŠ©è¯†åˆ«åœ¨å·¥ç¨‹åº”ç”¨ä¸­æ•´åˆLLMçš„æ”¹è¿›ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11949",
            "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
            "url": "https://huggingface.co/papers/2507.11949",
            "abstract": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.",
            "score": 2,
            "issue_id": 4862,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "60ad5621a3842ae5",
            "authors": [
                "Shuyang Xu",
                "Zhiyang Dou",
                "Mingyi Shi",
                "Liang Pan",
                "Leo Ho",
                "Jingbo Wang",
                "Yuan Liu",
                "Cheng Lin",
                "Yuexin Ma",
                "Wenping Wang",
                "Taku Komura"
            ],
            "affiliations": [
                "Macau University of Science and Technology",
                "Shanghai AI Lab",
                "ShanghaiTech University",
                "Texas A&M University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MOSPA - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SAM, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. MOSPA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging Sound and Motion: MOSPA for Realistic Human Animation",
                    "desc": "The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area."
                },
                "zh": {
                    "title": "ç©ºé—´éŸ³é¢‘é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆæ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶MOSPAï¼Œç”¨äºå»ºæ¨¡äººç±»åœ¨ç©ºé—´éŸ³é¢‘åˆºæ¿€ä¸‹çš„è¿åŠ¨ã€‚æˆ‘ä»¬åˆ›å»ºäº†é¦–ä¸ªç»¼åˆæ€§çš„ç©ºé—´éŸ³é¢‘é©±åŠ¨äººç±»è¿åŠ¨ï¼ˆSAMï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„ç©ºé—´éŸ³é¢‘ä¸è¿åŠ¨æ•°æ®ã€‚MOSPAé€šè¿‡æœ‰æ•ˆçš„èåˆæœºåˆ¶ï¼Œå‡†ç¡®æ•æ‰èº«ä½“è¿åŠ¨ä¸ç©ºé—´éŸ³é¢‘ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„äººç±»è¿åŠ¨ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09477",
            "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
            "url": "https://huggingface.co/papers/2507.09477",
            "abstract": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
            "score": 2,
            "issue_id": 4862,
            "pub_date": "2025-07-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ»Ñ",
                "en": "July 13",
                "zh": "7æœˆ13æ—¥"
            },
            "hash": "da4aa711048f0a7f",
            "authors": [
                "Yangning Li",
                "Weizhi Zhang",
                "Yuyao Yang",
                "Wei-Chieh Huang",
                "Yaozu Wu",
                "Junyu Luo",
                "Yuanchen Bei",
                "Henry Peng Zou",
                "Xiao Luo",
                "Yusheng Zhao",
                "Chunkit Chan",
                "Yankai Chen",
                "Zhongfen Deng",
                "Yinghui Li",
                "Hai-Tao Zheng",
                "Dongyuan Li",
                "Renhe Jiang",
                "Ming Zhang",
                "Yangqiu Song",
                "Philip S. Yu"
            ],
            "affiliations": [
                "HKUST",
                "Peking University",
                "The University of Tokyo",
                "Tsinghua University",
                "University of California, Los Angeles",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09477.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#rag"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Retrieval-Augmented Generation (RAG) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ RAG-Reasoning, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‡ĞµÑ€Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Synergized Retrieval and Reasoning",
                    "desc": "This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs."
                },
                "zh": {
                    "title": "æ¨ç†ä¸æ£€ç´¢çš„ååŒæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†ä¸æ£€ç´¢çš„ç»“åˆï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šè¡¨ç°ä¸è¶³ã€‚è®ºæ–‡æå‡ºäº†ç»Ÿä¸€çš„æ¨ç†-æ£€ç´¢è§†è§’ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å…ˆè¿›çš„æ¨ç†ä¼˜åŒ–RAGçš„æ¯ä¸ªé˜¶æ®µï¼Œå¹¶å¼ºè°ƒäº†æ–°å…´çš„ååŒRAG-æ¨ç†æ¡†æ¶ã€‚æœ€åï¼Œè®ºæ–‡åˆ†ç±»äº†æ–¹æ³•ã€æ•°æ®é›†å’Œå¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥æ„å»ºæ›´æœ‰æ•ˆã€é€‚åº”å¤šæ¨¡æ€ã€å¯ä¿¡èµ–å’Œä»¥äººä¸ºæœ¬çš„RAG-æ¨ç†ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09025",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "url": "https://huggingface.co/papers/2507.09025",
            "abstract": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
            "score": 2,
            "issue_id": 4861,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "3490901c2a32da3d",
            "authors": [
                "Chien Van Nguyen",
                "Ruiyi Zhang",
                "Hanieh Deilamsalehy",
                "Puneet Mathur",
                "Viet Dac Lai",
                "Haoliang Wang",
                "Jayakumar Subramanian",
                "Ryan A. Rossi",
                "Trung Bui",
                "Nikos Vlassis",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Lizard: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Lizard - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Lizard Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Lizard Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Lizard: Efficient Infinite-Context Generation for Transformers",
                    "desc": "Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks."
                },
                "zh": {
                    "title": "Lizardï¼šé«˜æ•ˆæ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "Lizardæ˜¯ä¸€ä¸ªçº¿æ€§åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬å˜ä¸ºçµæ´»çš„äºšäºŒæ¬¡æ¶æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ç§äºšäºŒæ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿsoftmaxæ³¨æ„åŠ›åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ æ—¶çš„å†…å­˜å’Œè®¡ç®—ç“¶é¢ˆï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚Lizardè¿˜ç»“åˆäº†é—¨æ§æ¨¡å—ï¼Œæ”¯æŒè‡ªé€‚åº”å†…å­˜æ§åˆ¶å’Œå¸¸é‡å†…å­˜æ¨ç†ï¼Œå¢å¼ºäº†æ¨¡å‹è®¾è®¡çš„çµæ´»æ€§ã€‚é€šè¿‡æ··åˆé—¨æ§çº¿æ€§æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ŒLizardèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–å’Œç»†ç²’åº¦çš„å±€éƒ¨äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-16.html",
    "link_next": "2025-07-18.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.07",
        "en": "07/18",
        "zh": "7æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}