{
    "date": {
        "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 16",
        "zh": "1æœˆ16æ—¥"
    },
    "time_utc": "2025-01-16 05:10",
    "weekday": 3,
    "issue_id": 1697,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08994",
            "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
            "url": "https://huggingface.co/papers/2501.08994",
            "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "0d164d45ba2a5c71",
            "authors": [
                "Chenyang Si",
                "Weichen Fan",
                "Zhengyao Lv",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore, 639798",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08994.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "RepVideo: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RepVideo - ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². RepVideo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RepVideo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Generation with Stable Representations",
                    "desc": "This paper presents RepVideo, a new framework designed to improve video generation using text-to-video diffusion models. It identifies issues with unstable semantic representations caused by variations in attention maps across different layers of the model. By accumulating features from neighboring layers, RepVideo creates more stable and enriched representations that enhance the model's ability to maintain consistency between adjacent frames. The results show that RepVideo significantly improves both the spatial accuracy of generated videos and their temporal coherence, leading to more realistic video outputs."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„RepVideoæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†RepVideoæ¡†æ¶ä»¥æ”¹å–„è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚ç ”ç©¶å‘ç°ä¸­é—´å±‚ç‰¹å¾çš„æ³¨æ„åŠ›å›¾å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™å¯¼è‡´è¯­ä¹‰è¡¨ç¤ºçš„ä¸ç¨³å®šæ€§ï¼Œè¿›è€Œå½±å“ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚RepVideoé€šè¿‡ä»ç›¸é‚»å±‚ç´¯ç§¯ç‰¹å¾ï¼Œå½¢æˆæ›´ä¸°å¯Œçš„è¡¨ç¤ºï¼Œä»è€Œæ•æ‰æ›´ç¨³å®šçš„è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepVideoæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç©ºé—´è¡¨ç°èƒ½åŠ›å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09019",
            "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
            "url": "https://huggingface.co/papers/2501.09019",
            "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "c4c991699f684865",
            "authors": [
                "Jingyuan Chen",
                "Fuchen Long",
                "Jie An",
                "Zhaofan Qiu",
                "Ting Yao",
                "Jiebo Luo",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Rochester, Rochester, NY USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09019.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ouroboros-Diffusion Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ouroboros-Diffusion. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¸ ÑÑĞ¶ĞµÑ‚Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Subject-Aware Cross-Frame Attention. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰ĞµĞµÑÑ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ouroboros-Diffusion Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Ouroboros-Diffusion: Enhancing Long Video Consistency and Coherence",
                    "desc": "The paper introduces Ouroboros-Diffusion, a new framework for improving long video generation using a pre-trained text-to-video model. It addresses the limitations of FIFO-Diffusion, particularly in maintaining long-range temporal consistency across video frames. The proposed method enhances structural consistency through a novel latent sampling technique and improves subject consistency with a Subject-Aware Cross-Frame Attention mechanism. Additionally, self-recurrent guidance is implemented to utilize information from previous frames, resulting in videos with better visual coherence and smoother transitions."
                },
                "zh": {
                    "title": "Ouroboros-Diffusionï¼šæå‡è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "FIFOè§†é¢‘æ‰©æ•£æ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„é•¿è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œä½†åœ¨ç”Ÿæˆè§†é¢‘æ—¶å¸¸å¸¸ç¼ºä¹é•¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºäº†Ouroboros-Diffusionæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ–°çš„æ½œåœ¨é‡‡æ ·æŠ€æœ¯å’Œä¸»é¢˜æ„ŸçŸ¥è·¨å¸§æ³¨æ„æœºåˆ¶ï¼Œå¢å¼ºäº†è§†é¢‘çš„ç»“æ„å’Œå†…å®¹ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†å¸§ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶é€šè¿‡è‡ªé€’å½’å¼•å¯¼æŠ€æœ¯åˆ©ç”¨å‰é¢æ¸…æ™°å¸§çš„ä¿¡æ¯æ¥æ”¹å–„åé¢å™ªå£°å¸§çš„å»å™ªæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOuroboros-Diffusionåœ¨ä¸»é¢˜ä¸€è‡´æ€§ã€è¿åŠ¨å¹³æ»‘æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08809",
            "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
            "url": "https://huggingface.co/papers/2501.08809",
            "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "d4d018c9adb2579c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#audio",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "XMusic: Ğ˜Ğ˜-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ XMusic - Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². XMusic ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: XProjector Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ XComposer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ XMIDI, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ‚Ñ‹ÑÑÑ‡ MIDI-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¶Ğ°Ğ½Ñ€Ğ¾Ğ². Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, XMusic Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸."
                },
                "en": {
                    "title": "XMusic: Emotionally Controlled Music Generation Made Easy!",
                    "desc": "This paper introduces XMusic, a new framework for generating symbolic music that can be controlled by emotional prompts. It includes two main components: XProjector, which converts various input types into musical elements, and XComposer, which generates and selects high-quality music. The framework uses a multi-task learning approach to ensure the generated music meets quality, emotional, and genre standards. Additionally, the authors created a large dataset, XMIDI, to support their research and demonstrate that XMusic outperforms existing methods in music generation."
                },
                "zh": {
                    "title": "XMusicï¼šæƒ…æ„Ÿå¯æ§çš„é«˜è´¨é‡éŸ³ä¹ç”Ÿæˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨å›¾åƒåˆæˆå’Œæ–‡æœ¬ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éŸ³ä¹ç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„ç¬¦å·éŸ³ä¹ç”Ÿæˆæ¡†æ¶XMusicï¼Œèƒ½å¤Ÿé€šè¿‡çµæ´»çš„æç¤ºç”Ÿæˆå¯æ§æƒ…æ„Ÿå’Œé«˜è´¨é‡çš„ç¬¦å·éŸ³ä¹ã€‚XMusicç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šXProjectorå’ŒXComposerï¼Œå‰è€…å°†å¤šç§æ¨¡æ€çš„æç¤ºè§£æä¸ºéŸ³ä¹å…ƒç´ ï¼Œåè€…åˆ™ç”Ÿæˆå’Œé€‰æ‹©é«˜è´¨é‡çš„éŸ³ä¹ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„XMIDIæ•°æ®é›†å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ¡ˆï¼ŒXMusicåœ¨éŸ³ä¹è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-15.html",
    "link_next": "2025-01-17.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† MiniMax-01 ç³»åˆ—ï¼ŒåŒ…æ‹¬ MiniMax-Text-01 å’Œ MiniMax-VL-01ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ–¹é¢å…·æœ‰å“è¶Šèƒ½åŠ›ã€‚æ ¸å¿ƒåœ¨äºé—ªç”µæ³¨æ„åŠ›å’Œå…¶é«˜æ•ˆæ‰©å±•ã€‚æˆ‘ä»¬å°†å…¶ä¸æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é›†æˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰ 32 ä¸ªä¸“å®¶å’Œ 4560 äº¿æ€»å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¼˜åŒ–çš„å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„è®¡ç®—é€šä¿¡é‡å æŠ€æœ¯ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ•°ç™¾äº¿å‚æ•°çš„æ¨¡å‹ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ã€‚MiniMax-Text-01 çš„ä¸Šä¸‹æ–‡çª—å£åœ¨è®­ç»ƒæœŸé—´å¯è¾¾åˆ° 100 ä¸‡ä¸ªæ ‡è®°ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´æ‰©å±•åˆ° 400 ä¸‡ä¸ªæ ‡è®°ã€‚MiniMax-VL-01 é€šè¿‡ä½¿ç”¨ 5120 äº¿è§†è§‰è¯­è¨€æ ‡è®°è¿›è¡ŒæŒç»­è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†å’Œå†…éƒ¨åŸºå‡†ä¸Šçš„æ€§èƒ½ä¸ GPT-4o å’Œ Claude-3.5-Sonnet ç›¸å½“ï¼ŒåŒæ—¶æä¾› 20-32 å€çš„ä¸Šä¸‹æ–‡çª—å£ã€‚æˆ‘ä»¬åœ¨ https://github.com/MiniMax-AI å…¬å¼€å‘å¸ƒäº† MiniMax-01ã€‚",
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
        "pinyin": "WÇ’men jiÃ¨shÃ o le MiniMax-01 xÃ¬liÃ¨, bÄokuÃ² MiniMax-Text-01 hÃ© MiniMax-VL-01. ZhÃ¨xiÄ“ mÃ³xÃ­ng zÃ i chÇ”lÇ chÃ¡ng shÃ ngxÃ¬awÃ©n fÄngmiÃ n jÃ¹yÇ’u zhuÃ³yuÃ¨ nÃ©nglÃ¬. HÃ©xÄ«n zÃ iyÃº shÇndiÇn zhÃ¹yÃ¬lÃ¬ hÃ© qÃ­ gÄoxiÃ o kuÃ²zhÇn. WÇ’men jiÄng qÃ­ yÇ” hÃ¹n hÃ© zhuÄnjiÄ mÃ³xÃ­ng (MoE) jÃ­chÃ©ng, chuÃ ngjiÃ n le yÄ«gÃ¨ jÃ¹yÇ’u 32 gÃ¨ zhuÄnjiÄ hÃ© 4560 yÃ¬ zÇ’ng cÄnshÃ¹ de mÃ³xÃ­ng. WÇ’men kÄifÄ le yÅuhuÃ  de bÃ¬ngxÃ­ng cÃ¨lÃ¼Ã¨ hÃ© gÄoxiÃ o de jÃ¬suÃ n tÅngxÃ¬n zhÃ²ngdiÃ© jÃ¬shÃ¹. ZhÃ¨ shÇ wÇ’men nÃ©nggÃ²u zÃ i shÃ¹bÇiyÃ¬ cÄnshÃ¹ de mÃ³xÃ­ng shÃ ng jÃ¬nxÃ­ng gÄoxiÃ o xÃ¹nliÃ n hÃ© tuÃ¬lÇ. MiniMax-Text-01 de shÃ ngxÃ¬awÃ©n chuÄngkÇ’u zÃ i xÃ¹nliÃ n qÄ«jiÄn kÄ› dÃ¡dÃ o 100 wÃ n gÃ¨ biÄojÃ¬, bÃ¬ng zÃ i tuÃ¬lÇ qÄ«jiÄn kuÃ²zhÇn dÃ o 400 wÃ n gÃ¨ biÄojÃ¬. MiniMax-VL-01 tÅngguÃ² shÇyÃ²ng 5120 yÃ¬ shÃ¬juÃ© yÇ”yÃ¡n biÄojÃ¬ jÃ¬nxÃ­ng chÃ­xÃ¹ xÃ¹nliÃ n. ShÃ¬yÃ n biÇomÃ­ng, wÇ’men de mÃ³xÃ­ng zÃ i biÄozhÇ”n hÃ© nÃ¨ibÃ¹ jÄ«zhÇ”n shÃ ng de xiÃ onÃ©nglÃ¬ yÇ” GPT-4o hÃ© Claude-3.5-Sonnet xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng 20-32 bÃ¨i de shÃ ngxÃ¬awÃ©n chuÄngkÇ’u. WÇ’men zÃ i https://github.com/MiniMax-AI gÅngkÄi fÄbÃ¹ le MiniMax-01.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"ç³»åˆ—\", \"pinyin\": \"xÃ¬ liÃ¨\", \"trans\": \"series\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n\", \"trans\": \"context\"},\n    {\"word\": \"å“è¶Š\", \"pinyin\": \"zhuÃ³ yuÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ© xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"é—ªç”µ\", \"pinyin\": \"shÇn diÃ n\", \"trans\": \"lightning\"},\n    {\"word\": \"æ³¨æ„åŠ›\", \"pinyin\": \"zhÃ¹ yÃ¬ lÃ¬\", \"trans\": \"attention\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ² zhÇn\", \"trans\": \"expand\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ä¸“å®¶\", \"pinyin\": \"zhuÄn jiÄ\", \"trans\": \"expert\"},\n    {\"word\": \"é›†æˆ\", \"pinyin\": \"jÃ­ chÃ©ng\", \"trans\": \"integrate\"},\n    {\"word\": \"å¹¶è¡Œ\", \"pinyin\": \"bÃ¬ng xÃ­ng\", \"trans\": \"parallel\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"é€šä¿¡\", \"pinyin\": \"tÅng xÃ¬n\", \"trans\": \"communication\"},\n    {\"word\": \"é‡å \", \"pinyin\": \"chÃ³ng diÃ©\", \"trans\": \"overlap\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"çª—å£\", \"pinyin\": \"chuÄng kÇ’u\", \"trans\": \"window\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄo jÃ¬\", \"trans\": \"token\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"è¯­è¨€\", \"pinyin\": \"yÇ” yÃ¡n\", \"trans\": \"language\"},\n    {\"word\": \"æŒç»­\", \"pinyin\": \"chÃ­ xÃ¹\", \"trans\": \"continuous\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"}\n]",
        "trans": "We introduced the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01. These models excel in handling long contexts, with a core focus on flash attention and its efficient scaling. We integrated them with a Mixture of Experts (MoE) model, creating a model with 32 experts and a total of 4560 billion parameters. We developed optimized parallel strategies and efficient computation-communication overlap techniques. This enables us to perform efficient training and inference on models with hundreds of billions of parameters. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and expands to 4 million tokens during inference. MiniMax-VL-01 undergoes continuous training using 5120 billion vision-language tokens. Experiments show that our models perform comparably to GPT-4o and Claude-3.5-Sonnet on standard and internal benchmarks while providing a 20-32 times larger context window. We have made MiniMax-01 publicly available at https://github.com/MiniMax-AI.",
        "update_ts": "2025-01-15 09:11"
    }
}