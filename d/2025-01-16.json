{
    "date": {
        "ru": "16 января",
        "en": "January 16",
        "zh": "1月16日"
    },
    "time_utc": "2025-01-16 14:09",
    "weekday": 3,
    "issue_id": 1706,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08828",
            "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
            "url": "https://huggingface.co/papers/2501.08828",
            "abstract": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.",
            "score": 12,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "bf9a6df8fecd4ec1",
            "authors": [
                "Kuicai Dong",
                "Yujing Chang",
                "Xin Deik Goh",
                "Dexun Li",
                "Ruiming Tang",
                "Yong Liu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08828.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "MMDocIR: Новый стандарт для мультимодального поиска документов",
                    "desc": "Статья представляет новый бенчмарк MMDocIR для оценки систем мультимодального поиска документов. Бенчмарк включает две задачи: поиск на уровне страниц и на уровне макетов. Датасет содержит экспертные аннотации для 1,685 вопросов и автоматически сгенерированные метки для 173,843 вопросов. Эксперименты показали, что визуальные ретриверы превосходят текстовые, а использование визуально-языковых моделей дает лучшие результаты, чем OCR-текст."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Document Retrieval with MMDocIR",
                    "desc": "This paper addresses the challenge of multi-modal document retrieval, which involves finding various types of content like figures and tables in large documents. It introduces a new benchmark called MMDocIR, which includes two tasks: page-level retrieval for finding relevant pages and layout-level retrieval for identifying specific layouts within those pages. The benchmark is supported by a comprehensive dataset with thousands of annotated questions, facilitating better training and evaluation of retrieval systems. The results show that visual retrieval methods outperform text-based methods, highlighting the importance of incorporating visual information in multi-modal retrieval tasks."
                },
                "zh": {
                    "title": "多模态文档检索的新基准MMDocIR",
                    "desc": "多模态文档检索旨在从大量文档中识别和提取各种形式的内容，如图形、表格、图表和布局信息。尽管其重要性显著，但目前缺乏有效评估多模态文档检索系统性能的基准。为了解决这一问题，本文提出了一个新的基准MMDocIR，包含页面级和布局级检索两个任务。通过严格的实验，我们发现视觉检索器的表现显著优于文本检索器，且MMDocIR训练集能有效促进多模态文档检索的训练过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08983",
            "title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities",
            "url": "https://huggingface.co/papers/2501.08983",
            "abstract": "3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.",
            "score": 8,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "39cd0826d4232170",
            "authors": [
                "Haozhe Xie",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore 637335"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08983.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Композиционная генерация 4D-городов с разделением динамики и статики",
                    "desc": "CityDreamer4D - это генеративная модель для создания неограниченных 4D-городов. Она разделяет генерацию динамических объектов (например, транспорта) и статических сцен (зданий, дорог). Модель использует разные типы нейронных полей для зданий, транспорта и фона, применяя специализированные генеративные хеш-сетки и периодические позиционные эмбеддинги. CityDreamer4D демонстрирует передовые результаты в генерации реалистичных 4D-городов и поддерживает различные приложения, включая редактирование объектов и городское моделирование."
                },
                "en": {
                    "title": "Revolutionizing Urban Landscapes: CityDreamer4D for Dynamic City Generation",
                    "desc": "This paper introduces CityDreamer4D, a generative model designed for creating unbounded 4D cities, which include both static and dynamic elements. The model distinguishes between dynamic objects like vehicles and static structures such as buildings, using specialized neural fields for each type. It employs a compact bird's-eye view (BEV) representation to generate realistic traffic scenarios and city layouts. Additionally, the paper provides extensive datasets for training, enabling various applications like instance editing and urban simulation while achieving high-quality results in 4D city generation."
                },
                "zh": {
                    "title": "CityDreamer4D：无限4D城市生成的新突破",
                    "desc": "近年来，3D场景生成受到了越来越多的关注，并取得了显著进展。生成4D城市比3D场景更具挑战性，因为城市环境中存在结构复杂、视觉多样的物体，如建筑和车辆。为了解决这些问题，我们提出了CityDreamer4D，这是一种专门用于生成无限4D城市的组合生成模型。该模型通过将动态物体与静态场景分离，并使用不同类型的神经场来组合城市中的所有物体，从而实现高质量的城市生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08994",
            "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
            "url": "https://huggingface.co/papers/2501.08994",
            "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
            "score": 8,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "0d164d45ba2a5c71",
            "authors": [
                "Chenyang Si",
                "Weichen Fan",
                "Zhengyao Lv",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore, 639798",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08994.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "RepVideo: стабильные представления для качественной генерации видео",
                    "desc": "Статья представляет RepVideo - улучшенную систему представлений для диффузионных моделей генерации видео на основе текста. Авторы обнаружили, что вариации в картах внимания между слоями приводят к нестабильным семантическим представлениям и снижают согласованность соседних кадров. RepVideo решает эту проблему путем накопления признаков из соседних слоев для создания обогащенных представлений. Эксперименты показывают, что RepVideo значительно улучшает способность генерировать точные пространственные образы и повышает временную согласованность при генерации видео."
                },
                "en": {
                    "title": "Enhancing Video Generation with Stable Representations",
                    "desc": "This paper presents RepVideo, a new framework designed to improve video generation using text-to-video diffusion models. It identifies issues with unstable semantic representations caused by variations in attention maps across different layers of the model. By accumulating features from neighboring layers, RepVideo creates more stable and enriched representations that enhance the model's ability to maintain consistency between adjacent frames. The results show that RepVideo significantly improves both the spatial accuracy of generated videos and their temporal coherence, leading to more realistic video outputs."
                },
                "zh": {
                    "title": "提升视频生成质量的RepVideo框架",
                    "desc": "本论文探讨了扩散模型在视频生成中的应用，提出了RepVideo框架以改善视频生成的质量。研究发现中间层特征的注意力图存在显著差异，这导致语义表示的不稳定性，进而影响相邻帧之间的相似性和时间一致性。RepVideo通过从相邻层累积特征，形成更丰富的表示，从而捕捉更稳定的语义信息。实验结果表明，RepVideo显著提高了生成视频的空间表现能力和时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08365",
            "title": "Towards Best Practices for Open Datasets for LLM Training",
            "url": "https://huggingface.co/papers/2501.08365",
            "abstract": "Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.   While this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.",
            "score": 6,
            "issue_id": 1702,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "90686080aa439157",
            "authors": [
                "Stefan Baack",
                "Stella Biderman",
                "Kasia Odrozek",
                "Aviya Skowron",
                "Ayah Bdeir",
                "Jillian Bommarito",
                "Jennifer Ding",
                "Maximilian Gahntz",
                "Paul Keller",
                "Pierre-Carl Langlais",
                "Greg Lindahl",
                "Sebastian Majstorovic",
                "Nik Marda",
                "Guilherme Penedo",
                "Maarten Van Segbroeck",
                "Jennifer Wang",
                "Leandro von Werra",
                "Mitchell Baker",
                "Julie Belião",
                "Kasia Chmielinski",
                "Marzieh Fadaee",
                "Lisa Gutermuth",
                "Hynek Kydlíček",
                "Greg Leppert",
                "EM Lewis-Jong",
                "Solana Larsen",
                "Shayne Longpre",
                "Angela Oduor Lungati",
                "Cullen Miller",
                "Victor Miller",
                "Max Ryabinin",
                "Kathleen Siminyu",
                "Andrew Strait",
                "Mark Surman",
                "Anna Tumadóttir",
                "Maurice Weber",
                "Rebecca Weiss",
                "Lee White",
                "Thomas Wolf"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08365.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#ethics",
                    "#data",
                    "#dataset"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Открытые данные для ответственного ИИ: вызовы и перспективы",
                    "desc": "Статья рассматривает проблему обучения больших языковых моделей (LLM) на данных без разрешения правообладателей. Анализируются юридические аспекты этой практики в разных странах и связанные с ней судебные иски. Отмечается тенденция к ограничению информации о наборах данных для обучения, что негативно влияет на прозрачность и подотчетность в сфере ИИ. Обсуждаются вызовы создания моделей на основе открытых данных, включая технические и социологические аспекты."
                },
                "en": {
                    "title": "Towards Transparent AI: The Need for Open Data Collaboration",
                    "desc": "This paper discusses the legal and ethical challenges surrounding the training of large language models (LLMs) using copyrighted data without permission. It highlights the varying legal frameworks across different countries, particularly the ambiguity in the United States compared to more defined rules in the EU and Japan. The authors argue that the trend of limiting information about training datasets undermines transparency and innovation in AI, making it difficult for researchers and stakeholders to understand the models. They propose that a shift towards using open access and public domain data is necessary, but emphasize the need for collaboration and investment in infrastructure to overcome the technical and sociological barriers involved."
                },
                "zh": {
                    "title": "推动开放许可数据的AI训练未来",
                    "desc": "许多人工智能公司在没有版权拥有者许可的情况下训练大型语言模型（LLMs）。不同国家对这种做法的合法性有不同的规定，欧盟和日本在某些限制下允许，而美国的法律环境则较为模糊。这种限制数据共享的信息趋势，妨碍了透明度、问责制和创新，影响了研究人员和受影响个体获取理解AI模型所需的信息。为了实现未来能够在开放许可数据上训练AI系统，需要在法律、技术和政策领域进行合作，并投资于元数据标准和数字化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07783",
            "title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding",
            "url": "https://huggingface.co/papers/2501.07783",
            "abstract": "Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP.",
            "score": 3,
            "issue_id": 1701,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "87295e912b5b0670",
            "authors": [
                "Zhaokai Wang",
                "Xizhou Zhu",
                "Xue Yang",
                "Gen Luo",
                "Hao Li",
                "Changyao Tian",
                "Wenhan Dou",
                "Junqi Ge",
                "Lewei Lu",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Sensetime",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07783.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективные многомасштабные сети для точного визуального восприятия",
                    "desc": "Статья представляет новую архитектуру нейронных сетей под названием Parameter-Inverted Image Pyramid Networks (PIIP). PIIP использует предобученные модели (ViT или CNN) в качестве ветвей для обработки многомасштабных изображений, где изображения с более высоким разрешением обрабатываются меньшими сетевыми ветвями для баланса вычислительных затрат и производительности. Авторы также предлагают новый механизм взаимодействия признаков между ветвями. PIIP демонстрирует превосходную производительность по сравнению с одноветвенными и существующими многоразрешающими подходами при меньших вычислительных затратах в задачах обнаружения объектов, сегментации, классификации изображений и мультимодального понимания."
                },
                "en": {
                    "title": "Efficient Multi-Scale Processing with PIIP Networks",
                    "desc": "This paper introduces Parameter-Inverted Image Pyramid Networks (PIIP), a new architecture designed to efficiently process multi-scale images for visual tasks. Unlike traditional methods that use a single large model for all resolutions, PIIP employs smaller branches for higher resolution images, reducing computational costs while maintaining performance. The architecture also features a unique cross-branch interaction mechanism to enhance feature integration across different scales. Experimental results demonstrate that PIIP outperforms existing methods in various tasks, achieving significant accuracy improvements with lower resource usage."
                },
                "zh": {
                    "title": "高效多尺度图像处理的新方法",
                    "desc": "本文提出了一种新的网络架构，称为参数反转图像金字塔网络（PIIP），旨在提高多尺度图像处理的效率。PIIP利用预训练模型作为分支，处理不同分辨率的图像，从而在性能和计算成本之间取得平衡。通过引入跨分支特征交互机制，PIIP能够有效整合来自不同空间尺度的信息。实验结果表明，PIIP在目标检测、分割和多模态理解等任务上表现优于现有方法，同时显著降低了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09012",
            "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
            "url": "https://huggingface.co/papers/2501.09012",
            "abstract": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.",
            "score": 3,
            "issue_id": 1699,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "e516a920b6534cc0",
            "authors": [
                "Ruixiang Jiang",
                "Changwen Chen"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09012.jpg",
            "data": {
                "categories": [
                    "#artificial intelligence",
                    "#reasoning",
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Искусственный интеллект учится оценивать искусство",
                    "desc": "Исследование посвящено использованию мультимодальных языковых моделей (MLLM) для оценки эстетики произведений искусства. Авторы создали набор данных MM-StyleBench для тестирования художественной стилизации и разработали метод моделирования человеческих предпочтений. Эксперименты выявили проблему галлюцинаций MLLM при оценке искусства, связанную с субъективностью ответов. Предложенный метод ArtCoT улучшает способность MLLM к рассуждениям об эстетике путем декомпозиции задач и использования конкретного языка."
                },
                "en": {
                    "title": "Enhancing MLLMs for Art Evaluation through Structured Reasoning",
                    "desc": "This paper investigates how Multimodal Large Language Models (MLLMs) can assess the aesthetics of artworks. The authors introduce MM-StyleBench, a new dataset designed to benchmark artistic stylization. They also create a method for modeling human preferences and analyze the correlation between MLLMs' evaluations and human judgments. The study highlights a hallucination problem in MLLMs when evaluating art and proposes ArtCoT, which improves reasoning by using task decomposition and specific language, providing insights for applications like style transfer and artistic image generation."
                },
                "zh": {
                    "title": "提升多模态大语言模型的艺术推理能力",
                    "desc": "本研究首次探讨了多模态大语言模型（MLLMs）在评估艺术作品美学时的推理能力。我们构建了一个新的高质量数据集MM-StyleBench，用于艺术风格化的基准测试。通过系统的相关性分析，我们发现MLLMs在艺术评估中存在固有的幻觉问题，且与人类偏好存在主观性关联。我们提出了ArtCoT方法，表明艺术特定任务分解和使用具体语言可以提升MLLMs的美学推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08809",
            "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
            "url": "https://huggingface.co/papers/2501.08809",
            "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.",
            "score": 3,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "d4d018c9adb2579c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#audio",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "XMusic: ИИ-композитор нового поколения с управляемыми эмоциями",
                    "desc": "Статья представляет XMusic - генерализованный фреймворк для генерации символической музыки, поддерживающий различные типы промптов. XMusic состоит из двух ключевых компонентов: XProjector для обработки промптов и XComposer для генерации музыки. Авторы также создали датасет XMIDI, содержащий более 100 тысяч MIDI-файлов с аннотациями эмоций и жанров. Согласно оценкам, XMusic значительно превосходит современные методы по качеству генерируемой музыки."
                },
                "en": {
                    "title": "XMusic: Emotionally Controlled Music Generation Made Easy!",
                    "desc": "This paper introduces XMusic, a new framework for generating symbolic music that can be controlled by emotional prompts. It includes two main components: XProjector, which converts various input types into musical elements, and XComposer, which generates and selects high-quality music. The framework uses a multi-task learning approach to ensure the generated music meets quality, emotional, and genre standards. Additionally, the authors created a large dataset, XMIDI, to support their research and demonstrate that XMusic outperforms existing methods in music generation."
                },
                "zh": {
                    "title": "XMusic：情感可控的高质量音乐生成",
                    "desc": "近年来，人工智能生成内容（AIGC）在图像合成和文本生成领域取得了显著进展，但在音乐生成方面仍面临挑战。本文提出了一种通用的符号音乐生成框架XMusic，能够通过灵活的提示生成可控情感和高质量的符号音乐。XMusic由两个核心组件组成：XProjector和XComposer，前者将多种模态的提示解析为音乐元素，后者则生成和选择高质量的音乐。通过构建大规模的XMIDI数据集和多任务学习方案，XMusic在音乐质量上显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08970",
            "title": "Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography",
            "url": "https://huggingface.co/papers/2501.08970",
            "abstract": "We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.",
            "score": 2,
            "issue_id": 1702,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "858fc03ac78b66c1",
            "authors": [
                "Ilia Shumailov",
                "Daniel Ramage",
                "Sarah Meiklejohn",
                "Peter Kairouz",
                "Florian Hartmann",
                "Borja Balle",
                "Eugene Bagdasarian"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08970.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#ethics",
                    "#architecture",
                    "#security",
                    "#inference"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Машинное обучение как доверенный посредник для безопасных вычислений",
                    "desc": "Статья представляет новый подход к безопасным вычислениям с использованием машинного обучения - Trusted Capable Model Environments (TCME). TCME предлагается как альтернатива традиционным криптографическим методам для обеспечения конфиденциальности при взаимодействии с ненадежными сторонами. Авторы утверждают, что мощные модели машинного обучения могут выполнять роль доверенной третьей стороны, позволяя проводить безопасные вычисления для приложений, которые ранее были невозможны. В статье описываются возможные применения TCME и обсуждаются текущие ограничения и перспективы развития этого подхода."
                },
                "en": {
                    "title": "Empowering Privacy with Trusted Machine Learning Models",
                    "desc": "This paper introduces Trusted Capable Model Environments (TCMEs) as a novel solution for secure computations involving untrusted parties. It suggests that advanced machine learning models can act as trusted intermediaries, allowing for private data sharing while maintaining privacy. The authors highlight how TCMEs can efficiently manage input/output constraints and control information flow, making them suitable for applications where traditional cryptographic methods fall short. They also present various use cases and acknowledge the limitations of their approach, paving the way for future developments in secure machine learning applications."
                },
                "zh": {
                    "title": "利用机器学习实现安全计算的新方法",
                    "desc": "本文探讨了在与不可信方互动时如何平衡隐私和计算效率。我们提出了可信能力模型环境（TCME），作为一种新的安全计算方法，利用机器学习模型充当可信第三方。TCME在输入/输出约束下进行交互，并通过显式的信息流控制和无状态性来保护隐私。我们展示了TCME在解决一些经典密码学问题上的潜力，并讨论了未来的实施路径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09019",
            "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
            "url": "https://huggingface.co/papers/2501.09019",
            "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
            "score": 2,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "c4c991699f684865",
            "authors": [
                "Jingyuan Chen",
                "Fuchen Long",
                "Jie An",
                "Zhaofan Qiu",
                "Ting Yao",
                "Jiebo Luo",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Rochester, Rochester, NY USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09019.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "🐍",
                "ru": {
                    "title": "Бесконечное видео: Ouroboros-Diffusion для непрерывной генерации согласованного контента",
                    "desc": "Эта статья представляет новый метод генерации видео произвольной длины под названием Ouroboros-Diffusion. Метод улучшает структурную и сюжетную согласованность видео с помощью нового подхода к выборке латентного пространства и механизма Subject-Aware Cross-Frame Attention. Авторы также вводят самоповторяющееся руководство, использующее информацию из предыдущих очищенных кадров для улучшения шумных кадров. Эксперименты на бенчмарке VBench показывают превосходство Ouroboros-Diffusion в сохранении согласованности субъектов, плавности движения и временной согласованности."
                },
                "en": {
                    "title": "Ouroboros-Diffusion: Enhancing Long Video Consistency and Coherence",
                    "desc": "The paper introduces Ouroboros-Diffusion, a new framework for improving long video generation using a pre-trained text-to-video model. It addresses the limitations of FIFO-Diffusion, particularly in maintaining long-range temporal consistency across video frames. The proposed method enhances structural consistency through a novel latent sampling technique and improves subject consistency with a Subject-Aware Cross-Frame Attention mechanism. Additionally, self-recurrent guidance is implemented to utilize information from previous frames, resulting in videos with better visual coherence and smoother transitions."
                },
                "zh": {
                    "title": "Ouroboros-Diffusion：提升视频生成一致性的创新框架",
                    "desc": "FIFO视频扩散是一种基于预训练文本到视频模型的长视频生成方法，但在生成视频时常常缺乏长时间的一致性。本文提出了Ouroboros-Diffusion框架，通过引入新的潜在采样技术和主题感知跨帧注意机制，增强了视频的结构和内容一致性。该方法确保了帧之间的平滑过渡，并通过自递归引导技术利用前面清晰帧的信息来改善后面噪声帧的去噪效果。实验结果表明，Ouroboros-Diffusion在主题一致性、运动平滑性和时间一致性方面优于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-01-15.html",
    "link_next": "2025-01-17.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1月15日"
    },
    "short_date_next": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#artificial intelligence": 1
    },
    "zh": {
        "text": "这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。",
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
        "pinyin": "这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。\n\nzhè piān wén zhāng jiè shào le duō mó tài wén dàng jiàn suǒ, zhǐ zài cóng dà liàng wén dàng zhōng shí bié hé jiàn suǒ tú biǎo, biǎo gé, tú xíng hé bù jú xìn xī. Jǐn guǎn qí zhòng yào xìng, quē fá yǒu xiào de jī zhǔn cè shì lái píng gū xì tǒng xìng néng. Wèi cǐ, wén zhāng tí chū le yī gè xīn de jī zhǔn cè shì MMDocIR, bāo kuò yè miàn jí hé bù jú jí jiàn suǒ rèn wù. MMDocIR bāo hán 1,685 gè zhuān jiā biāo zhù hé 173,843 gè zì dòng biāo zhù de wèn tí, shì yī gè zhòng yào de zī yuán. Shí yàn xiǎn shì, shì jué jiàn suǒ qì bǐ wén běn jiàn suǒ qì biǎo xiàn gèng hǎo, bìng qiě shǐ yòng VLM-text de wén běn jiàn suǒ qì yōu yú shǐ yòng OCR-text de jiàn suǒ qì.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '检索', 'pinyin': 'jiǎn suǒ', 'trans': 'retrieval'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'},\n{'word': '布局', 'pinyin': 'bù jiú', 'trans': 'layout'},\n{'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'},\n{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '页面级', 'pinyin': 'yè miàn jí', 'trans': 'page-level'},\n{'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotation'},\n{'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resource'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'},\n{'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'},\n{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]",
        "trans": "This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text.",
        "update_ts": "2025-01-16 09:10"
    }
}