{
    "date": {
        "ru": "27 января",
        "en": "January 27",
        "zh": "1月27日"
    },
    "time_utc": "2025-01-27 00:46",
    "weekday": 0,
    "issue_id": 1870,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.13200",
            "title": "SRMT: Shared Memory for Multi-agent Lifelong Pathfinding",
            "url": "https://huggingface.co/papers/2501.13200",
            "abstract": "Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the principal challenges in MARL is the need for explicit prediction of the agents' behavior to achieve cooperation. To resolve this issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly and coordinate their actions. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that requires agents to pass through a narrow corridor and on a POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a variety of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared recurrent memory into the transformer-based architectures can enhance coordination in decentralized multi-agent systems. The source code for training and evaluation is available on GitHub: https://github.com/Aloriosa/srmt.",
            "score": 53,
            "issue_id": 1846,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 января",
                "en": "January 22",
                "zh": "1月22日"
            },
            "hash": "52d8b3716543aa42",
            "authors": [
                "Alsu Sagirova",
                "Yuri Kuratov",
                "Mikhail Burtsev"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "London Institute for Mathematical Sciences, London, UK",
                "Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13200.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#rl",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SRMT: Улучшение координации в децентрализованных мультиагентных системах",
                    "desc": "В статье представлен новый подход к мультиагентному обучению с подкреплением (MARL) - Shared Recurrent Memory Transformer (SRMT). SRMT расширяет возможности трансформеров с памятью для мультиагентных систем, объединяя и глобально транслируя индивидуальную рабочую память агентов. Этот метод позволяет агентам неявно обмениваться информацией и координировать свои действия. SRMT показал превосходные результаты на задаче частично наблюдаемого мультиагентного поиска пути, превзойдя базовые алгоритмы обучения с подкреплением и продемонстрировав эффективную генерализацию."
                },
                "en": {
                    "title": "Enhancing Agent Coordination with Shared Memory Transformers",
                    "desc": "This paper introduces the Shared Recurrent Memory Transformer (SRMT), a novel approach in multi-agent reinforcement learning (MARL) that enhances cooperation among agents. SRMT utilizes a memory transformer architecture to allow agents to share and broadcast their individual memories, facilitating implicit communication and coordination. The effectiveness of SRMT is demonstrated through experiments on the Partially Observable Multi-Agent Pathfinding problem, where it outperforms traditional reinforcement learning methods, particularly in scenarios with sparse rewards. The results indicate that integrating shared memory into transformer models significantly improves the performance of decentralized multi-agent systems."
                },
                "zh": {
                    "title": "共享记忆提升多智能体协调能力",
                    "desc": "多智能体强化学习（MARL）在解决合作和竞争的多智能体问题上取得了显著进展。本文提出了一种共享递归记忆变换器（SRMT），通过汇聚和全局广播个体工作记忆，帮助智能体隐式交换信息并协调行动。我们在部分可观察的多智能体路径规划问题上评估了SRMT，结果显示其在稀疏奖励下表现优于多种强化学习基线，并且在训练时未见过的更长走廊上也能有效泛化。SRMT在多个基准任务中与最新的MARL、混合和基于规划的算法具有竞争力，表明共享递归记忆的引入可以增强去中心化多智能体系统的协调能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13629",
            "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
            "url": "https://huggingface.co/papers/2501.13629",
            "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.",
            "score": 37,
            "issue_id": 1842,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "d036f75a81877ded",
            "authors": [
                "Zhenghao Lin",
                "Zihao Tang",
                "Xiao Liu",
                "Yeyun Gong",
                "Yi Cheng",
                "Qi Chen",
                "Hang Li",
                "Ying Xin",
                "Ziyue Yang",
                "Kailai Yang",
                "Yu Yan",
                "Xiao Liang",
                "Shuai Lu",
                "Yiming Huang",
                "Zheheng Luo",
                "Lei Qu",
                "Xuan Feng",
                "Yaoxiang Wang",
                "Yuqing Xia",
                "Feiyang Chen",
                "Yuting Jiang",
                "Yasen Hu",
                "Hao Ni",
                "Binyang Li",
                "Guoshuai Zhao",
                "Jui-Hao Chiang",
                "Zhongxin Guo",
                "Chen Lin",
                "Kun Kuang",
                "Wenjie Li",
                "Yelong Shen",
                "Jian Jiao",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.13629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#long_context",
                    "#training",
                    "#synthetic",
                    "#data",
                    "#inference"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Sigma: эффективная ЯМ для системной области с инновационным механизмом внимания",
                    "desc": "Исследователи представили Sigma - эффективную большую языковую модель, специализированную для системной области. Модель использует новую архитектуру с DiffQKV-вниманием, которая оптимизирует компоненты Q, K и V механизма внимания для повышения эффективности. Sigma предобучена на тщательно собранных данных системной области объемом 6T токенов. На общих задачах модель показывает результаты на уровне современных аналогов, а в системной области значительно превосходит GPT-4 на новом бенчмарке AIMicius."
                },
                "en": {
                    "title": "Sigma: Revolutionizing System Domain Language Models with DiffQKV Attention",
                    "desc": "The paper presents Sigma, a specialized large language model designed for the system domain, utilizing a new architecture called DiffQKV attention. This innovative attention mechanism optimizes the Query, Key, and Value components to improve inference efficiency, particularly in long-context scenarios. Through extensive experiments, the authors show that Sigma achieves significant speed improvements, outperforming traditional models like GPT-4 in various tasks. The model is pre-trained on a vast dataset, including 19.5 billion tokens from the system domain, establishing a new benchmark for performance in this area."
                },
                "zh": {
                    "title": "Sigma：系统领域的高效语言模型",
                    "desc": "我们介绍了Sigma，这是一个高效的大型语言模型，专门针对系统领域。它采用了一种新颖的架构，包括DiffQKV注意力机制，并在我们精心收集的系统领域数据上进行了预训练。DiffQKV注意力通过优化注意力机制中的查询（Q）、键（K）和值（V）组件，显著提高了推理效率。实验结果表明，Sigma在系统领域的表现优于GPT-4，绝对提升幅度可达52.5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13918",
            "title": "Improving Video Generation with Human Feedback",
            "url": "https://huggingface.co/papers/2501.13918",
            "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and standard supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs. Project page: https://gongyeliu.github.io/videoalign.",
            "score": 35,
            "issue_id": 1849,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "933a6a47d8d5e20a",
            "authors": [
                "Jie Liu",
                "Gongye Liu",
                "Jiajun Liang",
                "Ziyang Yuan",
                "Xiaokun Liu",
                "Mingwu Zheng",
                "Xiele Wu",
                "Qiulin Wang",
                "Wenyu Qin",
                "Menghan Xia",
                "Xintao Wang",
                "Xiaohong Liu",
                "Fei Yang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Yujiu Yang",
                "Wanli Ouyang"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13918.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#video",
                    "#rlhf"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Улучшение генерации видео с помощью человеческой обратной связи и обучения с подкреплением",
                    "desc": "Данная работа представляет систематический подход к улучшению генерации видео с использованием обратной связи от людей. Авторы создали большой датасет человеческих предпочтений для современных моделей генерации видео и разработали многомерную модель оценки видео VideoReward. Они также предложили три алгоритма выравнивания для моделей на основе потоков: Flow-DPO, Flow-RWR и Flow-NRG. Эксперименты показали, что VideoReward значительно превосходит существующие модели оценки, а Flow-DPO демонстрирует лучшие результаты по сравнению с другими методами."
                },
                "en": {
                    "title": "Enhancing Video Generation with Human Feedback and Reward Models",
                    "desc": "This paper addresses challenges in video generation, particularly issues of motion smoothness and alignment with prompts. The authors propose a new pipeline that utilizes human feedback to enhance video generation models. They create a large dataset of human preferences and introduce VideoReward, a model that evaluates video quality based on these preferences. The study also presents three innovative algorithms for improving flow-based video generation, demonstrating that their methods outperform existing models and allow for personalized video quality adjustments."
                },
                "zh": {
                    "title": "优化视频生成，提升用户体验",
                    "desc": "本研究提出了一种系统化的视频生成模型优化方法，旨在解决视频生成中的不平滑运动和视频与提示之间的错位问题。我们构建了一个大规模的人类偏好数据集，专注于现代视频生成模型，并进行了多维度的成对注释。引入的VideoReward模型通过强化学习最大化奖励，并提出了三种对齐算法，以提高流模型的性能。实验结果表明，VideoReward在奖励模型中表现优异，Flow-DPO在性能上优于其他方法，满足用户个性化的视频质量需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13926",
            "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
            "url": "https://huggingface.co/papers/2501.13926",
            "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
            "score": 24,
            "issue_id": 1841,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "61611cbe661736ff",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Chengzhuo Tong",
                "Zhizheng Zhao",
                "Peng Gao",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "MMLab",
                "MiuLar Lab",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13926.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Рассуждения по цепочке мыслей открывают новые горизонты в генерации изображений",
                    "desc": "Статья исследует применение рассуждений по цепочке мыслей (Chain-of-Thought) для улучшения автореграссивной генерации изображений. Авторы предлагают три метода: масштабирование вычислений во время тестирования, оптимизацию предпочтений модели и интеграцию этих техник. Они также представляют новые модели вознаграждения PARM и PARM++, специально разработанные для генерации изображений. Результаты показывают значительное улучшение производительности базовой модели Show-o на 24% по сравнению с эталоном GenEval."
                },
                "en": {
                    "title": "Enhancing Image Generation with Chain-of-Thought Reasoning",
                    "desc": "This paper explores the use of Chain-of-Thought (CoT) reasoning to improve autoregressive image generation models. It investigates three main techniques: enhancing verification through increased computation, aligning model preferences using Direct Preference Optimization (DPO), and combining these methods for better outcomes. The authors introduce the Potential Assessment Reward Model (PARM) and its enhanced version PARM++, which help assess and correct image generation steps. The results show a significant performance boost, achieving a 24% improvement on the GenEval benchmark compared to previous models."
                },
                "zh": {
                    "title": "链式思维提升图像生成性能",
                    "desc": "本文探讨了链式思维（CoT）推理在自回归图像生成中的应用潜力。我们提出了三种技术：测试时计算的扩展、与直接偏好优化（DPO）对齐模型偏好，以及这些技术的整合。研究结果表明，这些方法可以有效结合，显著提升图像生成性能。此外，我们提出了潜力评估奖励模型（PARM）和PARM++，专门用于自回归图像生成，进一步提高了生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13826",
            "title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos",
            "url": "https://huggingface.co/papers/2501.13826",
            "abstract": "Humans acquire knowledge through three cognitive stages: perceiving information, comprehending knowledge, and adapting knowledge to solve novel problems. Videos serve as an effective medium for this learning process, facilitating a progression through these cognitive stages. However, existing video benchmarks fail to systematically evaluate the knowledge acquisition capabilities in Large Multimodal Models (LMMs). To address this gap, we introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU features a curated collection of 300 expert-level videos and 900 human-annotated questions across six disciplines, evaluating knowledge acquisition through stage-aligned question-answer pairs: Perception, Comprehension, and Adaptation. A proposed knowledge gain metric, {\\Delta}knowledge, quantifies improvement in performance after video viewing. Evaluation of LMMs reveals a steep decline in performance as cognitive demands increase and highlights a significant gap between human and model knowledge acquisition, underscoring the need for methods to enhance LMMs' capability to learn and adapt from videos.",
            "score": 18,
            "issue_id": 1848,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "4475243a608bc988",
            "authors": [
                "Kairui Hu",
                "Penghao Wu",
                "Fanyi Pu",
                "Wang Xiao",
                "Yuanhan Zhang",
                "Xiang Yue",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13826.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#science",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Новый рубеж в оценке мультимодального обучения ИИ",
                    "desc": "Статья представляет новый бенчмарк Video-MMMU для оценки способности больших мультимодальных моделей (LMM) приобретать знания из видео. Бенчмарк включает 300 экспертных видео и 900 вопросов по шести дисциплинам, оценивая восприятие, понимание и адаптацию знаний. Введена метрика ∆knowledge для измерения прироста знаний после просмотра видео. Результаты показывают значительный разрыв между человеческим и машинным обучением, подчеркивая необходимость улучшения LMM в области обучения на основе видео."
                },
                "en": {
                    "title": "Enhancing Knowledge Acquisition in LMMs through Video Learning",
                    "desc": "This paper introduces Video-MMMU, a benchmark designed to evaluate Large Multimodal Models (LMMs) in their ability to learn from videos. It focuses on three cognitive stages: perception, comprehension, and adaptation, using a set of 300 expert videos and 900 questions. The benchmark assesses how well LMMs can acquire knowledge through these stages, revealing a significant performance gap compared to humans. A new metric, Δknowledge, measures the improvement in LMM performance after watching videos, highlighting the need for better learning methods in these models."
                },
                "zh": {
                    "title": "提升多模态模型的视频知识获取能力",
                    "desc": "本文探讨了人类通过三个认知阶段获取知识的过程：感知信息、理解知识和适应知识以解决新问题。视频作为一种有效的学习媒介，能够促进这些认知阶段的进展。然而，现有的视频基准未能系统地评估大型多模态模型（LMMs）在知识获取方面的能力。为此，我们提出了Video-MMMU，这是一个多模态、多学科的基准，旨在评估LMMs从视频中获取和利用知识的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13919",
            "title": "Temporal Preference Optimization for Long-Form Video Understanding",
            "url": "https://huggingface.co/papers/2501.13919",
            "abstract": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website.",
            "score": 17,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "6e08b56893fb98a9",
            "authors": [
                "Rui Li",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Zeyu Wang",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13919.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "TPO: Улучшение временного понимания в видео-LMM без ручной разметки",
                    "desc": "Статья представляет новый метод под названием Temporal Preference Optimization (TPO) для улучшения временной привязки в видео-LMM моделях. TPO использует самообучение на основе предпочтений для различения хорошо и плохо привязанных во времени ответов. Метод работает на двух уровнях: локальная временная привязка для конкретных сегментов видео и комплексная для всей последовательности. Эксперименты на трех бенчмарках для длинных видео показали эффективность TPO для улучшения временного понимания в видео-LMM."
                },
                "en": {
                    "title": "Enhancing Temporal Understanding in Long Videos with TPO",
                    "desc": "This paper introduces Temporal Preference Optimization (TPO), a new framework aimed at improving how video large multimodal models (video-LMMs) understand time in long videos. TPO uses a self-training method that helps models learn to tell the difference between accurate and inaccurate timing responses by using specially curated preference datasets. These datasets focus on both specific video segments and the overall flow of the entire video, enhancing the model's ability to grasp temporal relationships. The results show that TPO significantly boosts performance on various benchmarks, making it a promising approach for better temporal reasoning in video analysis."
                },
                "zh": {
                    "title": "时间偏好优化：提升视频理解的关键",
                    "desc": "尽管视频大型多模态模型（video-LMMs）取得了显著进展，但在长视频中实现有效的时间定位仍然是一个挑战。为了解决这个问题，我们提出了一种新的后训练框架——时间偏好优化（TPO），旨在通过偏好学习增强视频-LMMs的时间定位能力。TPO采用自我训练的方法，利用精心策划的偏好数据集，使模型能够区分准确的时间响应和不太准确的时间响应。通过在这两个层次上优化偏好数据集，TPO显著提高了时间理解能力，同时减少了对手动标注数据的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13920",
            "title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models",
            "url": "https://huggingface.co/papers/2501.13920",
            "abstract": "With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models' performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each model's strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.",
            "score": 12,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "837193826ae51376",
            "authors": [
                "Jiayi Lei",
                "Renrui Zhang",
                "Xiangfei Hu",
                "Weifeng Lin",
                "Zhen Li",
                "Wenjian Sun",
                "Ruoyi Du",
                "Le Zhuo",
                "Zhongyu Li",
                "Xinyue Li",
                "Shitian Zhao",
                "Ziyu Guo",
                "Yiting Lu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13920.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Новый рубеж в оценке моделей текст-изображение: путь к универсальному ИИ",
                    "desc": "Эта статья посвящена оценке современных моделей преобразования текста в изображение (T2I). Авторы разработали новую систему оценки IMAGINE-E для тестирования шести ведущих моделей в пяти ключевых областях. Исследование выявило выдающиеся способности моделей FLUX.1 и Ideogram2.0 в структурированных задачах и задачах специфических доменов. Результаты подчеркивают растущий потенциал моделей T2I как универсальных инструментов искусственного интеллекта."
                },
                "en": {
                    "title": "Evaluating the Future of Text-to-Image Models",
                    "desc": "This paper discusses the advancements in text-to-image (T2I) models, particularly focusing on recent models like FLUX.1 and Ideogram2.0. These models not only excel in generating images from text prompts but also show versatility in various tasks such as image editing and video generation. The authors introduce a new evaluation framework called IMAGINE-E to assess the performance of six leading T2I models across multiple domains. The findings reveal that while some models perform exceptionally well in specific tasks, there is a need for better evaluation methods to fully understand their capabilities and limitations."
                },
                "zh": {
                    "title": "文本到图像模型的未来：通用性与评估的挑战",
                    "desc": "随着扩散模型的快速发展，文本到图像（T2I）模型在提示跟随和图像生成方面取得了显著进展。新推出的模型如FLUX.1和Ideogram2.0，以及Dall-E3和Stable Diffusion 3等，展示了在各种复杂任务中的卓越表现，提出了T2I模型是否朝着通用适用性发展的疑问。除了传统的图像生成，这些模型在可控生成、图像编辑、视频、音频、3D和运动生成等多个领域也展现了能力。为了全面评估这些模型的性能，我们开发了IMAGINE-E，并对六个主要模型进行了测试，强调了它们在不同领域的优势和局限性，特别是FLUX.1和Ideogram2.0在结构化和特定领域任务中的出色表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10799",
            "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
            "url": "https://huggingface.co/papers/2501.10799",
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",
            "score": 11,
            "issue_id": 1842,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 января",
                "en": "January 18",
                "zh": "1月18日"
            },
            "hash": "d43b005a69156930",
            "authors": [
                "Yen-Ting Lin",
                "Di Jin",
                "Tengyu Xu",
                "Tianhao Wu",
                "Sainbayar Sukhbaatar",
                "Chen Zhu",
                "Yun He",
                "Yun-Nung Chen",
                "Jason Weston",
                "Yuandong Tian",
                "Arash Rahnama",
                "Sinong Wang",
                "Hao Ma",
                "Han Fang"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National Taiwan University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10799.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Шаг за шагом к надежным математическим рассуждениям ИИ",
                    "desc": "Статья представляет новый подход к обучению больших языковых моделей (LLM) для математических рассуждений. Метод Step-KTO использует бинарную обратную связь как для промежуточных шагов рассуждения, так и для конечного результата. Это позволяет модели следовать логичному ходу мыслей, а не полагаться на поверхностные шаблоны. Эксперименты на сложных математических тестах показали значительное улучшение как точности конечного ответа, так и качества промежуточных шагов рассуждения."
                },
                "en": {
                    "title": "Enhancing Trustworthy Reasoning in LLMs with Step-KTO",
                    "desc": "This paper presents Step-KTO, a new training framework for large language models (LLMs) that enhances their mathematical reasoning abilities. Unlike previous methods that focus solely on the final answer, Step-KTO provides feedback on both the reasoning process and the outcome, promoting logical coherence. By evaluating intermediate reasoning steps alongside the final result, the framework helps LLMs avoid shortcuts and develop more reliable reasoning paths. Experiments show that Step-KTO significantly boosts accuracy and improves the quality of reasoning in challenging mathematical tasks, indicating its potential for creating more interpretable AI systems."
                },
                "zh": {
                    "title": "提升推理可信度的Step-KTO框架",
                    "desc": "大型语言模型（LLMs）在数学推理方面取得了显著成功。尽管链式思维提示和自一致性采样等方法有所进展，但这些方法往往只关注最终结果的正确性，而未能确保推理过程的连贯性和可靠性。本文提出了Step-KTO，这是一种结合过程级和结果级二元反馈的训练框架，旨在引导LLMs朝着更可信的推理轨迹发展。实验结果表明，Step-KTO显著提高了最终答案的准确性和中间推理步骤的质量，展示了逐步过程反馈在LLM训练中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10018",
            "title": "DiffuEraser: A Diffusion Model for Video Inpainting",
            "url": "https://huggingface.co/papers/2501.10018",
            "abstract": "Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as a prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, a video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning,which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency.",
            "score": 10,
            "issue_id": 1846,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "8ebb9334e60b0dd7",
            "authors": [
                "Xiaowen Li",
                "Haolan Xue",
                "Peiran Ren",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10018.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#long_context",
                    "#hallucinations",
                    "#cv"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "DiffuEraser: Улучшенное восстановление видео с помощью диффузионных моделей",
                    "desc": "DiffuEraser - это новая модель для восстановления видео на основе стабильной диффузии. Она использует предварительную информацию для инициализации и слабого кондиционирования, что помогает уменьшить шумовые артефакты. Модель расширяет временные рецептивные поля для улучшения временной согласованности при выводе длинных последовательностей. Экспериментальные результаты показывают, что DiffuEraser превосходит современные методы по полноте содержания и временной согласованности."
                },
                "en": {
                    "title": "Enhancing Video Inpainting with Diffusion Models for Better Consistency and Detail",
                    "desc": "This paper presents DiffuEraser, a novel video inpainting model that utilizes stable diffusion techniques to improve the restoration of masked regions in videos. By integrating prior information for initialization and weak conditioning, the model effectively reduces noise and visual artifacts. The authors enhance temporal consistency by expanding the temporal receptive fields and utilizing the smoothing properties of Video Diffusion Models. Experimental results show that DiffuEraser surpasses existing methods in terms of content completeness and temporal coherence, while also being efficient."
                },
                "zh": {
                    "title": "DiffuEraser：提升视频修复的细节与一致性",
                    "desc": "本文介绍了一种名为DiffuEraser的视频修复模型，基于稳定扩散技术，旨在用更丰富的细节和更连贯的结构填补被遮挡的区域。我们通过引入先验信息来提供初始化和弱条件，从而减少噪声伪影和抑制幻觉现象。为了提高长序列推理过程中的时间一致性，我们扩展了先验模型和DiffuEraser的时间感受野，并利用视频扩散模型的时间平滑特性进一步增强一致性。实验结果表明，我们的方法在内容完整性和时间一致性方面优于现有的最先进技术，同时保持了可接受的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13554",
            "title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt",
            "url": "https://huggingface.co/papers/2501.13554",
            "abstract": "Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.",
            "score": 8,
            "issue_id": 1852,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "15ba8f8e21d0e703",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#story_generation",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Один промпт - одна история: последовательная генерация изображений без дообучения",
                    "desc": "Статья представляет новый метод генерации последовательных изображений из текста под названием '1Prompt1Story'. Этот подход объединяет все промпты в единый вход для диффузионных моделей, сохраняя идентичность персонажей. Метод использует две новые техники: переоценку сингулярных значений и сохраняющее идентичность кросс-внимание. '1Prompt1Story' не требует дополнительного обучения и применим к различным конфигурациям диффузионных моделей."
                },
                "en": {
                    "title": "Consistent Storytelling in Text-to-Image Generation",
                    "desc": "This paper addresses the challenge of generating consistent images that preserve character identities in text-to-image (T2I) models. The authors introduce a novel method called \"One-Prompt-One-Story\" (1Prompt1Story), which allows for the concatenation of multiple prompts into a single input, enhancing the model's ability to maintain character consistency. They employ two innovative techniques, Singular-Value Reweighting and Identity-Preserving Cross-Attention, to refine the image generation process and ensure alignment with the input descriptions. The proposed method is evaluated against existing approaches, showing improved performance in both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "一提示一故事：提升文本到图像生成的一致性",
                    "desc": "本文提出了一种新的文本到图像生成方法，称为\"一提示一故事\"（1Prompt1Story），旨在解决生成过程中角色身份一致性的问题。该方法通过将所有提示合并为单个输入，初步保持角色身份，并利用两种新技术进行生成过程的优化。我们的方法不需要大量训练数据或对模型架构的修改，具有更广泛的适用性。实验结果表明，1Prompt1Story在定量和定性评估中均优于现有的一致性生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13824",
            "title": "Hallucinations Can Improve Large Language Models in Drug Discovery",
            "url": "https://huggingface.co/papers/2501.13824",
            "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been raised by researchers, yet their potential in areas where creativity is vital, such as drug discovery, merits exploration. In this paper, we come up with the hypothesis that hallucinations can improve LLMs in drug discovery. To verify this hypothesis, we use LLMs to describe the SMILES string of molecules in natural language and then incorporate these descriptions as part of the prompt to address specific tasks in drug discovery. Evaluated on seven LLMs and five classification tasks, our findings confirm the hypothesis: LLMs can achieve better performance with text containing hallucinations. Notably, Llama-3.1-8B achieves an 18.35% gain in ROC-AUC compared to the baseline without hallucination. Furthermore, hallucinations generated by GPT-4o provide the most consistent improvements across models. Additionally, we conduct empirical analyses and a case study to investigate key factors affecting performance and the underlying reasons. Our research sheds light on the potential use of hallucinations for LLMs and offers new perspectives for future research leveraging LLMs in drug discovery.",
            "score": 5,
            "issue_id": 1853,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "bd66442952551d3e",
            "authors": [
                "Shuzhou Yuan",
                "Michael Färber"
            ],
            "affiliations": [
                "Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Germany",
                "Dresden University of Technology, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13824.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#rlhf",
                    "#science",
                    "#hallucinations"
                ],
                "emoji": "💊",
                "ru": {
                    "title": "Галлюцинации LLM ускоряют разработку лекарств",
                    "desc": "Исследователи изучили потенциал галлюцинаций в больших языковых моделях (LLM) для улучшения процесса открытия новых лекарств. Они использовали LLM для описания молекул на естественном языке и включили эти описания в промпты для решения задач в области разработки лекарств. Эксперименты на семи LLM и пяти задачах классификации подтвердили гипотезу: модели показали лучшие результаты с текстами, содержащими галлюцинации. Исследование открывает новые перспективы использования LLM в фармацевтике."
                },
                "en": {
                    "title": "Harnessing Hallucinations: Boosting Drug Discovery with LLMs",
                    "desc": "This paper explores the idea that hallucinations in Large Language Models (LLMs) can enhance their performance in drug discovery tasks. The authors hypothesize that by using LLMs to generate natural language descriptions of molecular SMILES strings, they can improve the models' effectiveness in specific classification tasks. Their experiments show that LLMs, particularly Llama-3.1-8B, achieve significant performance gains when incorporating these hallucinated descriptions, with an 18.35% increase in ROC-AUC. The study provides insights into how hallucinations can be beneficial in creative applications like drug discovery, suggesting new avenues for future research."
                },
                "zh": {
                    "title": "利用幻觉提升药物发现中的大型语言模型表现",
                    "desc": "本研究探讨了大型语言模型（LLMs）在药物发现中的潜力，尤其是它们的幻觉现象。我们提出假设，幻觉可以提升LLMs在药物发现任务中的表现。通过将LLMs生成的分子SMILES字符串描述作为提示的一部分，我们在七个LLMs和五个分类任务上进行了评估。结果表明，包含幻觉的文本能显著提高模型性能，尤其是Llama-3.1-8B在ROC-AUC上比基线提高了18.35%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13452",
            "title": "EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion",
            "url": "https://huggingface.co/papers/2501.13452",
            "abstract": "Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low similarity issues, primarily due to their reliance on low-level facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) a two-stage training strategy, incorporating a stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering a more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos.",
            "score": 5,
            "issue_id": 1846,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "b98d987f7439b94b",
            "authors": [
                "Jiangchuan Wei",
                "Shiyue Yan",
                "Wenfeng Lin",
                "Boyuan Liu",
                "Renjie Chen",
                "Mingyu Guo"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13452.jpg",
            "data": {
                "categories": [
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "EchoVideo: Новый подход к генерации видео с сохранением идентичности",
                    "desc": "EchoVideo - это новый метод генерации видео с сохранением идентичности (IPT2V). Он использует модуль слияния изображения и текста (IITF) для интеграции семантических признаков и получения чистых представлений лиц. Применяется двухэтапная стратегия обучения со стохастическим использованием поверхностной информации о лицах. EchoVideo эффективно сохраняет идентичность лиц и целостность всего тела, демонстрируя отличные результаты в генерации качественных и контролируемых видео."
                },
                "en": {
                    "title": "EchoVideo: Enhancing Identity Preservation in Video Generation",
                    "desc": "The paper introduces EchoVideo, a novel approach to identity-preserving video generation that addresses common issues like 'copy-paste' artifacts and low similarity in generated videos. It utilizes an Identity Image-Text Fusion Module (IITF) to merge high-level semantic features from text, ensuring clean facial identity representations while avoiding irrelevant details. Additionally, a two-stage training strategy is implemented, which includes a stochastic method to balance the use of shallow facial information with high-level features. This results in improved fidelity and robustness in facial identity representation, leading to high-quality video generation with better controllability."
                },
                "zh": {
                    "title": "EchoVideo：提升视频生成的身份保留与质量",
                    "desc": "近年来，视频生成技术的进步对身份保留视频生成（IPT2V）产生了重要影响。然而，现有方法在生成过程中常常出现“复制粘贴”伪影和低相似度的问题，这主要是因为它们依赖于低级别的面部图像信息。为了解决这些挑战，我们提出了EchoVideo，采用了身份图像-文本融合模块（IITF）和两阶段训练策略，旨在平衡浅层特征的增强与高层特征的利用。实验表明，EchoVideo在生成高质量、可控性和保真度的视频方面表现出色，有效保留了面部身份和全身完整性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10979",
            "title": "Control LLM: Controlled Evolution for Intelligence Retention in LLM",
            "url": "https://huggingface.co/papers/2501.10979",
            "abstract": "Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is catastrophic forgetting (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose Control LLM, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge.   Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning (+14.4% on Math-Hard) and coding performance (+10% on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities (+10.6% on C-Eval, +6.8% on CMMLU, and +30.2% on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation (<4.3% on MMLU) compared to >35% in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products.   To support further research, we release the training and evaluation code (https://github.com/linkedin/ControlLLM) along with models trained on public datasets ( https://huggingface.co/ControlLLM) to the community.",
            "score": 3,
            "issue_id": 1858,
            "pub_date": "2025-01-19",
            "pub_date_card": {
                "ru": "19 января",
                "en": "January 19",
                "zh": "1月19日"
            },
            "hash": "dd48db75ab08337c",
            "authors": [
                "Haichao Wei",
                "Yunxiang Ren",
                "Zhoutong Fu",
                "Aman Lunia",
                "Yi-Lin Chen",
                "Alice Leung",
                "Ya Xu"
            ],
            "affiliations": [
                "LinkedIn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10979.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#math",
                    "#optimization",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Контроль над забыванием: новый метод обучения языковых моделей",
                    "desc": "Control LLM - это новый подход к обучению больших языковых моделей, который решает проблему катастрофического забывания при непрерывном предобучении и дообучении. Метод использует параллельные предобученные и расширенные блоки трансформера, интерполируя их скрытые состояния. Эксперименты показали значительное улучшение производительности в математических рассуждениях, программировании и многоязычных задачах без существенной потери изначальных возможностей. Подход успешно применен в продуктах LinkedIn и открыт для исследовательского сообщества."
                },
                "en": {
                    "title": "Enhancing LLMs Without Starting Over!",
                    "desc": "This paper introduces Control LLM, a new method designed to improve the performance of Large Language Models (LLMs) without the need for complete retraining. It addresses the issue of catastrophic forgetting that occurs during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT) by using parallel pre-trained transformer blocks and interpolation strategies to align hidden states. The results show that Control LLM significantly enhances performance in various tasks, including mathematical reasoning and coding, while maintaining strong original capabilities. The method has been validated through extensive experiments and is made available for further research, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "提升大型语言模型能力的新方法",
                    "desc": "大型语言模型（LLMs）需要大量计算资源，因此在不从头开始重新训练的情况下提升其能力至关重要。本文提出了一种新方法Control LLM，通过并行预训练和扩展的变换器块，利用插值策略对齐其隐藏状态，从而有效地保留现有任务的性能并无缝整合新知识。实验结果表明，Control LLM在连续预训练和连续监督微调中表现出色，显著提高了数学推理和编码性能，同时在多语言能力上也有显著提升。该方法在保持原有强大能力的同时，减少了数据和计算的需求，展示了其在开源模型中的领先地位。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13075",
            "title": "Evolution and The Knightian Blindspot of Machine Learning",
            "url": "https://huggingface.co/papers/2501.13075",
            "abstract": "This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.",
            "score": 3,
            "issue_id": 1845,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 января",
                "en": "January 22",
                "zh": "1月22日"
            },
            "hash": "5be12844b33bd729",
            "authors": [
                "Joel Lehman",
                "Elliot Meyerson",
                "Tarek El-Gaaly",
                "Kenneth O. Stanley",
                "Tarin Ziyaee"
            ],
            "affiliations": [
                "Cognizant AI Labs",
                "Second Nature AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13075.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#agents",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Преодоление неизвестного: уроки эволюции для машинного обучения",
                    "desc": "Статья утверждает, что машинное обучение упускает важный аспект общего интеллекта: устойчивость к качественно неизвестному будущему в открытом мире. Авторы сравнивают обучение с подкреплением (RL) и биологическую эволюцию, показывая, что RL часто не справляется с непредвиденными ситуациями. В статье исследуются предположения, лежащие в основе формализмов RL, и выявляются механизмы, с помощью которых эволюционные процессы способствуют устойчивости к новым и непредсказуемым вызовам. Авторы приходят к выводу, что хрупкость машинного обучения может быть результатом слепых пятен в его формализмах, и значительные улучшения могут быть достигнуты путем прямого противостояния проблеме неопределенности Найта."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing ML Robustness through Evolutionary Insights",
                    "desc": "This paper highlights a critical gap in machine learning (ML) regarding its ability to handle unknown future scenarios, which is essential for general intelligence. It draws parallels between reinforcement learning (RL) and biological evolution, emphasizing that while RL struggles with unforeseen situations, evolution naturally adapts to them. The authors argue that current ML formalisms overlook Knightian uncertainty, which limits the robustness of AI systems in open-world environments. They propose that by understanding and integrating evolutionary mechanisms, ML can improve its resilience to unpredictable challenges."
                },
                "zh": {
                    "title": "机器学习需面对未知不确定性挑战",
                    "desc": "这篇论文指出，机器学习（ML）在处理开放世界中的未知未来时，忽视了一个重要方面：对未知不确定性的鲁棒性。作者将这种鲁棒性与经济学中的奈特不确定性（Knightian Uncertainty）相联系，认为这是机器学习关键形式化中被排除的因素。通过对比强化学习（RL）与生物进化过程，论文强调了RL在开放世界情境中的局限性，并探讨了生物进化如何在没有明确理论的情况下，培养出适应复杂环境的能力。最后，作者认为，机器学习的脆弱性可能源于其形式化中的盲点，直接面对奈特不确定性挑战可能会带来显著的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13124",
            "title": "Debate Helps Weak-to-Strong Generalization",
            "url": "https://huggingface.co/papers/2501.13124",
            "abstract": "Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision. However, future superhuman models will surpass the capability of humans. Therefore, humans will only be able to weakly supervise superhuman models. This expected deficiency of human evaluation would weaken the safety of future AI systems. Scalable oversight and weak-to-strong generalization are two complementary approaches to tackle this issue. In this paper, we attempt to combine the strengths of these two approaches to further improve alignment. Specifically, we investigate ways of improving human supervision with a strong pretrained model and then supervise the strong model with enhanced weak human supervision. To make iterative empirical progress, we consider an analogy: can we use a strong model to improve weak model supervision and then use it to supervise the strong model? We empirically test it by finetuning a small weak model on ground truth labels with the additional help from a large strong model, and then finetuning the strong model on labels generated by the weak model. We find that debate can assist a weak model in extracting trustworthy information from an untrustworthy strong model, which provides leverage as context on samples when training a weak model. We also show that an ensemble of weak models helps exploit long arguments generated by strong model debaters and obtain a more robust supervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP benchmarks show that the combination approach leads to better alignment, which indicates that debate has the potential to help weak-to-strong generalization.",
            "score": 3,
            "issue_id": 1843,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "cacd0d01e3d119ee",
            "authors": [
                "Hao Lang",
                "Fei Huang",
                "Yongbin Li"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13124.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение контроля над ИИ: от слабого к сильному",
                    "desc": "Эта статья исследует методы улучшения контроля над сверхчеловеческими моделями искусственного интеллекта. Авторы предлагают комбинированный подход, используя сильную предобученную модель для улучшения слабого человеческого надзора, а затем применяя этот улучшенный надзор для обучения сильной модели. Эксперименты показывают, что метод дебатов помогает слабой модели извлекать достоверную информацию из ненадежной сильной модели. Результаты на бенчмарках OpenAI демонстрируют, что комбинированный подход приводит к лучшему выравниванию моделей с желаемым поведением."
                },
                "en": {
                    "title": "Enhancing AI Alignment through Model Debate and Supervision",
                    "desc": "This paper addresses the challenge of aligning superhuman AI models with desired behaviors, given that human supervision may be insufficient. It proposes a novel approach that combines scalable oversight with weak-to-strong generalization to enhance model alignment. The authors explore how a strong pretrained model can improve the supervision of a weak model, and in turn, how the weak model can provide valuable feedback to the strong model. Their experiments demonstrate that using debate between models can help extract reliable information, leading to improved alignment and performance on NLP tasks."
                },
                "zh": {
                    "title": "利用辩论提升AI模型的监督能力",
                    "desc": "本文探讨了如何在未来超人类模型的监督下改善人类的监督能力。由于人类的监督能力有限，未来的AI系统可能会面临安全性问题。我们提出了一种结合可扩展监督和弱到强泛化的方法，通过强大的预训练模型来增强人类的监督。实验结果表明，辩论可以帮助弱模型从强模型中提取可靠信息，从而提高监督的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11858",
            "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
            "url": "https://huggingface.co/papers/2501.11858",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
            "score": 2,
            "issue_id": 1862,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "af76793f3055f7e0",
            "authors": [
                "Zhili Cheng",
                "Yuge Tu",
                "Ran Li",
                "Shiqi Dai",
                "Jinyi Hu",
                "Shengding Hu",
                "Jiahao Li",
                "Yang Shi",
                "Tianyu Yu",
                "Weize Chen",
                "Lei Shi",
                "Maosong Sun"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11858.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EmbodiedEval: Новый рубеж в оценке воплощенных возможностей MLLM",
                    "desc": "Статья представляет новый комплексный бенчмарк EmbodiedEval для оценки мультимодальных больших языковых моделей (MLLM) в задачах воплощенного искусственного интеллекта. EmbodiedEval включает 328 разнообразных задач в 125 трехмерных сценах, охватывающих навигацию, взаимодействие с объектами, социальное взаимодействие и ответы на вопросы. Оценка современных MLLM на EmbodiedEval выявила значительное отставание от человеческого уровня в воплощенных задачах. Результаты демонстрируют ограничения существующих MLLM и предоставляют insights для их дальнейшего развития."
                },
                "en": {
                    "title": "Empowering MLLMs with Interactive Evaluation for Embodied Tasks",
                    "desc": "This paper introduces EmbodiedEval, a new evaluation benchmark designed for Multimodal Large Language Models (MLLMs) in the context of embodied tasks. Unlike previous benchmarks that rely on static images or videos, EmbodiedEval offers a diverse set of 328 interactive tasks across 125 3D scenes, allowing for a more comprehensive assessment of MLLMs' capabilities. The tasks are categorized into five areas: navigation, object interaction, social interaction, attribute question answering, and spatial question answering, which helps evaluate different aspects of embodied AI. The findings reveal that current MLLMs fall short of human performance in these tasks, highlighting the need for further advancements in their embodied capabilities."
                },
                "zh": {
                    "title": "全面评估多模态大型语言模型的具身能力",
                    "desc": "多模态大型语言模型（MLLMs）在智能体领域取得了显著进展，但现有的评估基准主要依赖静态图像或视频，限制了对交互场景的评估。为了更全面地评估MLLMs的能力，我们提出了EmbodiedEval，这是一个包含328个任务和125个多样化3D场景的互动评估基准。该基准涵盖了导航、物体交互、社交互动等五大类任务，旨在全面评估智能体的不同能力。通过对最先进的MLLMs进行评估，我们发现它们在具身任务上与人类水平存在显著差距，揭示了现有模型的局限性，为未来的发展提供了重要见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10283",
            "title": "GSTAR: Gaussian Surface Tracking and Reconstruction",
            "url": "https://huggingface.co/papers/2501.10283",
            "abstract": "3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GSTAR maintains the mesh topology and tracks the meshes using Gaussians. In regions where topology changes, GSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and the generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://eth-ait.github.io/GSTAR/.",
            "score": 1,
            "issue_id": 1847,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "2ce1394526d61cff",
            "authors": [
                "Chengwei Zheng",
                "Lixin Xue",
                "Juan Zarate",
                "Jie Song"
            ],
            "affiliations": [
                "ETH Zurich",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10283.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "GSTAR: Гауссово сплаттинг для динамических 3D-сцен",
                    "desc": "GSTAR - это новый метод, который позволяет достичь фотореалистичного рендеринга, точной реконструкции поверхности и надежного 3D-трекинга для динамических сцен с изменяющейся топологией. Метод связывает гауссианы с гранями меша для представления динамических объектов и адаптивно отвязывает их в областях с изменяющейся топологией. GSTAR также вводит метод поверхностного потока сцены для надежной инициализации трекинга между кадрами. Эксперименты показывают эффективность метода в отслеживании и реконструкции динамических поверхностей."
                },
                "en": {
                    "title": "GSTAR: Dynamic Surface Tracking with 3D Gaussian Splatting",
                    "desc": "The paper presents GSTAR, a new method for rendering and tracking dynamic surfaces using 3D Gaussian Splatting. It effectively handles changes in surface topology, such as when surfaces appear or disappear, by binding Gaussians to mesh faces. For consistent topologies, GSTAR maintains the mesh structure, while it adaptively unbinds Gaussians in areas with topology changes to ensure accurate surface reconstruction. The method also includes a surface-based scene flow technique for improved tracking across frames, demonstrating its effectiveness in various applications."
                },
                "zh": {
                    "title": "GSTAR：动态场景中的高效3D跟踪与重建",
                    "desc": "3D高斯点技术使得静态场景的照片级真实感渲染变得高效。最近的研究扩展了这些方法，以支持表面重建和跟踪。然而，使用3D高斯点跟踪动态表面仍然面临挑战，因为表面可能出现、消失或分裂。为了解决这些问题，我们提出了GSTAR，这是一种新方法，能够在拓扑变化的动态场景中实现照片级真实感渲染、准确的表面重建和可靠的3D跟踪。"
                }
            }
        }
    ],
    "link_prev": "2025-01-24.html",
    "link_next": "2025-01-28.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1月24日"
    },
    "short_date_next": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1月28日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 6,
        "#multimodal": 4,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了多智能体强化学习（MARL）在解决合作和竞争问题上的进展。MARL的主要挑战是需要显式预测智能体行为以实现合作。为解决这个问题，作者提出了共享循环记忆变压器（SRMT），通过汇集和全局广播个体工作记忆，使智能体能够隐式交换信息并协调行动。SRMT在部分可观察的多智能体路径规划问题和POGEMA基准任务集上表现出色，特别是在稀疏奖励下。",
        "title": "SRMT: Shared Memory for Multi-agent Lifelong Pathfinding",
        "pinyin": "这篇文章介绍了多智能体强化学习（MARL）在解决合作和竞争问题上的进展。MARL的主要挑战是需要显式预测智能体行为以实现合作。为解决这个问题，作者提出了共享循环记忆变压器（SRMT），通过汇集和全局广播个体工作记忆，使智能体能够隐式交换信息并协调行动。SRMT在部分可观察的多智能体路径规划问题和POGEMA基准任务集上表现出色，特别是在稀疏奖励下。\n\nzhè piān wén zhāng jiè shào le duō zhì néng tǐ qiáng huà xué xí (MARL) zài jiě jué hé zuò yǔ jìng zhēng wèn tí shàng de jìn zhǎn. MARL de zhǔ yào tiáo zhàn shì xū yào xiǎn shì yù cè zhì néng tǐ xíng wéi yǐ shí xiàn hé zuò. wèi jiě jué zhè gè wèn tí, zuò zhě tí chū le gòng xiǎng xún huán jì yì biàn yā sū (SRMT), tōng guò huì jí hé quán jú guǎng bō gè tǐ gōng zuò jì yì, shǐ zhì néng tǐ néng gòu yǐn shì jiāo huàn xìn xī bìng xié tiáo xíng dòng. SRMT zài bù fēn kě guān chá de duō zhì néng tǐ lù jìng guī huà wèn tí hé POGEMA jī zhǔn rèn wù jí shàng biǎo xiàn chū sè, tè bié shì zài xī shū jiǎng lì xià.",
        "vocab": "[{'word': '多智能体强化学习', 'pinyin': 'duō zhìnéngtǐ qiánghuà xuéxí', 'trans': 'multi-agent reinforcement learning'}, {'word': '合作', 'pinyin': 'hézuò', 'trans': 'cooperation'}, {'word': '竞争', 'pinyin': 'jìngzhēng', 'trans': 'competition'}, {'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'}, {'word': '显式', 'pinyin': 'xiǎnshì', 'trans': 'explicit'}, {'word': '预测', 'pinyin': 'yùcè', 'trans': 'predict'}, {'word': '行为', 'pinyin': 'xíngwéi', 'trans': 'behavior'}, {'word': '共享', 'pinyin': 'gòngxiǎng', 'trans': 'share'}, {'word': '循环', 'pinyin': 'xúnhuán', 'trans': 'cyclic'}, {'word': '记忆', 'pinyin': 'jìyì', 'trans': 'memory'}, {'word': '变压器', 'pinyin': 'biànyāqì', 'trans': 'transformer'}, {'word': '汇集', 'pinyin': 'huìjí', 'trans': 'gather'}, {'word': '全局', 'pinyin': 'quánjú', 'trans': 'global'}, {'word': '广播', 'pinyin': 'guǎngbō', 'trans': 'broadcast'}, {'word': '个体', 'pinyin': 'gètǐ', 'trans': 'individual'}, {'word': '工作', 'pinyin': 'gōngzuò', 'trans': 'work'}, {'word': '隐式', 'pinyin': 'yǐnshì', 'trans': 'implicit'}, {'word': '交换', 'pinyin': 'jiāohuàn', 'trans': 'exchange'}, {'word': '协调', 'pinyin': 'xiétiáo', 'trans': 'coordinate'}, {'word': '行动', 'pinyin': 'xíngdòng', 'trans': 'action'}, {'word': '部分', 'pinyin': 'bùfen', 'trans': 'partial'}, {'word': '可观察', 'pinyin': 'kě guānchá', 'trans': 'observable'}, {'word': '路径规划', 'pinyin': 'lùjìng guīhuà', 'trans': 'path planning'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '任务集', 'pinyin': 'rènwù jí', 'trans': 'task set'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chūsè', 'trans': 'outstanding'}, {'word': '稀疏', 'pinyin': 'xīshū', 'trans': 'sparse'}, {'word': '奖励', 'pinyin': 'jiǎnglì', 'trans': 'reward'}]",
        "trans": "This article discusses the advancements of Multi-Agent Reinforcement Learning (MARL) in addressing cooperative and competitive problems. The main challenge of MARL is the need to explicitly predict agent behaviors to achieve cooperation. To address this issue, the authors propose the Shared Recurrent Memory Transformer (SRMT), which aggregates and globally broadcasts individual working memories, enabling agents to implicitly exchange information and coordinate actions. SRMT performs well in partially observable multi-agent path planning problems and the POGEMA benchmark task set, particularly in sparse reward scenarios.",
        "update_ts": "2025-01-26 12:36"
    }
}