{
    "date": {
        "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 27",
        "zh": "1æœˆ27æ—¥"
    },
    "time_utc": "2025-01-27 00:46",
    "weekday": 0,
    "issue_id": 1870,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.13200",
            "title": "SRMT: Shared Memory for Multi-agent Lifelong Pathfinding",
            "url": "https://huggingface.co/papers/2501.13200",
            "abstract": "Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the principal challenges in MARL is the need for explicit prediction of the agents' behavior to achieve cooperation. To resolve this issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly and coordinate their actions. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that requires agents to pass through a narrow corridor and on a POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a variety of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared recurrent memory into the transformer-based architectures can enhance coordination in decentralized multi-agent systems. The source code for training and evaluation is available on GitHub: https://github.com/Aloriosa/srmt.",
            "score": 53,
            "issue_id": 1846,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "52d8b3716543aa42",
            "authors": [
                "Alsu Sagirova",
                "Yuri Kuratov",
                "Mikhail Burtsev"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "London Institute for Mathematical Sciences, London, UK",
                "Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13200.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#rl",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SRMT: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (MARL) - Shared Recurrent Memory Transformer (SRMT). SRMT Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. SRMT Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Agent Coordination with Shared Memory Transformers",
                    "desc": "This paper introduces the Shared Recurrent Memory Transformer (SRMT), a novel approach in multi-agent reinforcement learning (MARL) that enhances cooperation among agents. SRMT utilizes a memory transformer architecture to allow agents to share and broadcast their individual memories, facilitating implicit communication and coordination. The effectiveness of SRMT is demonstrated through experiments on the Partially Observable Multi-Agent Pathfinding problem, where it outperforms traditional reinforcement learning methods, particularly in scenarios with sparse rewards. The results indicate that integrating shared memory into transformer models significantly improves the performance of decentralized multi-agent systems."
                },
                "zh": {
                    "title": "å…±äº«è®°å¿†æå‡å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›",
                    "desc": "å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³åˆä½œå’Œç«äº‰çš„å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…±äº«é€’å½’è®°å¿†å˜æ¢å™¨ï¼ˆSRMTï¼‰ï¼Œé€šè¿‡æ±‡èšå’Œå…¨å±€å¹¿æ’­ä¸ªä½“å·¥ä½œè®°å¿†ï¼Œå¸®åŠ©æ™ºèƒ½ä½“éšå¼äº¤æ¢ä¿¡æ¯å¹¶åè°ƒè¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’é—®é¢˜ä¸Šè¯„ä¼°äº†SRMTï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ç¨€ç–å¥–åŠ±ä¸‹è¡¨ç°ä¼˜äºå¤šç§å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿èµ°å»Šä¸Šä¹Ÿèƒ½æœ‰æ•ˆæ³›åŒ–ã€‚SRMTåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸­ä¸æœ€æ–°çš„MARLã€æ··åˆå’ŒåŸºäºè§„åˆ’çš„ç®—æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œè¡¨æ˜å…±äº«é€’å½’è®°å¿†çš„å¼•å…¥å¯ä»¥å¢å¼ºå»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åè°ƒèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13629",
            "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
            "url": "https://huggingface.co/papers/2501.13629",
            "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.",
            "score": 37,
            "issue_id": 1842,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "d036f75a81877ded",
            "authors": [
                "Zhenghao Lin",
                "Zihao Tang",
                "Xiao Liu",
                "Yeyun Gong",
                "Yi Cheng",
                "Qi Chen",
                "Hang Li",
                "Ying Xin",
                "Ziyue Yang",
                "Kailai Yang",
                "Yu Yan",
                "Xiao Liang",
                "Shuai Lu",
                "Yiming Huang",
                "Zheheng Luo",
                "Lei Qu",
                "Xuan Feng",
                "Yaoxiang Wang",
                "Yuqing Xia",
                "Feiyang Chen",
                "Yuting Jiang",
                "Yasen Hu",
                "Hao Ni",
                "Binyang Li",
                "Guoshuai Zhao",
                "Jui-Hao Chiang",
                "Zhongxin Guo",
                "Chen Lin",
                "Kun Kuang",
                "Wenjie Li",
                "Yelong Shen",
                "Jian Jiao",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.13629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#long_context",
                    "#training",
                    "#synthetic",
                    "#data",
                    "#inference"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Sigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¯Ğœ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Sigma - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ DiffQKV-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Q, K Ğ¸ V Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Sigma Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 6T Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIMicius."
                },
                "en": {
                    "title": "Sigma: Revolutionizing System Domain Language Models with DiffQKV Attention",
                    "desc": "The paper presents Sigma, a specialized large language model designed for the system domain, utilizing a new architecture called DiffQKV attention. This innovative attention mechanism optimizes the Query, Key, and Value components to improve inference efficiency, particularly in long-context scenarios. Through extensive experiments, the authors show that Sigma achieves significant speed improvements, outperforming traditional models like GPT-4 in various tasks. The model is pre-trained on a vast dataset, including 19.5 billion tokens from the system domain, establishing a new benchmark for performance in this area."
                },
                "zh": {
                    "title": "Sigmaï¼šç³»ç»Ÿé¢†åŸŸçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Sigmaï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹ç³»ç»Ÿé¢†åŸŸã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼ŒåŒ…æ‹¬DiffQKVæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åœ¨æˆ‘ä»¬ç²¾å¿ƒæ”¶é›†çš„ç³»ç»Ÿé¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚DiffQKVæ³¨æ„åŠ›é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰å’Œå€¼ï¼ˆVï¼‰ç»„ä»¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSigmaåœ¨ç³»ç»Ÿé¢†åŸŸçš„è¡¨ç°ä¼˜äºGPT-4ï¼Œç»å¯¹æå‡å¹…åº¦å¯è¾¾52.5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13918",
            "title": "Improving Video Generation with Human Feedback",
            "url": "https://huggingface.co/papers/2501.13918",
            "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and standard supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs. Project page: https://gongyeliu.github.io/videoalign.",
            "score": 35,
            "issue_id": 1849,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "933a6a47d8d5e20a",
            "authors": [
                "Jie Liu",
                "Gongye Liu",
                "Jiajun Liang",
                "Ziyang Yuan",
                "Xiaokun Liu",
                "Mingwu Zheng",
                "Xiele Wu",
                "Qiulin Wang",
                "Wenyu Qin",
                "Menghan Xia",
                "Xintao Wang",
                "Xiaohong Liu",
                "Fei Yang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Yujiu Yang",
                "Wanli Ouyang"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13918.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#video",
                    "#rlhf"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ VideoReward. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²: Flow-DPO, Flow-RWR Ğ¸ Flow-NRG. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VideoReward Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Flow-DPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Generation with Human Feedback and Reward Models",
                    "desc": "This paper addresses challenges in video generation, particularly issues of motion smoothness and alignment with prompts. The authors propose a new pipeline that utilizes human feedback to enhance video generation models. They create a large dataset of human preferences and introduce VideoReward, a model that evaluates video quality based on these preferences. The study also presents three innovative algorithms for improving flow-based video generation, demonstrating that their methods outperform existing models and allow for personalized video quality adjustments."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è§†é¢‘ç”Ÿæˆï¼Œæå‡ç”¨æˆ·ä½“éªŒ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„ä¸å¹³æ»‘è¿åŠ¨å’Œè§†é¢‘ä¸æç¤ºä¹‹é—´çš„é”™ä½é—®é¢˜ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„äººç±»åå¥½æ•°æ®é›†ï¼Œä¸“æ³¨äºç°ä»£è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†å¤šç»´åº¦çš„æˆå¯¹æ³¨é‡Šã€‚å¼•å…¥çš„VideoRewardæ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æœ€å¤§åŒ–å¥–åŠ±ï¼Œå¹¶æå‡ºäº†ä¸‰ç§å¯¹é½ç®—æ³•ï¼Œä»¥æé«˜æµæ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoRewardåœ¨å¥–åŠ±æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒFlow-DPOåœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–çš„è§†é¢‘è´¨é‡éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13926",
            "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
            "url": "https://huggingface.co/papers/2501.13926",
            "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
            "score": 24,
            "issue_id": 1841,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "61611cbe661736ff",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Chengzhuo Tong",
                "Zhizheng Zhao",
                "Peng Gao",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "MMLab",
                "MiuLar Lab",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13926.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ PARM Ğ¸ PARM++, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Show-o Ğ½Ğ° 24% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ GenEval."
                },
                "en": {
                    "title": "Enhancing Image Generation with Chain-of-Thought Reasoning",
                    "desc": "This paper explores the use of Chain-of-Thought (CoT) reasoning to improve autoregressive image generation models. It investigates three main techniques: enhancing verification through increased computation, aligning model preferences using Direct Preference Optimization (DPO), and combining these methods for better outcomes. The authors introduce the Potential Assessment Reward Model (PARM) and its enhanced version PARM++, which help assess and correct image generation steps. The results show a significant performance boost, achieving a 24% improvement on the GenEval benchmark compared to previous models."
                },
                "zh": {
                    "title": "é“¾å¼æ€ç»´æå‡å›¾åƒç”Ÿæˆæ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§æŠ€æœ¯ï¼šæµ‹è¯•æ—¶è®¡ç®—çš„æ‰©å±•ã€ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹é½æ¨¡å‹åå¥½ï¼Œä»¥åŠè¿™äº›æŠ€æœ¯çš„æ•´åˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥æœ‰æ•ˆç»“åˆï¼Œæ˜¾è‘—æå‡å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåŠ›è¯„ä¼°å¥–åŠ±æ¨¡å‹ï¼ˆPARMï¼‰å’ŒPARM++ï¼Œä¸“é—¨ç”¨äºè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13826",
            "title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos",
            "url": "https://huggingface.co/papers/2501.13826",
            "abstract": "Humans acquire knowledge through three cognitive stages: perceiving information, comprehending knowledge, and adapting knowledge to solve novel problems. Videos serve as an effective medium for this learning process, facilitating a progression through these cognitive stages. However, existing video benchmarks fail to systematically evaluate the knowledge acquisition capabilities in Large Multimodal Models (LMMs). To address this gap, we introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU features a curated collection of 300 expert-level videos and 900 human-annotated questions across six disciplines, evaluating knowledge acquisition through stage-aligned question-answer pairs: Perception, Comprehension, and Adaptation. A proposed knowledge gain metric, {\\Delta}knowledge, quantifies improvement in performance after video viewing. Evaluation of LMMs reveals a steep decline in performance as cognitive demands increase and highlights a significant gap between human and model knowledge acquisition, underscoring the need for methods to enhance LMMs' capability to learn and adapt from videos.",
            "score": 18,
            "issue_id": 1848,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "4475243a608bc988",
            "authors": [
                "Kairui Hu",
                "Penghao Wu",
                "Fanyi Pu",
                "Wang Xiao",
                "Yuanhan Zhang",
                "Xiang Yue",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13826.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#science",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Video-MMMU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 300 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 900 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° âˆ†knowledge Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LMM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Knowledge Acquisition in LMMs through Video Learning",
                    "desc": "This paper introduces Video-MMMU, a benchmark designed to evaluate Large Multimodal Models (LMMs) in their ability to learn from videos. It focuses on three cognitive stages: perception, comprehension, and adaptation, using a set of 300 expert videos and 900 questions. The benchmark assesses how well LMMs can acquire knowledge through these stages, revealing a significant performance gap compared to humans. A new metric, Î”knowledge, measures the improvement in LMM performance after watching videos, highlighting the need for better learning methods in these models."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†é¢‘çŸ¥è¯†è·å–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†äººç±»é€šè¿‡ä¸‰ä¸ªè®¤çŸ¥é˜¶æ®µè·å–çŸ¥è¯†çš„è¿‡ç¨‹ï¼šæ„ŸçŸ¥ä¿¡æ¯ã€ç†è§£çŸ¥è¯†å’Œé€‚åº”çŸ¥è¯†ä»¥è§£å†³æ–°é—®é¢˜ã€‚è§†é¢‘ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å­¦ä¹ åª’ä»‹ï¼Œèƒ½å¤Ÿä¿ƒè¿›è¿™äº›è®¤çŸ¥é˜¶æ®µçš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æœªèƒ½ç³»ç»Ÿåœ°è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨çŸ¥è¯†è·å–æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Video-MMMUï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šå­¦ç§‘çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LMMsä»è§†é¢‘ä¸­è·å–å’Œåˆ©ç”¨çŸ¥è¯†çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13919",
            "title": "Temporal Preference Optimization for Long-Form Video Understanding",
            "url": "https://huggingface.co/papers/2501.13919",
            "abstract": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website.",
            "score": 17,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "6e08b56893fb98a9",
            "authors": [
                "Rui Li",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Zeyu Wang",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13919.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "TPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LMM Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Temporal Preference Optimization (TPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LMM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. TPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ´Ğ»Ñ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LMM."
                },
                "en": {
                    "title": "Enhancing Temporal Understanding in Long Videos with TPO",
                    "desc": "This paper introduces Temporal Preference Optimization (TPO), a new framework aimed at improving how video large multimodal models (video-LMMs) understand time in long videos. TPO uses a self-training method that helps models learn to tell the difference between accurate and inaccurate timing responses by using specially curated preference datasets. These datasets focus on both specific video segments and the overall flow of the entire video, enhancing the model's ability to grasp temporal relationships. The results show that TPO significantly boosts performance on various benchmarks, making it a promising approach for better temporal reasoning in video analysis."
                },
                "zh": {
                    "title": "æ—¶é—´åå¥½ä¼˜åŒ–ï¼šæå‡è§†é¢‘ç†è§£çš„å…³é”®",
                    "desc": "å°½ç®¡è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆvideo-LMMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿è§†é¢‘ä¸­å®ç°æœ‰æ•ˆçš„æ—¶é—´å®šä½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒæ¡†æ¶â€”â€”æ—¶é—´åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åå¥½å­¦ä¹ å¢å¼ºè§†é¢‘-LMMsçš„æ—¶é—´å®šä½èƒ½åŠ›ã€‚TPOé‡‡ç”¨è‡ªæˆ‘è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„åå¥½æ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å‡†ç¡®çš„æ—¶é—´å“åº”å’Œä¸å¤ªå‡†ç¡®çš„æ—¶é—´å“åº”ã€‚é€šè¿‡åœ¨è¿™ä¸¤ä¸ªå±‚æ¬¡ä¸Šä¼˜åŒ–åå¥½æ•°æ®é›†ï¼ŒTPOæ˜¾è‘—æé«˜äº†æ—¶é—´ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹æ‰‹åŠ¨æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13920",
            "title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models",
            "url": "https://huggingface.co/papers/2501.13920",
            "abstract": "With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models' performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each model's strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.",
            "score": 12,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "837193826ae51376",
            "authors": [
                "Jiayi Lei",
                "Renrui Zhang",
                "Xiangfei Hu",
                "Weifeng Lin",
                "Zhen Li",
                "Wenjian Sun",
                "Ruoyi Du",
                "Le Zhuo",
                "Zhongyu Li",
                "Xinyue Li",
                "Shitian Zhao",
                "Ziyu Guo",
                "Yiting Lu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13920.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ (T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ IMAGINE-E Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆĞµÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ FLUX.1 Ğ¸ Ideogram2.0 Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T2I ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Evaluating the Future of Text-to-Image Models",
                    "desc": "This paper discusses the advancements in text-to-image (T2I) models, particularly focusing on recent models like FLUX.1 and Ideogram2.0. These models not only excel in generating images from text prompts but also show versatility in various tasks such as image editing and video generation. The authors introduce a new evaluation framework called IMAGINE-E to assess the performance of six leading T2I models across multiple domains. The findings reveal that while some models perform exceptionally well in specific tasks, there is a need for better evaluation methods to fully understand their capabilities and limitations."
                },
                "zh": {
                    "title": "æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æœªæ¥ï¼šé€šç”¨æ€§ä¸è¯„ä¼°çš„æŒ‘æˆ˜",
                    "desc": "éšç€æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨æç¤ºè·Ÿéšå’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æ–°æ¨å‡ºçš„æ¨¡å‹å¦‚FLUX.1å’ŒIdeogram2.0ï¼Œä»¥åŠDall-E3å’ŒStable Diffusion 3ç­‰ï¼Œå±•ç¤ºäº†åœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°ï¼Œæå‡ºäº†T2Iæ¨¡å‹æ˜¯å¦æœç€é€šç”¨é€‚ç”¨æ€§å‘å±•çš„ç–‘é—®ã€‚é™¤äº†ä¼ ç»Ÿçš„å›¾åƒç”Ÿæˆï¼Œè¿™äº›æ¨¡å‹åœ¨å¯æ§ç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ã€éŸ³é¢‘ã€3Då’Œè¿åŠ¨ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸä¹Ÿå±•ç°äº†èƒ½åŠ›ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†IMAGINE-Eï¼Œå¹¶å¯¹å…­ä¸ªä¸»è¦æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå¼ºè°ƒäº†å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸçš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯FLUX.1å’ŒIdeogram2.0åœ¨ç»“æ„åŒ–å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10799",
            "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
            "url": "https://huggingface.co/papers/2501.10799",
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",
            "score": 11,
            "issue_id": 1842,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "d43b005a69156930",
            "authors": [
                "Yen-Ting Lin",
                "Di Jin",
                "Tengyu Xu",
                "Tianhao Wu",
                "Sainbayar Sukhbaatar",
                "Chen Zhu",
                "Yun He",
                "Yun-Nung Chen",
                "Jason Weston",
                "Yuandong Tian",
                "Arash Rahnama",
                "Sinong Wang",
                "Hao Ma",
                "Han Fang"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National Taiwan University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10799.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¨Ğ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Step-KTO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ…Ğ¾Ğ´Ñƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Trustworthy Reasoning in LLMs with Step-KTO",
                    "desc": "This paper presents Step-KTO, a new training framework for large language models (LLMs) that enhances their mathematical reasoning abilities. Unlike previous methods that focus solely on the final answer, Step-KTO provides feedback on both the reasoning process and the outcome, promoting logical coherence. By evaluating intermediate reasoning steps alongside the final result, the framework helps LLMs avoid shortcuts and develop more reliable reasoning paths. Experiments show that Step-KTO significantly boosts accuracy and improves the quality of reasoning in challenging mathematical tasks, indicating its potential for creating more interpretable AI systems."
                },
                "zh": {
                    "title": "æå‡æ¨ç†å¯ä¿¡åº¦çš„Step-KTOæ¡†æ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡é“¾å¼æ€ç»´æç¤ºå’Œè‡ªä¸€è‡´æ€§é‡‡æ ·ç­‰æ–¹æ³•æœ‰æ‰€è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€åªå…³æ³¨æœ€ç»ˆç»“æœçš„æ­£ç¡®æ€§ï¼Œè€Œæœªèƒ½ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„è¿è´¯æ€§å’Œå¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†Step-KTOï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¿‡ç¨‹çº§å’Œç»“æœçº§äºŒå…ƒåé¦ˆçš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å¼•å¯¼LLMsæœç€æ›´å¯ä¿¡çš„æ¨ç†è½¨è¿¹å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStep-KTOæ˜¾è‘—æé«˜äº†æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œå±•ç¤ºäº†é€æ­¥è¿‡ç¨‹åé¦ˆåœ¨LLMè®­ç»ƒä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10018",
            "title": "DiffuEraser: A Diffusion Model for Video Inpainting",
            "url": "https://huggingface.co/papers/2501.10018",
            "abstract": "Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as a prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, a video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning,which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency.",
            "score": 10,
            "issue_id": 1846,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "8ebb9334e60b0dd7",
            "authors": [
                "Xiaowen Li",
                "Haolan Xue",
                "Peiran Ren",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10018.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#long_context",
                    "#hallucinations",
                    "#cv"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DiffuEraser: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DiffuEraser - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiffuEraser Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Inpainting with Diffusion Models for Better Consistency and Detail",
                    "desc": "This paper presents DiffuEraser, a novel video inpainting model that utilizes stable diffusion techniques to improve the restoration of masked regions in videos. By integrating prior information for initialization and weak conditioning, the model effectively reduces noise and visual artifacts. The authors enhance temporal consistency by expanding the temporal receptive fields and utilizing the smoothing properties of Video Diffusion Models. Experimental results show that DiffuEraser surpasses existing methods in terms of content completeness and temporal coherence, while also being efficient."
                },
                "zh": {
                    "title": "DiffuEraserï¼šæå‡è§†é¢‘ä¿®å¤çš„ç»†èŠ‚ä¸ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDiffuEraserçš„è§†é¢‘ä¿®å¤æ¨¡å‹ï¼ŒåŸºäºç¨³å®šæ‰©æ•£æŠ€æœ¯ï¼Œæ—¨åœ¨ç”¨æ›´ä¸°å¯Œçš„ç»†èŠ‚å’Œæ›´è¿è´¯çš„ç»“æ„å¡«è¡¥è¢«é®æŒ¡çš„åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å…ˆéªŒä¿¡æ¯æ¥æä¾›åˆå§‹åŒ–å’Œå¼±æ¡ä»¶ï¼Œä»è€Œå‡å°‘å™ªå£°ä¼ªå½±å’ŒæŠ‘åˆ¶å¹»è§‰ç°è±¡ã€‚ä¸ºäº†æé«˜é•¿åºåˆ—æ¨ç†è¿‡ç¨‹ä¸­çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æ‰©å±•äº†å…ˆéªŒæ¨¡å‹å’ŒDiffuEraserçš„æ—¶é—´æ„Ÿå—é‡ï¼Œå¹¶åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å¹³æ»‘ç‰¹æ€§è¿›ä¸€æ­¥å¢å¼ºä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…å®¹å®Œæ•´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13554",
            "title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt",
            "url": "https://huggingface.co/papers/2501.13554",
            "abstract": "Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.",
            "score": 8,
            "issue_id": 1852,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "15ba8f8e21d0e703",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#story_generation",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ - Ğ¾Ğ´Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ: Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ '1Prompt1Story'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. '1Prompt1Story' Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Consistent Storytelling in Text-to-Image Generation",
                    "desc": "This paper addresses the challenge of generating consistent images that preserve character identities in text-to-image (T2I) models. The authors introduce a novel method called \"One-Prompt-One-Story\" (1Prompt1Story), which allows for the concatenation of multiple prompts into a single input, enhancing the model's ability to maintain character consistency. They employ two innovative techniques, Singular-Value Reweighting and Identity-Preserving Cross-Attention, to refine the image generation process and ensure alignment with the input descriptions. The proposed method is evaluated against existing approaches, showing improved performance in both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "ä¸€æç¤ºä¸€æ•…äº‹ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸º\"ä¸€æç¤ºä¸€æ•…äº‹\"ï¼ˆ1Prompt1Storyï¼‰ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆè¿‡ç¨‹ä¸­è§’è‰²èº«ä»½ä¸€è‡´æ€§çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ‰€æœ‰æç¤ºåˆå¹¶ä¸ºå•ä¸ªè¾“å…¥ï¼Œåˆæ­¥ä¿æŒè§’è‰²èº«ä»½ï¼Œå¹¶åˆ©ç”¨ä¸¤ç§æ–°æŠ€æœ¯è¿›è¡Œç”Ÿæˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æˆ–å¯¹æ¨¡å‹æ¶æ„çš„ä¿®æ”¹ï¼Œå…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ1Prompt1Storyåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„ä¸€è‡´æ€§ç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13824",
            "title": "Hallucinations Can Improve Large Language Models in Drug Discovery",
            "url": "https://huggingface.co/papers/2501.13824",
            "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been raised by researchers, yet their potential in areas where creativity is vital, such as drug discovery, merits exploration. In this paper, we come up with the hypothesis that hallucinations can improve LLMs in drug discovery. To verify this hypothesis, we use LLMs to describe the SMILES string of molecules in natural language and then incorporate these descriptions as part of the prompt to address specific tasks in drug discovery. Evaluated on seven LLMs and five classification tasks, our findings confirm the hypothesis: LLMs can achieve better performance with text containing hallucinations. Notably, Llama-3.1-8B achieves an 18.35% gain in ROC-AUC compared to the baseline without hallucination. Furthermore, hallucinations generated by GPT-4o provide the most consistent improvements across models. Additionally, we conduct empirical analyses and a case study to investigate key factors affecting performance and the underlying reasons. Our research sheds light on the potential use of hallucinations for LLMs and offers new perspectives for future research leveraging LLMs in drug discovery.",
            "score": 5,
            "issue_id": 1853,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "bd66442952551d3e",
            "authors": [
                "Shuzhou Yuan",
                "Michael FÃ¤rber"
            ],
            "affiliations": [
                "Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Germany",
                "Dresden University of Technology, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13824.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#rlhf",
                    "#science",
                    "#hallucinations"
                ],
                "emoji": "ğŸ’Š",
                "ru": {
                    "title": "Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ LLM ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ LLM Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ LLM Ğ¸ Ğ¿ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² Ñ„Ğ°Ñ€Ğ¼Ğ°Ñ†ĞµĞ²Ñ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Harnessing Hallucinations: Boosting Drug Discovery with LLMs",
                    "desc": "This paper explores the idea that hallucinations in Large Language Models (LLMs) can enhance their performance in drug discovery tasks. The authors hypothesize that by using LLMs to generate natural language descriptions of molecular SMILES strings, they can improve the models' effectiveness in specific classification tasks. Their experiments show that LLMs, particularly Llama-3.1-8B, achieve significant performance gains when incorporating these hallucinated descriptions, with an 18.35% increase in ROC-AUC. The study provides insights into how hallucinations can be beneficial in creative applications like drug discovery, suggesting new avenues for future research."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¹»è§‰æå‡è¯ç‰©å‘ç°ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯ç‰©å‘ç°ä¸­çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯å®ƒä»¬çš„å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬æå‡ºå‡è®¾ï¼Œå¹»è§‰å¯ä»¥æå‡LLMsåœ¨è¯ç‰©å‘ç°ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å°†LLMsç”Ÿæˆçš„åˆ†å­SMILESå­—ç¬¦ä¸²æè¿°ä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ä¸ƒä¸ªLLMså’Œäº”ä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒåŒ…å«å¹»è§‰çš„æ–‡æœ¬èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯Llama-3.1-8Båœ¨ROC-AUCä¸Šæ¯”åŸºçº¿æé«˜äº†18.35%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13452",
            "title": "EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion",
            "url": "https://huggingface.co/papers/2501.13452",
            "abstract": "Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low similarity issues, primarily due to their reliance on low-level facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) a two-stage training strategy, incorporating a stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering a more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos.",
            "score": 5,
            "issue_id": 1846,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "b98d987f7439b94b",
            "authors": [
                "Jiangchuan Wei",
                "Shiyue Yan",
                "Wenfeng Lin",
                "Boyuan Liu",
                "Renjie Chen",
                "Mingyu Guo"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13452.jpg",
            "data": {
                "categories": [
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "EchoVideo: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "EchoVideo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (IPT2V). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° (IITF) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ»Ğ¸Ñ†Ğ°Ñ…. EchoVideo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ñ† Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ»Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "EchoVideo: Enhancing Identity Preservation in Video Generation",
                    "desc": "The paper introduces EchoVideo, a novel approach to identity-preserving video generation that addresses common issues like 'copy-paste' artifacts and low similarity in generated videos. It utilizes an Identity Image-Text Fusion Module (IITF) to merge high-level semantic features from text, ensuring clean facial identity representations while avoiding irrelevant details. Additionally, a two-stage training strategy is implemented, which includes a stochastic method to balance the use of shallow facial information with high-level features. This results in improved fidelity and robustness in facial identity representation, leading to high-quality video generation with better controllability."
                },
                "zh": {
                    "title": "EchoVideoï¼šæå‡è§†é¢‘ç”Ÿæˆçš„èº«ä»½ä¿ç•™ä¸è´¨é‡",
                    "desc": "è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥å¯¹èº«ä»½ä¿ç•™è§†é¢‘ç”Ÿæˆï¼ˆIPT2Vï¼‰äº§ç”Ÿäº†é‡è¦å½±å“ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸¸å¸¸å‡ºç°â€œå¤åˆ¶ç²˜è´´â€ä¼ªå½±å’Œä½ç›¸ä¼¼åº¦çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºä½çº§åˆ«çš„é¢éƒ¨å›¾åƒä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EchoVideoï¼Œé‡‡ç”¨äº†èº«ä»½å›¾åƒ-æ–‡æœ¬èåˆæ¨¡å—ï¼ˆIITFï¼‰å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å¹³è¡¡æµ…å±‚ç‰¹å¾çš„å¢å¼ºä¸é«˜å±‚ç‰¹å¾çš„åˆ©ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒEchoVideoåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¯æ§æ€§å’Œä¿çœŸåº¦çš„è§†é¢‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆä¿ç•™äº†é¢éƒ¨èº«ä»½å’Œå…¨èº«å®Œæ•´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10979",
            "title": "Control LLM: Controlled Evolution for Intelligence Retention in LLM",
            "url": "https://huggingface.co/papers/2501.10979",
            "abstract": "Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is catastrophic forgetting (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose Control LLM, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge.   Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning (+14.4% on Math-Hard) and coding performance (+10% on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities (+10.6% on C-Eval, +6.8% on CMMLU, and +30.2% on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation (<4.3% on MMLU) compared to >35% in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products.   To support further research, we release the training and evaluation code (https://github.com/linkedin/ControlLLM) along with models trained on public datasets ( https://huggingface.co/ControlLLM) to the community.",
            "score": 3,
            "issue_id": 1858,
            "pub_date": "2025-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "dd48db75ab08337c",
            "authors": [
                "Haichao Wei",
                "Yunxiang Ren",
                "Zhoutong Fu",
                "Aman Lunia",
                "Yi-Lin Chen",
                "Alice Leung",
                "Ya Xu"
            ],
            "affiliations": [
                "LinkedIn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10979.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#math",
                    "#optimization",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Control LLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ² Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ñ… LinkedIn Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Enhancing LLMs Without Starting Over!",
                    "desc": "This paper introduces Control LLM, a new method designed to improve the performance of Large Language Models (LLMs) without the need for complete retraining. It addresses the issue of catastrophic forgetting that occurs during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT) by using parallel pre-trained transformer blocks and interpolation strategies to align hidden states. The results show that Control LLM significantly enhances performance in various tasks, including mathematical reasoning and coding, while maintaining strong original capabilities. The method has been validated through extensive experiments and is made available for further research, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå› æ­¤åœ¨ä¸ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æå‡å…¶èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•Control LLMï¼Œé€šè¿‡å¹¶è¡Œé¢„è®­ç»ƒå’Œæ‰©å±•çš„å˜æ¢å™¨å—ï¼Œåˆ©ç”¨æ’å€¼ç­–ç•¥å¯¹é½å…¶éšè—çŠ¶æ€ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¿ç•™ç°æœ‰ä»»åŠ¡çš„æ€§èƒ½å¹¶æ— ç¼æ•´åˆæ–°çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒControl LLMåœ¨è¿ç»­é¢„è®­ç»ƒå’Œè¿ç»­ç›‘ç£å¾®è°ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ•°å­¦æ¨ç†å’Œç¼–ç æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¤šè¯­è¨€èƒ½åŠ›ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒåŸæœ‰å¼ºå¤§èƒ½åŠ›çš„åŒæ—¶ï¼Œå‡å°‘äº†æ•°æ®å’Œè®¡ç®—çš„éœ€æ±‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆåœ°ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13075",
            "title": "Evolution and The Knightian Blindspot of Machine Learning",
            "url": "https://huggingface.co/papers/2501.13075",
            "abstract": "This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.",
            "score": 3,
            "issue_id": 1845,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "5be12844b33bd729",
            "authors": [
                "Joel Lehman",
                "Elliot Meyerson",
                "Tarek El-Gaaly",
                "Kenneth O. Stanley",
                "Tarin Ziyaee"
            ],
            "affiliations": [
                "Cognizant AI Labs",
                "Second Nature AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13075.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#agents",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾: ÑƒÑ€Ğ¾ĞºĞ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ°ÑĞ¿ĞµĞºÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼Ñƒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ RL Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ² RL, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´ÑÑ‚ Ğº Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ, Ñ‡Ñ‚Ğ¾ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ»ĞµĞ¿Ñ‹Ñ… Ğ¿ÑÑ‚ĞµĞ½ Ğ² ĞµĞ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¼Ğ°Ñ…, Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞĞ°Ğ¹Ñ‚Ğ°."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing ML Robustness through Evolutionary Insights",
                    "desc": "This paper highlights a critical gap in machine learning (ML) regarding its ability to handle unknown future scenarios, which is essential for general intelligence. It draws parallels between reinforcement learning (RL) and biological evolution, emphasizing that while RL struggles with unforeseen situations, evolution naturally adapts to them. The authors argue that current ML formalisms overlook Knightian uncertainty, which limits the robustness of AI systems in open-world environments. They propose that by understanding and integrating evolutionary mechanisms, ML can improve its resilience to unpredictable challenges."
                },
                "zh": {
                    "title": "æœºå™¨å­¦ä¹ éœ€é¢å¯¹æœªçŸ¥ä¸ç¡®å®šæ€§æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨å¤„ç†å¼€æ”¾ä¸–ç•Œä¸­çš„æœªçŸ¥æœªæ¥æ—¶ï¼Œå¿½è§†äº†ä¸€ä¸ªé‡è¦æ–¹é¢ï¼šå¯¹æœªçŸ¥ä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚ä½œè€…å°†è¿™ç§é²æ£’æ€§ä¸ç»æµå­¦ä¸­çš„å¥ˆç‰¹ä¸ç¡®å®šæ€§ï¼ˆKnightian Uncertaintyï¼‰ç›¸è”ç³»ï¼Œè®¤ä¸ºè¿™æ˜¯æœºå™¨å­¦ä¹ å…³é”®å½¢å¼åŒ–ä¸­è¢«æ’é™¤çš„å› ç´ ã€‚é€šè¿‡å¯¹æ¯”å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç”Ÿç‰©è¿›åŒ–è¿‡ç¨‹ï¼Œè®ºæ–‡å¼ºè°ƒäº†RLåœ¨å¼€æ”¾ä¸–ç•Œæƒ…å¢ƒä¸­çš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†ç”Ÿç‰©è¿›åŒ–å¦‚ä½•åœ¨æ²¡æœ‰æ˜ç¡®ç†è®ºçš„æƒ…å†µä¸‹ï¼ŒåŸ¹å…»å‡ºé€‚åº”å¤æ‚ç¯å¢ƒçš„èƒ½åŠ›ã€‚æœ€åï¼Œä½œè€…è®¤ä¸ºï¼Œæœºå™¨å­¦ä¹ çš„è„†å¼±æ€§å¯èƒ½æºäºå…¶å½¢å¼åŒ–ä¸­çš„ç›²ç‚¹ï¼Œç›´æ¥é¢å¯¹å¥ˆç‰¹ä¸ç¡®å®šæ€§æŒ‘æˆ˜å¯èƒ½ä¼šå¸¦æ¥æ˜¾è‘—çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13124",
            "title": "Debate Helps Weak-to-Strong Generalization",
            "url": "https://huggingface.co/papers/2501.13124",
            "abstract": "Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision. However, future superhuman models will surpass the capability of humans. Therefore, humans will only be able to weakly supervise superhuman models. This expected deficiency of human evaluation would weaken the safety of future AI systems. Scalable oversight and weak-to-strong generalization are two complementary approaches to tackle this issue. In this paper, we attempt to combine the strengths of these two approaches to further improve alignment. Specifically, we investigate ways of improving human supervision with a strong pretrained model and then supervise the strong model with enhanced weak human supervision. To make iterative empirical progress, we consider an analogy: can we use a strong model to improve weak model supervision and then use it to supervise the strong model? We empirically test it by finetuning a small weak model on ground truth labels with the additional help from a large strong model, and then finetuning the strong model on labels generated by the weak model. We find that debate can assist a weak model in extracting trustworthy information from an untrustworthy strong model, which provides leverage as context on samples when training a weak model. We also show that an ensemble of weak models helps exploit long arguments generated by strong model debaters and obtain a more robust supervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP benchmarks show that the combination approach leads to better alignment, which indicates that debate has the potential to help weak-to-strong generalization.",
            "score": 3,
            "issue_id": 1843,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "cacd0d01e3d119ee",
            "authors": [
                "Hao Lang",
                "Fei Huang",
                "Yongbin Li"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13124.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ˜Ğ˜: Ğ¾Ñ‚ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğº ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ ÑĞ²ĞµÑ€Ñ…Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑÑ‚Ğ¾Ñ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ±Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… OpenAI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing AI Alignment through Model Debate and Supervision",
                    "desc": "This paper addresses the challenge of aligning superhuman AI models with desired behaviors, given that human supervision may be insufficient. It proposes a novel approach that combines scalable oversight with weak-to-strong generalization to enhance model alignment. The authors explore how a strong pretrained model can improve the supervision of a weak model, and in turn, how the weak model can provide valuable feedback to the strong model. Their experiments demonstrate that using debate between models can help extract reliable information, leading to improved alignment and performance on NLP tasks."
                },
                "zh": {
                    "title": "åˆ©ç”¨è¾©è®ºæå‡AIæ¨¡å‹çš„ç›‘ç£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨æœªæ¥è¶…äººç±»æ¨¡å‹çš„ç›‘ç£ä¸‹æ”¹å–„äººç±»çš„ç›‘ç£èƒ½åŠ›ã€‚ç”±äºäººç±»çš„ç›‘ç£èƒ½åŠ›æœ‰é™ï¼Œæœªæ¥çš„AIç³»ç»Ÿå¯èƒ½ä¼šé¢ä¸´å®‰å…¨æ€§é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¯æ‰©å±•ç›‘ç£å’Œå¼±åˆ°å¼ºæ³›åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹æ¥å¢å¼ºäººç±»çš„ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾©è®ºå¯ä»¥å¸®åŠ©å¼±æ¨¡å‹ä»å¼ºæ¨¡å‹ä¸­æå–å¯é ä¿¡æ¯ï¼Œä»è€Œæé«˜ç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11858",
            "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
            "url": "https://huggingface.co/papers/2501.11858",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
            "score": 2,
            "issue_id": 1862,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "af76793f3055f7e0",
            "authors": [
                "Zhili Cheng",
                "Yuge Tu",
                "Ran Li",
                "Shiqi Dai",
                "Jinyi Hu",
                "Shengding Hu",
                "Jiahao Li",
                "Yang Shi",
                "Tianyu Yu",
                "Weize Chen",
                "Lei Shi",
                "Maosong Sun"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11858.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "EmbodiedEval: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EmbodiedEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. EmbodiedEval Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 328 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 125 Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ½Ğ° EmbodiedEval Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ insights Ğ´Ğ»Ñ Ğ¸Ñ… Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering MLLMs with Interactive Evaluation for Embodied Tasks",
                    "desc": "This paper introduces EmbodiedEval, a new evaluation benchmark designed for Multimodal Large Language Models (MLLMs) in the context of embodied tasks. Unlike previous benchmarks that rely on static images or videos, EmbodiedEval offers a diverse set of 328 interactive tasks across 125 3D scenes, allowing for a more comprehensive assessment of MLLMs' capabilities. The tasks are categorized into five areas: navigation, object interaction, social interaction, attribute question answering, and spatial question answering, which helps evaluate different aspects of embodied AI. The findings reveal that current MLLMs fall short of human performance in these tasks, highlighting the need for further advancements in their embodied capabilities."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…·èº«èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ™ºèƒ½ä½“é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦ä¾èµ–é™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œé™åˆ¶äº†å¯¹äº¤äº’åœºæ™¯çš„è¯„ä¼°ã€‚ä¸ºäº†æ›´å…¨é¢åœ°è¯„ä¼°MLLMsçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†EmbodiedEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«328ä¸ªä»»åŠ¡å’Œ125ä¸ªå¤šæ ·åŒ–3Dåœºæ™¯çš„äº’åŠ¨è¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¯¼èˆªã€ç‰©ä½“äº¤äº’ã€ç¤¾äº¤äº’åŠ¨ç­‰äº”å¤§ç±»ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“çš„ä¸åŒèƒ½åŠ›ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨å…·èº«ä»»åŠ¡ä¸Šä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„å‘å±•æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10283",
            "title": "GSTAR: Gaussian Surface Tracking and Reconstruction",
            "url": "https://huggingface.co/papers/2501.10283",
            "abstract": "3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GSTAR maintains the mesh topology and tracks the meshes using Gaussians. In regions where topology changes, GSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and the generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://eth-ait.github.io/GSTAR/.",
            "score": 1,
            "issue_id": 1847,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "2ce1394526d61cff",
            "authors": [
                "Chengwei Zheng",
                "Lixin Xue",
                "Juan Zarate",
                "Jie Song"
            ],
            "affiliations": [
                "ETH Zurich",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10283.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "GSTAR: Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½",
                    "desc": "GSTAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ñ Ğ³Ñ€Ğ°Ğ½ÑĞ¼Ğ¸ Ğ¼ĞµÑˆĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹. GSTAR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "GSTAR: Dynamic Surface Tracking with 3D Gaussian Splatting",
                    "desc": "The paper presents GSTAR, a new method for rendering and tracking dynamic surfaces using 3D Gaussian Splatting. It effectively handles changes in surface topology, such as when surfaces appear or disappear, by binding Gaussians to mesh faces. For consistent topologies, GSTAR maintains the mesh structure, while it adaptively unbinds Gaussians in areas with topology changes to ensure accurate surface reconstruction. The method also includes a surface-based scene flow technique for improved tracking across frames, demonstrating its effectiveness in various applications."
                },
                "zh": {
                    "title": "GSTARï¼šåŠ¨æ€åœºæ™¯ä¸­çš„é«˜æ•ˆ3Dè·Ÿè¸ªä¸é‡å»º",
                    "desc": "3Dé«˜æ–¯ç‚¹æŠ€æœ¯ä½¿å¾—é™æ€åœºæ™¯çš„ç…§ç‰‡çº§çœŸå®æ„Ÿæ¸²æŸ“å˜å¾—é«˜æ•ˆã€‚æœ€è¿‘çš„ç ”ç©¶æ‰©å±•äº†è¿™äº›æ–¹æ³•ï¼Œä»¥æ”¯æŒè¡¨é¢é‡å»ºå’Œè·Ÿè¸ªã€‚ç„¶è€Œï¼Œä½¿ç”¨3Dé«˜æ–¯ç‚¹è·Ÿè¸ªåŠ¨æ€è¡¨é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¡¨é¢å¯èƒ½å‡ºç°ã€æ¶ˆå¤±æˆ–åˆ†è£‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GSTARï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ‹“æ‰‘å˜åŒ–çš„åŠ¨æ€åœºæ™¯ä¸­å®ç°ç…§ç‰‡çº§çœŸå®æ„Ÿæ¸²æŸ“ã€å‡†ç¡®çš„è¡¨é¢é‡å»ºå’Œå¯é çš„3Dè·Ÿè¸ªã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-24.html",
    "link_next": "2025-01-28.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 6,
        "#multimodal": 4,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³åˆä½œå’Œç«äº‰é—®é¢˜ä¸Šçš„è¿›å±•ã€‚MARLçš„ä¸»è¦æŒ‘æˆ˜æ˜¯éœ€è¦æ˜¾å¼é¢„æµ‹æ™ºèƒ½ä½“è¡Œä¸ºä»¥å®ç°åˆä½œã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†å…±äº«å¾ªç¯è®°å¿†å˜å‹å™¨ï¼ˆSRMTï¼‰ï¼Œé€šè¿‡æ±‡é›†å’Œå…¨å±€å¹¿æ’­ä¸ªä½“å·¥ä½œè®°å¿†ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿéšå¼äº¤æ¢ä¿¡æ¯å¹¶åè°ƒè¡ŒåŠ¨ã€‚SRMTåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’é—®é¢˜å’ŒPOGEMAåŸºå‡†ä»»åŠ¡é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–å¥–åŠ±ä¸‹ã€‚",
        "title": "SRMT: Shared Memory for Multi-agent Lifelong Pathfinding",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³åˆä½œå’Œç«äº‰é—®é¢˜ä¸Šçš„è¿›å±•ã€‚MARLçš„ä¸»è¦æŒ‘æˆ˜æ˜¯éœ€è¦æ˜¾å¼é¢„æµ‹æ™ºèƒ½ä½“è¡Œä¸ºä»¥å®ç°åˆä½œã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†å…±äº«å¾ªç¯è®°å¿†å˜å‹å™¨ï¼ˆSRMTï¼‰ï¼Œé€šè¿‡æ±‡é›†å’Œå…¨å±€å¹¿æ’­ä¸ªä½“å·¥ä½œè®°å¿†ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿéšå¼äº¤æ¢ä¿¡æ¯å¹¶åè°ƒè¡ŒåŠ¨ã€‚SRMTåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’é—®é¢˜å’ŒPOGEMAåŸºå‡†ä»»åŠ¡é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–å¥–åŠ±ä¸‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le duÅ zhÃ¬ nÃ©ng tÇ qiÃ¡ng huÃ  xuÃ© xÃ­ (MARL) zÃ i jiÄ› juÃ© hÃ© zuÃ² yÇ” jÃ¬ng zhÄ“ng wÃ¨n tÃ­ shÃ ng de jÃ¬n zhÇn. MARL de zhÇ” yÃ o tiÃ¡o zhÃ n shÃ¬ xÅ« yÃ o xiÇn shÃ¬ yÃ¹ cÃ¨ zhÃ¬ nÃ©ng tÇ xÃ­ng wÃ©i yÇ shÃ­ xiÃ n hÃ© zuÃ². wÃ¨i jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le gÃ²ng xiÇng xÃºn huÃ¡n jÃ¬ yÃ¬ biÃ n yÄ sÅ« (SRMT), tÅng guÃ² huÃ¬ jÃ­ hÃ© quÃ¡n jÃº guÇng bÅ gÃ¨ tÇ gÅng zuÃ² jÃ¬ yÃ¬, shÇ zhÃ¬ nÃ©ng tÇ nÃ©ng gÃ²u yÇn shÃ¬ jiÄo huÃ n xÃ¬n xÄ« bÃ¬ng xiÃ© tiÃ¡o xÃ­ng dÃ²ng. SRMT zÃ i bÃ¹ fÄ“n kÄ› guÄn chÃ¡ de duÅ zhÃ¬ nÃ©ng tÇ lÃ¹ jÃ¬ng guÄ« huÃ  wÃ¨n tÃ­ hÃ© POGEMA jÄ« zhÇ”n rÃ¨n wÃ¹ jÃ­ shÃ ng biÇo xiÃ n chÅ« sÃ¨, tÃ¨ biÃ© shÃ¬ zÃ i xÄ« shÅ« jiÇng lÃ¬ xiÃ .",
        "vocab": "[{'word': 'å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'duÅ zhÃ¬nÃ©ngtÇ qiÃ¡nghuÃ  xuÃ©xÃ­', 'trans': 'multi-agent reinforcement learning'}, {'word': 'åˆä½œ', 'pinyin': 'hÃ©zuÃ²', 'trans': 'cooperation'}, {'word': 'ç«äº‰', 'pinyin': 'jÃ¬ngzhÄ“ng', 'trans': 'competition'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇozhÃ n', 'trans': 'challenge'}, {'word': 'æ˜¾å¼', 'pinyin': 'xiÇnshÃ¬', 'trans': 'explicit'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹cÃ¨', 'trans': 'predict'}, {'word': 'è¡Œä¸º', 'pinyin': 'xÃ­ngwÃ©i', 'trans': 'behavior'}, {'word': 'å…±äº«', 'pinyin': 'gÃ²ngxiÇng', 'trans': 'share'}, {'word': 'å¾ªç¯', 'pinyin': 'xÃºnhuÃ¡n', 'trans': 'cyclic'}, {'word': 'è®°å¿†', 'pinyin': 'jÃ¬yÃ¬', 'trans': 'memory'}, {'word': 'å˜å‹å™¨', 'pinyin': 'biÃ nyÄqÃ¬', 'trans': 'transformer'}, {'word': 'æ±‡é›†', 'pinyin': 'huÃ¬jÃ­', 'trans': 'gather'}, {'word': 'å…¨å±€', 'pinyin': 'quÃ¡njÃº', 'trans': 'global'}, {'word': 'å¹¿æ’­', 'pinyin': 'guÇngbÅ', 'trans': 'broadcast'}, {'word': 'ä¸ªä½“', 'pinyin': 'gÃ¨tÇ', 'trans': 'individual'}, {'word': 'å·¥ä½œ', 'pinyin': 'gÅngzuÃ²', 'trans': 'work'}, {'word': 'éšå¼', 'pinyin': 'yÇnshÃ¬', 'trans': 'implicit'}, {'word': 'äº¤æ¢', 'pinyin': 'jiÄohuÃ n', 'trans': 'exchange'}, {'word': 'åè°ƒ', 'pinyin': 'xiÃ©tiÃ¡o', 'trans': 'coordinate'}, {'word': 'è¡ŒåŠ¨', 'pinyin': 'xÃ­ngdÃ²ng', 'trans': 'action'}, {'word': 'éƒ¨åˆ†', 'pinyin': 'bÃ¹fen', 'trans': 'partial'}, {'word': 'å¯è§‚å¯Ÿ', 'pinyin': 'kÄ› guÄnchÃ¡', 'trans': 'observable'}, {'word': 'è·¯å¾„è§„åˆ’', 'pinyin': 'lÃ¹jÃ¬ng guÄ«huÃ ', 'trans': 'path planning'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'}, {'word': 'ä»»åŠ¡é›†', 'pinyin': 'rÃ¨nwÃ¹ jÃ­', 'trans': 'task set'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}, {'word': 'ç¨€ç–', 'pinyin': 'xÄ«shÅ«', 'trans': 'sparse'}, {'word': 'å¥–åŠ±', 'pinyin': 'jiÇnglÃ¬', 'trans': 'reward'}]",
        "trans": "This article discusses the advancements of Multi-Agent Reinforcement Learning (MARL) in addressing cooperative and competitive problems. The main challenge of MARL is the need to explicitly predict agent behaviors to achieve cooperation. To address this issue, the authors propose the Shared Recurrent Memory Transformer (SRMT), which aggregates and globally broadcasts individual working memories, enabling agents to implicitly exchange information and coordinate actions. SRMT performs well in partially observable multi-agent path planning problems and the POGEMA benchmark task set, particularly in sparse reward scenarios.",
        "update_ts": "2025-01-26 12:36"
    }
}