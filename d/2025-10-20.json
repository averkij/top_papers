{
    "date": {
        "ru": "20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 20",
        "zh": "10æœˆ20æ—¥"
    },
    "time_utc": "2025-10-20 00:56",
    "weekday": 0,
    "issue_id": 6497,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.04849",
            "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
            "url": "https://huggingface.co/papers/2510.04849",
            "abstract": "PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.",
            "score": 100,
            "issue_id": 6476,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "ae839101b9ffc0a4",
            "authors": [
                "Elisei Rykov",
                "Kseniia Petrushina",
                "Maksim Savkin",
                "Valerii Olisov",
                "Artem Vazhentsev",
                "Kseniia Titova",
                "Alexander Panchenko",
                "Vasily Konovalov",
                "Julia Belikova"
            ],
            "affiliations": [
                "AIRI",
                "MWS AI",
                "Moscow Institute of Physics and Technology",
                "Sber AI Lab",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04849.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#low_resource",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸ„",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¹Ğ¼Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ: Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² 14 ÑĞ·Ñ‹ĞºĞ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ PsiloQA â€” ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² LLM Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· GPT-4o, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ encoder-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…. PsiloQA Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ cross-lingual Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection with PsiloQA: A Multilingual Approach",
                    "desc": "PsiloQA is a new multilingual dataset designed to improve the detection of hallucinations in large language models (LLMs) across 14 different languages. Unlike previous benchmarks that only focus on English and evaluate at the sequence level, PsiloQA provides detailed annotations at the span level, allowing for a more precise assessment of factual accuracy. The dataset is created using an automated pipeline that generates question-answer pairs and identifies hallucinated spans by comparing model outputs to correct answers. Our findings show that encoder-based models perform best in detecting these hallucinations, highlighting the dataset's potential for enhancing multilingual LLM applications."
                },
                "zh": {
                    "title": "PsiloQAï¼šå¤šè¯­è¨€å¹»è§‰æ£€æµ‹çš„æ–°çªç ´",
                    "desc": "PsiloQAæ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼Œä¸“æ³¨äºæ£€æµ‹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ã€‚å®ƒæ¶µç›–äº†14ç§è¯­è¨€ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆäº†å¸¦æœ‰è·¨åº¦çº§å¹»è§‰çš„é—®ç­”å¯¹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç¼–ç å™¨çš„æ¨¡å‹åœ¨å¹»è§‰æ£€æµ‹ä¸­è¡¨ç°æœ€ä½³ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œè·¨è¯­è¨€æ³›åŒ–ã€‚è¯¥æ•°æ®é›†çš„æ„å»ºæˆæœ¬ä½äºäººå·¥æ ‡æ³¨çš„æ•°æ®é›†ï¼Œæ¨åŠ¨äº†å¤šè¯­è¨€ç¯å¢ƒä¸­å¹»è§‰æ£€æµ‹çš„è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14545",
            "title": "Agentic Entropy-Balanced Policy Optimization",
            "url": "https://huggingface.co/papers/2510.14545",
            "abstract": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
            "score": 90,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "ac2984c140493beb",
            "authors": [
                "Guanting Dong",
                "Licheng Bao",
                "Zhongyuan Wang",
                "Kangzhi Zhao",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Jinghan Yang",
                "Hangyu Mao",
                "Fuzheng Zhang",
                "Kun Gai",
                "Guorui Zhou",
                "Yutao Zhu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14545.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "AEPO â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. AEPO Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ° 14 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-14B Ñ AEPO Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ 7 mainstream RL Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 1000 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Balancing Entropy for Smarter Web Agents with AEPO",
                    "desc": "The paper introduces AEPO, an innovative agentic reinforcement learning algorithm that tackles issues related to entropy in training web agents. It highlights how traditional methods can lead to training collapse due to over-reliance on entropy signals. AEPO features a dynamic rollout mechanism that balances sampling budgets and a policy optimization technique that preserves important gradients. The results demonstrate AEPO's superior performance across multiple datasets, showcasing its ability to enhance training stability and diversity in web agents."
                },
                "zh": {
                    "title": "å¹³è¡¡ç†µï¼Œæå‡ä»£ç†å­¦ä¹ æ€§èƒ½",
                    "desc": "AEPOæ˜¯ä¸€ç§ä»£ç†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ç½‘ç»œä»£ç†è®­ç»ƒä¸­çš„ç†µç›¸å…³æŒ‘æˆ˜ã€‚è¯¥ç®—æ³•é€šè¿‡åŠ¨æ€å¹³è¡¡ç†µï¼Œåœ¨å›æ»šå’Œç­–ç•¥æ›´æ–°é˜¶æ®µæé«˜æ€§èƒ½å’Œç¨³å®šæ€§ã€‚AEPOçš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬åŠ¨æ€ç†µå¹³è¡¡å›æ»šæœºåˆ¶å’Œç†µå¹³è¡¡ç­–ç•¥ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢è¿‡åº¦åˆ†æ”¯é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAEPOåœ¨14ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äº7ç§ä¸»æµå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14975",
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "url": "https://huggingface.co/papers/2510.14975",
            "abstract": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  \t\t\t\t\tAI-generated summary \t\t\t\t Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
            "score": 72,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "f35e52daa72d6eaa",
            "authors": [
                "Hengyuan Xu",
                "Wei Cheng",
                "Peng Xing",
                "Yixiao Fang",
                "Shuhan Wu",
                "Rui Wang",
                "Xianfang Zeng",
                "Daxin Jiang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14975.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ±ĞµĞ· ĞºĞ¾Ğ¿Ğ¸Ğ¿Ğ°ÑÑ‚Ğ°: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ĞºĞ¾Ğ¿Ğ¸Ğ¿Ğ°ÑÑ‚Ğ°Â» Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ğ³Ğ´Ğ° AI Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ»Ğ¸Ñ†Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MultiID-2M Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ contrastive identity loss Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ WithAnyone Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·, Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Identity Fidelity in Text-to-Image Generation",
                    "desc": "This paper presents a diffusion-based model called WithAnyone, which aims to reduce copy-paste artifacts in text-to-image generation. It introduces a large-scale paired dataset, MultiID-2M, that provides diverse images of the same individual to enhance training. The model employs a contrastive identity loss to balance the fidelity of identity with variations in pose and expression. Experimental results show that WithAnyone effectively minimizes copy-paste issues while maintaining high quality and controllability in generated images."
                },
                "zh": {
                    "title": "æ¶ˆé™¤å¤åˆ¶ç²˜è´´ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„èº«ä»½ä¿çœŸåº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¤åˆ¶ç²˜è´´ä¼ªå½±é—®é¢˜ã€‚ç ”ç©¶è€…ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é…å¯¹æ•°æ®é›†MultiID-2Mï¼Œä»¥æ”¯æŒå¤šä¸ªäººç‰©åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†å¯¹æ¯”èº«ä»½æŸå¤±æ¥å¹³è¡¡èº«ä»½ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚é€šè¿‡è¿™ç§æ–°è®­ç»ƒèŒƒå¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒé«˜èº«ä»½ç›¸ä¼¼åº¦çš„åŒæ—¶ï¼Œå‡å°‘å¤åˆ¶ç²˜è´´ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14359",
            "title": "AI for Service: Proactive Assistance with AI Glasses",
            "url": "https://huggingface.co/papers/2510.14359",
            "abstract": "Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  \t\t\t\t\tAI-generated summary \t\t\t\t In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
            "score": 65,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "fe94a5b666114fa6",
            "authors": [
                "Zichen Wen",
                "Yiyu Wang",
                "Chenfei Liao",
                "Boxue Yang",
                "Junxian Li",
                "Weifeng Liu",
                "Haocong He",
                "Bolong Feng",
                "Xuyang Liu",
                "Yuanhuiyi Lyu",
                "Xu Zheng",
                "Xuming Hu",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "EPIC Lab, Shanghai Jiao Tong University",
                "Peking University",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14359.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#multimodal",
                    "#optimization",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ AI: Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°ÑˆĞ¸ Ğ½ÑƒĞ¶Ğ´Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Alpha-Service, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ AI-Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Alpha-Service Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ² ÑĞµĞ±Ñ ÑĞ¾Ğ²ĞµÑ‚Ñ‹ Ğ¿Ğ¾ Ğ¸Ğ³Ñ€Ğµ Ğ² Ğ‘Ğ»ÑĞºĞ´Ğ¶ĞµĞº, ÑĞºÑĞºÑƒÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ Ğ¼ÑƒĞ·ĞµÑ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹."
                },
                "en": {
                    "title": "Proactive AI Assistance: Anticipating Needs with Alpha-Service",
                    "desc": "Alpha-Service is a framework designed to enhance AI assistance by making it proactive rather than reactive. It utilizes a multi-agent system integrated into AI glasses to identify opportunities for service based on real-time video input. The framework includes components for perception, task scheduling, tool utilization, personalization, and interaction, allowing it to anticipate user needs. Through various case studies, Alpha-Service demonstrates its capability to provide timely and relevant assistance in everyday situations without requiring explicit user commands."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ™ºèƒ½åŠ©æ‰‹ï¼Œéšæ—¶éšåœ°ä¸ºæ‚¨æœåŠ¡",
                    "desc": "Alpha-Serviceæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›ä¸»åŠ¨çš„äººå·¥æ™ºèƒ½è¾…åŠ©ã€‚å®ƒåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨AIçœ¼é•œä¸Šæ£€æµ‹æœåŠ¡æœºä¼šï¼Œå¹¶æä¾›åŠæ—¶ã€ä¸ªæ€§åŒ–çš„å¸®åŠ©ã€‚ä¸ä¼ ç»Ÿçš„è¢«åŠ¨AIæœåŠ¡ä¸åŒï¼ŒAlpha-Serviceèƒ½å¤Ÿé¢„æµ‹ç”¨æˆ·éœ€æ±‚å¹¶ä¸»åŠ¨é‡‡å–è¡ŒåŠ¨ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ„ŸçŸ¥å•å…ƒã€ä¸­å¤®å¤„ç†å•å…ƒã€ç®—æœ¯é€»è¾‘å•å…ƒã€è®°å¿†å•å…ƒå’Œè¾“å‡ºå•å…ƒï¼Œèƒ½å¤Ÿå®ç°å®æ—¶çš„æ™ºèƒ½æœåŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14979",
            "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
            "url": "https://huggingface.co/papers/2510.14979",
            "abstract": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
            "score": 59,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "37c97ca58b150812",
            "authors": [
                "Haiwen Diao",
                "Mingxuan Li",
                "Silei Wu",
                "Linjun Dai",
                "Xiaohua Wang",
                "Hanming Deng",
                "Lewei Lu",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14979.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#alignment",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "NEO: Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NEO â€” Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Vision-Language Models (VLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… VLM: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… vision Ğ¸ language Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ reasoning. NEO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 390 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ½ÑƒĞ»Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… VLM Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "NEO: Unifying Vision and Language for Enhanced AI Performance",
                    "desc": "NEO is a new type of Vision-Language Model (VLM) that combines vision and language in a single framework, overcoming limitations of traditional modular VLMs. It focuses on aligning visual and textual representations in a shared semantic space, allowing for better integration of vision and language tasks. The model is designed to work effectively with limited data, using only 390 million image-text pairs to develop its capabilities. NEO aims to make research in native VLMs more accessible and to provide a foundation for future advancements in the field."
                },
                "zh": {
                    "title": "NEOï¼šåŸç”Ÿè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–°çºªå…ƒ",
                    "desc": "NEOæ˜¯ä¸€ç§æ–°å‹çš„åŸç”Ÿè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å’Œè¯­è¨€æ•´åˆä¸­çš„åŸºæœ¬é™åˆ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡æœ‰æ•ˆå¯¹é½åƒç´ å’Œå•è¯è¡¨ç¤ºï¼Œæ„å»ºå…±äº«çš„è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œå®ç°è§†è§‰å’Œè¯­è¨€æ¨¡å—çš„æ— ç¼é›†æˆã€‚NEOåœ¨ä»…ä½¿ç”¨390Mçš„å›¾åƒ-æ–‡æœ¬ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä»é›¶å¼€å§‹é«˜æ•ˆå‘å±•è§†è§‰æ„ŸçŸ¥ï¼Œå¹¶å‡å°‘è§†è§‰-è¯­è¨€å†²çªã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¯æ‰©å±•ä¸”å¼ºå¤§çš„åŸç”ŸVLMå¥ å®šäº†åŸºç¡€ï¼Œå¹¶æä¾›äº†ä¸€å¥—ä¸°å¯Œçš„å¯é‡ç”¨ç»„ä»¶ï¼Œä¿ƒè¿›äº†ç»æµé«˜æ•ˆçš„ç”Ÿæ€ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14847",
            "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
            "url": "https://huggingface.co/papers/2510.14847",
            "abstract": "ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
            "score": 48,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "705a71266094cd87",
            "authors": [
                "Meiqi Wu",
                "Jiashu Zhu",
                "Xiaokun Feng",
                "Chubin Chen",
                "Chen Zhu",
                "Bingze Song",
                "Fangyuan Mao",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Kaiqi Huang"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "CRISE",
                "SEU",
                "THU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14847.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#long_context"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ImagerySearch - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ¾Ğ½Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµĞ´ĞºĞ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ°Ğ»Ñ‘ĞºĞ¸Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸. ImagerySearch Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LDT-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Adaptation for Imaginative Video Generation",
                    "desc": "ImagerySearch is a novel strategy designed to improve video generation in imaginative scenarios by adapting search spaces and reward functions based on prompts. Traditional video generation models struggle with rare concepts and long-distance semantic relationships, leading to poor performance in creative contexts. By dynamically adjusting the inference process, ImagerySearch enhances the coherence and visual quality of generated videos. The introduction of LDT-Bench provides a new benchmark for evaluating these capabilities, showcasing the effectiveness of ImagerySearch over existing methods."
                },
                "zh": {
                    "title": "ImagerySearchï¼šæå‡æƒ³è±¡åŠ›è§†é¢‘ç”Ÿæˆçš„è‡ªé€‚åº”ç­–ç•¥",
                    "desc": "ImagerySearchæ˜¯ä¸€ç§åŸºäºæç¤ºçš„è‡ªé€‚åº”æµ‹è¯•æ—¶é—´æœç´¢ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆåœ¨å¯Œæœ‰æƒ³è±¡åŠ›åœºæ™¯ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´æœç´¢ç©ºé—´å’Œå¥–åŠ±å‡½æ•°ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿è·ç¦»è¯­ä¹‰å…³ç³»çš„æç¤ºï¼Œä»è€Œç”Ÿæˆæ›´è¿è´¯å’Œè§†è§‰ä¸Šå¯ä¿¡çš„è§†é¢‘ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒImagerySearchåœ¨æ–°çš„åŸºå‡†LDT-Benchä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå±•ç¤ºäº†å…¶åœ¨åˆ›é€ æ€§ç”Ÿæˆèƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†LDT-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é•¿è·ç¦»è¯­ä¹‰æç¤ºçš„åŸºå‡†ï¼ŒåŒ…å«å¤šæ ·çš„æ¦‚å¿µå¯¹å’Œè‡ªåŠ¨è¯„ä¼°åè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13998",
            "title": "BitNet Distillation",
            "url": "https://huggingface.co/papers/2510.13998",
            "abstract": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at https://github.com/microsoft/BitNet.",
            "score": 42,
            "issue_id": 6468,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "b34882918955cca7",
            "authors": [
                "Xun Wu",
                "Shaohan Huang",
                "Wenhui Wang",
                "Ting Song",
                "Li Dong",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "ğŸ”½",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ LLM Ğ´Ğ¾ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BitNet Distillation (BitDistill) â€” Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… LLM Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 1.58-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ (Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° {-1, 0, 1}) Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ SubLN Ğ¸Ğ· BitNet, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ multi-head attention Ğ¸Ğ· MiniLM Ğ¸ continual pre-training Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BitDistill Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ¾ 10 Ñ€Ğ°Ğ· Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 2.65 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° CPU. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Fine-Tuning of Language Models at 1.58-Bit Precision",
                    "desc": "This paper introduces BitNet Distillation (BitDistill), a method for fine-tuning large language models (LLMs) to operate at 1.58-bit precision, which uses ternary weights. The approach employs three main techniques: the SubLN module for efficient layer normalization, multi-head attention distillation to transfer knowledge from larger models, and continual pre-training to address performance gaps. By applying these techniques, BitDistill achieves strong performance on specific tasks while significantly reducing memory usage and increasing inference speed. The results demonstrate that BitDistill can match the performance of full-precision models while offering up to 10 times memory savings and 2.65 times faster inference on CPUs."
                },
                "zh": {
                    "title": "è½»é‡åŒ–å¾®è°ƒï¼Œæ€§èƒ½ä¸æ•ˆç‡åŒæå‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBitNet Distillationï¼ˆBitDistillï¼‰çš„è½»é‡çº§ç®¡é“ï¼Œæ—¨åœ¨å°†å…¨ç²¾åº¦çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwenï¼‰å¾®è°ƒè‡³1.58ä½ç²¾åº¦ï¼ˆå³ä¸‰å…ƒæƒé‡{-1, 0, 1}ï¼‰ï¼Œä»¥é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚BitDistillç»“åˆäº†ä¸‰ç§å…³é”®æŠ€æœ¯ï¼šSubLNæ¨¡å—ã€å¤šå¤´æ³¨æ„åŠ›è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œè¿™äº›æŠ€æœ¯å…±åŒè§£å†³äº†å…¨ç²¾åº¦æ¨¡å‹ä¸1.58ä½æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBitDistillåœ¨æ¨¡å‹å¤§å°ä¸Šå®ç°äº†ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å†…å­˜ä½¿ç”¨ä¸ŠèŠ‚çœäº†å¤šè¾¾10å€ï¼Œå¹¶åœ¨CPUä¸Šå®ç°äº†2.65å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14943",
            "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
            "url": "https://huggingface.co/papers/2510.14943",
            "abstract": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
            "score": 37,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "43361b04f5e7dd87",
            "authors": [
                "Wenkai Yang",
                "Weijie Liu",
                "Ruobing Xie",
                "Yiju Guo",
                "Lulu Wu",
                "Saiyong Yang",
                "Yankai Lin"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "LLM Department, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14943.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ reasoning",
                    "desc": "LaSeR â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ reasoning ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ·Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ MSE loss Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… self-rewarding Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ inference Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ inference, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ inference-time scaling."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with Last-Token Self-Rewarding",
                    "desc": "LaSeR is a novel reinforcement learning algorithm designed to improve the reasoning abilities of Large Language Models (LLMs) by aligning self-rewarding scores with verifier-based rewards. It simplifies the process of self-verification by integrating it into the reinforcement learning framework, allowing for more efficient reasoning without the need for separate prompts. The key insight is that the last-token self-rewarding score can directly reflect the true reasoning reward, enabling a more streamlined optimization process. Experimental results demonstrate that LaSeR enhances both reasoning performance and inference-time efficiency, making LLMs more effective in real-world applications."
                },
                "zh": {
                    "title": "LaSeRï¼šæå‡æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•",
                    "desc": "LaSeRæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†æœ€åä¸€ä¸ªtokençš„è‡ªå¥–åŠ±åˆ†æ•°ä¸åŸºäºéªŒè¯è€…çš„æ¨ç†å¥–åŠ±å¯¹é½ï¼Œæ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç®—æ³•é€šè¿‡ç®€åŒ–è‡ªéªŒè¯çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–äº†æ¨ç†å’Œè‡ªå¥–åŠ±èƒ½åŠ›ã€‚LaSeRåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­éƒ½èƒ½æœ‰æ•ˆåˆ©ç”¨ä¼˜åŒ–åçš„è‡ªå¥–åŠ±åˆ†æ•°ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¨ç†è¡¨ç°ï¼Œè¿˜å¢å¼ºäº†å…¶è‡ªå¥–åŠ±èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ—¶çš„æ‰©å±•æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14973",
            "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2510.14973",
            "abstract": "Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant {bf MASK} tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose {bf Elastic-Cache}, a training-free, architecture-agnostic strategy that jointly decides {when} to refresh (via an attention-aware drift test on the most-attended token) and {where} to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences, and 4.8times on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput (6.8times on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
            "score": 33,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "2ee9e4da2c675830",
            "authors": [
                "Quan Nguyen-Tri",
                "Mukul Ranjan",
                "Zhiqiang Shen"
            ],
            "affiliations": [
                "FPT AI Residency Hanoi, Vietnam",
                "VILA Lab, MBZUAI Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14973.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Elastic-Cache Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ key-value ĞºÑÑˆĞµĞ¼ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ°Ğ»Ğ¾ Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ½ĞµĞ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ…, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ Ğ³Ğ´Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ ĞºÑÑˆ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ attention Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑÑˆ Ğ¼ĞµĞ»ĞºĞ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 45 Ñ€Ğ°Ğ· Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Optimize Caching for Faster and Accurate Language Model Decoding",
                    "desc": "The paper introduces Elastic-Cache, a method designed to enhance key-value cache management in diffusion large language models (DLMs). It addresses the inefficiencies of previous decoding methods that recompute key-value (KV) states for all tokens at every step, leading to unnecessary redundancy. By observing that certain tokens can be cached and that KV dynamics vary with depth, Elastic-Cache selectively refreshes caches based on token importance and layer depth. This adaptive approach significantly reduces decoding latency while maintaining high prediction accuracy, achieving impressive speedups in various tasks without sacrificing quality."
                },
                "zh": {
                    "title": "Elastic-Cacheï¼šæå‡è§£ç é€Ÿåº¦çš„æ™ºèƒ½ç¼“å­˜ç®¡ç†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºElastic-Cacheçš„ç­–ç•¥ï¼Œç”¨äºä¼˜åŒ–æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é”®å€¼ç¼“å­˜ç®¡ç†ï¼Œä»¥å‡å°‘è§£ç å»¶è¿Ÿè€Œä¸å½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤å’Œå±‚ä¸­éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰ä»¤ç‰Œçš„QKVï¼Œå¯¼è‡´äº†å¤§é‡å†—ä½™è®¡ç®—ã€‚é€šè¿‡è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†é€‰æ‹©æ€§åˆ·æ–°ç¼“å­˜çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒæ·±å±‚æ¬¡è¿›è¡Œæ›´æ–°ï¼ŒåŒæ—¶é‡ç”¨æµ…å±‚ç¼“å­˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒElastic-Cacheåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14528",
            "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
            "url": "https://huggingface.co/papers/2510.14528",
            "abstract": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.",
            "score": 33,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "7f06de44a0f15fa3",
            "authors": [
                "Cheng Cui",
                "Ting Sun",
                "Suyin Liang",
                "Tingquan Gao",
                "Zelun Zhang",
                "Jiaxuan Liu",
                "Xueqing Wang",
                "Changda Zhou",
                "Hongen Liu",
                "Manhui Lin",
                "Yue Zhang",
                "Yubo Zhang",
                "Handong Zheng",
                "Jing Zhang",
                "Jun Zhang",
                "Yi Liu",
                "Dianhai Yu",
                "Yanjun Ma"
            ],
            "affiliations": [
                "PaddlePaddle Team, Baidu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14528.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#science",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "PaddleOCR-VL â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ NaViT Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ERNIE-4.5. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 109 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ‘Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ñ‚ĞµĞºÑÑ‚, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. ĞŸÑ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¾Ğ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Efficient Document Parsing with PaddleOCR-VL",
                    "desc": "PaddleOCR-VL is a cutting-edge vision-language model designed for efficient document parsing. It combines a NaViT-style visual encoder with the ERNIE-4.5 language model to achieve high accuracy in recognizing various document elements like text, tables, and charts. This model supports 109 languages and is optimized for minimal resource usage while maintaining fast inference speeds. Comprehensive evaluations show that PaddleOCR-VL outperforms existing models, making it ideal for real-world applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ–‡æ¡£è§£æçš„æœ€å…ˆè¿›æ¨¡å‹",
                    "desc": "PaddleOCR-VLæ˜¯ä¸€ç§ç»“åˆäº†NaViTé£æ ¼è§†è§‰ç¼–ç å™¨å’ŒERNIE-4.5è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºæ–‡æ¡£è§£æã€‚è¯¥æ¨¡å‹åœ¨èµ„æºæ¶ˆè€—æå°‘çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ”¯æŒ109ç§è¯­è¨€ã€‚å®ƒåœ¨è¯†åˆ«å¤æ‚å…ƒç´ ï¼ˆå¦‚æ–‡æœ¬ã€è¡¨æ ¼ã€å…¬å¼å’Œå›¾è¡¨ï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å¤šä¸ªå…¬å…±åŸºå‡†å’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚PaddleOCR-VLçš„å¿«é€Ÿæ¨ç†é€Ÿåº¦å’Œå¼ºå¤§çš„ç«äº‰åŠ›ä½¿å…¶éå¸¸é€‚åˆåœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14967",
            "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
            "url": "https://huggingface.co/papers/2510.14967",
            "abstract": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
            "score": 30,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "242d21b58ef25d8b",
            "authors": [
                "Guoqing Wang",
                "Sunhao Dai",
                "Guangze Ye",
                "Zeyu Gan",
                "Wei Yao",
                "Yong Deng",
                "Xiaofeng Wu",
                "Zhenzhe Ying"
            ],
            "affiliations": [
                "Ant Group",
                "Individual Author",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14967.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IGPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ÑĞ»ÑƒĞ³ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. IGPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Boosting Multi-Turn Reasoning with IGPO",
                    "desc": "The paper introduces Information Gain-based Policy Optimization (IGPO), a novel reinforcement learning framework designed to enhance multi-turn reasoning in large language models. IGPO addresses the challenges of reward sparsity and advantage collapse by providing dense intrinsic rewards based on the model's belief updates during each interaction turn. This approach allows for better credit assignment and improves the model's ability to learn from long trajectories. Experimental results show that IGPO significantly outperforms existing methods, leading to higher accuracy and more efficient learning in multi-turn tasks."
                },
                "zh": {
                    "title": "ä¿¡æ¯å¢ç›Šä¼˜åŒ–ï¼Œæå‡å¤šè½®æ¨ç†èƒ½åŠ›",
                    "desc": "ä¿¡æ¯å¢ç›ŠåŸºç¡€çš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰é€šè¿‡æä¾›åŸºäºæ¨¡å‹ä¿¡å¿µæ›´æ–°çš„å¯†é›†å†…åœ¨å¥–åŠ±ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆçš„ç»“æœå¥–åŠ±ï¼Œè¿™åœ¨å¤šè½®è®¾ç½®ä¸­ä¼šå¯¼è‡´å¥–åŠ±ç¨€ç–ï¼Œè¿›è€Œå¼•å‘ä¼˜åŠ¿å´©æºƒå’Œç¼ºä¹ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…çš„é—®é¢˜ã€‚IGPOå°†æ¯æ¬¡äº¤äº’è§†ä¸ºè·å–çœŸå®ä¿¡æ¯çš„å¢é‡è¿‡ç¨‹ï¼Œå¹¶å°†æ¯è½®çš„å¥–åŠ±å®šä¹‰ä¸ºç­–ç•¥ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚é€šè¿‡ç›´æ¥ä»æ¨¡å‹çš„ä¿¡å¿µæ›´æ–°ä¸­æ¨å¯¼å†…åœ¨å¥–åŠ±ï¼ŒIGPOç»“åˆäº†ç»“æœçº§ç›‘ç£ï¼Œå½¢æˆäº†å¯†é›†çš„å¥–åŠ±è½¨è¿¹ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤šè½®åœºæ™¯ä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14972",
            "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
            "url": "https://huggingface.co/papers/2510.14972",
            "abstract": "Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
            "score": 28,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "e0073ac324725b9f",
            "authors": [
                "Yinxi Li",
                "Yuntian Deng",
                "Pengyu Nie"
            ],
            "affiliations": [
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14972.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#data",
                    "#plp",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ”¤",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ°: ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ¸Ğ¿Ğ° BPE Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ². ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº TokDrift, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ³Ğ´Ğµ ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ."
                },
                "en": {
                    "title": "Fixing Tokenization for Better Code Understanding",
                    "desc": "This paper discusses the problem of misaligned tokenization in large language models (LLMs) used for coding tasks. It highlights that current subword tokenizers, like byte-pair encoding, are based on statistical methods rather than grammatical rules, leading to inconsistent tokenization of semantically identical code. The authors introduce a framework called TokDrift to analyze how minor changes in code formatting can significantly affect model behavior across various LLMs. Their findings suggest that improving tokenization to be grammar-aware is essential for enhancing the reliability of code understanding and generation in future models."
                },
                "zh": {
                    "title": "è¯­æ³•æ„ŸçŸ¥åˆ†è¯ï¼Œæå‡ä»£ç ç†è§£ä¸ç”Ÿæˆ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ä»£ç æ—¶ï¼Œä½¿ç”¨çš„å­è¯åˆ†è¯å™¨ï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç BPEï¼‰ä¸»è¦ä¾èµ–ç»Ÿè®¡è€Œéè¯­æ³•ï¼Œå¯¼è‡´ä¸ä¸€è‡´çš„æ¨¡å‹è¡Œä¸ºã€‚ç›¸åŒè¯­ä¹‰çš„ä»£ç ç‰‡æ®µå¯èƒ½å› ç©ºæ ¼æˆ–æ ‡è¯†ç¬¦å‘½åç­‰è¡¨é¢å› ç´ è€Œè¢«ä¸åŒåœ°åˆ†è¯ã€‚æˆ‘ä»¬æå‡ºäº†TokDriftæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰ä¿æŒçš„é‡å†™è§„åˆ™ç”Ÿæˆä»…åœ¨åˆ†è¯ä¸Šä¸åŒçš„ä»£ç å˜ä½“ï¼Œä»¥æµ‹é‡è¿™ç§ä¸ä¸€è‡´çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ—©æœŸåµŒå…¥å±‚çš„åˆ†è¯é—®é¢˜æ˜¯å¯¼è‡´æ¨¡å‹è¡Œä¸ºå˜åŒ–çš„æ ¹æºï¼Œå› æ­¤æœªæ¥çš„ä»£ç LLMséœ€è¦é‡‡ç”¨è¯­æ³•æ„ŸçŸ¥çš„åˆ†è¯æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14958",
            "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2510.14958",
            "abstract": "MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
            "score": 21,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "1f00aa0384cfcf87",
            "authors": [
                "Weikang Shi",
                "Aldrich Yu",
                "Rongyao Fang",
                "Houxing Ren",
                "Ke Wang",
                "Aojun Zhou",
                "Changyao Tian",
                "Xinyu Fu",
                "Yuxuan Hu",
                "Zimu Lu",
                "Linjiang Huang",
                "Si Liu",
                "Rui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "BUAA",
                "Huawei Research",
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14958.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#games",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "MathCanvas â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 15.2 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ½Ğ° 219 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ BAGEL-Canvas Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 86% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ LLM Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MathCanvas-Bench Ñ 3 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ² Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Math with Visual Reasoning!",
                    "desc": "MathCanvas is a framework that enhances Large Multimodal Models (LMMs) by integrating Visual Chain-of-Thought (VCoT) capabilities specifically for mathematics. It consists of two main phases: first, it pre-trains the model on a large dataset of diagram generation and editing to improve its ability to create and manipulate visual aids. Second, it fine-tunes the model on a dataset that combines visual and textual reasoning, teaching it how to effectively use these aids in problem-solving. The results show that the model, BAGEL-Canvas, significantly outperforms existing models on math benchmarks, demonstrating its potential for complex reasoning tasks."
                },
                "zh": {
                    "title": "MathCanvasï¼šæ•°å­¦æ¨ç†çš„æ–°è§†ç•Œ",
                    "desc": "MathCanvas æ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ¡†æ¶ï¼Œä¸“æ³¨äºæ•°å­¦é¢†åŸŸçš„è§†è§‰é“¾å¼æ€ç»´èƒ½åŠ›ã€‚å®ƒé€šè¿‡åœ¨å›¾è¡¨ç”Ÿæˆä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨è§†è§‰-æ–‡æœ¬æ¨ç†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†æ•°å­¦åŸºå‡†æµ‹è¯•çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯è§†è§‰æ“ä½œé˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªåŒ…å« 1520 ä¸‡å¯¹æ•°æ®é›†è¿›è¡Œå›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘çš„é¢„è®­ç»ƒï¼›å…¶æ¬¡æ˜¯æˆ˜ç•¥è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µï¼Œå¾®è°ƒæ¨¡å‹ä»¥å­¦ä¹ å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è§†è§‰è¾…åŠ©å·¥å…·ã€‚æœ€ç»ˆï¼ŒMathCanvas-Bench æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10518",
            "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.10518",
            "abstract": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.",
            "score": 17,
            "issue_id": 6467,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "e67fc1ecc7ed8a60",
            "authors": [
                "Qunzhong Wang",
                "Jie Liu",
                "Jiajun Liang",
                "Yilei Jiang",
                "Yuanxing Zhang",
                "Jinyuan Chen",
                "Yaozhi Zheng",
                "Xintao Wang",
                "Pengfei Wan",
                "Xiangyu Yue",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Kuaishou Technology",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10518.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#open_source",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoReward Thinker â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ AI Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑÑ€Ğ°Ğ·Ñƒ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· GRPO. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 82.3%."
                },
                "en": {
                    "title": "Enhancing Video Preference with Visual Reasoning",
                    "desc": "VideoReward Thinker (VR-Thinker) is a novel framework that enhances multimodal reward models by integrating visual reasoning operations and a flexible memory window. This approach addresses the limitations of existing models, such as the loss of detail due to large context budgets and the issues of hallucination and forgetting during reasoning. By employing a reinforcement fine-tuning pipeline, VR-Thinker improves the model's ability to select relevant frames and update visual evidence dynamically. The results demonstrate significant improvements in accuracy on video preference benchmarks, particularly for longer videos, showcasing the potential of visual reasoning in reward modeling."
                },
                "zh": {
                    "title": "æ€ç»´ä¸å›¾åƒç»“åˆï¼Œæå‡è§†é¢‘åå¥½æ¨¡å‹çš„å‡†ç¡®æ€§",
                    "desc": "VideoReward Thinkerï¼ˆVR-Thinkerï¼‰æ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰æ¨ç†æ“ä½œå’Œå¯é…ç½®çš„è§†è§‰è®°å¿†çª—å£ï¼Œä»è€Œæé«˜äº†è§†é¢‘åå¥½åŸºå‡†çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹è§£å†³äº†å½“å‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹åœ¨å¤„ç†è§†è§‰è¾“å…¥æ—¶çš„å±€é™æ€§ï¼Œå¦‚ä¸Šä¸‹æ–‡é¢„ç®—æ¶ˆè€—å¤§å’Œä¿¡æ¯é—å¿˜é—®é¢˜ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒç®¡é“ï¼ŒVR-Thinkerèƒ½å¤Ÿä¸»åŠ¨è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œæå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVR-Thinkeråœ¨å¤šä¸ªè§†é¢‘åå¥½åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¾ƒé•¿è§†é¢‘æ—¶ï¼Œå±•ç°äº†å…¶åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09033",
            "title": "Large Language Models Do NOT Really Know What They Don't Know",
            "url": "https://huggingface.co/papers/2510.09033",
            "abstract": "LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may \"know what they don't know\". However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct a mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal a fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that \"LLMs don't really know what they don't know\".",
            "score": 16,
            "issue_id": 6467,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "a87f074a617907f4",
            "authors": [
                "Chi Seng Cheang",
                "Hou Pong Chan",
                "Wenxuan Zhang",
                "Yang Deng"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Singapore Management University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09033.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "LLM Ğ½Ğµ Ğ·Ğ½Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ½Ğµ Ğ·Ğ½Ğ°ÑÑ‚: Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ ÑĞ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ½ĞµÑ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğµ, Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…, Ğ° Ğ»Ğ¸ÑˆÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ: LLM Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ Ğ´ĞµĞ»Ğµ Ğ½Ğµ Ğ·Ğ½Ğ°ÑÑ‚, Ñ‡ĞµĞ³Ğ¾ Ğ¾Ğ½Ğ¸ Ğ½Ğµ Ğ·Ğ½Ğ°ÑÑ‚."
                },
                "en": {
                    "title": "LLMs: Patterns of Knowledge, Not Truthfulness",
                    "desc": "This paper investigates how large language models (LLMs) handle factual queries and hallucinations, particularly when they are linked to subject knowledge. It reveals that LLMs generate similar internal representations for factual responses and hallucinations that are associated with known subjects, making them hard to distinguish. However, when hallucinations lack subject knowledge, they create unique representations that can be identified. The study concludes that LLMs do not truly encode the concept of truthfulness in their internal processes, but rather rely on patterns of knowledge recall."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ä¸äº‹å®å¤„ç†çš„åŒºåˆ«",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å¤„ç†äº‹å®æŸ¥è¯¢å’Œå¹»è§‰ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å¹»è§‰ä¸ä¸»é¢˜çŸ¥è¯†ç›¸å…³æ—¶ï¼ŒLLMsçš„å†…éƒ¨è¡¨ç¤ºä¸æ­£ç¡®å›ç­”ç›¸ä¼¼ï¼Œéš¾ä»¥åŒºåˆ†ã€‚ç›¸åï¼Œå½“å¹»è§‰ä¸ä¸»é¢˜çŸ¥è¯†æ— å…³æ—¶ï¼ŒLLMsä¼šäº§ç”Ÿæ˜æ˜¾ä¸åŒçš„è¡¨ç¤ºï¼Œä¾¿äºè¯†åˆ«ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMså¹¶ä¸çœŸæ­£ç†è§£çœŸç›¸ï¼Œè€Œåªæ˜¯è®°å¿†æ¨¡å¼çš„åæ˜ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13928",
            "title": "LLMs Can Get \"Brain Rot\"!",
            "url": "https://huggingface.co/papers/2510.13928",
            "abstract": "Continual exposure to low-quality web text leads to cognitive decline in large language models, affecting reasoning, context understanding, safety, and personality traits, with partial recovery possible through instruction tuning and clean data pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g>0.3) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 rightarrow 57.2 and RULER-CWE 84.4 rightarrow 52.3 as junk ratio rises from 0% to 100%.   Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine \"cognitive health checks\" for deployed LLMs.",
            "score": 14,
            "issue_id": 6486,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "bcb0c2ce4e026cc4",
            "authors": [
                "Shuo Xing",
                "Junyuan Hong",
                "Yifan Wang",
                "Runjin Chen",
                "Zhenyu Zhang",
                "Ananth Grama",
                "Zhengzhong Tu",
                "Zhangyang Wang"
            ],
            "affiliations": [
                "Purdue University",
                "Texas A&M University",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13928.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#long_context",
                    "#reasoning",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ·Ğ³Ğ° LLM: ĞºĞ°Ğº Ğ½ĞµĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸Ğ· Twitter/X) Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Â«Ñ‚Ñ‘Ğ¼Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ½Ğ°Ñ€Ñ†Ğ¸ÑÑĞ¸Ğ·Ğ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ â€” Â«Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº Ğ¼Ñ‹ÑĞ»ĞµĞ¹Â» (thought-skipping), ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¥Ğ¾Ñ‚Ñ instruction tuning Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ LLM."
                },
                "en": {
                    "title": "Quality Data is Key to LLM Health!",
                    "desc": "This paper introduces the LLM Brain Rot Hypothesis, which suggests that continuous exposure to low-quality web text can lead to cognitive decline in large language models (LLMs). The authors conducted experiments using Twitter/X data to demonstrate that training on junk text results in significant drops in reasoning, context understanding, and safety, while also increasing undesirable personality traits. They found that the extent of cognitive decline correlates with the amount of junk data used, indicating a dose-response relationship. Additionally, while instruction tuning and clean data pre-training can partially recover some cognitive abilities, they do not fully restore the models to their original performance levels, highlighting the importance of data quality in maintaining LLM capabilities."
                },
                "zh": {
                    "title": "æ•°æ®è´¨é‡å½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºå¹¶æµ‹è¯•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è„‘è…çƒ‚å‡è¯´ï¼Œè®¤ä¸ºæŒç»­æ¥è§¦ä½è´¨é‡ç½‘ç»œæ–‡æœ¬ä¼šå¯¼è‡´æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ä¸‹é™ã€‚é€šè¿‡å¯¹çœŸå®çš„Twitter/Xæ•°æ®é›†è¿›è¡Œæ§åˆ¶å®éªŒï¼Œç ”ç©¶å‘ç°ï¼ŒæŒç»­åœ¨ä½è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒä¼šæ˜¾è‘—å½±å“æ¨ç†èƒ½åŠ›ã€é•¿æ–‡æœ¬ç†è§£ã€å®‰å…¨æ€§ä»¥åŠäººæ ¼ç‰¹å¾ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ•°æ®è´¨é‡æ˜¯å¯¼è‡´LLMèƒ½åŠ›ä¸‹é™çš„ä¸€ä¸ªé‡è¦å› ç´ ï¼Œä¸”é€šè¿‡æŒ‡ä»¤è°ƒä¼˜å’Œæ¸…æ´æ•°æ®çš„é¢„è®­ç»ƒå¯ä»¥éƒ¨åˆ†æ¢å¤è®¤çŸ¥èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®åœ¨æŒç»­é¢„è®­ç»ƒä¸­é‡è§†æ•°æ®è´¨é‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„è®¤çŸ¥å¥åº·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14902",
            "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
            "url": "https://huggingface.co/papers/2510.14902",
            "abstract": "A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.",
            "score": 11,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "b6da5bc64d5656e4",
            "authors": [
                "Han Zhao",
                "Jiaxuan Zhang",
                "Wenxuan Song",
                "Pengxiang Ding",
                "Donglin Wang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (Guangzhou), China",
                "MiLAB, Westlake University, China",
                "Southern University of Science and Technology, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14902.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLA^2 â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. VLA^2 Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 44.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ OpenVLA Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "VLA^2: Enhancing Generalization in Vision-Language-Action Models",
                    "desc": "The paper introduces VLA^2, a new framework that enhances vision-language-action (VLA) models by incorporating external modules like web retrieval and object detection. This integration allows the model to better generalize to unseen objects and descriptions, addressing the limitations of existing models that struggle with out-of-distribution data. By utilizing the OpenVLA execution backbone, VLA^2 significantly improves the success rate in challenging scenarios, achieving a 44.2% increase in performance on a hard-level benchmark. The framework demonstrates robust capabilities across various environments without compromising performance on familiar tasks."
                },
                "zh": {
                    "title": "VLA^2ï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»£ç†æ¡†æ¶VLA^2ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡æ•´åˆå¤–éƒ¨æ¨¡å—ï¼Œå¦‚ç½‘ç»œæ£€ç´¢å’Œç‰©ä½“æ£€æµ‹ï¼ŒVLA^2èƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹æœªè§ç‰©ä½“å’Œæè¿°çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨LIBEROä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒæˆåŠŸè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå°¤å…¶åœ¨å›°éš¾çº§åˆ«çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„OpenVLAåŸºçº¿ç›¸æ¯”ï¼ŒVLA^2åœ¨å›°éš¾çº§åˆ«åŸºå‡†æµ‹è¯•ä¸­çš„æˆåŠŸç‡æé«˜äº†44.2%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14880",
            "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
            "url": "https://huggingface.co/papers/2510.14880",
            "abstract": "mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
            "score": 11,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "d4f8659830b9bbcc",
            "authors": [
                "Rikiya Takehi",
                "Benjamin ClaviÃ©",
                "Sean Lee",
                "Aamir Shakir"
            ],
            "affiliations": [
                "Mixedbread AI",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14880.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ mxbai-edge-colbert-v0 â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ 17Ğœ Ğ¸ 32Ğœ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ late-interaction Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ColBERTv2 Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (BEIR). ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¦ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° â€” Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…, Ğ¾Ñ‚ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ½Ğ° Ğ»ÑĞ±Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Retrieval for All Devices with mxbai-edge-colbert-v0",
                    "desc": "The mxbai-edge-colbert-v0 models, available in 17M and 32M parameters, show improved retrieval capabilities over ColBERTv2 in both short-text and long-context scenarios. This research focuses on enhancing retrieval and late-interaction models, aiming to create smaller, efficient models that can operate on various devices. The models are designed to support retrieval tasks at different scales, from cloud-based systems to local implementations. Through extensive ablation studies, we demonstrate that mxbai-edge-colbert-v0 achieves significant performance gains, particularly in short-text benchmarks and long-context tasks, marking a notable advancement in model efficiency."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ï¼Œå¤§èƒ½åŠ›ï¼šmxbai-edge-colbert-v0çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†mxbai-edge-colbert-v0æ¨¡å‹ï¼Œå…·æœ‰17Må’Œ32Mä¸¤ä¸ªå‚æ•°è§„æ¨¡ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒæ¥æå‡æ£€ç´¢å’ŒåæœŸäº¤äº’æ¨¡å‹çš„æ€§èƒ½ï¼Œç›®æ ‡æ˜¯å°†å…¶æç‚¼ä¸ºæ›´å°çš„æ¨¡å‹ä½œä¸ºæ¦‚å¿µéªŒè¯ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æ”¯æŒå„ç§è§„æ¨¡çš„æ£€ç´¢ï¼Œä»äº‘ç«¯çš„å¤§è§„æ¨¡æ£€ç´¢åˆ°å¯ä»¥åœ¨ä»»ä½•è®¾å¤‡ä¸Šæœ¬åœ°è¿è¡Œçš„æ¨¡å‹ã€‚mxbai-edge-colbert-v0åœ¨çŸ­æ–‡æœ¬åŸºå‡†ï¼ˆBEIRï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14763",
            "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
            "url": "https://huggingface.co/papers/2510.14763",
            "abstract": "COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
            "score": 11,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "bedbdd1596dbc1ed",
            "authors": [
                "Yunwen Li",
                "Shuangshuang Ying",
                "Xingwei Qu",
                "Xin Li",
                "Sheng Jin",
                "Minghao Liu",
                "Zhoufutu Wen",
                "Tianyu Zheng",
                "Xeron Du",
                "Qiguang Chen",
                "Jiajun Shi",
                "Wangchunshu Zhou",
                "Jiazhan Feng",
                "Wanjun Zhong",
                "Libo Qin",
                "Stephen Huang",
                "Wanxiang Che",
                "Chenghua Lin",
                "Eli Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.14763.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#multilingual",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "ĞšÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ AI Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ COIG-Writer Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¸ÑÑŒĞ¼Ñƒ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1665 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ process supervision (Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°) Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 1:12. ĞšÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ (Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ² 89 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼). Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Creative Writing with COIG-Writer",
                    "desc": "The paper introduces COIG-Writer, a dataset designed to enhance creative writing in Chinese by providing insights into the thought processes behind writing. It emphasizes the importance of process supervision and general-purpose data in improving the performance of large language models, especially in non-English contexts. The dataset includes curated triplets that consist of prompts, reasoning documentation, and final texts, allowing for a deeper understanding of creative writing. Key findings indicate that a balance of creative and general samples is crucial for optimal performance, and that cultural context significantly affects creative capabilities."
                },
                "zh": {
                    "title": "åˆ›æ„å†™ä½œçš„æˆåŠŸæºäºé€»è¾‘ä¸è¯­è¨€çš„ç»“åˆ",
                    "desc": "COIG-Writeræ˜¯ä¸€ä¸ªä¸­æ–‡åˆ›æ„å†™ä½œæ•°æ®é›†ï¼Œå¼ºè°ƒè¿‡ç¨‹ç›‘ç£å’Œé€šç”¨æ•°æ®å¯¹åˆ›æ„å†™ä½œçš„é‡è¦æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ–‡åŒ–èƒŒæ™¯å’Œè¯æ±‡å¤šæ ·æ€§ä¼šå½±å“åˆ›ä½œè¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ã€‚è¯¥æ•°æ®é›†åŒ…å«1665ä¸ªç²¾å¿ƒç­–åˆ’çš„ä¸‰å…ƒç»„ï¼Œè®°å½•äº†åˆ›ä½œè¿‡ç¨‹ä¸­çš„æ¨ç†å’Œæœ€ç»ˆæ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ›æ„å†™ä½œçš„æˆåŠŸä¾èµ–äºå™äº‹é€»è¾‘å’Œè¯­è¨€è¡¨è¾¾çš„ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13217",
            "title": "LLM-guided Hierarchical Retrieval",
            "url": "https://huggingface.co/papers/2510.13217",
            "abstract": "LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
            "score": 11,
            "issue_id": 6467,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "60e89137064e6fba",
            "authors": [
                "Nilesh Gupta",
                "Wei-Cheng Chang",
                "Ngot Bui",
                "Cho-Jui Hsieh",
                "Inderjit S. Dhillon"
            ],
            "affiliations": [
                "Google",
                "UCLA",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13217.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ñ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "LATTICE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ information retrieval, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³Ğ»Ğ¾Ğ¼ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ²Ğ¸Ğ·Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ LLM Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµÑ€ĞµĞ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ LLM Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BRIGHT Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Recall@100 Ğ½Ğ° 9% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ zero-shot baseline."
                },
                "en": {
                    "title": "LATTICE: Navigating Large Document Collections with Semantic Precision",
                    "desc": "LATTICE is a hierarchical retrieval framework designed to improve the efficiency and accuracy of reasoning over large document collections. It organizes documents into a semantic tree structure, allowing for logarithmic search complexity during retrieval. The framework operates in two phases: an offline phase that builds the semantic hierarchy and an online phase where a search LLM navigates this structure. By using a novel traversal algorithm to calibrate relevance scores, LATTICE achieves state-of-the-art performance on reasoning-intensive benchmarks without requiring extensive training."
                },
                "zh": {
                    "title": "LATTICEï¼šé«˜æ•ˆçš„å±‚æ¬¡æ£€ç´¢æ¡†æ¶",
                    "desc": "LATTICEæ˜¯ä¸€ç§å±‚æ¬¡æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯­ä¹‰æ ‘ç»“æ„å’Œéå†ç®—æ³•æé«˜å¯¹å¤§å‹æ–‡æ¡£é›†åˆçš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¦»çº¿é˜¶æ®µå°†æ–‡æ¡£ç»„ç»‡æˆè¯­ä¹‰å±‚æ¬¡ï¼Œåœ¨çº¿é˜¶æ®µåˆ™é€šè¿‡æœç´¢LLMåœ¨æ ‘ç»“æ„ä¸­å¯¼èˆªã€‚ä¸ºäº†å…‹æœæ¨¡å‹çš„ç›¸å…³æ€§åˆ¤æ–­å™ªå£°å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼ŒLATTICEæå‡ºäº†ä¸€ç§éå†ç®—æ³•ï¼Œèƒ½å¤Ÿä»å±€éƒ¨LLMè¾“å‡ºä¸­ä¼°è®¡æ ¡å‡†çš„æ½œåœ¨ç›¸å…³æ€§åˆ†æ•°ã€‚è¯¥æ¡†æ¶åœ¨æ¨ç†å¯†é›†å‹çš„BRIGHTåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶-shotæ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14276",
            "title": "Qwen3Guard Technical Report",
            "url": "https://huggingface.co/papers/2510.14276",
            "abstract": "Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
            "score": 10,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "9d825e50f189ea5b",
            "authors": [
                "Haiquan Zhao",
                "Chenhan Yuan",
                "Fei Huang",
                "Xiaomeng Hu",
                "Yichang Zhang",
                "An Yang",
                "Bowen Yu",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin",
                "Baosong Yang",
                "Chen Cheng",
                "Jialong Tang",
                "Jiandong Jiang",
                "Jianwei Zhang",
                "Jijie Xu",
                "Ming Yan",
                "Minmin Sun",
                "Pei Zhang",
                "Pengjun Xie",
                "Qiaoyu Tang",
                "Qin Zhu",
                "Rong Zhang",
                "Shibin Wu",
                "Shuo Zhang",
                "Tao He",
                "Tianyi Tang",
                "Tingyu Xia",
                "Wei Liao",
                "Weizhou Shen",
                "Wenbiao Yin",
                "Wenmeng Zhou",
                "Wenyuan Yu",
                "Xiaobin Wang",
                "Xiaodong Deng",
                "Xiaodong Xu",
                "Xinyu Zhang",
                "Yang Liu",
                "Yeqiu Li",
                "Yi Zhang",
                "Yong Jiang",
                "Yu Wan",
                "Yuxin Zhou"
            ],
            "affiliations": [
                "Anthropic",
                "Meta-AI",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14276.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#alignment",
                    "#training",
                    "#ethics",
                    "#multilingual",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° LLM Ñ Ñ‚Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Qwen3Guard â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ² Ğ´Ğ²ÑƒÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ…. Generative Qwen3Guard ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ (Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹, ÑĞ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¹, Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Stream Qwen3Guard Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ 119 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€Ñ‘Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… (0.6B, 4B, 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ…, ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Multilingual Safety for Language Models: Real-Time Monitoring and Fine-Grained Judgments",
                    "desc": "Qwen3Guard presents advanced safety guardrail models designed for large language models (LLMs) that enhance output safety through multilingual support. It addresses the limitations of existing models by providing fine-grained tri-class judgments (safe, controversial, unsafe) instead of just binary classifications, allowing for better alignment with diverse safety policies. Additionally, it introduces real-time token-level safety monitoring, enabling timely interventions during text generation, which is crucial for preventing harmful outputs. The models are scalable, available in multiple sizes, and support a wide range of languages, ensuring effective safety moderation in global applications."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€å®‰å…¨é˜²æŠ¤ï¼Œå®æ—¶ç›‘æ§è¾“å‡ºå®‰å…¨",
                    "desc": "Qwen3Guard æ˜¯ä¸€ç§å¤šè¯­è¨€å®‰å…¨é˜²æŠ¤æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œç»†ç²’åº¦çš„ä¸‰ç±»åˆ¤æ–­å’Œå®æ—¶çš„ä»¤ç‰Œçº§å®‰å…¨ç›‘æ§ã€‚å®ƒè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æä¾›æ›´ç»†è‡´çš„å®‰å…¨åˆ†ç±»ï¼ˆå®‰å…¨ã€äº‰è®®ã€ä¸å®‰å…¨ï¼‰ï¼Œè€Œä¸æ˜¯ç®€å•çš„äºŒå…ƒæ ‡ç­¾ï¼›äºŒæ˜¯æ”¯æŒåœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶å®‰å…¨æ£€æŸ¥ï¼Œé¿å…äº†æœ‰å®³è¾“å‡ºçš„é£é™©ã€‚è¯¥æ¨¡å‹æ”¯æŒ119ç§è¯­è¨€å’Œæ–¹è¨€ï¼Œé€‚ç”¨äºå…¨çƒå¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨ç®¡ç†ã€‚ç»è¿‡è¯„ä¼°ï¼ŒQwen3Guard åœ¨å®‰å…¨åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14980",
            "title": "Agentic Design of Compositional Machines",
            "url": "https://huggingface.co/papers/2510.14980",
            "abstract": "State-of-the-art LLMs are benchmarked in a machine design testbed, BesiegeField, highlighting the need for reinforcement learning to improve spatial reasoning, strategic assembly, and instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
            "score": 9,
            "issue_id": 6481,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "a0f11b00ed15f95e",
            "authors": [
                "Wenqian Zhang",
                "Weiyang Liu",
                "Zhen Liu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The Chinese University of Hong Kong (Shenzhen)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14980.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#dataset",
                    "#games"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ LLM ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ ĞºĞ°Ğº Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ BesiegeField Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ³Ñ€Ñ‹ Besiege Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¸Ğ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ workflow Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ¼ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑĞ±Ğ¾Ñ€ĞºĞµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ open-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ RL-Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Empowering LLMs for Machine Design with Reinforcement Learning",
                    "desc": "This paper explores the capabilities of large language models (LLMs) in the context of machine design using a new testbed called BesiegeField. The study emphasizes the importance of reinforcement learning (RL) to enhance LLMs' abilities in spatial reasoning, strategic assembly, and following instructions. By benchmarking these models in a simulated environment, the authors identify the limitations of current open-source LLMs in performing complex design tasks. The paper also discusses the creation of a cold-start dataset and RL finetuning experiments to address these challenges and improve LLM performance in machine design."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›æœºå™¨è®¾è®¡çš„æœªæ¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æœºå™¨è®¾è®¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´æ¨ç†ã€æˆ˜ç•¥ç»„è£…å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›æ–¹é¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºBesiegeFieldçš„æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°åŸºäºæœºå™¨æ„å»ºæ¸¸æˆBesiegeï¼Œæ”¯æŒåŸºäºéƒ¨ä»¶çš„æ„å»ºå’Œç‰©ç†æ¨¡æ‹Ÿã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›çš„LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨æœºå™¨è®¾è®¡ä»»åŠ¡ä¸­å­˜åœ¨ä¸è¶³ä¹‹å¤„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæ”¹è¿›çš„é€”å¾„ï¼Œå¹¶è¿›è¡Œäº†ç›¸å…³å®éªŒä»¥åº”å¯¹è¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†ä¹‹é—´çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14616",
            "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
            "url": "https://huggingface.co/papers/2510.14616",
            "abstract": "Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
            "score": 9,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "184cb21903d17746",
            "authors": [
                "Shuangshuang Ying",
                "Yunwen Li",
                "Xingwei Qu",
                "Xin Li",
                "Sheng Jin",
                "Minghao Liu",
                "Zhoufutu Wen",
                "Xeron Du",
                "Tianyu Zheng",
                "Yichi Zhang",
                "Letian Ni",
                "Yuyang Cheng",
                "Qiguang Chen",
                "Jingzhe Ding",
                "Shengda Long",
                "Wangchunshu Zhou",
                "Jiazhan Feng",
                "Wanjun Zhong",
                "Libo Qin",
                "Ge Zhang",
                "Wenhao Huang",
                "Wanxiang Che",
                "Chenghua Lin"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14616.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#reasoning",
                    "#low_resource",
                    "#dataset",
                    "#story_generation"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ WritingPreferenceBench â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 1800 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ³Ğ´Ğµ ÑƒĞ±Ñ€Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ reward models Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ 52.7%, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ 81.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 8B Ğ´Ğ¾ 27B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ° Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¶Ğ°Ğ½Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ğ» Ğ¾Ñ‚ 18% Ğ´Ğ¾ 82% Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ RLHF ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ€Ğ¾Ğ´Ğµ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·Ğ¾Ğ½Ğ°Ğ½ÑĞ° â€” Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€ÑĞ¼Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Creativity: Reasoning Chains Enhance Preference Learning",
                    "desc": "This paper discusses the limitations of current preference learning methods in evaluating creative writing, particularly when objective quality signals are absent. It introduces a new dataset, WritingPreferenceBench, which includes 1,800 human-annotated preference pairs across various genres. The study finds that traditional sequence-based reward models and zero-shot language models perform poorly, achieving only around 52-54% accuracy. In contrast, generative reward models that utilize explicit reasoning chains significantly outperform these methods, achieving 81.8% accuracy, highlighting the importance of intermediate reasoning in assessing subjective quality in creative writing."
                },
                "zh": {
                    "title": "ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼šåˆ›æ„å†™ä½œçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨åˆ›æ„å†™ä½œåå¥½å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶ä¼˜äºåŸºäºåºåˆ—çš„å¥–åŠ±æ¨¡å‹å’Œé›¶-shotè¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„åå¥½å­¦ä¹ æ–¹æ³•åœ¨å»é™¤å®¢è§‚è´¨é‡ä¿¡å·åï¼Œå‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†WritingPreferenceBenchæ•°æ®é›†ï¼ŒåŒ…å«1800å¯¹äººç±»æ ‡æ³¨çš„åå¥½å¯¹ï¼Œæ¶µç›–8ç§åˆ›æ„å†™ä½œç±»å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆå¥–åŠ±æ¨¡å‹é€šè¿‡æ˜ç¡®çš„æ¨ç†é“¾å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¼ºè°ƒäº†ä¸­é—´æ¨ç†åœ¨æ•æ‰ä¸»è§‚è´¨é‡ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14240",
            "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in\n  the Wild",
            "url": "https://huggingface.co/papers/2510.14240",
            "abstract": "LiveResearchBench and DeepEval provide a comprehensive framework for evaluating deep research systems across various domains, focusing on real-time web search, synthesis, and citation-grounded long-form reports.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.",
            "score": 9,
            "issue_id": 6484,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "3bd7c61a80eb7624",
            "authors": [
                "Jiayu Wang",
                "Yifei Ming",
                "Riya Dulepet",
                "Qinglin Chen",
                "Austin Xu",
                "Zixuan Ke",
                "Frederic Sala",
                "Aws Albarghouthi",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "Stanford University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14240.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#survey",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LiveResearchBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 100 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ¾Ñ‚ĞµĞ½ Ğ²ĞµĞ±-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ñ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ DeepEval â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 17 Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ single-agent Ğ¸ multi-agent Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Evaluating Deep Research Systems with LiveResearchBench and DeepEval",
                    "desc": "The paper introduces LiveResearchBench and DeepEval, frameworks designed to evaluate deep research systems that generate comprehensive reports by synthesizing information from live web sources. It emphasizes the need for user-centric, dynamic, unambiguous, and multi-faceted tasks to assess these systems effectively. LiveResearchBench consists of 100 expert-curated tasks that require extensive real-time web search, while DeepEval provides a suite of evaluation protocols focusing on content quality and report accuracy. The study evaluates 17 advanced deep research systems, identifying their strengths and weaknesses to improve future research capabilities."
                },
                "zh": {
                    "title": "æ·±åº¦ç ”ç©¶ç³»ç»Ÿè¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†LiveResearchBenchå’ŒDeepEvalè¿™ä¸¤ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ç³»ç»Ÿåœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶ç½‘ç»œæœç´¢å’Œä¿¡æ¯ç»¼åˆæ–¹é¢ã€‚æ·±åº¦ç ”ç©¶æ˜¯æŒ‡é€šè¿‡æœç´¢å’Œç»¼åˆæ¥è‡ªæ•°ç™¾ä¸ªå®æ—¶ç½‘ç»œæºçš„ä¿¡æ¯ï¼Œç”Ÿæˆå…¨é¢ä¸”æœ‰å¼•ç”¨ä¾æ®çš„æŠ¥å‘Šã€‚ä¸ºäº†æœ‰æ•ˆè¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œæ–‡ç« æå‡ºäº†å››ä¸ªåŸåˆ™ï¼šä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒã€åŠ¨æ€æ›´æ–°ã€æ˜ç¡®æ— æ­§ä¹‰ä»¥åŠå¤šæ–¹é¢çš„æœç´¢éœ€æ±‚ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¡†æ¶ï¼Œç ”ç©¶è€…èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„æ€§èƒ½ï¼Œè¯†åˆ«å…¶ä¼˜ç¼ºç‚¹ï¼Œå¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14807",
            "title": "SimKO: Simple Pass@K Policy Optimization",
            "url": "https://huggingface.co/papers/2510.14807",
            "abstract": "Simple Pass@K Optimization (SimKO) addresses over-concentration in reinforcement learning with verifiable rewards (RLVR) by asymmetrically adjusting token probabilities, enhancing exploration and pass@K performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.",
            "score": 8,
            "issue_id": 6480,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "31835f5c7d68f61d",
            "authors": [
                "Ruotian Peng",
                "Yi Ren",
                "Zhouliang Yu",
                "Weiyang Liu",
                "Yandong Wen"
            ],
            "affiliations": [
                "CUHK",
                "University of British Columbia",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14807.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² RLVR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SimKO",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Simple Pass@K Optimization (SimKO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ pass@K. SimKO ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ¿-K ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ¿-1 Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ pass@K Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Boosting Exploration in RLVR with SimKO",
                    "desc": "The paper introduces Simple Pass@K Optimization (SimKO) to tackle the issue of over-concentration in reinforcement learning with verifiable rewards (RLVR). It identifies that existing RLVR methods tend to favor exploitation, leading to improved pass@1 but poorer pass@K performance for K greater than 1. The authors analyze token-level probability distributions and find that the top-1 candidate accumulates too much probability, which negatively impacts exploration. SimKO addresses this by asymmetrically adjusting probabilities, enhancing exploration and improving pass@K outcomes across various benchmarks."
                },
                "zh": {
                    "title": "ç®€å•Pass@Kä¼˜åŒ–ï¼šæå‡æ¢ç´¢èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "ç®€å•çš„Pass@Kä¼˜åŒ–ï¼ˆSimKOï¼‰æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ‰€å¸¦æ¥çš„è¿‡åº¦é›†ä¸­é—®é¢˜ã€‚ç°æœ‰çš„RLVRæ–¹æ³•å¾€å¾€åå‘äºåˆ©ç”¨è€Œéæ¢ç´¢ï¼Œå¯¼è‡´åœ¨pass@1è¡¨ç°è‰¯å¥½ä½†åœ¨pass@Kï¼ˆK>1ï¼‰è¡¨ç°ä¸ä½³ã€‚é€šè¿‡åˆ†æè®­ç»ƒåŠ¨æ€ï¼Œæˆ‘ä»¬å‘ç°æ¦‚ç‡é›†ä¸­æ•ˆåº”ä½¿å¾—æ’åç¬¬ä¸€çš„å€™é€‰é¡¹æ¦‚ç‡ä¸æ–­å¢åŠ ï¼ŒæŠ‘åˆ¶äº†å…¶ä»–å€™é€‰é¡¹çš„æ¦‚ç‡ã€‚SimKOé€šè¿‡ä¸å¯¹ç§°è°ƒæ•´å€™é€‰é¡¹çš„æ¦‚ç‡ï¼Œå¢å¼ºäº†æ¢ç´¢èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†pass@Kçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14300",
            "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
            "url": "https://huggingface.co/papers/2510.14300",
            "abstract": "AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
            "score": 8,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "7c55fed8dcc26cc0",
            "authors": [
                "Weijie Shen",
                "Yitian Liu",
                "Yuhao Wu",
                "Zhixuan Liang",
                "Sijia Gu",
                "Dehui Wang",
                "Tian Nian",
                "Lei Xu",
                "Yusen Qin",
                "Jiangmiao Pang",
                "Xinping Guan",
                "Xiaokang Yang",
                "Yao Mu"
            ],
            "affiliations": [
                "D-Robotics",
                "Key Laboratory of System Control and Information Processing, Ministry of Education of China",
                "MoE key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
                "School of Automation and Intelligent Sensing, Shanghai Jiao Tong University",
                "School of Computer Science, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory",
                "Shanghai Key Laboratory of Integrated Administration Technologies for Information Security",
                "The University of Hong Kong",
                "Tongji University",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14300.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#agi",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° AdaMoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Mixture-of-Experts Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²ĞµÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ğ° Ğ½Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Â«Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²ÑÑ‘Â». ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 1.8% Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO, 9.3% Ğ² RoboTwin Ğ¸ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ 21.5% Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ÑƒĞ»Ñ."
                },
                "en": {
                    "title": "Collaborative Expertise for Enhanced Robotic Manipulation",
                    "desc": "AdaMoE is a Mixture-of-Experts architecture designed to enhance Vision-Language-Action (VLA) models for robotic manipulation. It effectively utilizes pretrained weights from dense VLA models to improve computational efficiency and performance. By implementing a decoupling technique, AdaMoE allows for collaborative expert utilization, where multiple experts can contribute to decision-making without competing against each other. This innovative approach leads to significant performance improvements in real-world robotic tasks, demonstrating its practical effectiveness."
                },
                "zh": {
                    "title": "AdaMoEï¼šæå‡æœºå™¨äººæ“ä½œçš„ä¸“å®¶æ··åˆæ¶æ„",
                    "desc": "AdaMoEæ˜¯ä¸€ç§ä¸“å®¶æ··åˆæ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæƒé‡å’Œæé«˜è®¡ç®—æ•ˆç‡æ¥å¢å¼ºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»å¤´è®­ç»ƒVLAæ¨¡å‹æ‰€éœ€çš„é«˜è®¡ç®—èµ„æºå’Œæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚AdaMoEé€šè¿‡å°†å‰é¦ˆå±‚æ›¿æ¢ä¸ºç¨€ç–æ¿€æ´»çš„ä¸“å®¶å±‚ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„å®¹é‡ä¸è®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaMoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å®é™…åº”ç”¨ä¸­å®ç°äº†21.5%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13054",
            "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
            "url": "https://huggingface.co/papers/2510.13054",
            "abstract": "",
            "score": 8,
            "issue_id": 6468,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "6612b26b86cfae4d",
            "authors": [
                "Ankit Goyal",
                "Hugo Hadfield",
                "Xuning Yang",
                "Valts Blukis",
                "Fabio Ramos"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13054.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ñ‚ĞµĞºÑÑ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ä¸æ•ˆç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾é€‰æ‹©æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œè¿™é¡¹ç ”ç©¶ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12764",
            "title": "AnyUp: Universal Feature Upsampling",
            "url": "https://huggingface.co/papers/2510.12764",
            "abstract": "AnyUp is a feature-agnostic upsampling method that generalizes across different vision features and resolutions without requiring re-training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.",
            "score": 8,
            "issue_id": 6481,
            "pub_date": "2025-10-14",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            },
            "hash": "c797c8b259a33385",
            "authors": [
                "Thomas Wimmer",
                "Prune Truong",
                "Marie-Julie Rakotosaona",
                "Michael Oechsle",
                "Federico Tombari",
                "Bernt Schiele",
                "Jan Eric Lenssen"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google",
                "Max Planck Institute for Informatics",
                "TU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12764.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ¿ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ñ‡ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "AnyUp â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ¿ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ„Ğ¸Ñ‡ĞµĞ¹ (DINO, CLIP Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸) Ğ¸ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ feature extractor, AnyUp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ feature-agnostic, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ downstream Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "AnyUp: Universal Upsampling for Vision Features Without Retraining",
                    "desc": "AnyUp is a novel upsampling method designed to enhance vision features without the need for retraining on specific encoders. Unlike traditional learning-based upsamplers that require separate training for each feature extractor, AnyUp operates in a feature-agnostic manner, allowing it to generalize across various feature types and resolutions. This approach not only improves the quality of upsampled features but also maintains the semantic integrity of the original features. Our experiments demonstrate that AnyUp achieves state-of-the-art performance while being efficient and applicable to a diverse set of downstream tasks."
                },
                "zh": {
                    "title": "AnyUpï¼šæ— ç‰¹å¾ä¾èµ–çš„é«˜æ•ˆä¸Šé‡‡æ ·æ–¹æ³•",
                    "desc": "AnyUpæ˜¯ä¸€ç§ç‰¹å¾æ— å…³çš„ä¸Šé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸åŒçš„è§†è§‰ç‰¹å¾å’Œåˆ†è¾¨ç‡ä¸‹ä½¿ç”¨ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„ç‰¹å¾ä¸Šé‡‡æ ·å™¨ï¼Œå¦‚DINOæˆ–CLIPï¼Œéœ€è¦é’ˆå¯¹æ¯ä¸ªç‰¹å¾æå–å™¨è¿›è¡Œé‡æ–°è®­ç»ƒï¼Œå› æ­¤åœ¨æ¨ç†æ—¶æ— æ³•æ³›åŒ–åˆ°ä¸åŒçš„ç‰¹å¾ç±»å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨ç†æ—¶çš„ç‰¹å¾æ— å…³ä¸Šé‡‡æ ·æ¶æ„ï¼Œä»¥ç¼“è§£è¿™ä¸€é™åˆ¶å¹¶æé«˜ä¸Šé‡‡æ ·è´¨é‡ã€‚åœ¨å®éªŒä¸­ï¼ŒAnyUpåœ¨ä¸Šé‡‡æ ·ç‰¹å¾æ–¹é¢è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒçš„ç‰¹å¾ç±»å‹ï¼Œå¹¶åœ¨é«˜æ•ˆçš„åŒæ—¶ä¿æŒç‰¹å¾è¯­ä¹‰ï¼Œæ˜“äºåº”ç”¨äºå¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14974",
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "url": "https://huggingface.co/papers/2510.14974",
            "abstract": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models (pi-Flow). pi-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard ell_2 flow matching loss. By simply mimicking the teacher's behavior, pi-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256^2, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
            "score": 6,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "8bb31ac0ea75d5e3",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "affiliations": [
                "Adobe Research",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14974.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ pi-Flow â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ flow-based Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ policy (ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ FID 2.85 Ğ½Ğ° ImageNet Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞµÑ‚Ğ¸ Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° FLUX.1 Ğ¸ Qwen-Image."
                },
                "en": {
                    "title": "Dynamic Policies for Enhanced Image Generation",
                    "desc": "This paper introduces policy-based flow models (pi-Flow) that enhance image generation by effectively transferring knowledge from teacher models to student models. By modifying the output layer of the student model to predict a network-free policy, pi-Flow generates dynamic flow velocities that improve both the speed and accuracy of the image generation process. The authors propose a novel imitation distillation method that aligns the student's policy with the teacher's trajectory, thus avoiding the common trade-off between quality and diversity in generated images. The results demonstrate that pi-Flow achieves superior performance on benchmark datasets, outperforming existing methods in both image quality and diversity."
                },
                "zh": {
                    "title": "åŸºäºç­–ç•¥çš„æµæ¨¡å‹ï¼šæå‡å›¾åƒç”Ÿæˆè´¨é‡ä¸å¤šæ ·æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç­–ç•¥çš„æµæ¨¡å‹ï¼ˆpi-Flowï¼‰ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡å°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†æç‚¼åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œpi-Flowèƒ½å¤ŸåŠ¨æ€é¢„æµ‹æµé€Ÿï¼Œä»è€Œæ”¹å–„ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿®æ”¹å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºå±‚ï¼Œä½¿å…¶åœ¨ä¸€ä¸ªæ—¶é—´æ­¥é•¿å†…é¢„æµ‹æ— ç½‘ç»œç­–ç•¥ï¼Œå¹¶åœ¨æœªæ¥çš„å­æ­¥éª¤ä¸­ç”ŸæˆåŠ¨æ€æµé€Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œpi-Flowåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‡ æ­¥ç”Ÿæˆæ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†è´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14955",
            "title": "RealDPO: Real or Not Real, that is the Preference",
            "url": "https://huggingface.co/papers/2510.14955",
            "abstract": "RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
            "score": 6,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "9d501f9fad1593d2",
            "authors": [
                "Guo Cheng",
                "Danni Yang",
                "Ziqi Huang",
                "Jianlou Si",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14955.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ AI-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "RealDPO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ supervised fine-tuning, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Direct Preference Optimization (DPO), ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RealAction-5K Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Motion Realism in Video Generation with RealDPO",
                    "desc": "RealDPO is a new method for improving how video generative models create realistic motions by using real-world data for preference learning. It addresses the challenge of generating smooth and contextually accurate movements, which has been a limitation in current models. By using Direct Preference Optimization (DPO) and a special loss function, RealDPO allows the model to learn from real videos and correct its mistakes iteratively. The introduction of the RealAction-5K dataset provides high-quality examples of human activities, further enhancing the model's ability to produce lifelike motion in videos."
                },
                "zh": {
                    "title": "çœŸå®æ•°æ®é©±åŠ¨çš„è¿åŠ¨åˆæˆä¼˜åŒ–",
                    "desc": "RealDPOæ˜¯ä¸€ç§æ–°é¢–çš„åå¥½å­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„è¿åŠ¨çœŸå®æ„Ÿã€‚å®ƒé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œè¿­ä»£è‡ªæˆ‘ä¿®æ­£çš„æ–¹æ³•ï¼Œè§£å†³äº†ç”Ÿæˆå¤æ‚è¿åŠ¨æ—¶çš„æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒRealDPOä½¿ç”¨å®šåˆ¶çš„æŸå¤±å‡½æ•°ï¼Œæä¾›æ›´æœ‰æ•ˆçš„åé¦ˆï¼Œä»è€Œæé«˜è¿åŠ¨åˆæˆçš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹æ¯”çœŸå®è§†é¢‘å’Œæ¨¡å‹è¾“å‡ºçš„é”™è¯¯ï¼ŒRealDPOèƒ½å¤Ÿé€æ­¥æ”¹è¿›è¿åŠ¨è´¨é‡ï¼Œæ˜¾è‘—æå‡è§†é¢‘è´¨é‡å’Œè¿åŠ¨çœŸå®æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14211",
            "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
            "url": "https://huggingface.co/papers/2510.14211",
            "abstract": "LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
            "score": 6,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "2fca134be90f8d31",
            "authors": [
                "Beomseok Kang",
                "Jiwon Song",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14211.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#training",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº ÑĞ»Ğ¾Ñ‘Ğ²",
                    "desc": "LiteStage â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ½ÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºÑƒ ÑĞ»Ğ¾Ñ‘Ğ², Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 1.70 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 4%, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Speed Up Multi-Stage Reasoning with LiteStage!",
                    "desc": "LiteStage is a framework designed to improve the efficiency of multi-stage reasoning in small language models by optimizing how layers are used. It addresses the challenges of varying sensitivity to layer skipping and the issue of generating unnecessary output tokens. By implementing a two-part approach that includes an offline search for optimal layer budgets and an online method for early exits based on confidence, LiteStage minimizes latency while maintaining accuracy. Experiments demonstrate that it can significantly speed up processing times with only a small decrease in accuracy compared to previous methods."
                },
                "zh": {
                    "title": "LiteStageï¼šæå‡æ¨ç†é€Ÿåº¦çš„æ™ºèƒ½å±‚è·³è¿‡æ¡†æ¶",
                    "desc": "LiteStageæ˜¯ä¸€ä¸ªå…³æ³¨å»¶è¿Ÿçš„å±‚è·³è¿‡æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šé˜¶æ®µæ¨ç†çš„æ•ˆç‡ã€‚å®ƒé€šè¿‡åˆ†é…æœ€ä½³å±‚é¢„ç®—å’ŒæŠ‘åˆ¶å†—ä½™è¾“å‡ºæ ‡è®°ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå°çš„å‡†ç¡®æ€§æŸå¤±ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é˜¶æ®µæ€§ç¦»çº¿æœç´¢å’ŒåŸºäºç½®ä¿¡åº¦çš„åœ¨çº¿ç”Ÿæˆæ—©æœŸé€€å‡ºç­–ç•¥ï¼Œä»¥è§£å†³ç°æœ‰æŠ€æœ¯åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLiteStageåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾1.70å€çš„åŠ é€Ÿï¼Œä¸”å‡†ç¡®æ€§æŸå¤±ä½äº4.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14978",
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "url": "https://huggingface.co/papers/2510.14978",
            "abstract": "A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "f8c41e0883a370dc",
            "authors": [
                "Nupur Kumari",
                "Sheng-Yu Wang",
                "Nanxuan Zhao",
                "Yotam Nitzan",
                "Yuheng Li",
                "Krishna Kumar Singh",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu",
                "Xun Huang"
            ],
            "affiliations": [
                "Adobe",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14978.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#synthetic",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ VLM",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ-Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ). ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ñ ĞµÑ‘ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ distribution matching loss (DMD), Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ‘ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ supervised Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ RL-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Flow-GRPO."
                },
                "en": {
                    "title": "Unpaired Image Editing: A New Era with Diffusion Models and Vision-Language Feedback",
                    "desc": "This paper introduces a novel training method for image editing models that eliminates the need for paired data, which is often difficult to obtain. The approach utilizes unrolled diffusion models and incorporates feedback from vision-language models (VLMs) to optimize the editing process. By evaluating edits based on natural language instructions, the VLM provides direct gradients for training, ensuring that the edits are both accurate and visually coherent. The proposed method achieves performance comparable to traditional supervised models while avoiding the pitfalls of synthetic training pairs, demonstrating its effectiveness through rigorous benchmarking and ablation studies."
                },
                "zh": {
                    "title": "æ— é…å¯¹æ•°æ®çš„å›¾åƒç¼–è¾‘æ–°èŒƒå¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¨¡å‹è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å±•å¼€çš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰-è¯­è¨€åé¦ˆï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹å®ç°ä¸ç›‘ç£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–äºå¤§é‡è¾“å…¥-ç›®æ ‡å¯¹çš„ç›‘ç£å¾®è°ƒï¼Œè¿™åœ¨æ•°æ®æ”¶é›†ä¸Šå­˜åœ¨ç“¶é¢ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥ä¼˜åŒ–å‡ æ­¥æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹çš„åé¦ˆï¼Œæ¶ˆé™¤äº†å¯¹é…å¯¹æ•°æ®çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ²¡æœ‰é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸ä½¿ç”¨å¤§é‡é…å¯¹æ•°æ®è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14969",
            "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
            "url": "https://huggingface.co/papers/2510.14969",
            "abstract": "",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "95b0c7bd0991a16b",
            "authors": [
                "Yiming Wang",
                "Da Yin",
                "Yuedong Cui",
                "Ruichen Zheng",
                "Zhiqian Li",
                "Zongyu Lin",
                "Di Wu",
                "Xueqing Wu",
                "Chenchen Ye",
                "Yu Zhou",
                "Kai-Wei Chang"
            ],
            "affiliations": [
                "Harvard University",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14969.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ° â€” Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ¸ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ³Ğ»Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞšĞ¾Ğ³Ğ´Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14949",
            "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
            "url": "https://huggingface.co/papers/2510.14949",
            "abstract": "A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.  \t\t\t\t\tAI-generated summary \t\t\t\t Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
            "score": 5,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "23527b0261302314",
            "authors": [
                "Yu Zhou",
                "Sohyun An",
                "Haikang Deng",
                "Da Yin",
                "Clark Peng",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Nanyun Peng"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#low_resource",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 4200 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 32-48% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº fine-tuning Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ´Ğ°ÑÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ğ¼ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°Ñ… Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ (+34.4%) Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ñ‘Ğ¼."
                },
                "en": {
                    "title": "Bridging Dialects: Enhancing Generative Models Without Compromise",
                    "desc": "This paper addresses the challenge of improving multimodal generative models' performance when processing dialectal textual input without harming their effectiveness on Standard American English (SAE). The authors introduce a new benchmark that includes a diverse set of prompts from six English dialects, revealing significant performance drops in existing models when dialect words are used. They propose an innovative encoder-based mitigation strategy that enables models to learn dialect features while maintaining SAE performance. Experimental results demonstrate that their approach can enhance dialect performance significantly, achieving parity with SAE without degrading its quality."
                },
                "zh": {
                    "title": "æå‡æ–¹è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œä¿ç•™æ ‡å‡†è‹±è¯­è¡¨ç°",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’ŒåŸºäºç¼–ç å™¨çš„ç¼“è§£ç­–ç•¥ï¼Œä»¥æé«˜å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨æ–¹è¨€æ–‡æœ¬è¾“å…¥ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä¸å½±å“æ ‡å‡†ç¾å¼è‹±è¯­çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ¶µç›–å…­ç§å¸¸è§è‹±è¯­æ–¹è¨€çš„å¤§è§„æ¨¡åŸºå‡†ï¼Œå¹¶æ”¶é›†äº†4200å¤šä¸ªç‹¬ç‰¹çš„æç¤ºè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨ä½¿ç”¨æ–¹è¨€è¯æ±‡æ—¶ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦å¯è¾¾32.26%è‡³48.17%ã€‚æˆ‘ä»¬çš„ç¼–ç å™¨ç­–ç•¥èƒ½å¤Ÿè®©æ¨¡å‹è¯†åˆ«æ–°çš„æ–¹è¨€ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒæ ‡å‡†ç¾å¼è‹±è¯­çš„æ€§èƒ½å‡ ä¹ä¸å—å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13996",
            "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
            "url": "https://huggingface.co/papers/2510.13996",
            "abstract": "The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible.",
            "score": 5,
            "issue_id": 6471,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "83d28be82371955f",
            "authors": [
                "Lukas Gienapp",
                "Christopher SchrÃ¶der",
                "Stefan Schweter",
                "Christopher Akiki",
                "Ferdinand Schlatt",
                "Arden Zimmermann",
                "Phillipe GenÃªt",
                "Martin Potthast"
            ],
            "affiliations": [
                "Friedrich-Schiller-UniversitÃ¤t Jena",
                "German National Library",
                "Independent Researcher",
                "InfAI and ScaDS.AI",
                "Leipzig University and ScaDS.AI",
                "University of Kassel, hessian.AI, and ScaDS.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13996.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "ğŸ‡©ğŸ‡ª",
                "ru": {
                    "title": "ĞĞµĞ¼ĞµÑ†ĞºĞ¸Ğµ Ğ¾Ğ±Ñ‰Ğ¸Ğ½Ñ‹: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ German Commons â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆÑƒÑ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 154,56 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· 41 Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑĞµĞ¼ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²: ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ²ĞµĞ±-Ñ‚ĞµĞºÑÑ‚Ñ‹. Ğ’ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¸ CC-BY-SA 4.0 Ğ¸Ğ»Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ĞµĞ³Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ»Ğ°Ñ German Commons Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Empowering German Language Models with Open Data",
                    "desc": "The German Commons is a comprehensive dataset designed to support the training of German language models by providing openly licensed text. It includes 154.56 billion tokens sourced from 41 different domains, ensuring a diverse range of topics such as legal, scientific, and cultural content. The dataset is meticulously processed to maintain high quality through filtering and deduplication, making it suitable for machine learning applications. By offering legally compliant data, the German Commons fills a significant gap in the availability of German language resources for AI development."
                },
                "zh": {
                    "title": "å¾·å›½å…¬å…±æ•°æ®é›†ï¼šå¼€æ”¾å¾·è¯­æ¨¡å‹çš„å…³é”®",
                    "desc": "å¾·å›½å…¬å…±æ•°æ®é›†æä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¼€æ”¾è®¸å¯çš„å¾·è¯­æ–‡æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å¾·è¯­è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†æ±‡é›†äº†æ¥è‡ª41ä¸ªæ¥æºçš„æ–‡æœ¬ï¼Œæ¶µç›–æ³•å¾‹ã€ç§‘å­¦ã€æ–‡åŒ–ã€æ”¿æ²»ã€æ–°é—»ã€ç»æµå’Œç½‘ç»œç­‰ä¸ƒä¸ªé¢†åŸŸï¼Œå…±è®¡1545.6äº¿ä¸ªé«˜è´¨é‡æ ‡è®°ã€‚é€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ¥æºå’Œä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤ï¼Œè¯¥æ•°æ®é›†ç¡®ä¿äº†æ–‡æœ¬çš„ä¸€è‡´æ€§å’Œåˆæ³•æ€§ï¼Œé€‚åˆç”¨äºè¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œå†åˆ†å‘ã€‚å¾·å›½å…¬å…±æ•°æ®é›†çš„å‘å¸ƒä¸ºå¼€æ”¾è®¸å¯çš„å¾·è¯­é¢„è®­ç»ƒæ•°æ®å¡«è¡¥äº†é‡è¦ç©ºç™½ï¼Œä¿ƒè¿›äº†çœŸæ­£å¼€æ”¾çš„å¾·è¯­è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13454",
            "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
            "url": "https://huggingface.co/papers/2510.13454",
            "abstract": "VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "b26ad07b318d2b9a",
            "authors": [
                "Hyojun Go",
                "Dominik Narnhofer",
                "Goutam Bhat",
                "Prune Truong",
                "Federico Tombari",
                "Konrad Schindler"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13454.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#alignment",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº 3D Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾: ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½",
                    "desc": "VIST3A â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ latent text-to-video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ model stitching, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ direct reward finetuning, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ text-to-3D Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Gaussian splats Ğ¸ pointmap."
                },
                "en": {
                    "title": "Transforming Text into Stunning 3D Scenes with VIST3A!",
                    "desc": "VIST3A is a novel framework that integrates latent text-to-video models with 3D reconstruction systems to create detailed 3D scenes from textual descriptions. It addresses the challenge of effectively combining these two components while preserving their learned knowledge through a process called model stitching. Additionally, VIST3A ensures that the outputs from the text-to-video generator are compatible with the 3D decoder using a technique known as direct reward finetuning. The results demonstrate significant improvements over previous text-to-3D methods, allowing for the generation of high-quality 3D representations and pointmaps."
                },
                "zh": {
                    "title": "VIST3Aï¼šæ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯çš„æ–°æ–¹æ³•",
                    "desc": "VIST3Aæ˜¯ä¸€ä¸ªç»“åˆæ½œåœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å’Œ3Dé‡å»ºç³»ç»Ÿçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡å‹æ‹¼æ¥æŠ€æœ¯ï¼Œå°†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå™¨ä¸3Dè§£ç å™¨æœ‰æ•ˆç»“åˆï¼Œä¿ç•™äº†å„è‡ªçš„çŸ¥è¯†ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºå¯ä»¥è§£ç ä¸ºä¸€è‡´çš„3Då‡ ä½•å½¢çŠ¶ï¼ŒVIST3Aè¿˜é‡‡ç”¨äº†ç›´æ¥å¥–åŠ±å¾®è°ƒæŠ€æœ¯è¿›è¡Œå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIST3Aåœ¨ä¸åŒçš„è§†é¢‘ç”Ÿæˆå™¨å’Œ3Dé‡å»ºæ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10472",
            "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the\n  Importance of Exploration Breadth",
            "url": "https://huggingface.co/papers/2510.10472",
            "abstract": "FML-bench evaluates automatic machine learning research agents on diverse fundamental problems using a unified framework with multiple metrics, highlighting the importance of broad exploration strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at https://github.com/qrzou/FML-bench.",
            "score": 5,
            "issue_id": 6482,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "3b1ff47aec462986",
            "authors": [
                "Qiran Zou",
                "Hou Hei Lam",
                "Wenhao Zhao",
                "Yiming Tang",
                "Tingting Chen",
                "Samson Yu",
                "Tianyi Zhang",
                "Chang Liu",
                "Xiangyang Ji",
                "Dianbo Liu"
            ],
            "affiliations": [
                "National University of Singapore",
                "Tsinghua University",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10472.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#survey",
                    "#optimization",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¨Ğ¸Ñ€Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ FML-bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ¸Ğ´ĞµĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ML Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑƒĞ·ĞºĞ¾Ğ¹, Ğ½Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "FML-bench: Broad Exploration for Better ML Research",
                    "desc": "FML-bench is a new benchmark designed to evaluate automatic machine learning research agents on a variety of fundamental problems. It aims to address the limitations of existing benchmarks by focusing on diverse tasks and reducing the coding burden for researchers. The framework includes multiple metrics to assess the performance of these agents comprehensively. The findings indicate that agents using broad exploration strategies achieve better research outcomes compared to those that concentrate on narrow, incremental improvements."
                },
                "zh": {
                    "title": "å¹¿æ³›æ¢ç´¢ï¼Œæå‡è‡ªåŠ¨æœºå™¨å­¦ä¹ ç ”ç©¶çš„æ•ˆæœ",
                    "desc": "FML-benchæ˜¯ä¸€ä¸ªè¯„ä¼°è‡ªåŠ¨æœºå™¨å­¦ä¹ ç ”ç©¶ä»£ç†çš„åŸºå‡†ï¼Œæ¶µç›–äº†å…«ä¸ªå¤šæ ·åŒ–çš„åŸºç¡€æœºå™¨å­¦ä¹ ç ”ç©¶é—®é¢˜ã€‚å®ƒæ—¨åœ¨å‡å°‘ç¼–ç è´Ÿæ‹…ï¼Œå¼ºè°ƒåŸºç¡€é—®é¢˜è€Œéç‰¹å®šåº”ç”¨æ¡ˆä¾‹ï¼Œå¹¶æä¾›é«˜ä»»åŠ¡å¤šæ ·æ€§ã€‚é€šè¿‡ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶å’Œäº”ä¸ªäº’è¡¥çš„æŒ‡æ ‡ï¼ŒFML-benchå…¨é¢è¯„ä¼°ä»£ç†çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨å¹¿æ³›æ¢ç´¢ç­–ç•¥çš„ä»£ç†åœ¨ç ”ç©¶æˆæœä¸Šä¼˜äºä¸“æ³¨äºç‹­çª„ä½†æ·±å…¥æ¢ç´¢çš„ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14961",
            "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
            "url": "https://huggingface.co/papers/2510.14961",
            "abstract": "A new diffusion forcing sampler accelerates token generation in recurrent-depth language models, offering a 5x speedup without tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
            "score": 4,
            "issue_id": 6478,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "ca9713b9a228bba8",
            "authors": [
                "Jonas Geiping",
                "Xinyu Yang",
                "Guinan Su"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "Electrical and Computer Engineering Carnegie Mellon University",
                "Max-Planck Institute for Intelligent Systems, TÃ¼bingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14961.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ (Ğ³Ğ´Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑÑĞ¼Ğ¿Ğ»ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 3.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ°ĞºĞ¾Ğ¹-Ğ»Ğ¸Ğ±Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Accelerating Token Generation with Diffusion Forcing Sampler",
                    "desc": "This paper introduces a new method called the diffusion forcing sampler, which significantly speeds up the process of generating tokens in recurrent-depth language models. By leveraging the similarities between recurrent-depth and diffusion models, the sampler allows for the generation of new tokens during each forward pass while refining previous tokens in parallel. This approach results in a fivefold increase in speed without requiring any adjustments to the existing model architecture. The findings suggest that recurrent-depth models can be effectively treated as advanced diffusion language models, enhancing their efficiency in language tasks."
                },
                "zh": {
                    "title": "åŠ é€Ÿé€’å½’æ·±åº¦æ¨¡å‹çš„æ‰©æ•£é‡‡æ ·å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£å¼ºåˆ¶é‡‡æ ·å™¨ï¼Œèƒ½å¤ŸåŠ é€Ÿé€’å½’æ·±åº¦è¯­è¨€æ¨¡å‹ä¸­çš„æ ‡è®°ç”Ÿæˆï¼Œé€Ÿåº¦æå‡å¯è¾¾5å€ï¼Œæ— éœ€è°ƒä¼˜ã€‚é€’å½’æ·±åº¦æ¨¡å‹é€šè¿‡é‡å¤å±‚çš„è®¡ç®—èƒ½åŠ›æ¥å¢å¼ºå…¶æ€§èƒ½ï¼Œé€‚ç”¨äºç°ä»£è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚æˆ‘ä»¬æ¢è®¨äº†é€’å½’æ·±åº¦æ¨¡å‹ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†æ–°çš„é‡‡æ ·å™¨ã€‚è¯¥é‡‡æ ·å™¨åœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­è§£ç æ–°æ ‡è®°ï¼ŒåŒæ—¶é€šè¿‡é€’å½’å¹¶è¡Œåœ°è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›æ ‡è®°çš„æ½œåœ¨çŠ¶æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14913",
            "title": "Budget-aware Test-time Scaling via Discriminative Verification",
            "url": "https://huggingface.co/papers/2510.14913",
            "abstract": "A hybrid approach combining discriminative verification with self-consistency outperforms generative verification in test-time scaling for large language models, achieving higher accuracy within a fixed compute budget.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
            "score": 4,
            "issue_id": 6486,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "7b1d93574c553153",
            "authors": [
                "Kyle Montgomery",
                "Sijun Tan",
                "Yuqi Chen",
                "Siyuan Zhuang",
                "Tianjun Zhang",
                "Raluca Ada Popa",
                "Chenguang Wang"
            ],
            "affiliations": [
                "UC Berkeley",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14913.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ test-time scaling. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ self-consistency. ĞŸÑ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° 15.3% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AIME2025 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Accuracy with Budget-Friendly Hybrid Verification",
                    "desc": "This paper presents a hybrid approach that combines discriminative verification with self-consistency to improve the performance of large language models during test-time scaling. Traditional methods often rely on generative verification, which can be computationally expensive and impractical. The authors show that while discriminative verifiers alone may not perform as well, their combination with self-consistency leads to significant improvements in accuracy without exceeding a fixed compute budget. The results indicate that this hybrid method can outperform generative verification by up to 15.3% on the AIME2025 benchmark, making it a more efficient choice for real-world applications."
                },
                "zh": {
                    "title": "æ··åˆéªŒè¯ï¼šé«˜æ•ˆæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°†åˆ¤åˆ«éªŒè¯ä¸è‡ªä¸€è‡´æ€§ç›¸ç»“åˆï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„ç”ŸæˆéªŒè¯æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°åˆ¤åˆ«éªŒè¯åœ¨å•ç‹¬ä½¿ç”¨æ—¶å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œä½†ä¸è‡ªä¸€è‡´æ€§ç»“åˆåï¼Œå½¢æˆäº†ä¸€ç§é«˜æ•ˆçš„æµ‹è¯•æ—¶æ‰©å±•æœºåˆ¶ã€‚è¯¥æ··åˆæ–¹æ³•åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œå‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„ç”ŸæˆéªŒè¯æ–¹æ³•é«˜å‡º15.3%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13697",
            "title": "On Pretraining for Project-Level Code Completion",
            "url": "https://huggingface.co/papers/2510.13697",
            "abstract": "Extending the context window and adapting to a new rotary positional embedding scaling parameter improve repository-level code completion in OpenCoder, achieving performance comparable to larger models with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t Repository-level pretraining is commonly used to enable large language models for code to leverage codebase-wide context. This enhances their ability to generate accurate and context-aware code completions. In this work, we investigate how different repository-processing strategies affect in-context learning in OpenCoder, a 1.5B-parameter model. We extend its context window from 4,096 to 16,384 tokens by training on additional 1B tokens of curated repository-level data. Despite relying on a smaller dataset than competing models (which often use hundreds of billions of tokens), our model achieves comparable performance on the Long Code Arena benchmark. We find that various repository-processing techniques yield similarly strong results, with the primary gain coming from adapting to a new rotary positional embedding (RoPE) scaling parameter. Finally, we show that a simpler file-level training approach at the original sequence length remains highly effective, opening up repository-level code completion research to settings with more constrained data and compute resources.",
            "score": 4,
            "issue_id": 6476,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "d964acf6c0110803",
            "authors": [
                "Maksim Sapronov",
                "Evgeniy Glukhov"
            ],
            "affiliations": [
                "JetBrains Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13697.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#long_context",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#data",
                    "#architecture"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OpenCoder Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ñ 4,096 Ğ´Ğ¾ 16,384 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ rotary positional embedding (RoPE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Ğ²ÑĞµĞ³Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ñ‚ĞµĞ½ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ¾Ñ‡ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Code Completion with Extended Context and Efficient Training",
                    "desc": "This paper discusses improvements in code completion using the OpenCoder model by extending its context window and adapting a new rotary positional embedding scaling parameter. By increasing the context window from 4,096 to 16,384 tokens and training on an additional 1 billion tokens, the model can leverage more codebase-wide context for better performance. Despite using less data than larger models, OpenCoder achieves results comparable to those models on the Long Code Arena benchmark. The findings suggest that simpler training methods can also be effective, making advanced code completion techniques accessible even with limited resources."
                },
                "zh": {
                    "title": "æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œæå‡ä»£ç è¡¥å…¨æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ‰©å±•ä¸Šä¸‹æ–‡çª—å£å’Œè°ƒæ•´æ—‹è½¬ä½ç½®åµŒå…¥çš„ç¼©æ”¾å‚æ•°æ¥æé«˜OpenCoderçš„ä»£ç è¡¥å…¨æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡çª—å£ä»4096ä¸ªæ ‡è®°æ‰©å±•åˆ°16384ä¸ªæ ‡è®°ï¼Œå¹¶åœ¨é¢å¤–çš„10äº¿ä¸ªæ ‡è®°çš„ç²¾å¿ƒç­–åˆ’çš„ä»£ç åº“æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å°½ç®¡ä½¿ç”¨çš„æ•°æ®é›†æ¯”ç«äº‰æ¨¡å‹å°ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹åœ¨Long Code ArenaåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨ä¸åŒçš„ä»£ç åº“å¤„ç†ç­–ç•¥å¯ä»¥è·å¾—å¼ºå¤§çš„ç»“æœï¼Œå°¤å…¶æ˜¯é€‚åº”æ–°çš„æ—‹è½¬ä½ç½®åµŒå…¥ç¼©æ”¾å‚æ•°å¸¦æ¥äº†ä¸»è¦çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14976",
            "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
            "url": "https://huggingface.co/papers/2510.14976",
            "abstract": "Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
            "score": 3,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "dea3c35c6b10157d",
            "authors": [
                "Shaowei Liu",
                "Chuan Guo",
                "Bing Zhou",
                "Jian Wang"
            ],
            "affiliations": [
                "Snap Inc.",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14976.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ñ‹ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ°",
                    "desc": "Ponimator â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ· Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ğ¸Ğ· motion capture Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° conditional diffusion models: Ğ¾Ğ´Ğ¸Ğ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ prior'Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ·Ğ°Ğ¼ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… mocap Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Transforming Motion Capture into Interactive Animation",
                    "desc": "Ponimator is a framework that utilizes conditional diffusion models to create and synthesize interactive poses based on motion capture data. It focuses on close-contact human interactions, allowing for the generation of dynamic motion sequences and interactive poses. The framework consists of two main components: a pose animator for generating motion sequences and a pose generator for synthesizing poses from various inputs. Through empirical testing, Ponimator shows its ability to effectively transfer interaction knowledge to various animation tasks, enhancing versatility in animation applications."
                },
                "zh": {
                    "title": "Ponimatorï¼šäº’åŠ¨åŠ¨ç”»çš„æ™ºèƒ½ç”Ÿæˆ",
                    "desc": "Ponimator æ˜¯ä¸€ä¸ªåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºä»åŠ¨ä½œæ•æ‰æ•°æ®ç”Ÿæˆå’Œåˆæˆäº’åŠ¨å§¿åŠ¿ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è¿‘è·ç¦»äººé™…äº’åŠ¨å§¿åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¸®åŠ©äººç±»ç›´è§‚æ¨æµ‹äº’åŠ¨çš„åŠ¨æ€ã€‚Ponimator é€šè¿‡ä¸¤ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥å®ç°ï¼šä¸€ä¸ªç”¨äºç”ŸæˆåŠ¨æ€è¿åŠ¨åºåˆ—ï¼Œå¦ä¸€ä¸ªç”¨äºä»å•ä¸€å§¿åŠ¿æˆ–æ–‡æœ¬åˆæˆäº’åŠ¨å§¿åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPonimator åœ¨å¤šç§æ•°æ®é›†å’Œåº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14919",
            "title": "Predicting Task Performance with Context-aware Scaling Laws",
            "url": "https://huggingface.co/papers/2510.14919",
            "abstract": "A framework models downstream performance of large language models as a function of training compute and context, offering insights into efficient design for long-context tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
            "score": 3,
            "issue_id": 6486,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "05f9185d4f9b9de7",
            "authors": [
                "Kyle Montgomery",
                "David Park",
                "Jianhong Tu",
                "Michael Bendersky",
                "Beliz Gunel",
                "Dawn Song",
                "Chenguang Wang"
            ],
            "affiliations": [
                "Databricks",
                "Google DeepMind",
                "UC Berkeley",
                "UC Santa Cruz",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14919.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#reasoning",
                    "#machine_translation",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ—Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ framework Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° downstream Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° upstream Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ cross-entropy loss, Ğ½Ğ¾ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Framework Ğ±Ñ‹Ğ» Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… Llama-2-7B Ğ¸ Llama-2-13B Ğ½Ğ° 65,500 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ long-context LLM."
                },
                "en": {
                    "title": "Optimizing Large Language Models: Context and Compute Unleashed!",
                    "desc": "This paper introduces a new framework that helps understand how the performance of large language models (LLMs) on specific tasks depends on the amount of training compute and the context provided during training. Traditional scaling laws focus on metrics like loss but do not adequately explain how well models perform on real-world tasks, especially when context length varies. The authors validate their framework using data from Llama-2 models across various tasks, showing that it can predict performance accurately as context increases. This research provides insights that can help design more efficient LLMs for tasks requiring long contexts."
                },
                "zh": {
                    "title": "é«˜æ•ˆè®¾è®¡é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè€ƒè™‘äº†è®­ç»ƒè®¡ç®—èµ„æºå’Œä¸Šä¸‹æ–‡çš„å½±å“ã€‚ä¼ ç»Ÿçš„ç¼©æ”¾æ³•åˆ™æ— æ³•æœ‰æ•ˆæ•æ‰ä¸Šä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å®è¯éªŒè¯ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹çš„æ¨¡å‹è¡¨ç°ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå¯¹Llama-2-7Bå’ŒLlama-2-13Bè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨è®­ç»ƒè®¡ç®—èµ„æºçš„å˜åŒ–ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤ç ”ç©¶ä¸ºè®¾è®¡é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14252",
            "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
            "url": "https://huggingface.co/papers/2510.14252",
            "abstract": "The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
            "score": 2,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "b6535f904cb7a0c8",
            "authors": [
                "Jihao Zhao",
                "Zhiyuan Ji",
                "Simin Niu",
                "Hanyu Wang",
                "Feiyu Xiong",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "School of Information, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14252.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#multimodal",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MoM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ \"Ğ²Ğ¾ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ\" Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (SLM) Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ retrieval Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Transforming Text Processing: From Passive Chunking to Proactive Understanding",
                    "desc": "The MoM framework improves the Retrieval-Augmented Generation (RAG) approach by shifting from passive text chunking to an active understanding of documents. This new method allows large language models (LLMs) to create structured document memories, enhancing their reasoning and knowledge retention. Additionally, small language models (SLMs) are trained to develop reading skills similar to humans through proactive exploration of content. The framework employs advanced techniques like multi-path sampling and reverse reasoning to optimize document memory retrieval and improve text processing capabilities."
                },
                "zh": {
                    "title": "ä¸»åŠ¨ç†è§£ï¼Œæ„å»ºæ–‡æ¡£è®°å¿†çš„æœªæ¥",
                    "desc": "MoMæ¡†æ¶é€šè¿‡å°†æ–‡æœ¬å¤„ç†ä»è¢«åŠ¨åˆ†å—è½¬å˜ä¸ºä¸»åŠ¨ç†è§£ï¼Œå¢å¼ºäº†RAGçš„èƒ½åŠ›ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–çš„æ–‡æ¡£è®°å¿†ï¼Œå¹¶ä½¿å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å‘å±•å‡ºç±»äººé˜…è¯»èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„RAGèŒƒå¼åœ¨ç†è§£ç›¸å…³æ–‡æœ¬å—æ—¶å­˜åœ¨å±€é™ï¼Œé™åˆ¶äº†çŸ¥è¯†å†…åŒ–çš„æ·±åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å®šä¹‰äº†æ–‡æ¡£è®°å¿†æå–çš„è¿‡ç¨‹ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»åœ¨é˜…è¯»æ—¶çš„è®¤çŸ¥è¿‡ç¨‹ã€‚é€šè¿‡å¤šè·¯å¾„é‡‡æ ·å’Œå¤šè§’åº¦è¯„ä¼°æœºåˆ¶ï¼ŒMoMæ¡†æ¶æœ‰æ•ˆå¤„ç†å¤šé¢†åŸŸæ–‡æ¡£ï¼Œå¸®åŠ©SLMsä¸»åŠ¨æ¢ç´¢å’Œæ„å»ºæ–‡æ¡£è®°å¿†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13913",
            "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty\n  Enhancement Mechanisms",
            "url": "https://huggingface.co/papers/2510.13913",
            "abstract": "A two-pronged data synthesis pipeline generates complex question-answer pairs, enabling the training of more effective web-based research agents with higher diversity in tool use.  \t\t\t\t\tAI-generated summary \t\t\t\t Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.",
            "score": 2,
            "issue_id": 6482,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "19ea6eddc0efed68",
            "authors": [
                "Shrey Pandit",
                "Xuan-Phi Nguyen",
                "Yifei Ming",
                "Austin Xu",
                "Jiayu Wang",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13913.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#long_context",
                    "#synthetic",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ²ĞµĞ±-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ¾ Ñ‚ĞµÑ… Ğ¿Ğ¾Ñ€, Ğ¿Ğ¾ĞºĞ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğµ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ°Ñ‚ÑŒÑÑ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ°Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Web Agents with Complex Question-Answer Synthesis",
                    "desc": "This paper presents a novel two-pronged data synthesis pipeline designed to create complex question-answer pairs for training web-based research agents. The approach focuses on progressively increasing the complexity of tasks until a baseline agent fails, ensuring that the generated data captures the necessary intricacies for long-horizon reasoning. By validating factuality and exploring alternative answers, the pipeline enhances the quality and diversity of the training data. Experiments demonstrate that agents trained on this synthesized dataset outperform those trained on existing datasets, achieving greater tool-use diversity and improved performance."
                },
                "zh": {
                    "title": "åŒç®¡é½ä¸‹ï¼Œæå‡ç½‘ç»œç ”ç©¶ä»£ç†çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç®¡é½ä¸‹çš„æ•°æ®åˆæˆæµç¨‹ï¼Œç”¨äºç”Ÿæˆå¤æ‚çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œä»¥è®­ç»ƒæ›´æœ‰æ•ˆçš„ç½‘ç»œç ”ç©¶ä»£ç†ã€‚è¯¥æµç¨‹é€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§ï¼Œç›´åˆ°åŸºçº¿ä»£ç†å¤±è´¥ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„æ•°æ®èƒ½å¤Ÿæ•æ‰åˆ°é•¿æ—¶é—´æ¨ç†æ‰€éœ€çš„å¤æ‚æ€§ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶æ•°æ®çš„éš¾åº¦å’Œè´¨é‡ï¼Œé¿å…äº†æ•°æ®å’Œè®­ç»ƒæ•ˆæœæ··æ·†çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ•°æ®é›†è¾ƒå°ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒå‡ºçš„ç½‘ç»œä»£ç†åœ¨å·¥å…·ä½¿ç”¨çš„å¤šæ ·æ€§å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06694",
            "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
            "url": "https://huggingface.co/papers/2510.06694",
            "abstract": "SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
            "score": 2,
            "issue_id": 6472,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "d1a64ad898f0ebc3",
            "authors": [
                "Jipeng Lyu",
                "Jiahua Dong",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06694.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°ÑĞºĞ°Ğ´Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "SCas4D â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Gaussian Splatting. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ²Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 100 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ°Ğ´Ñ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Dynamic Scene Modeling with SCas4D",
                    "desc": "SCas4D is a novel framework designed for efficiently modeling dynamic scenes using 3D Gaussian Splatting. It focuses on capturing hierarchical deformation patterns, which allows for faster convergence and high-quality outputs. By refining deformations from a coarse to a fine level, SCas4D can achieve results comparable to existing methods while significantly reducing training iterations. This approach is particularly effective in tasks such as self-supervised articulated object segmentation and novel view synthesis."
                },
                "zh": {
                    "title": "é«˜æ•ˆåŠ¨æ€åœºæ™¯å»ºæ¨¡çš„å±‚æ¬¡åŒ–ä¼˜åŒ–æ¡†æ¶",
                    "desc": "SCas4Dæ˜¯ä¸€ç§çº§è”ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨3Dé«˜æ–¯ç‚¹äº‘æœ‰æ•ˆå»ºæ¨¡åŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–å˜å½¢æ¨¡å¼ï¼Œå¿«é€Ÿæ”¶æ•›å¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°é«˜è´¨é‡ç»“æœã€‚SCas4Dçš„å…³é”®åœ¨äºçœŸå®ä¸–ç•Œçš„å˜å½¢é€šå¸¸å‘ˆç°å±‚æ¬¡åŒ–æ¨¡å¼ï¼Œå¤šä¸ªé«˜æ–¯å…±äº«ç›¸ä¼¼çš„å˜æ¢ã€‚é€šè¿‡ä»ç²—åˆ°ç»†é€æ­¥ä¼˜åŒ–å˜å½¢ï¼ŒSCas4Dåœ¨æ¯ä¸ªæ—¶é—´å¸§å†…ä»…éœ€100æ¬¡è¿­ä»£å³å¯æ”¶æ•›ï¼Œä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°ä»…ä¸ºç°æœ‰æ–¹æ³•çš„äº”åˆ†ä¹‹ä¸€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14942",
            "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
            "url": "https://huggingface.co/papers/2510.14942",
            "abstract": "GroundedPRM uses Monte Carlo Tree Search and external validation to improve multi-step reasoning in LLMs with fewer, higher-quality annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
            "score": 1,
            "issue_id": 6481,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "49ee4eff0ffba5f0",
            "authors": [
                "Yao Zhang",
                "Yu Wu",
                "Haowei Zhang",
                "Weiguo Li",
                "Haokun Chen",
                "Jingpei Wu",
                "Guohao Li",
                "Zhen Han",
                "Volker Tresp"
            ],
            "affiliations": [
                "Fudan University",
                "LMU Munich",
                "Munich Center for Machine Learning",
                "Technical University of Munich",
                "University Heidelberg",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14942.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GroundedPRM",
                    "desc": "GroundedPRM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ GroundedPRM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ¼Ğ°Ğ»ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Step Reasoning with GroundedPRM",
                    "desc": "GroundedPRM is a novel framework designed to enhance multi-step reasoning in Large Language Models (LLMs) by utilizing Monte Carlo Tree Search (MCTS) and external validation methods. It addresses the challenges of noisy rewards and misalignment in existing Process Reward Models (PRMs) by constructing structured reasoning paths and validating intermediate steps with external tools. This approach allows for fine-grained credit assignment and reduces hallucination in supervision, leading to higher factual fidelity. GroundedPRM demonstrates significant performance improvements with fewer annotations, making it a scalable solution for effective process-level reasoning in LLMs."
                },
                "zh": {
                    "title": "GroundedPRMï¼šé«˜æ•ˆçš„å¤šæ­¥æ¨ç†æ¡†æ¶",
                    "desc": "GroundedPRMæ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œå¤–éƒ¨éªŒè¯æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç»“æ„åŒ–çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å¥–åŠ±å™ªå£°ï¼Œå¹¶å®ç°ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ã€‚å®ƒè¿˜é€šè¿‡å¤–éƒ¨å·¥å…·éªŒè¯æ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œæ¶ˆé™¤è™šå‡ç›‘ç£ï¼Œä»è€Œæä¾›åŸºäºæ‰§è¡Œçš„æ­£ç¡®æ€§ä¿¡å·ã€‚æœ€ç»ˆï¼ŒGroundedPRMåœ¨ä»…ä½¿ç”¨40Kè‡ªåŠ¨æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ï¼Œå±•ç¤ºäº†é«˜æ•ˆä¸”å¯éªŒè¯çš„è¿‡ç¨‹çº§æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14351",
            "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
            "url": "https://huggingface.co/papers/2510.14351",
            "abstract": "Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
            "score": 1,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "cc92d03746a5b325",
            "authors": [
                "Perapard Ngokpol",
                "Kun Kerdthaisong",
                "Pasin Buakhaw",
                "Pitikorn Khlaisamniang",
                "Supasate Vorathammathorn",
                "Piyalitt Ittichaiwong",
                "Nutchanon Yongsatianchot"
            ],
            "affiliations": [
                "Artificial Intelligence Association of Thailand",
                "Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University",
                "School of Biomedical Engineering & Imaging Sciences, Kings College London",
                "Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University",
                "Thammasat School of Engineering, Thammasat University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14351.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#multimodal",
                    "#alignment",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "ğŸ¦¸",
                "ru": {
                    "title": "Ğ¡ÑƒĞ¿ĞµÑ€Ğ³ĞµÑ€Ğ¾Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²ÑĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ…: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° LLM Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Beyond One World Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ¿ĞµÑ€Ğ³ĞµÑ€Ğ¾ĞµĞ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹ (ĞºĞ¾Ğ¼Ğ¸ĞºÑÑ‹, Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹). Benchmark Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 30 Ğ³ĞµÑ€Ğ¾ĞµĞ² Ğ² 90 Ğ²ĞµÑ€ÑĞ¸ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: chain-of-thought Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñƒ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñƒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ…; Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°; Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´ĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸ Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¸ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Think-Act Matching Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ»ÑƒĞ¶Ğ° Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Evaluating LLMs in Superhero Roleplay Across Canons",
                    "desc": "The paper introduces the Beyond One World benchmark, which assesses large language models (LLMs) on their ability to accurately portray superheroes from different canons, such as Marvel and DC. It consists of two main tasks: Canon Events, which tests the models' factual recall of significant character events, and Moral Dilemmas, which evaluates their ethical reasoning in complex scenarios. The study highlights the challenges LLMs face in maintaining consistency across various character versions and proposes a new metric called Think-Act Matching to measure the alignment between a model's reasoning and its actions. The findings reveal that while some models can think or act well, achieving proficiency in both remains a significant challenge."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¶…çº§è‹±é›„è§’è‰²æ‰®æ¼”ä¸­çš„ä¸€è‡´æ€§ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸º\"Beyond One World\"çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒèƒŒæ™¯ä¸‹å‡†ç¡®æç»˜ç‰¹å®šç‰ˆæœ¬è¶…çº§è‹±é›„çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–äº†30ä¸ªæ ‡å¿—æ€§è‹±é›„å’Œ90ä¸ªç‰¹å®šç‰ˆæœ¬ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»è¦ä»»åŠ¡ï¼šäº‹å®å›å¿†å’Œé“å¾·å›°å¢ƒã€‚ç ”ç©¶å‘ç°ï¼Œé“¾å¼æ€ç»´æç¤ºå¯ä»¥æé«˜è¾ƒå¼±æ¨¡å‹çš„å™äº‹è¿è´¯æ€§ï¼Œä½†å¯èƒ½é™ä½å¼ºæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ€è€ƒå’Œè¡ŒåŠ¨æ–¹é¢å¾€å¾€è¡¨ç°å‡ºè‰²ï¼Œä½†å¾ˆå°‘åŒæ—¶å…¼é¡¾ä¸¤è€…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14095",
            "title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
            "url": "https://huggingface.co/papers/2510.14095",
            "abstract": "Transformer networks are enhanced with four architectural mechanisms to improve out-of-distribution generalization and algorithmic reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.",
            "score": 1,
            "issue_id": 6483,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "7b973056587ce859",
            "authors": [
                "Awni Altabaa",
                "Siyu Chen",
                "John Lafferty",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "Department of Statistics & Data Science, Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14095.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ§ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Transformer-ÑĞµÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ²Ğ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ GSM8K Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Enhancing Transformers for Better Out-of-Distribution Reasoning",
                    "desc": "This paper focuses on improving the ability of Transformer networks to generalize beyond their training data, particularly in reasoning tasks. It introduces four new architectural mechanisms: input-adaptive recurrence, algorithmic supervision, anchored latent representations, and an error-correction mechanism. These enhancements aim to enable better out-of-distribution (OOD) generalization and improve the reasoning capabilities of language models. The authors also provide an analysis to explain how these mechanisms contribute to the improved performance in OOD scenarios."
                },
                "zh": {
                    "title": "å¢å¼ºå˜æ¢å™¨ç½‘ç»œçš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å››ç§æ¶æ„æœºåˆ¶æ¥å¢å¼ºå˜æ¢å™¨ç½‘ç»œçš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›å’Œç®—æ³•æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨GSM8Ké£æ ¼çš„æ¨¡å—åŒ–ç®—æœ¯ä»»åŠ¡ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œåˆ†æäº†å˜æ¢å™¨ç½‘ç»œåœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–æ–¹é¢çš„è¡¨ç°ã€‚æå‡ºçš„å››ç§æœºåˆ¶åŒ…æ‹¬ï¼šè¾“å…¥è‡ªé€‚åº”é€’å½’ã€ç®—æ³•ç›‘ç£ã€é€šè¿‡ç¦»æ•£ç“¶é¢ˆé”šå®šçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»¥åŠæ˜¾å¼çš„é”™è¯¯ä¿®æ­£æœºåˆ¶ã€‚è¿™äº›æœºåˆ¶å…±åŒæ„æˆäº†ä¸€ç§æ¶æ„æ–¹æ³•ï¼Œä½¿å˜æ¢å™¨ç½‘ç»œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç¨³å¥çš„ç®—æ³•æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13910",
            "title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval\n  Augmented Generation Systems",
            "url": "https://huggingface.co/papers/2510.13910",
            "abstract": "RAGCap-Bench evaluates intermediate tasks in agentic RAG workflows, highlighting the importance of enhancing these capabilities for better end-to-end performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that \"slow-thinking\" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.",
            "score": 1,
            "issue_id": 6473,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "fcc3e047868a25f7",
            "authors": [
                "Jingru Lin",
                "Chen Zhang",
                "Stephen Y. Liu",
                "Haizhou Li"
            ],
            "affiliations": [
                "National University of Singapore, Singapore",
                "The Chinese University of Hong Kong, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#rag",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RAGCap-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. RAG (Retrieval-Augmented Generation) Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ LLM Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ LLM Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ RAGCap Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Agentic RAG Workflows for Superior Performance",
                    "desc": "The paper introduces RAGCap-Bench, a benchmark designed to evaluate the intermediate tasks in agentic Retrieval-Augmented Generation (RAG) workflows. It highlights the need for improving the reasoning capabilities of Large Language Models (LLMs) to enhance their performance on complex queries. By analyzing outputs from advanced systems, the authors identify essential tasks and common errors, creating a taxonomy for better evaluation. The findings suggest that models that take a more deliberate approach to reasoning perform better overall, validating the importance of focusing on intermediate capabilities."
                },
                "zh": {
                    "title": "æå‡ä»£ç† RAG èƒ½åŠ›ï¼Œä¼˜åŒ–æ•´ä½“æ€§èƒ½",
                    "desc": "RAGCap-Bench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä»£ç† RAG å·¥ä½œæµä¸­é—´ä»»åŠ¡çš„åŸºå‡†ï¼Œå¼ºè°ƒæå‡è¿™äº›èƒ½åŠ›å¯¹æ•´ä½“æ€§èƒ½çš„é‡è¦æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åŠ¨æ€æ£€ç´¢å¤–éƒ¨ä¿¡æ¯æ¥ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®é™åˆ¶ï¼Œå¦‚äº‹å®é”™è¯¯å’Œè¿‡æ—¶çŸ¥è¯†ã€‚å°½ç®¡ä»£ç† RAG ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šè·³é—®é¢˜å’Œä¸­é—´æ¨ç†èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰æ›´å¼º RAGCap æ€§èƒ½çš„â€œæ…¢æ€è€ƒâ€æ¨¡å‹åœ¨æ•´ä½“ç»“æœä¸Šè¡¨ç°æ›´ä½³ï¼ŒéªŒè¯äº†åŸºå‡†çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10390",
            "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
            "url": "https://huggingface.co/papers/2510.10390",
            "abstract": "RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting either dangerous overconfidence or overcaution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarks -- RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) -- and our complete generation framework to enable continued, dynamic evaluation of this critical capability.",
            "score": 1,
            "issue_id": 6469,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "0545dfda4fd8c317",
            "authors": [
                "Aashiq Muhamed",
                "Leonardo F. R. Ribeiro",
                "Markus Dreyer",
                "Virginia Smith",
                "Mona T. Diab"
            ],
            "affiliations": [
                "Amazon AGI",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10390.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#hallucinations",
                    "#alignment",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸš«",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Â«Ğ½Ğµ Ğ·Ğ½Ğ°ÑÂ»: Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ¼ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ½Ğ¸Ğ¶Ğµ 50% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½ÑƒÑ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ»Ğ¸Ğ±Ğ¾ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ÑÑ Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ RefusalBench â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 176 ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· alignment, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM."
                },
                "en": {
                    "title": "RefusalBench: Enhancing Selective Refusal in Language Models",
                    "desc": "This paper introduces RefusalBench, a tool designed to assess how well language models in Retrieval-Augmented Generation (RAG) systems can refuse to answer questions based on incorrect context. The study shows that even advanced models often fail to refuse appropriately, with accuracy dropping below 50% in complex tasks. It highlights that traditional benchmarks are inadequate, as models can exploit specific dataset features rather than genuinely understanding refusal. By using a systematic approach with various perturbation strategies, the authors provide insights into the failure patterns and suggest that selective refusal can be improved through targeted training."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„é€‰æ‹©æ€§æ‹’ç»èƒ½åŠ›",
                    "desc": "RefusalBench æ˜¯ä¸€ä¸ªè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ RAG ç³»ç»Ÿä¸­é€‰æ‹©æ€§æ‹’ç»èƒ½åŠ›çš„å·¥å…·ï¼Œä½¿ç”¨ç¨‹åºç”Ÿæˆçš„æµ‹è¯•æ¡ˆä¾‹æ¥æ­ç¤ºç³»ç»Ÿæ€§å¤±è´¥æ¨¡å¼å¹¶æä¾›æ”¹è¿›è·¯å¾„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤šæ–‡æ¡£ä»»åŠ¡ä¸­çš„æ‹’ç»å‡†ç¡®ç‡ä¹Ÿä½äº 50%ï¼Œå¹¶ä¸”è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡æˆ–è¿‡åº¦è°¨æ…çš„å€¾å‘ã€‚é™æ€åŸºå‡†æµ‹è¯•æ— æ³•å¯é è¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œå› ä¸ºæ¨¡å‹ä¼šåˆ©ç”¨æ•°æ®é›†ç‰¹å®šçš„ä¼ªå½±å¹¶è®°å¿†æµ‹è¯•å®ä¾‹ã€‚æˆ‘ä»¬æå‡ºçš„ RefusalBench é‡‡ç”¨ç”Ÿæˆæ€§æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶è¯­è¨€æ‰°åŠ¨ç¨‹åºåŒ–åˆ›å»ºè¯Šæ–­æµ‹è¯•æ¡ˆä¾‹ï¼Œæ­ç¤ºäº†æ‹’ç»èƒ½åŠ›çš„å¯è®­ç»ƒæ€§å’Œå¯¹é½æ•æ„Ÿæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13161",
            "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM\n  Inference",
            "url": "https://huggingface.co/papers/2510.13161",
            "abstract": "Mirror Speculative Decoding accelerates large language model inference by parallelizing speculative execution across heterogeneous accelerators and using multi-token speculative streaming to reduce draft latency without compromising acceptance rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding accelerates LLM inference by using a draft model to look ahead, but gains are capped by the cost of autoregressive draft generation: increasing draft size elevates acceptance rates but introduces additional latency overhead exacerbating the speed-accuracy tradeoff. Prior methods (Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade acceptance or introduce overheads that limit scaling. We present Mirror Speculative Decoding (Mirror-SD), an inference algorithm that breaks the latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from early-exit signals in parallel with the target model's suffix and explicitly maps computation across heterogeneous accelerators (GPU and NPU) to exploit cross-device parallelism. The draft speculates forward continuations for the target to verify, while the target simultaneously speculates correction paths for the draft, converting speculation into two complementary execution pipelines. To further cut draft latency without weakening acceptance semantics, we add speculative streaming so the draft emits multiple tokens per step. This dual strategy of parallel heterogeneous execution plus multi-token speculative streaming pushes speculative decoding toward its ideal regime of high acceptance with low overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving 2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative improvement over the strongest baseline, EAGLE3.",
            "score": 0,
            "issue_id": 6481,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "db6a1a4635cfeda2",
            "authors": [
                "Nikhil Bhendawade",
                "Kumari Nishu",
                "Arnav Kundu",
                "Chris Bartels",
                "Minsik Cho",
                "Irina Belousova"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13161.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸª",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞµ Ğ·ĞµÑ€ĞºĞ°Ğ»Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ",
                    "desc": "Mirror Speculative Decoding - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑÑ… (GPU Ğ¸ NPU), Ğ³Ğ´Ğµ draft-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ multi-token speculative streaming Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ draft-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑˆĞ°Ğ³, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 14B Ğ´Ğ¾ 66B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2.8-5.8 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ½Ğ° 30% Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ baseline EAGLE3."
                },
                "en": {
                    "title": "Accelerating Inference with Mirror-SD: Speed Meets Accuracy!",
                    "desc": "Mirror Speculative Decoding (Mirror-SD) is a novel inference algorithm designed to enhance the performance of large language models (LLMs) by optimizing the speculative decoding process. It achieves this by parallelizing execution across different types of hardware accelerators, such as GPUs and NPUs, and implementing multi-token speculative streaming to minimize latency. This approach allows the draft model to generate multiple tokens at once while maintaining high acceptance rates, effectively breaking the traditional latency-acceptance tradeoff. As a result, Mirror-SD significantly accelerates inference times, achieving impressive speedups on large-scale models without compromising accuracy."
                },
                "zh": {
                    "title": "é•œåƒæ¨æµ‹è§£ç ï¼šåŠ é€Ÿæ¨ç†çš„æ–°çªç ´",
                    "desc": "é•œåƒæ¨æµ‹è§£ç ï¼ˆMirror-SDï¼‰æ˜¯ä¸€ç§åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„æ–°ç®—æ³•ï¼Œé€šè¿‡åœ¨å¼‚æ„åŠ é€Ÿå™¨ä¸Šå¹¶è¡Œæ‰§è¡Œæ¨æµ‹ï¼Œæ˜¾è‘—é™ä½è‰ç¨¿å»¶è¿Ÿè€Œä¸å½±å“æ¥å—ç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ—©æœŸé€€å‡ºä¿¡å·å¹¶è¡Œå¯åŠ¨å®Œæ•´çš„åˆ†æ”¯å›æ»šï¼ŒåŒæ—¶åœ¨ç›®æ ‡æ¨¡å‹çš„åç¼€ä¸Šè¿›è¡Œè®¡ç®—ï¼Œå……åˆ†åˆ©ç”¨è·¨è®¾å¤‡çš„å¹¶è¡Œæ€§ã€‚é•œåƒæ¨æµ‹è§£ç è¿˜å¼•å…¥äº†å¤šæ ‡è®°æ¨æµ‹æµï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘è‰ç¨¿å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒé«˜æ¥å—ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirror-SDåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†2.8åˆ°5.8å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å¼ºåŸºçº¿EAGLE3ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-17.html",
    "link_next": "2025-10-21.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 15,
        "#data": 7,
        "#benchmark": 21,
        "#agents": 10,
        "#cv": 7,
        "#rl": 7,
        "#rlhf": 5,
        "#rag": 4,
        "#plp": 1,
        "#inference": 7,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 10,
        "#healthcare": 0,
        "#training": 23,
        "#robotics": 1,
        "#agi": 4,
        "#games": 3,
        "#interpretability": 6,
        "#reasoning": 18,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 1,
        "#optimization": 28,
        "#survey": 2,
        "#diffusion": 6,
        "#alignment": 7,
        "#story_generation": 2,
        "#hallucinations": 4,
        "#long_context": 7,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 3,
        "#science": 3,
        "#low_resource": 7
    }
}