{
    "date": {
        "ru": "20 –æ–∫—Ç—è–±—Ä—è",
        "en": "October 20",
        "zh": "10Êúà20Êó•"
    },
    "time_utc": "2025-10-20 00:56",
    "weekday": 0,
    "issue_id": 6497,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.04849",
            "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
            "url": "https://huggingface.co/papers/2510.04849",
            "abstract": "PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.",
            "score": 100,
            "issue_id": 6476,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 6",
                "zh": "10Êúà6Êó•"
            },
            "hash": "ae839101b9ffc0a4",
            "authors": [
                "Elisei Rykov",
                "Kseniia Petrushina",
                "Maksim Savkin",
                "Valerii Olisov",
                "Artem Vazhentsev",
                "Kseniia Titova",
                "Alexander Panchenko",
                "Vasily Konovalov",
                "Julia Belikova"
            ],
            "affiliations": [
                "AIRI",
                "MWS AI",
                "Moscow Institute of Physics and Technology",
                "Sber AI Lab",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04849.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#low_resource",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "üçÑ",
                "ru": {
                    "title": "–ü–æ–π–º–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—é: –¥–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ 14 —è–∑—ã–∫–∞—Ö",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PsiloQA ‚Äî –∫—Ä—É–ø–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LLM –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 14 —è–∑—ã–∫–æ–≤. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ —á–µ—Ä–µ–∑ GPT-4o, –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø—É—Ç—ë–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ encoder-–º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Å–µ—Ö —è–∑—ã–∫–∞—Ö. PsiloQA –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π cross-lingual –ø–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –Ω–∞ –¥—Ä—É–≥–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection with PsiloQA: A Multilingual Approach",
                    "desc": "PsiloQA is a new multilingual dataset designed to improve the detection of hallucinations in large language models (LLMs) across 14 different languages. Unlike previous benchmarks that only focus on English and evaluate at the sequence level, PsiloQA provides detailed annotations at the span level, allowing for a more precise assessment of factual accuracy. The dataset is created using an automated pipeline that generates question-answer pairs and identifies hallucinated spans by comparing model outputs to correct answers. Our findings show that encoder-based models perform best in detecting these hallucinations, highlighting the dataset's potential for enhancing multilingual LLM applications."
                },
                "zh": {
                    "title": "PsiloQAÔºöÂ§öËØ≠Ë®ÄÂπªËßâÊ£ÄÊµãÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "PsiloQAÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºå‰∏ìÊ≥®‰∫éÊ£ÄÊµãÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇÂÆÉÊ∂µÁõñ‰∫Ü14ÁßçËØ≠Ë®ÄÔºåÂπ∂ÈÄöËøáËá™Âä®ÂåñÊµÅÁ®ãÁîüÊàê‰∫ÜÂ∏¶ÊúâË∑®Â∫¶Á∫ßÂπªËßâÁöÑÈóÆÁ≠îÂØπ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÊ®°ÂûãÂú®ÂπªËßâÊ£ÄÊµã‰∏≠Ë°®Áé∞ÊúÄ‰Ω≥ÔºåËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åË∑®ËØ≠Ë®ÄÊ≥õÂåñ„ÄÇËØ•Êï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ÊàêÊú¨‰Ωé‰∫é‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÔºåÊé®Âä®‰∫ÜÂ§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÂπªËßâÊ£ÄÊµãÁöÑËøõÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14545",
            "title": "Agentic Entropy-Balanced Policy Optimization",
            "url": "https://huggingface.co/papers/2510.14545",
            "abstract": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
            "score": 90,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "ac2984c140493beb",
            "authors": [
                "Guanting Dong",
                "Licheng Bao",
                "Zhongyuan Wang",
                "Kangzhi Zhao",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Jinghan Yang",
                "Hangyu Mao",
                "Fuzheng Zhang",
                "Kun Gai",
                "Guorui Zhou",
                "Yutao Zhu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14545.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#optimization"
                ],
                "emoji": "‚öñÔ∏è",
                "ru": {
                    "title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "AEPO ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –ê–ª–≥–æ—Ä–∏—Ç–º –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–µ—Ö–∞–Ω–∏–∑–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –≤—ã—Å–æ–∫–æ—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö. AEPO –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–ª–∞–ø—Å –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ —ç–Ω—Ç—Ä–æ–ø–∏–∏, –∏ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏. –ù–∞ 14 –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –º–æ–¥–µ–ª—å Qwen3-14B —Å AEPO –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è 7 mainstream RL –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 1000 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤."
                },
                "en": {
                    "title": "Balancing Entropy for Smarter Web Agents with AEPO",
                    "desc": "The paper introduces AEPO, an innovative agentic reinforcement learning algorithm that tackles issues related to entropy in training web agents. It highlights how traditional methods can lead to training collapse due to over-reliance on entropy signals. AEPO features a dynamic rollout mechanism that balances sampling budgets and a policy optimization technique that preserves important gradients. The results demonstrate AEPO's superior performance across multiple datasets, showcasing its ability to enhance training stability and diversity in web agents."
                },
                "zh": {
                    "title": "Âπ≥Ë°°ÁÜµÔºåÊèêÂçá‰ª£ÁêÜÂ≠¶‰π†ÊÄßËÉΩ",
                    "desc": "AEPOÊòØ‰∏ÄÁßç‰ª£ÁêÜÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÁΩëÁªú‰ª£ÁêÜËÆ≠ÁªÉ‰∏≠ÁöÑÁÜµÁõ∏ÂÖ≥ÊåëÊàò„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøáÂä®ÊÄÅÂπ≥Ë°°ÁÜµÔºåÂú®ÂõûÊªöÂíåÁ≠ñÁï•Êõ¥Êñ∞Èò∂ÊÆµÊèêÈ´òÊÄßËÉΩÂíåÁ®≥ÂÆöÊÄß„ÄÇAEPOÁöÑ‰∏§‰∏™Ê†∏ÂøÉÁªÑ‰ª∂ÂåÖÊã¨Âä®ÊÄÅÁÜµÂπ≥Ë°°ÂõûÊªöÊú∫Âà∂ÂíåÁÜµÂπ≥Ë°°Á≠ñÁï•‰ºòÂåñÔºåËÉΩÂ§üÊúâÊïàÈò≤Ê≠¢ËøáÂ∫¶ÂàÜÊîØÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAEPOÂú®14‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫é7Áßç‰∏ªÊµÅÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14975",
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "url": "https://huggingface.co/papers/2510.14975",
            "abstract": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  \t\t\t\t\tAI-generated summary \t\t\t\t Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
            "score": 72,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "f35e52daa72d6eaa",
            "authors": [
                "Hengyuan Xu",
                "Wei Cheng",
                "Peng Xing",
                "Yixiao Fang",
                "Shuhan Wu",
                "Rui Wang",
                "Xianfang Zeng",
                "Daxin Jiang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14975.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∏—Ü –±–µ–∑ –∫–æ–ø–∏–ø–∞—Å—Ç–∞: –±–∞–ª–∞–Ω—Å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É ¬´–∫–æ–ø–∏–ø–∞—Å—Ç–∞¬ª –≤ text-to-image –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ AI –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –ª–∏—Ü–æ –≤–º–µ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MultiID-2M —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –ª—é–¥–µ–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ contrastive identity loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å WithAnyone –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞—É—á–∏–ª–∞—Å—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø–æ–∑, –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ –æ—Å–≤–µ—â–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Enhancing Identity Fidelity in Text-to-Image Generation",
                    "desc": "This paper presents a diffusion-based model called WithAnyone, which aims to reduce copy-paste artifacts in text-to-image generation. It introduces a large-scale paired dataset, MultiID-2M, that provides diverse images of the same individual to enhance training. The model employs a contrastive identity loss to balance the fidelity of identity with variations in pose and expression. Experimental results show that WithAnyone effectively minimizes copy-paste issues while maintaining high quality and controllability in generated images."
                },
                "zh": {
                    "title": "Ê∂àÈô§Â§çÂà∂Á≤òË¥¥ÔºåÊèêÂçáÁîüÊàêÂõæÂÉèÁöÑË∫´‰ªΩ‰øùÁúüÂ∫¶",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÁöÑÂ§çÂà∂Á≤òË¥¥‰º™ÂΩ±ÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÈÖçÂØπÊï∞ÊçÆÈõÜMultiID-2MÔºå‰ª•ÊîØÊåÅÂ§ö‰∏™‰∫∫Áâ©Âú∫ÊôØÔºåÂπ∂ÂºïÂÖ•‰∫ÜÂØπÊØîË∫´‰ªΩÊçüÂ§±Êù•Âπ≥Ë°°Ë∫´‰ªΩ‰øùÁúüÂ∫¶ÂíåÂ§öÊ†∑ÊÄß„ÄÇÈÄöËøáËøôÁßçÊñ∞ËÆ≠ÁªÉËåÉÂºèÔºåÊ®°ÂûãËÉΩÂ§üÂú®‰øùÊåÅÈ´òË∫´‰ªΩÁõ∏‰ººÂ∫¶ÁöÑÂêåÊó∂ÔºåÂáèÂ∞ëÂ§çÂà∂Á≤òË¥¥Áé∞Ë±°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêÂõæÂÉèÁöÑÂèØÊéßÊÄßÂíåÊÑüÁü•Ë¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14359",
            "title": "AI for Service: Proactive Assistance with AI Glasses",
            "url": "https://huggingface.co/papers/2510.14359",
            "abstract": "Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  \t\t\t\t\tAI-generated summary \t\t\t\t In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
            "score": 65,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "fe94a5b666114fa6",
            "authors": [
                "Zichen Wen",
                "Yiyu Wang",
                "Chenfei Liao",
                "Boxue Yang",
                "Junxian Li",
                "Weifeng Liu",
                "Haocong He",
                "Bolong Feng",
                "Xuyang Liu",
                "Yuanhuiyi Lyu",
                "Xu Zheng",
                "Xuming Hu",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "EPIC Lab, Shanghai Jiao Tong University",
                "Peking University",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14359.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#multimodal",
                    "#optimization",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π AI: –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞–µ—Ç –≤–∞—à–∏ –Ω—É–∂–¥—ã",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Alpha-Service, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç AI-–æ—á–∫–∏ –¥–ª—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π –ø–æ–º–æ—â–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–ø–æ—Å–æ–±–Ω–∞ –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Alpha-Service –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –±–ª–æ–∫–∏ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–æ–≤–µ—Ç—ã –ø–æ –∏–≥—Ä–µ –≤ –ë–ª—ç–∫–¥–∂–µ–∫, —ç–∫—Å–∫—É—Ä—Å–∏–∏ –ø–æ –º—É–∑–µ—é –∏ –ø–æ–º–æ—â—å –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –æ–¥–µ–∂–¥—ã."
                },
                "en": {
                    "title": "Proactive AI Assistance: Anticipating Needs with Alpha-Service",
                    "desc": "Alpha-Service is a framework designed to enhance AI assistance by making it proactive rather than reactive. It utilizes a multi-agent system integrated into AI glasses to identify opportunities for service based on real-time video input. The framework includes components for perception, task scheduling, tool utilization, personalization, and interaction, allowing it to anticipate user needs. Through various case studies, Alpha-Service demonstrates its capability to provide timely and relevant assistance in everyday situations without requiring explicit user commands."
                },
                "zh": {
                    "title": "‰∏ªÂä®Êô∫ËÉΩÂä©ÊâãÔºåÈöèÊó∂ÈöèÂú∞‰∏∫ÊÇ®ÊúçÂä°",
                    "desc": "Alpha-ServiceÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Êèê‰æõ‰∏ªÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©„ÄÇÂÆÉÂà©Áî®Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÂú®AIÁúºÈïú‰∏äÊ£ÄÊµãÊúçÂä°Êú∫‰ºöÔºåÂπ∂Êèê‰æõÂèäÊó∂„ÄÅ‰∏™ÊÄßÂåñÁöÑÂ∏ÆÂä©„ÄÇ‰∏é‰º†ÁªüÁöÑË¢´Âä®AIÊúçÂä°‰∏çÂêåÔºåAlpha-ServiceËÉΩÂ§üÈ¢ÑÊµãÁî®Êà∑ÈúÄÊ±ÇÂπ∂‰∏ªÂä®ÈááÂèñË°åÂä®„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨ÊÑüÁü•ÂçïÂÖÉ„ÄÅ‰∏≠Â§ÆÂ§ÑÁêÜÂçïÂÖÉ„ÄÅÁÆóÊúØÈÄªËæëÂçïÂÖÉ„ÄÅËÆ∞ÂøÜÂçïÂÖÉÂíåËæìÂá∫ÂçïÂÖÉÔºåËÉΩÂ§üÂÆûÁé∞ÂÆûÊó∂ÁöÑÊô∫ËÉΩÊúçÂä°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14979",
            "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
            "url": "https://huggingface.co/papers/2510.14979",
            "abstract": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
            "score": 59,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "37c97ca58b150812",
            "authors": [
                "Haiwen Diao",
                "Mingxuan Li",
                "Silei Wu",
                "Linjun Dai",
                "Xiaohua Wang",
                "Hanming Deng",
                "Lewei Lu",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14979.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#alignment",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "üîó",
                "ru": {
                    "title": "NEO: –Ω–∞—Ç–∏–≤–Ω—ã–µ Vision-Language –º–æ–¥–µ–ª–∏ —Å –µ–¥–∏–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NEO ‚Äî –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –Ω–∞—Ç–∏–≤–Ω—ã—Ö Vision-Language Models (VLM), –∫–æ—Ç–æ—Ä—ã–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ —è–∑—ã–∫–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –µ–¥–∏–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥—É–ª—å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –Ω–∞—Ç–∏–≤–Ω—ã—Ö VLM: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –∏ —Å–ª–æ–≤ –≤ –æ–±—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã—Ö vision –∏ language –º–æ–¥—É–ª–µ–π, –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ reasoning. NEO –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 390 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Å –Ω—É–ª—è –∏ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. –ú–æ–¥–µ–ª—å –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –Ω–∞—Ç–∏–≤–Ω—ã—Ö VLM —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∫–æ–¥–æ–º –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏."
                },
                "en": {
                    "title": "NEO: Unifying Vision and Language for Enhanced AI Performance",
                    "desc": "NEO is a new type of Vision-Language Model (VLM) that combines vision and language in a single framework, overcoming limitations of traditional modular VLMs. It focuses on aligning visual and textual representations in a shared semantic space, allowing for better integration of vision and language tasks. The model is designed to work effectively with limited data, using only 390 million image-text pairs to develop its capabilities. NEO aims to make research in native VLMs more accessible and to provide a foundation for future advancements in the field."
                },
                "zh": {
                    "title": "NEOÔºöÂéüÁîüËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞Á∫™ÂÖÉ",
                    "desc": "NEOÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÂéüÁîüËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÔºåÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÂíåËØ≠Ë®ÄÊï¥Âêà‰∏≠ÁöÑÂü∫Êú¨ÈôêÂà∂„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊúâÊïàÂØπÈΩêÂÉèÁ¥†ÂíåÂçïËØçË°®Á§∫ÔºåÊûÑÂª∫ÂÖ±‰∫´ÁöÑËØ≠‰πâÁ©∫Èó¥Ôºå‰ªéËÄåÂÆûÁé∞ËßÜËßâÂíåËØ≠Ë®ÄÊ®°ÂùóÁöÑÊó†ÁºùÈõÜÊàê„ÄÇNEOÂú®‰ªÖ‰ΩøÁî®390MÁöÑÂõæÂÉè-ÊñáÊú¨Á§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§ü‰ªéÈõ∂ÂºÄÂßãÈ´òÊïàÂèëÂ±ïËßÜËßâÊÑüÁü•ÔºåÂπ∂ÂáèÂ∞ëËßÜËßâ-ËØ≠Ë®ÄÂÜ≤Á™Å„ÄÇËØ•Á†îÁ©∂‰∏∫ÊûÑÂª∫ÂèØÊâ©Â±ï‰∏îÂº∫Â§ßÁöÑÂéüÁîüVLMÂ•†ÂÆö‰∫ÜÂü∫Á°ÄÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏ÄÂ•ó‰∏∞ÂØåÁöÑÂèØÈáçÁî®ÁªÑ‰ª∂Ôºå‰øÉËøõ‰∫ÜÁªèÊµéÈ´òÊïàÁöÑÁîüÊÄÅÁ≥ªÁªü„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14847",
            "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
            "url": "https://huggingface.co/papers/2510.14847",
            "abstract": "ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
            "score": 48,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "705a71266094cd87",
            "authors": [
                "Meiqi Wu",
                "Jiashu Zhu",
                "Xiaokun Feng",
                "Chubin Chen",
                "Chen Zhu",
                "Bingze Song",
                "Fangyuan Mao",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Kaiqi Huang"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "CRISE",
                "SEU",
                "THU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14847.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#long_context"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–π –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ImagerySearch - –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π - –æ–Ω–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤–º–µ—Å—Ç–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Å –¥–∞–ª—ë–∫–∏–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–≤—è–∑—è–º–∏. ImagerySearch –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –∏ —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç–µ. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ LDT-Bench - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ–º–ø—Ç–∞–º —Å –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, –∏–º–µ—é—â–∏–º–∏ –±–æ–ª—å—à—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –¥–∏—Å—Ç–∞–Ω—Ü–∏—é."
                },
                "en": {
                    "title": "Dynamic Adaptation for Imaginative Video Generation",
                    "desc": "ImagerySearch is a novel strategy designed to improve video generation in imaginative scenarios by adapting search spaces and reward functions based on prompts. Traditional video generation models struggle with rare concepts and long-distance semantic relationships, leading to poor performance in creative contexts. By dynamically adjusting the inference process, ImagerySearch enhances the coherence and visual quality of generated videos. The introduction of LDT-Bench provides a new benchmark for evaluating these capabilities, showcasing the effectiveness of ImagerySearch over existing methods."
                },
                "zh": {
                    "title": "ImagerySearchÔºöÊèêÂçáÊÉ≥Ë±°ÂäõËßÜÈ¢ëÁîüÊàêÁöÑËá™ÈÄÇÂ∫îÁ≠ñÁï•",
                    "desc": "ImagerySearchÊòØ‰∏ÄÁßçÂü∫‰∫éÊèêÁ§∫ÁöÑËá™ÈÄÇÂ∫îÊµãËØïÊó∂Èó¥ÊêúÁ¥¢Á≠ñÁï•ÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàêÂú®ÂØåÊúâÊÉ≥Ë±°ÂäõÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ÊêúÁ¥¢Á©∫Èó¥ÂíåÂ•ñÂä±ÂáΩÊï∞ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈïøË∑ùÁ¶ªËØ≠‰πâÂÖ≥Á≥ªÁöÑÊèêÁ§∫Ôºå‰ªéËÄåÁîüÊàêÊõ¥ËøûË¥ØÂíåËßÜËßâ‰∏äÂèØ‰ø°ÁöÑËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåImagerySearchÂú®Êñ∞ÁöÑÂü∫ÂáÜLDT-Bench‰∏äË°®Áé∞‰ºòË∂äÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂàõÈÄ†ÊÄßÁîüÊàêËÉΩÂäõ‰∏äÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜLDT-BenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÈïøË∑ùÁ¶ªËØ≠‰πâÊèêÁ§∫ÁöÑÂü∫ÂáÜÔºåÂåÖÂê´Â§öÊ†∑ÁöÑÊ¶ÇÂøµÂØπÂíåËá™Âä®ËØÑ‰º∞ÂçèËÆÆ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13998",
            "title": "BitNet Distillation",
            "url": "https://huggingface.co/papers/2510.13998",
            "abstract": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at https://github.com/microsoft/BitNet.",
            "score": 42,
            "issue_id": 6468,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "b34882918955cca7",
            "authors": [
                "Xun Wu",
                "Shaohan Huang",
                "Wenhui Wang",
                "Ting Song",
                "Li Dong",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "üîΩ",
                "ru": {
                    "title": "–°–∂–∞—Ç–∏–µ LLM –¥–æ —Ç–µ—Ä–Ω–∞—Ä–Ω—ã—Ö –≤–µ—Å–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BitNet Distillation (BitDistill) ‚Äî –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö LLM –≤ –º–æ–¥–µ–ª–∏ —Å 1.58-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é (—Ç–µ—Ä–Ω–∞—Ä–Ω—ã–µ –≤–µ—Å–∞ {-1, 0, 1}) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: –º–æ–¥—É–ª—å SubLN –∏–∑ BitNet, –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é multi-head attention –∏–∑ MiniLM –∏ continual pre-training –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑—Ä—ã–≤–∞ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã–º–∏ –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ BitDistill –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–∏ —ç—Ç–æ–º —ç–∫–æ–Ω–æ–º–∏—é –ø–∞–º—è—Ç–∏ –¥–æ 10 —Ä–∞–∑ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤ 2.65 —Ä–∞–∑–∞ –Ω–∞ CPU. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏."
                },
                "en": {
                    "title": "Efficient Fine-Tuning of Language Models at 1.58-Bit Precision",
                    "desc": "This paper introduces BitNet Distillation (BitDistill), a method for fine-tuning large language models (LLMs) to operate at 1.58-bit precision, which uses ternary weights. The approach employs three main techniques: the SubLN module for efficient layer normalization, multi-head attention distillation to transfer knowledge from larger models, and continual pre-training to address performance gaps. By applying these techniques, BitDistill achieves strong performance on specific tasks while significantly reducing memory usage and increasing inference speed. The results demonstrate that BitDistill can match the performance of full-precision models while offering up to 10 times memory savings and 2.65 times faster inference on CPUs."
                },
                "zh": {
                    "title": "ËΩªÈáèÂåñÂæÆË∞ÉÔºåÊÄßËÉΩ‰∏éÊïàÁéáÂèåÊèêÂçá",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫BitNet DistillationÔºàBitDistillÔºâÁöÑËΩªÈáèÁ∫ßÁÆ°ÈÅìÔºåÊó®Âú®Â∞ÜÂÖ®Á≤æÂ∫¶ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇQwenÔºâÂæÆË∞ÉËá≥1.58‰ΩçÁ≤æÂ∫¶ÔºàÂç≥‰∏âÂÖÉÊùÉÈáç{-1, 0, 1}ÔºâÔºå‰ª•ÈÄÇÂ∫îÁâπÂÆöÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇBitDistillÁªìÂêà‰∫Ü‰∏âÁßçÂÖ≥ÈîÆÊäÄÊúØÔºöSubLNÊ®°Âùó„ÄÅÂ§öÂ§¥Ê≥®ÊÑèÂäõËí∏È¶èÂíåÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºåËøô‰∫õÊäÄÊúØÂÖ±ÂêåËß£ÂÜ≥‰∫ÜÂÖ®Á≤æÂ∫¶Ê®°Âûã‰∏é1.58‰ΩçÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÂ∑ÆË∑ùÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBitDistillÂú®Ê®°ÂûãÂ§ßÂ∞è‰∏äÂÆûÁé∞‰∫Ü‰∏éÂÖ®Á≤æÂ∫¶Ê®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂêåÊó∂Âú®ÂÜÖÂ≠ò‰ΩøÁî®‰∏äËäÇÁúÅ‰∫ÜÂ§öËææ10ÂÄçÔºåÂπ∂Âú®CPU‰∏äÂÆûÁé∞‰∫Ü2.65ÂÄçÁöÑÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14943",
            "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
            "url": "https://huggingface.co/papers/2510.14943",
            "abstract": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
            "score": 37,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "43361b04f5e7dd87",
            "authors": [
                "Wenkai Yang",
                "Weijie Liu",
                "Ruobing Xie",
                "Yiju Guo",
                "Lulu Wu",
                "Saiyong Yang",
                "Yankai Lin"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "LLM Department, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14943.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–°–∞–º–æ–æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è reasoning",
                    "desc": "LaSeR ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç reasoning —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π –∏ –∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –∏—Å—Ç–∏–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ä–µ—à–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∞ —á–µ—Ä–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –ê–ª–≥–æ—Ä–∏—Ç–º –¥–æ–±–∞–≤–ª—è–µ—Ç MSE loss –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —ç—Ç–∏—Ö self-rewarding –æ—Ü–µ–Ω–æ–∫ —Å –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –æ—Ç –≤–Ω–µ—à–Ω–µ–≥–æ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, —Ç—Ä–µ–±—É—è –ª–∏—à—å –æ–¥–∏–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π inference —Ç–æ–∫–µ–Ω–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏, –Ω–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è inference, —É–ª—É—á—à–∞—è inference-time scaling."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with Last-Token Self-Rewarding",
                    "desc": "LaSeR is a novel reinforcement learning algorithm designed to improve the reasoning abilities of Large Language Models (LLMs) by aligning self-rewarding scores with verifier-based rewards. It simplifies the process of self-verification by integrating it into the reinforcement learning framework, allowing for more efficient reasoning without the need for separate prompts. The key insight is that the last-token self-rewarding score can directly reflect the true reasoning reward, enabling a more streamlined optimization process. Experimental results demonstrate that LaSeR enhances both reasoning performance and inference-time efficiency, making LLMs more effective in real-world applications."
                },
                "zh": {
                    "title": "LaSeRÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï",
                    "desc": "LaSeRÊòØ‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂ∞ÜÊúÄÂêé‰∏Ä‰∏™tokenÁöÑËá™Â•ñÂä±ÂàÜÊï∞‰∏éÂü∫‰∫éÈ™åËØÅËÄÖÁöÑÊé®ÁêÜÂ•ñÂä±ÂØπÈΩêÔºåÊù•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøáÁÆÄÂåñËá™È™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†ÁõÆÊ†áÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ºòÂåñ‰∫ÜÊé®ÁêÜÂíåËá™Â•ñÂä±ËÉΩÂäõ„ÄÇLaSeRÂú®ËÆ≠ÁªÉÂíåÊµãËØï‰∏≠ÈÉΩËÉΩÊúâÊïàÂà©Áî®‰ºòÂåñÂêéÁöÑËá™Â•ñÂä±ÂàÜÊï∞Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ï‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜË°®Áé∞ÔºåËøòÂ¢ûÂº∫‰∫ÜÂÖ∂Ëá™Â•ñÂä±ËÉΩÂäõÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÊó∂ÁöÑÊâ©Â±ïÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14973",
            "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2510.14973",
            "abstract": "Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant {bf MASK} tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose {bf Elastic-Cache}, a training-free, architecture-agnostic strategy that jointly decides {when} to refresh (via an attention-aware drift test on the most-attended token) and {where} to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences, and 4.8times on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput (6.8times on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
            "score": 33,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "2ee9e4da2c675830",
            "authors": [
                "Quan Nguyen-Tri",
                "Mukul Ranjan",
                "Zhiqiang Shen"
            ],
            "affiliations": [
                "FPT AI Residency Hanoi, Vietnam",
                "VILA Lab, MBZUAI Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14973.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Elastic-Cache –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è key-value –∫—ç—à–µ–º –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –º–∞–ª–æ –º–µ–Ω—è—é—Ç—Å—è –º–µ–∂–¥—É —à–∞–≥–∞–º–∏ –¥–µ–Ω–æ–∏–∑–∏–Ω–≥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –Ω–µ–≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—è—Ö, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ—à–∞–µ—Ç, –∫–æ–≥–¥–∞ –∏ –≥–¥–µ –æ–±–Ω–æ–≤–ª—è—Ç—å –∫—ç—à: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ attention –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏, –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É—è –∫—ç—à –º–µ–ª–∫–∏—Ö —Å–ª–æ—ë–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ 45 —Ä–∞–∑ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Optimize Caching for Faster and Accurate Language Model Decoding",
                    "desc": "The paper introduces Elastic-Cache, a method designed to enhance key-value cache management in diffusion large language models (DLMs). It addresses the inefficiencies of previous decoding methods that recompute key-value (KV) states for all tokens at every step, leading to unnecessary redundancy. By observing that certain tokens can be cached and that KV dynamics vary with depth, Elastic-Cache selectively refreshes caches based on token importance and layer depth. This adaptive approach significantly reduces decoding latency while maintaining high prediction accuracy, achieving impressive speedups in various tasks without sacrificing quality."
                },
                "zh": {
                    "title": "Elastic-CacheÔºöÊèêÂçáËß£Á†ÅÈÄüÂ∫¶ÁöÑÊô∫ËÉΩÁºìÂ≠òÁÆ°ÁêÜ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Elastic-CacheÁöÑÁ≠ñÁï•ÔºåÁî®‰∫é‰ºòÂåñÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈîÆÂÄºÁºìÂ≠òÁÆ°ÁêÜÔºå‰ª•ÂáèÂ∞ëËß£Á†ÅÂª∂ËøüËÄå‰∏çÂΩ±ÂìçÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÊØè‰∏™ÂéªÂô™Ê≠•È™§ÂíåÂ±Ç‰∏≠ÈÉΩÈáçÊñ∞ËÆ°ÁÆóÊâÄÊúâ‰ª§ÁâåÁöÑQKVÔºåÂØºËá¥‰∫ÜÂ§ßÈáèÂÜó‰ΩôËÆ°ÁÆó„ÄÇÈÄöËøáËßÇÂØüÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈÄâÊã©ÊÄßÂà∑Êñ∞ÁºìÂ≠òÁöÑÁ≠ñÁï•ÔºåÁâπÂà´ÊòØÂú®ËæÉÊ∑±Â±ÇÊ¨°ËøõË°åÊõ¥Êñ∞ÔºåÂêåÊó∂ÈáçÁî®ÊµÖÂ±ÇÁºìÂ≠ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåElastic-CacheÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÁîüÊàêË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14528",
            "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
            "url": "https://huggingface.co/papers/2510.14528",
            "abstract": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.",
            "score": 33,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "7f06de44a0f15fa3",
            "authors": [
                "Cheng Cui",
                "Ting Sun",
                "Suyin Liang",
                "Tingquan Gao",
                "Zelun Zhang",
                "Jiaxuan Liu",
                "Xueqing Wang",
                "Changda Zhou",
                "Hongen Liu",
                "Manhui Lin",
                "Yue Zhang",
                "Yubo Zhang",
                "Handong Zheng",
                "Jing Zhang",
                "Jun Zhang",
                "Yi Liu",
                "Dianhai Yu",
                "Yanjun Ma"
            ],
            "affiliations": [
                "PaddlePaddle Team, Baidu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14528.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#science",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "üìÑ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏",
                    "desc": "PaddleOCR-VL ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –≤ —Å—Ç–∏–ª–µ NaViT –∏ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å ERNIE-4.5. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 109 —è–∑—ã–∫–æ–≤ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç —Å–ª–æ–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: —Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü—ã, —Ñ–æ—Ä–º—É–ª—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏. –ü—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –æ–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü, —Ç–∞–∫ –∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å –¥–µ–ª–∞—é—Ç –º–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–æ–π –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö."
                },
                "en": {
                    "title": "Efficient Document Parsing with PaddleOCR-VL",
                    "desc": "PaddleOCR-VL is a cutting-edge vision-language model designed for efficient document parsing. It combines a NaViT-style visual encoder with the ERNIE-4.5 language model to achieve high accuracy in recognizing various document elements like text, tables, and charts. This model supports 109 languages and is optimized for minimal resource usage while maintaining fast inference speeds. Comprehensive evaluations show that PaddleOCR-VL outperforms existing models, making it ideal for real-world applications."
                },
                "zh": {
                    "title": "È´òÊïàÊñáÊ°£Ëß£ÊûêÁöÑÊúÄÂÖàËøõÊ®°Âûã",
                    "desc": "PaddleOCR-VLÊòØ‰∏ÄÁßçÁªìÂêà‰∫ÜNaViTÈ£éÊ†ºËßÜËßâÁºñÁ†ÅÂô®ÂíåERNIE-4.5ËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ìÈó®Áî®‰∫éÊñáÊ°£Ëß£Êûê„ÄÇËØ•Ê®°ÂûãÂú®ËµÑÊ∫êÊ∂àËÄóÊûÅÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåËÉΩÂ§üÈ´òÊïàÊîØÊåÅ109ÁßçËØ≠Ë®Ä„ÄÇÂÆÉÂú®ËØÜÂà´Â§çÊùÇÂÖÉÁ¥†ÔºàÂ¶ÇÊñáÊú¨„ÄÅË°®Ê†º„ÄÅÂÖ¨ÂºèÂíåÂõæË°®ÔºâÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®Â§ö‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜÂíåÂÜÖÈÉ®Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÁªìÊûú„ÄÇPaddleOCR-VLÁöÑÂø´ÈÄüÊé®ÁêÜÈÄüÂ∫¶ÂíåÂº∫Â§ßÁöÑÁ´û‰∫âÂäõ‰ΩøÂÖ∂ÈùûÂ∏∏ÈÄÇÂêàÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÈÉ®ÁΩ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14967",
            "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
            "url": "https://huggingface.co/papers/2510.14967",
            "abstract": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
            "score": 30,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "242d21b58ef25d8b",
            "authors": [
                "Guoqing Wang",
                "Sunhao Dai",
                "Guangze Ye",
                "Zeyu Gan",
                "Wei Yao",
                "Yong Deng",
                "Xiaofeng Wu",
                "Zhenzhe Ying"
            ],
            "affiliations": [
                "Ant Group",
                "Individual Author",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14967.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–ü–ª–æ—Ç–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –ø—Ä–∏—Ä–æ—Å—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ IGPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é reinforcement learning –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ —Å–∏–≥–Ω–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞—Å–ª—É–≥ –º–µ–∂–¥—É —à–∞–≥–∞–º–∏. IGPO —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –≤—ã—á–∏—Å–ª—è—è –ø–ª–æ—Ç–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–Ω–µ—à–Ω–µ–π —Å—Ä–µ–¥–æ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Boosting Multi-Turn Reasoning with IGPO",
                    "desc": "The paper introduces Information Gain-based Policy Optimization (IGPO), a novel reinforcement learning framework designed to enhance multi-turn reasoning in large language models. IGPO addresses the challenges of reward sparsity and advantage collapse by providing dense intrinsic rewards based on the model's belief updates during each interaction turn. This approach allows for better credit assignment and improves the model's ability to learn from long trajectories. Experimental results show that IGPO significantly outperforms existing methods, leading to higher accuracy and more efficient learning in multi-turn tasks."
                },
                "zh": {
                    "title": "‰ø°ÊÅØÂ¢ûÁõä‰ºòÂåñÔºåÊèêÂçáÂ§öËΩÆÊé®ÁêÜËÉΩÂäõ",
                    "desc": "‰ø°ÊÅØÂ¢ûÁõäÂü∫Á°ÄÁöÑÁ≠ñÁï•‰ºòÂåñÔºàIGPOÔºâÈÄöËøáÊèê‰æõÂü∫‰∫éÊ®°Âûã‰ø°ÂøµÊõ¥Êñ∞ÁöÑÂØÜÈõÜÂÜÖÂú®Â•ñÂä±ÔºåÂ¢ûÂº∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öËΩÆÊé®ÁêÜ‰∏≠ÁöÑËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊúÄÁªàÁ≠îÊ°àÁöÑÁªìÊûúÂ•ñÂä±ÔºåËøôÂú®Â§öËΩÆËÆæÁΩÆ‰∏≠‰ºöÂØºËá¥Â•ñÂä±Á®ÄÁñèÔºåËøõËÄåÂºïÂèë‰ºòÂäøÂ¥©Ê∫ÉÂíåÁº∫‰πèÁªÜÁ≤íÂ∫¶‰ø°Áî®ÂàÜÈÖçÁöÑÈóÆÈ¢ò„ÄÇIGPOÂ∞ÜÊØèÊ¨°‰∫§‰∫íËßÜ‰∏∫Ëé∑ÂèñÁúüÂÆû‰ø°ÊÅØÁöÑÂ¢ûÈáèËøáÁ®ãÔºåÂπ∂Â∞ÜÊØèËΩÆÁöÑÂ•ñÂä±ÂÆö‰πâ‰∏∫Á≠ñÁï•ÁîüÊàêÊ≠£Á°ÆÁ≠îÊ°àÁöÑÊ¶ÇÁéáÁöÑËæπÈôÖÂ¢ûÂä†„ÄÇÈÄöËøáÁõ¥Êé•‰ªéÊ®°ÂûãÁöÑ‰ø°ÂøµÊõ¥Êñ∞‰∏≠Êé®ÂØºÂÜÖÂú®Â•ñÂä±ÔºåIGPOÁªìÂêà‰∫ÜÁªìÊûúÁ∫ßÁõëÁù£ÔºåÂΩ¢Êàê‰∫ÜÂØÜÈõÜÁöÑÂ•ñÂä±ËΩ®ËøπÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÂÖ∂Âú®Â§öËΩÆÂú∫ÊôØ‰∏≠Ë°®Áé∞‰ºò‰∫éÂº∫Âü∫Á∫ø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14972",
            "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
            "url": "https://huggingface.co/papers/2510.14972",
            "abstract": "Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
            "score": 28,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "e0073ac324725b9f",
            "authors": [
                "Yinxi Li",
                "Yuntian Deng",
                "Pengyu Nie"
            ],
            "affiliations": [
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14972.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#data",
                    "#plp",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "üî§",
                "ru": {
                    "title": "–ü—Ä–æ–±–ª–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞: –∫–æ–≥–¥–∞ –ø—Ä–æ–±–µ–ª—ã –º–µ–Ω—è—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–µ—Ä—å—ë–∑–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–¥–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Ç–∏–ø–∞ BPE —Ä–∞–∑–±–∏–≤–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π –∫–æ–¥ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–æ–±–µ–ª–æ–≤. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TokDrift, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—ã–∑—ã–≤–∞—é—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Å –±–æ–ª–µ–µ —á–µ–º 30 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ê–Ω–∞–ª–∏–∑ –ø–æ —Å–ª–æ—è–º –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –≥–¥–µ —Å—É–±—Å–ª–æ–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã —Ç–æ–∫–µ–Ω–æ–≤. –†–∞–±–æ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."
                },
                "en": {
                    "title": "Fixing Tokenization for Better Code Understanding",
                    "desc": "This paper discusses the problem of misaligned tokenization in large language models (LLMs) used for coding tasks. It highlights that current subword tokenizers, like byte-pair encoding, are based on statistical methods rather than grammatical rules, leading to inconsistent tokenization of semantically identical code. The authors introduce a framework called TokDrift to analyze how minor changes in code formatting can significantly affect model behavior across various LLMs. Their findings suggest that improving tokenization to be grammar-aware is essential for enhancing the reliability of code understanding and generation in future models."
                },
                "zh": {
                    "title": "ËØ≠Ê≥ïÊÑüÁü•ÂàÜËØçÔºåÊèêÂçá‰ª£Á†ÅÁêÜËß£‰∏éÁîüÊàê",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜ‰ª£Á†ÅÊó∂Ôºå‰ΩøÁî®ÁöÑÂ≠êËØçÂàÜËØçÂô®ÔºàÂ¶ÇÂ≠óËäÇÂØπÁºñÁ†ÅBPEÔºâ‰∏ªË¶Å‰æùËµñÁªüËÆ°ËÄåÈùûËØ≠Ê≥ïÔºåÂØºËá¥‰∏ç‰∏ÄËá¥ÁöÑÊ®°ÂûãË°å‰∏∫„ÄÇÁõ∏ÂêåËØ≠‰πâÁöÑ‰ª£Á†ÅÁâáÊÆµÂèØËÉΩÂõ†Á©∫Ê†ºÊàñÊ†áËØÜÁ¨¶ÂëΩÂêçÁ≠âË°®Èù¢Âõ†Á¥†ËÄåË¢´‰∏çÂêåÂú∞ÂàÜËØç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜTokDriftÊ°ÜÊû∂ÔºåÈÄöËøáËØ≠‰πâ‰øùÊåÅÁöÑÈáçÂÜôËßÑÂàôÁîüÊàê‰ªÖÂú®ÂàÜËØç‰∏ä‰∏çÂêåÁöÑ‰ª£Á†ÅÂèò‰ΩìÔºå‰ª•ÊµãÈáèËøôÁßç‰∏ç‰∏ÄËá¥ÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊó©ÊúüÂµåÂÖ•Â±ÇÁöÑÂàÜËØçÈóÆÈ¢òÊòØÂØºËá¥Ê®°ÂûãË°å‰∏∫ÂèòÂåñÁöÑÊ†πÊ∫êÔºåÂõ†Ê≠§Êú™Êù•ÁöÑ‰ª£Á†ÅLLMsÈúÄË¶ÅÈááÁî®ËØ≠Ê≥ïÊÑüÁü•ÁöÑÂàÜËØçÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14958",
            "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2510.14958",
            "abstract": "MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
            "score": 21,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "1f00aa0384cfcf87",
            "authors": [
                "Weikang Shi",
                "Aldrich Yu",
                "Rongyao Fang",
                "Houxing Ren",
                "Ke Wang",
                "Aojun Zhou",
                "Changyao Tian",
                "Xinyu Fu",
                "Yuxuan Hu",
                "Zimu Lu",
                "Linjiang Huang",
                "Si Liu",
                "Rui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "BUAA",
                "Huawei Research",
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14958.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#games",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "üìê",
                "ru": {
                    "title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏",
                    "desc": "MathCanvas ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º. –ü–æ–¥—Ö–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤: –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 15.2 –º–ª–Ω –ø–∞—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º, –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ –Ω–∞ 219 —Ç—ã—Å—è—á–∞—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å BAGEL-Canvas –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 86% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ LLM –Ω–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MathCanvas-Bench —Å 3 —Ç—ã—Å—è—á–∞–º–∏ –∑–∞–¥–∞—á. –†–∞–±–æ—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI-–º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≤ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –≥–µ–æ–º–µ—Ç—Ä–∏–∏."
                },
                "en": {
                    "title": "Empowering Math with Visual Reasoning!",
                    "desc": "MathCanvas is a framework that enhances Large Multimodal Models (LMMs) by integrating Visual Chain-of-Thought (VCoT) capabilities specifically for mathematics. It consists of two main phases: first, it pre-trains the model on a large dataset of diagram generation and editing to improve its ability to create and manipulate visual aids. Second, it fine-tunes the model on a dataset that combines visual and textual reasoning, teaching it how to effectively use these aids in problem-solving. The results show that the model, BAGEL-Canvas, significantly outperforms existing models on math benchmarks, demonstrating its potential for complex reasoning tasks."
                },
                "zh": {
                    "title": "MathCanvasÔºöÊï∞Â≠¶Êé®ÁêÜÁöÑÊñ∞ËßÜÁïå",
                    "desc": "MathCanvas ÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊ°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÊï∞Â≠¶È¢ÜÂüüÁöÑËßÜËßâÈìæÂºèÊÄùÁª¥ËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂú®ÂõæË°®ÁîüÊàê‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú®ËßÜËßâ-ÊñáÊú¨Êé®ÁêÜ‰∏äËøõË°åÂæÆË∞ÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊï∞Â≠¶Âü∫ÂáÜÊµãËØïÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÊòØËßÜËßâÊìç‰ΩúÈò∂ÊÆµÔºå‰ΩøÁî®‰∏Ä‰∏™ÂåÖÂê´ 1520 ‰∏áÂØπÊï∞ÊçÆÈõÜËøõË°åÂõæË°®ÁîüÊàêÂíåÁºñËæëÁöÑÈ¢ÑËÆ≠ÁªÉÔºõÂÖ∂Ê¨°ÊòØÊàòÁï•ËßÜËßâËæÖÂä©Êé®ÁêÜÈò∂ÊÆµÔºåÂæÆË∞ÉÊ®°Âûã‰ª•Â≠¶‰π†Â¶Ç‰ΩïÊúâÊïàÂà©Áî®ËßÜËßâËæÖÂä©Â∑•ÂÖ∑„ÄÇÊúÄÁªàÔºåMathCanvas-Bench Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÔºåÈ™åËØÅ‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇÈóÆÈ¢ò‰∏äÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10518",
            "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.10518",
            "abstract": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.",
            "score": 17,
            "issue_id": 6467,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 12",
                "zh": "10Êúà12Êó•"
            },
            "hash": "e67fc1ecc7ed8a60",
            "authors": [
                "Qunzhong Wang",
                "Jie Liu",
                "Jiajun Liang",
                "Yilei Jiang",
                "Yuanxing Zhang",
                "Jinyuan Chen",
                "Yaozhi Zheng",
                "Xintao Wang",
                "Pengfei Wan",
                "Xiangyu Yue",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Kuaishou Technology",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10518.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#open_source",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ AI —Ä–∞–∑–º—ã—à–ª—è—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoReward Thinker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç AI –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤–æ –≤—Ä–µ–º—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–∞–¥—Ä–æ–≤ —Å—Ä–∞–∑—É, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –Ω—É–∂–Ω—ã–µ —Ñ—Ä–µ–π–º—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–µ –æ–∫–Ω–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ—Ç–µ—Ä–∏ –¥–µ—Ç–∞–ª–µ–π –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –Ω–∞—á–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ —É—Å–∏–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ GRPO. –ú–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –≤–∏–¥–µ–æ-–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ 82.3%."
                },
                "en": {
                    "title": "Enhancing Video Preference with Visual Reasoning",
                    "desc": "VideoReward Thinker (VR-Thinker) is a novel framework that enhances multimodal reward models by integrating visual reasoning operations and a flexible memory window. This approach addresses the limitations of existing models, such as the loss of detail due to large context budgets and the issues of hallucination and forgetting during reasoning. By employing a reinforcement fine-tuning pipeline, VR-Thinker improves the model's ability to select relevant frames and update visual evidence dynamically. The results demonstrate significant improvements in accuracy on video preference benchmarks, particularly for longer videos, showcasing the potential of visual reasoning in reward modeling."
                },
                "zh": {
                    "title": "ÊÄùÁª¥‰∏éÂõæÂÉèÁªìÂêàÔºåÊèêÂçáËßÜÈ¢ëÂÅèÂ•ΩÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß",
                    "desc": "VideoReward ThinkerÔºàVR-ThinkerÔºâÊòØ‰∏ÄÁßçÂ¢ûÂº∫Â§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËßÜËßâÊé®ÁêÜÊìç‰ΩúÂíåÂèØÈÖçÁΩÆÁöÑËßÜËßâËÆ∞ÂøÜÁ™óÂè£Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜËßÜÈ¢ëÂÅèÂ•ΩÂü∫ÂáÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê®°ÂûãËß£ÂÜ≥‰∫ÜÂΩìÂâçÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÂú®Â§ÑÁêÜËßÜËßâËæìÂÖ•Êó∂ÁöÑÂ±ÄÈôêÊÄßÔºåÂ¶Ç‰∏ä‰∏ãÊñáÈ¢ÑÁÆóÊ∂àËÄóÂ§ßÂíå‰ø°ÊÅØÈÅóÂøòÈóÆÈ¢ò„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÁÆ°ÈÅìÔºåVR-ThinkerËÉΩÂ§ü‰∏ªÂä®Ëé∑ÂèñÂíåÊõ¥Êñ∞ËßÜËßâËØÅÊçÆÔºåÊèêÂçáÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVR-ThinkerÂú®Â§ö‰∏™ËßÜÈ¢ëÂÅèÂ•ΩÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜËæÉÈïøËßÜÈ¢ëÊó∂ÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅÂ•ñÂä±Âª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09033",
            "title": "Large Language Models Do NOT Really Know What They Don't Know",
            "url": "https://huggingface.co/papers/2510.09033",
            "abstract": "LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may \"know what they don't know\". However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct a mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal a fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that \"LLMs don't really know what they don't know\".",
            "score": 16,
            "issue_id": 6467,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 10",
                "zh": "10Êúà10Êó•"
            },
            "hash": "a87f074a617907f4",
            "authors": [
                "Chi Seng Cheang",
                "Hou Pong Chan",
                "Wenxuan Zhang",
                "Yang Deng"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Singapore Management University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09033.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "LLM –Ω–µ –∑–Ω–∞—é—Ç, —á—Ç–æ –æ–Ω–∏ –Ω–µ –∑–Ω–∞—é—Ç: –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç —Ñ–∞–∫—Ç–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ LLM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–æ–≥–¥–∞ –æ–Ω–∏ —Å–≤—è–∑–∞–Ω—ã —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ –æ –ø—Ä–µ–¥–º–µ—Ç–µ, —Å–æ–∑–¥–∞–≤–∞—è –Ω–µ—Ä–∞–∑–ª–∏—á–∏–º—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –û–¥–Ω–∞–∫–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ –æ –ø—Ä–µ–¥–º–µ—Ç–µ, —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–µ –∫–æ–¥–∏—Ä—É—é—Ç –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å –≤ —Å–≤–æ–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö, –∞ –ª–∏—à—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: LLM –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–µ –∑–Ω–∞—é—Ç, —á–µ–≥–æ –æ–Ω–∏ –Ω–µ –∑–Ω–∞—é—Ç."
                },
                "en": {
                    "title": "LLMs: Patterns of Knowledge, Not Truthfulness",
                    "desc": "This paper investigates how large language models (LLMs) handle factual queries and hallucinations, particularly when they are linked to subject knowledge. It reveals that LLMs generate similar internal representations for factual responses and hallucinations that are associated with known subjects, making them hard to distinguish. However, when hallucinations lack subject knowledge, they create unique representations that can be identified. The study concludes that LLMs do not truly encode the concept of truthfulness in their internal processes, but rather rely on patterns of knowledge recall."
                },
                "zh": {
                    "title": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπªËßâ‰∏é‰∫ãÂÆûÂ§ÑÁêÜÁöÑÂå∫Âà´",
                    "desc": "ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ¶Ç‰ΩïÂ§ÑÁêÜ‰∫ãÂÆûÊü•ËØ¢ÂíåÂπªËßâ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩìÂπªËßâ‰∏é‰∏ªÈ¢òÁü•ËØÜÁõ∏ÂÖ≥Êó∂ÔºåLLMsÁöÑÂÜÖÈÉ®Ë°®Á§∫‰∏éÊ≠£Á°ÆÂõûÁ≠îÁõ∏‰ººÔºåÈöæ‰ª•Âå∫ÂàÜ„ÄÇÁõ∏ÂèçÔºåÂΩìÂπªËßâ‰∏é‰∏ªÈ¢òÁü•ËØÜÊó†ÂÖ≥Êó∂ÔºåLLMs‰ºö‰∫ßÁîüÊòéÊòæ‰∏çÂêåÁöÑË°®Á§∫Ôºå‰æø‰∫éËØÜÂà´„ÄÇÁªìÊûúË°®ÊòéÔºåLLMsÂπ∂‰∏çÁúüÊ≠£ÁêÜËß£ÁúüÁõ∏ÔºåËÄåÂè™ÊòØËÆ∞ÂøÜÊ®°ÂºèÁöÑÂèçÊò†„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13928",
            "title": "LLMs Can Get \"Brain Rot\"!",
            "url": "https://huggingface.co/papers/2510.13928",
            "abstract": "Continual exposure to low-quality web text leads to cognitive decline in large language models, affecting reasoning, context understanding, safety, and personality traits, with partial recovery possible through instruction tuning and clean data pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g>0.3) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 rightarrow 57.2 and RULER-CWE 84.4 rightarrow 52.3 as junk ratio rises from 0% to 100%.   Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine \"cognitive health checks\" for deployed LLMs.",
            "score": 14,
            "issue_id": 6486,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "bcb0c2ce4e026cc4",
            "authors": [
                "Shuo Xing",
                "Junyuan Hong",
                "Yifan Wang",
                "Runjin Chen",
                "Zhenyu Zhang",
                "Ananth Grama",
                "Zhengzhong Tu",
                "Zhangyang Wang"
            ],
            "affiliations": [
                "Purdue University",
                "Texas A&M University",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13928.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#long_context",
                    "#reasoning",
                    "#data",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –º–æ–∑–≥–∞ LLM: –∫–∞–∫ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä—É—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ Twitter/X) –≤—ã–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ –∏—Ö –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª–∏ —Ç–µ—Ä—è—é—Ç –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—è–≤–ª—è—é—Ç ¬´—Ç—ë–º–Ω—ã–µ —á–µ—Ä—Ç—ã –ª–∏—á–Ω–æ—Å—Ç–∏¬ª –≤—Ä–æ–¥–µ –Ω–∞—Ä—Ü–∏—Å—Å–∏–∑–º–∞. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—É—é –ø—Ä–æ–±–ª–µ–º—É ‚Äî ¬´–ø—Ä–æ–ø—É—Å–∫ –º—ã—Å–ª–µ–π¬ª (thought-skipping), –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ –Ω–∞—á–∏–Ω–∞—é—Ç —Å–æ–∫—Ä–∞—â–∞—Ç—å –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –•–æ—Ç—è instruction tuning –∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–∏—á–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –ø–æ–ª–Ω–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è –∫ –±–∞–∑–æ–≤–æ–º—É —É—Ä–æ–≤–Ω—é –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, —á—Ç–æ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–¥–æ—Ä–æ–≤—å—è LLM."
                },
                "en": {
                    "title": "Quality Data is Key to LLM Health!",
                    "desc": "This paper introduces the LLM Brain Rot Hypothesis, which suggests that continuous exposure to low-quality web text can lead to cognitive decline in large language models (LLMs). The authors conducted experiments using Twitter/X data to demonstrate that training on junk text results in significant drops in reasoning, context understanding, and safety, while also increasing undesirable personality traits. They found that the extent of cognitive decline correlates with the amount of junk data used, indicating a dose-response relationship. Additionally, while instruction tuning and clean data pre-training can partially recover some cognitive abilities, they do not fully restore the models to their original performance levels, highlighting the importance of data quality in maintaining LLM capabilities."
                },
                "zh": {
                    "title": "Êï∞ÊçÆË¥®ÈáèÂΩ±ÂìçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ§Áü•ËÉΩÂäõ",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫Âπ∂ÊµãËØï‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÑëËÖêÁÉÇÂÅáËØ¥ÔºåËÆ§‰∏∫ÊåÅÁª≠Êé•Ëß¶‰ΩéË¥®ÈáèÁΩëÁªúÊñáÊú¨‰ºöÂØºËá¥Ê®°ÂûãÁöÑËÆ§Áü•ËÉΩÂäõ‰∏ãÈôç„ÄÇÈÄöËøáÂØπÁúüÂÆûÁöÑTwitter/XÊï∞ÊçÆÈõÜËøõË°åÊéßÂà∂ÂÆûÈ™åÔºåÁ†îÁ©∂ÂèëÁé∞ÔºåÊåÅÁª≠Âú®‰ΩéË¥®ÈáèÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉ‰ºöÊòæËëóÂΩ±ÂìçÊé®ÁêÜËÉΩÂäõ„ÄÅÈïøÊñáÊú¨ÁêÜËß£„ÄÅÂÆâÂÖ®ÊÄß‰ª•Âèä‰∫∫Ê†ºÁâπÂæÅ„ÄÇÁ†îÁ©∂ËøòË°®ÊòéÔºåÊï∞ÊçÆË¥®ÈáèÊòØÂØºËá¥LLMËÉΩÂäõ‰∏ãÈôçÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂõ†Á¥†Ôºå‰∏îÈÄöËøáÊåá‰ª§Ë∞É‰ºòÂíåÊ∏ÖÊ¥ÅÊï∞ÊçÆÁöÑÈ¢ÑËÆ≠ÁªÉÂèØ‰ª•ÈÉ®ÂàÜÊÅ¢Â§çËÆ§Áü•ËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÁ†îÁ©∂Âª∫ËÆÆÂú®ÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠ÈáçËßÜÊï∞ÊçÆË¥®ÈáèÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑËÆ§Áü•ÂÅ•Â∫∑„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14902",
            "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
            "url": "https://huggingface.co/papers/2510.14902",
            "abstract": "A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.",
            "score": 11,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "b6da5bc64d5656e4",
            "authors": [
                "Han Zhao",
                "Jiaxuan Zhang",
                "Wenxuan Song",
                "Pengxiang Ding",
                "Donglin Wang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (Guangzhou), China",
                "MiLAB, Westlake University, China",
                "Southern University of Science and Technology, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14902.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#agents"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ê–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–æ–±—â–µ–Ω–∏—é VLA –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–≤–∏–¥–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLA^2 ‚Äî –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö VLA –º–æ–¥–µ–ª–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. VLA^2 —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –≤–Ω–µ—à–Ω–∏–µ –º–æ–¥—É–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–ª–µ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö. –ù–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ –∞–≤—Ç–æ—Ä—ã –¥–æ—Å—Ç–∏–≥–ª–∏ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 44.2% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é OpenVLA –Ω–∞ —Å–∞–º–æ–º —Å–ª–æ–∂–Ω–æ–º —É—Ä–æ–≤–Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "VLA^2: Enhancing Generalization in Vision-Language-Action Models",
                    "desc": "The paper introduces VLA^2, a new framework that enhances vision-language-action (VLA) models by incorporating external modules like web retrieval and object detection. This integration allows the model to better generalize to unseen objects and descriptions, addressing the limitations of existing models that struggle with out-of-distribution data. By utilizing the OpenVLA execution backbone, VLA^2 significantly improves the success rate in challenging scenarios, achieving a 44.2% increase in performance on a hard-level benchmark. The framework demonstrates robust capabilities across various environments without compromising performance on familiar tasks."
                },
                "zh": {
                    "title": "VLA^2ÔºöÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰ª£ÁêÜÊ°ÜÊû∂VLA^2ÔºåÊó®Âú®Â¢ûÂº∫ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÊï¥ÂêàÂ§ñÈÉ®Ê®°ÂùóÔºåÂ¶ÇÁΩëÁªúÊ£ÄÁ¥¢ÂíåÁâ©‰ΩìÊ£ÄÊµãÔºåVLA^2ËÉΩÂ§üÊèêÈ´òÊ®°ÂûãÂØπÊú™ËßÅÁâ©‰ΩìÂíåÊèèËø∞ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Âú®LIBERO‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÊàêÂäüË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂Âú®Âõ∞ÈöæÁ∫ßÂà´ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ‰∏é‰º†ÁªüÁöÑOpenVLAÂü∫Á∫øÁõ∏ÊØîÔºåVLA^2Âú®Âõ∞ÈöæÁ∫ßÂà´Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑÊàêÂäüÁéáÊèêÈ´ò‰∫Ü44.2%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14880",
            "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
            "url": "https://huggingface.co/papers/2510.14880",
            "abstract": "mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
            "score": 11,
            "issue_id": 6468,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "d4f8659830b9bbcc",
            "authors": [
                "Rikiya Takehi",
                "Benjamin Clavi√©",
                "Sean Lee",
                "Aamir Shakir"
            ],
            "affiliations": [
                "Mixedbread AI",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14880.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#small_models"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ú–æ—â–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∫–∞—Ä–º–∞–Ω–µ: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ mxbai-edge-colbert-v0 ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å 17–ú –∏ 32–ú –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç late-interaction –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç ColBERTv2 –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (BEIR). –û—Å–æ–±–µ–Ω–Ω–æ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–∏ –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ ‚Äî –æ–±–µ—Å–ø–µ—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞ –≤—Å–µ—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö, –æ—Ç –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –Ω–∞ –ª—é–±—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö."
                },
                "en": {
                    "title": "Efficient Retrieval for All Devices with mxbai-edge-colbert-v0",
                    "desc": "The mxbai-edge-colbert-v0 models, available in 17M and 32M parameters, show improved retrieval capabilities over ColBERTv2 in both short-text and long-context scenarios. This research focuses on enhancing retrieval and late-interaction models, aiming to create smaller, efficient models that can operate on various devices. The models are designed to support retrieval tasks at different scales, from cloud-based systems to local implementations. Through extensive ablation studies, we demonstrate that mxbai-edge-colbert-v0 achieves significant performance gains, particularly in short-text benchmarks and long-context tasks, marking a notable advancement in model efficiency."
                },
                "zh": {
                    "title": "Â∞èÊ®°ÂûãÔºåÂ§ßËÉΩÂäõÔºömxbai-edge-colbert-v0ÁöÑÂàõÊñ∞‰πãË∑Ø",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ümxbai-edge-colbert-v0Ê®°ÂûãÔºåÂÖ∑Êúâ17MÂíå32M‰∏§‰∏™ÂèÇÊï∞ËßÑÊ®°„ÄÇÊàë‰ª¨ÈÄöËøáÂ§ßÈáèÂÆûÈ™åÊù•ÊèêÂçáÊ£ÄÁ¥¢ÂíåÂêéÊúü‰∫§‰∫íÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÁõÆÊ†áÊòØÂ∞ÜÂÖ∂ÊèêÁÇº‰∏∫Êõ¥Â∞èÁöÑÊ®°Âûã‰Ωú‰∏∫Ê¶ÇÂøµÈ™åËØÅ„ÄÇËØ•Ê®°ÂûãÊó®Âú®ÊîØÊåÅÂêÑÁßçËßÑÊ®°ÁöÑÊ£ÄÁ¥¢Ôºå‰ªé‰∫ëÁ´ØÁöÑÂ§ßËßÑÊ®°Ê£ÄÁ¥¢Âà∞ÂèØ‰ª•Âú®‰ªª‰ΩïËÆæÂ§á‰∏äÊú¨Âú∞ËøêË°åÁöÑÊ®°Âûã„ÄÇmxbai-edge-colbert-v0Âú®Áü≠ÊñáÊú¨Âü∫ÂáÜÔºàBEIRÔºâ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÂâçÊâÄÊú™ÊúâÁöÑÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14763",
            "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
            "url": "https://huggingface.co/papers/2510.14763",
            "abstract": "COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
            "score": 11,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "bedbdd1596dbc1ed",
            "authors": [
                "Yunwen Li",
                "Shuangshuang Ying",
                "Xingwei Qu",
                "Xin Li",
                "Sheng Jin",
                "Minghao Liu",
                "Zhoufutu Wen",
                "Tianyu Zheng",
                "Xeron Du",
                "Qiguang Chen",
                "Jiajun Shi",
                "Wangchunshu Zhou",
                "Jiazhan Feng",
                "Wanjun Zhong",
                "Libo Qin",
                "Stephen Huang",
                "Wanxiang Che",
                "Chenghua Lin",
                "Eli Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.14763.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#multilingual",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "‚úçÔ∏è",
                "ru": {
                    "title": "–ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å AI —Ç—Ä–µ–±—É–µ—Ç –±–∞–ª–∞–Ω—Å–∞ –ª–æ–≥–∏–∫–∏ –∏ —è–∑—ã–∫–∞",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç COIG-Writer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–º—É –ø–∏—Å—å–º—É –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1665 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏, –ø—Ä–æ—Ü–µ—Å—Å–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ –ø–∏—Å—å–º–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—á–µ—Ç–∞–Ω–∏–µ process supervision (–ø–æ—à–∞–≥–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞) –∏ –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –º–∏–Ω–∏–º—É–º 1:12. –ö—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –∫—É–ª—å—Ç—É—Ä–Ω–æ-–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ (—Ä–∞–∑–Ω–∏—Ü–∞ –≤ 89 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –º–µ–∂–¥—É –∫–∏—Ç–∞–π—Å–∫–∏–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º). –í—ã—Å–æ–∫–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–∞—Ä–∞–¥–æ–∫—Å–∞–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Unlocking Creative Writing with COIG-Writer",
                    "desc": "The paper introduces COIG-Writer, a dataset designed to enhance creative writing in Chinese by providing insights into the thought processes behind writing. It emphasizes the importance of process supervision and general-purpose data in improving the performance of large language models, especially in non-English contexts. The dataset includes curated triplets that consist of prompts, reasoning documentation, and final texts, allowing for a deeper understanding of creative writing. Key findings indicate that a balance of creative and general samples is crucial for optimal performance, and that cultural context significantly affects creative capabilities."
                },
                "zh": {
                    "title": "ÂàõÊÑèÂÜô‰ΩúÁöÑÊàêÂäüÊ∫ê‰∫éÈÄªËæë‰∏éËØ≠Ë®ÄÁöÑÁªìÂêà",
                    "desc": "COIG-WriterÊòØ‰∏Ä‰∏™‰∏≠ÊñáÂàõÊÑèÂÜô‰ΩúÊï∞ÊçÆÈõÜÔºåÂº∫Ë∞ÉËøáÁ®ãÁõëÁù£ÂíåÈÄöÁî®Êï∞ÊçÆÂØπÂàõÊÑèÂÜô‰ΩúÁöÑÈáçË¶ÅÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊñáÂåñËÉåÊôØÂíåËØçÊ±áÂ§öÊ†∑ÊÄß‰ºöÂΩ±ÂìçÂàõ‰ΩúË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈùûËã±ËØ≠ÁéØÂ¢É‰∏≠„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´1665‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑ‰∏âÂÖÉÁªÑÔºåËÆ∞ÂΩï‰∫ÜÂàõ‰ΩúËøáÁ®ã‰∏≠ÁöÑÊé®ÁêÜÂíåÊúÄÁªàÊñáÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂàõÊÑèÂÜô‰ΩúÁöÑÊàêÂäü‰æùËµñ‰∫éÂèô‰∫ãÈÄªËæëÂíåËØ≠Ë®ÄË°®ËææÁöÑÁªìÂêà„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13217",
            "title": "LLM-guided Hierarchical Retrieval",
            "url": "https://huggingface.co/papers/2510.13217",
            "abstract": "LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
            "score": 11,
            "issue_id": 6467,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "60e89137064e6fba",
            "authors": [
                "Nilesh Gupta",
                "Wei-Cheng Chang",
                "Ngot Bui",
                "Cho-Jui Hsieh",
                "Inderjit S. Dhillon"
            ],
            "affiliations": [
                "Google",
                "UCLA",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13217.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üå≥",
                "ru": {
                    "title": "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é",
                    "desc": "LATTICE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è information retrieval, –∫–æ—Ç–æ—Ä—ã–π –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–∏–¥–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ –æ—Ñ–ª–∞–π–Ω —Å—Ç—Ä–æ–∏—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∏–ª–∏ –¥–∏–≤–∏–∑–∏–≤–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é, –∑–∞—Ç–µ–º LLM –Ω–∞–≤–∏–≥–∏—Ä—É–µ—Ç –ø–æ —ç—Ç–æ–º—É –¥–µ—Ä–µ–≤—É –æ–Ω–ª–∞–π–Ω. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—Ö–æ–¥–∞ –¥–µ—Ä–µ–≤–∞, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–ª–∏–±—Ä—É–µ—Ç —à—É–º–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç LLM –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏—Ö –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –º–µ—Ç—Ä–∏–∫—É. –ü–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ BRIGHT –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, —É–ª—É—á—à–∞—è Recall@100 –Ω–∞ 9% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º zero-shot baseline."
                },
                "en": {
                    "title": "LATTICE: Navigating Large Document Collections with Semantic Precision",
                    "desc": "LATTICE is a hierarchical retrieval framework designed to improve the efficiency and accuracy of reasoning over large document collections. It organizes documents into a semantic tree structure, allowing for logarithmic search complexity during retrieval. The framework operates in two phases: an offline phase that builds the semantic hierarchy and an online phase where a search LLM navigates this structure. By using a novel traversal algorithm to calibrate relevance scores, LATTICE achieves state-of-the-art performance on reasoning-intensive benchmarks without requiring extensive training."
                },
                "zh": {
                    "title": "LATTICEÔºöÈ´òÊïàÁöÑÂ±ÇÊ¨°Ê£ÄÁ¥¢Ê°ÜÊû∂",
                    "desc": "LATTICEÊòØ‰∏ÄÁßçÂ±ÇÊ¨°Ê£ÄÁ¥¢Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËØ≠‰πâÊ†ëÁªìÊûÑÂíåÈÅçÂéÜÁÆóÊ≥ïÊèêÈ´òÂØπÂ§ßÂûãÊñáÊ°£ÈõÜÂêàÁöÑÊé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê°ÜÊû∂ÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÁ¶ªÁ∫øÈò∂ÊÆµÂ∞ÜÊñáÊ°£ÁªÑÁªáÊàêËØ≠‰πâÂ±ÇÊ¨°ÔºåÂú®Á∫øÈò∂ÊÆµÂàôÈÄöËøáÊêúÁ¥¢LLMÂú®Ê†ëÁªìÊûÑ‰∏≠ÂØºËà™„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÊ®°ÂûãÁöÑÁõ∏ÂÖ≥ÊÄßÂà§Êñ≠Âô™Â£∞Âíå‰∏ä‰∏ãÊñá‰æùËµñÊÄßÔºåLATTICEÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÅçÂéÜÁÆóÊ≥ïÔºåËÉΩÂ§ü‰ªéÂ±ÄÈÉ®LLMËæìÂá∫‰∏≠‰º∞ËÆ°Ê†°ÂáÜÁöÑÊΩúÂú®Áõ∏ÂÖ≥ÊÄßÂàÜÊï∞„ÄÇËØ•Ê°ÜÊû∂Âú®Êé®ÁêÜÂØÜÈõÜÂûãÁöÑBRIGHTÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÈõ∂-shotÊÄßËÉΩÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊîπËøõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14276",
            "title": "Qwen3Guard Technical Report",
            "url": "https://huggingface.co/papers/2510.14276",
            "abstract": "Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
            "score": 10,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "9d825e50f189ea5b",
            "authors": [
                "Haiquan Zhao",
                "Chenhan Yuan",
                "Fei Huang",
                "Xiaomeng Hu",
                "Yichang Zhang",
                "An Yang",
                "Bowen Yu",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin",
                "Baosong Yang",
                "Chen Cheng",
                "Jialong Tang",
                "Jiandong Jiang",
                "Jianwei Zhang",
                "Jijie Xu",
                "Ming Yan",
                "Minmin Sun",
                "Pei Zhang",
                "Pengjun Xie",
                "Qiaoyu Tang",
                "Qin Zhu",
                "Rong Zhang",
                "Shibin Wu",
                "Shuo Zhang",
                "Tao He",
                "Tianyi Tang",
                "Tingyu Xia",
                "Wei Liao",
                "Weizhou Shen",
                "Wenbiao Yin",
                "Wenmeng Zhou",
                "Wenyuan Yu",
                "Xiaobin Wang",
                "Xiaodong Deng",
                "Xiaodong Xu",
                "Xinyu Zhang",
                "Yang Liu",
                "Yeqiu Li",
                "Yi Zhang",
                "Yong Jiang",
                "Yu Wan",
                "Yuxin Zhou"
            ],
            "affiliations": [
                "Anthropic",
                "Meta-AI",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14276.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#alignment",
                    "#training",
                    "#ethics",
                    "#multilingual",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –∑–∞—â–∏—Ç–∞ LLM —Å —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏",
                    "desc": "Qwen3Guard ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ –≤ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö. Generative Qwen3Guard –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π, —Å–ø–æ—Ä–Ω—ã–π, –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π) –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –¥–µ–ª–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. Stream Qwen3Guard –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–º–µ—à–∏–≤–∞—Ç—å—Å—è –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞. –ú–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç 119 —è–∑—ã–∫–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ç—Ä—ë—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö (0.6B, 4B, 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö, –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
                },
                "en": {
                    "title": "Multilingual Safety for Language Models: Real-Time Monitoring and Fine-Grained Judgments",
                    "desc": "Qwen3Guard presents advanced safety guardrail models designed for large language models (LLMs) that enhance output safety through multilingual support. It addresses the limitations of existing models by providing fine-grained tri-class judgments (safe, controversial, unsafe) instead of just binary classifications, allowing for better alignment with diverse safety policies. Additionally, it introduces real-time token-level safety monitoring, enabling timely interventions during text generation, which is crucial for preventing harmful outputs. The models are scalable, available in multiple sizes, and support a wide range of languages, ensuring effective safety moderation in global applications."
                },
                "zh": {
                    "title": "Â§öËØ≠Ë®ÄÂÆâÂÖ®Èò≤Êä§ÔºåÂÆûÊó∂ÁõëÊéßËæìÂá∫ÂÆâÂÖ®",
                    "desc": "Qwen3Guard ÊòØ‰∏ÄÁßçÂ§öËØ≠Ë®ÄÂÆâÂÖ®Èò≤Êä§Ê®°ÂûãÔºåËÉΩÂ§üËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑ‰∏âÁ±ªÂà§Êñ≠ÂíåÂÆûÊó∂ÁöÑ‰ª§ÁâåÁ∫ßÂÆâÂÖ®ÁõëÊéß„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢òÔºö‰∏ÄÊòØÊèê‰æõÊõ¥ÁªÜËá¥ÁöÑÂÆâÂÖ®ÂàÜÁ±ªÔºàÂÆâÂÖ®„ÄÅ‰∫âËÆÆ„ÄÅ‰∏çÂÆâÂÖ®ÔºâÔºåËÄå‰∏çÊòØÁÆÄÂçïÁöÑ‰∫åÂÖÉÊ†áÁ≠æÔºõ‰∫åÊòØÊîØÊåÅÂú®ÊñáÊú¨ÁîüÊàêËøáÁ®ã‰∏≠ËøõË°åÂÆûÊó∂ÂÆâÂÖ®Ê£ÄÊü•ÔºåÈÅøÂÖç‰∫ÜÊúâÂÆ≥ËæìÂá∫ÁöÑÈ£éÈô©„ÄÇËØ•Ê®°ÂûãÊîØÊåÅ119ÁßçËØ≠Ë®ÄÂíåÊñπË®ÄÔºåÈÄÇÁî®‰∫éÂÖ®ÁêÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÁÆ°ÁêÜ„ÄÇÁªèËøáËØÑ‰º∞ÔºåQwen3Guard Âú®ÂÆâÂÖ®ÂàÜÁ±ªÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14980",
            "title": "Agentic Design of Compositional Machines",
            "url": "https://huggingface.co/papers/2510.14980",
            "abstract": "State-of-the-art LLMs are benchmarked in a machine design testbed, BesiegeField, highlighting the need for reinforcement learning to improve spatial reasoning, strategic assembly, and instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
            "score": 9,
            "issue_id": 6481,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "a0f11b00ed15f95e",
            "authors": [
                "Wenqian Zhang",
                "Weiyang Liu",
                "Zhen Liu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The Chinese University of Hong Kong (Shenzhen)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14980.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#dataset",
                    "#games"
                ],
                "emoji": "üîß",
                "ru": {
                    "title": "–£—á–∏–º LLM –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –º–∞—à–∏–Ω—ã –∫–∞–∫ –∏–Ω–∂–µ–Ω–µ—Ä—ã",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É BesiegeField –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–≥—Ä—ã Besiege –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–∞—à–∏–Ω—ã –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM —Å –∞–≥–µ–Ω—Ç–Ω—ã–º–∏ workflow –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –∏–º –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è, —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Å–±–æ—Ä–∫–µ –∏ —Ç–æ—á–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –¢–µ–∫—É—â–∏–µ open-source –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º –¥–∏–∑–∞–π–Ω–µ –º–∞—à–∏–Ω —Å —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Å–∏–º—É–ª—è—Ü–∏–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reinforcement learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ç–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, —Å–æ–∑–¥–∞–≤ –Ω–∞—á–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø—Ä–æ–≤–µ–¥—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å RL-—Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–æ–º."
                },
                "en": {
                    "title": "Empowering LLMs for Machine Design with Reinforcement Learning",
                    "desc": "This paper explores the capabilities of large language models (LLMs) in the context of machine design using a new testbed called BesiegeField. The study emphasizes the importance of reinforcement learning (RL) to enhance LLMs' abilities in spatial reasoning, strategic assembly, and following instructions. By benchmarking these models in a simulated environment, the authors identify the limitations of current open-source LLMs in performing complex design tasks. The paper also discusses the creation of a cold-start dataset and RL finetuning experiments to address these challenges and improve LLM performance in machine design."
                },
                "zh": {
                    "title": "Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÊú∫Âô®ËÆæËÆ°ÁöÑÊú™Êù•",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊú∫Âô®ËÆæËÆ°‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Á©∫Èó¥Êé®ÁêÜ„ÄÅÊàòÁï•ÁªÑË£ÖÂíåÈÅµÂæ™Êåá‰ª§ÁöÑËÉΩÂäõÊñπÈù¢„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âêç‰∏∫BesiegeFieldÁöÑÊµãËØïÂπ≥Âè∞ÔºåËØ•Âπ≥Âè∞Âü∫‰∫éÊú∫Âô®ÊûÑÂª∫Ê∏∏ÊàèBesiegeÔºåÊîØÊåÅÂü∫‰∫éÈÉ®‰ª∂ÁöÑÊûÑÂª∫ÂíåÁâ©ÁêÜÊ®°Êãü„ÄÇÈÄöËøáÂØπÂΩìÂâçÊúÄÂÖàËøõÁöÑLLMsËøõË°åÂü∫ÂáÜÊµãËØïÔºåÊàë‰ª¨ÂèëÁé∞ÂÆÉ‰ª¨Âú®Êú∫Âô®ËÆæËÆ°‰ªªÂä°‰∏≠Â≠òÂú®‰∏çË∂≥‰πãÂ§Ñ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰Ωú‰∏∫ÊîπËøõÁöÑÈÄîÂæÑÔºåÂπ∂ËøõË°å‰∫ÜÁõ∏ÂÖ≥ÂÆûÈ™å‰ª•Â∫îÂØπËØ≠Ë®Ä„ÄÅÊú∫Âô®ËÆæËÆ°ÂíåÁâ©ÁêÜÊé®ÁêÜ‰πãÈó¥ÁöÑÊåëÊàò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14616",
            "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
            "url": "https://huggingface.co/papers/2510.14616",
            "abstract": "Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
            "score": 9,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "184cb21903d17746",
            "authors": [
                "Shuangshuang Ying",
                "Yunwen Li",
                "Xingwei Qu",
                "Xin Li",
                "Sheng Jin",
                "Minghao Liu",
                "Zhoufutu Wen",
                "Xeron Du",
                "Tianyu Zheng",
                "Yichi Zhang",
                "Letian Ni",
                "Yuyang Cheng",
                "Qiguang Chen",
                "Jingzhe Ding",
                "Shengda Long",
                "Wangchunshu Zhou",
                "Jiazhan Feng",
                "Wanjun Zhong",
                "Libo Qin",
                "Ge Zhang",
                "Wenhao Huang",
                "Wanxiang Che",
                "Chenghua Lin"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14616.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#reasoning",
                    "#low_resource",
                    "#dataset",
                    "#story_generation"
                ],
                "emoji": "‚úçÔ∏è",
                "ru": {
                    "title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ WritingPreferenceBench ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1800 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, –≥–¥–µ —É–±—Ä–∞–ª–∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ—Å—Ç–∞–≤–∏–ª–∏ —Ç–æ–ª—å–∫–æ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ reward models –ø–æ–∫–∞–∑–∞–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Å–µ–≥–æ 52.7%, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —è–≤–Ω—ã–º–∏ —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–æ—Å—Ç–∏–≥–ª–∏ 81.8% —Ç–æ—á–Ω–æ—Å—Ç–∏. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ —Å 8B –¥–æ 27B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ —É–ª—É—á—à–∏–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∞ —Ä–∞–∑–±—Ä–æ—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∂–∞–Ω—Ä–∞–º–∏ –¥–æ—Å—Ç–∏–≥–∞–ª –æ—Ç 18% –¥–æ 82% –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π RLHF —É—á–∏—Ç—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –æ—à–∏–±–∫–∏, –∞ –Ω–µ –ø–æ–Ω–∏–º–∞—Ç—å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤—Ä–æ–¥–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ ‚Äî –¥–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ –Ω–µ –ø—Ä—è–º–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è."
                },
                "en": {
                    "title": "Unlocking Creativity: Reasoning Chains Enhance Preference Learning",
                    "desc": "This paper discusses the limitations of current preference learning methods in evaluating creative writing, particularly when objective quality signals are absent. It introduces a new dataset, WritingPreferenceBench, which includes 1,800 human-annotated preference pairs across various genres. The study finds that traditional sequence-based reward models and zero-shot language models perform poorly, achieving only around 52-54% accuracy. In contrast, generative reward models that utilize explicit reasoning chains significantly outperform these methods, achieving 81.8% accuracy, highlighting the importance of intermediate reasoning in assessing subjective quality in creative writing."
                },
                "zh": {
                    "title": "ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºöÂàõÊÑèÂÜô‰ΩúÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÂ•ñÂä±Ê®°ÂûãÂú®ÂàõÊÑèÂÜô‰ΩúÂÅèÂ•ΩÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞ÔºåÂèëÁé∞ÂÖ∂‰ºò‰∫éÂü∫‰∫éÂ∫èÂàóÁöÑÂ•ñÂä±Ê®°ÂûãÂíåÈõ∂-shotËØ≠Ë®ÄÊ®°Âûã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂéªÈô§ÂÆ¢ËßÇË¥®Èáè‰ø°Âè∑ÂêéÔºåÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôç„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜWritingPreferenceBenchÊï∞ÊçÆÈõÜÔºåÂåÖÂê´1800ÂØπ‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂÅèÂ•ΩÂØπÔºåÊ∂µÁõñ8ÁßçÂàõÊÑèÂÜô‰ΩúÁ±ªÂûã„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÁîüÊàêÂ•ñÂä±Ê®°ÂûãÈÄöËøáÊòéÁ°ÆÁöÑÊé®ÁêÜÈìæÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÁéáÔºåÂº∫Ë∞É‰∫Ü‰∏≠Èó¥Êé®ÁêÜÂú®ÊçïÊçâ‰∏ªËßÇË¥®Èáè‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14240",
            "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in\n  the Wild",
            "url": "https://huggingface.co/papers/2510.14240",
            "abstract": "LiveResearchBench and DeepEval provide a comprehensive framework for evaluating deep research systems across various domains, focusing on real-time web search, synthesis, and citation-grounded long-form reports.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.",
            "score": 9,
            "issue_id": 6484,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "3bd7c61a80eb7624",
            "authors": [
                "Jiayu Wang",
                "Yifei Ming",
                "Riya Dulepet",
                "Qinglin Chen",
                "Austin Xu",
                "Zixuan Ke",
                "Frederic Sala",
                "Aws Albarghouthi",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "Stanford University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14240.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#survey",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LiveResearchBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 100 —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-—Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ–∏—Å–∫–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Å–æ—Ç–µ–Ω –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω DeepEval ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –ø–æ–ª–Ω–æ—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, —Ç–æ—á–Ω–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ 17 –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º, –≤–∫–ª—é—á–∞—è single-agent –∏ multi-agent —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—è–≤–∏–ª–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–∞–¥—ë–∂–Ω—ã—Ö AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π."
                },
                "en": {
                    "title": "Evaluating Deep Research Systems with LiveResearchBench and DeepEval",
                    "desc": "The paper introduces LiveResearchBench and DeepEval, frameworks designed to evaluate deep research systems that generate comprehensive reports by synthesizing information from live web sources. It emphasizes the need for user-centric, dynamic, unambiguous, and multi-faceted tasks to assess these systems effectively. LiveResearchBench consists of 100 expert-curated tasks that require extensive real-time web search, while DeepEval provides a suite of evaluation protocols focusing on content quality and report accuracy. The study evaluates 17 advanced deep research systems, identifying their strengths and weaknesses to improve future research capabilities."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶Á†îÁ©∂Á≥ªÁªüËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜLiveResearchBenchÂíåDeepEvalËøô‰∏§‰∏™Ê°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞Ê∑±Â∫¶Á†îÁ©∂Á≥ªÁªüÂú®Â§ö‰∏™È¢ÜÂüüÁöÑË°®Áé∞ÔºåÁâπÂà´ÊòØÂú®ÂÆûÊó∂ÁΩëÁªúÊêúÁ¥¢Âíå‰ø°ÊÅØÁªºÂêàÊñπÈù¢„ÄÇÊ∑±Â∫¶Á†îÁ©∂ÊòØÊåáÈÄöËøáÊêúÁ¥¢ÂíåÁªºÂêàÊù•Ëá™Êï∞Áôæ‰∏™ÂÆûÊó∂ÁΩëÁªúÊ∫êÁöÑ‰ø°ÊÅØÔºåÁîüÊàêÂÖ®Èù¢‰∏îÊúâÂºïÁî®‰æùÊçÆÁöÑÊä•Âëä„ÄÇ‰∏∫‰∫ÜÊúâÊïàËØÑ‰º∞Ëøô‰∏ÄËÉΩÂäõÔºåÊñáÁ´†ÊèêÂá∫‰∫ÜÂõõ‰∏™ÂéüÂàôÔºö‰ª•Áî®Êà∑‰∏∫‰∏≠ÂøÉ„ÄÅÂä®ÊÄÅÊõ¥Êñ∞„ÄÅÊòéÁ°ÆÊó†Ê≠ß‰πâ‰ª•ÂèäÂ§öÊñπÈù¢ÁöÑÊêúÁ¥¢ÈúÄÊ±Ç„ÄÇÈÄöËøáËøô‰∏§‰∏™Ê°ÜÊû∂ÔºåÁ†îÁ©∂ËÄÖËÉΩÂ§üÁ≥ªÁªüÂú∞ËØÑ‰º∞Ê∑±Â∫¶Á†îÁ©∂Á≥ªÁªüÁöÑÊÄßËÉΩÔºåËØÜÂà´ÂÖ∂‰ºòÁº∫ÁÇπÔºåÂπ∂Êé®Âä®ËØ•È¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14807",
            "title": "SimKO: Simple Pass@K Policy Optimization",
            "url": "https://huggingface.co/papers/2510.14807",
            "abstract": "Simple Pass@K Optimization (SimKO) addresses over-concentration in reinforcement learning with verifiable rewards (RLVR) by asymmetrically adjusting token probabilities, enhancing exploration and pass@K performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.",
            "score": 8,
            "issue_id": 6480,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "31835f5c7d68f61d",
            "authors": [
                "Ruotian Peng",
                "Yi Ren",
                "Zhouliang Yu",
                "Weiyang Liu",
                "Yandong Wen"
            ],
            "affiliations": [
                "CUHK",
                "University of British Columbia",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14807.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ RLVR —Å –ø–æ–º–æ—â—å—é SimKO",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –¥–ª—è LLM. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Simple Pass@K Optimization (SimKO), –∫–æ—Ç–æ—Ä—ã–π –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å pass@K. SimKO —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ–ø-K –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø—Ä–∏ –≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö –∏ —Å–Ω–∏–∂–∞–µ—Ç –¥–ª—è —Ç–æ–ø-1 –ø—Ä–∏ –Ω–µ–≤–µ—Ä–Ω—ã—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–≤—ã—Å–∏—Ç—å pass@K –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ª–æ–≥–∏–∫–µ."
                },
                "en": {
                    "title": "Boosting Exploration in RLVR with SimKO",
                    "desc": "The paper introduces Simple Pass@K Optimization (SimKO) to tackle the issue of over-concentration in reinforcement learning with verifiable rewards (RLVR). It identifies that existing RLVR methods tend to favor exploitation, leading to improved pass@1 but poorer pass@K performance for K greater than 1. The authors analyze token-level probability distributions and find that the top-1 candidate accumulates too much probability, which negatively impacts exploration. SimKO addresses this by asymmetrically adjusting probabilities, enhancing exploration and improving pass@K outcomes across various benchmarks."
                },
                "zh": {
                    "title": "ÁÆÄÂçïPass@K‰ºòÂåñÔºöÊèêÂçáÊé¢Á¥¢ËÉΩÂäõÁöÑËß£ÂÜ≥ÊñπÊ°à",
                    "desc": "ÁÆÄÂçïÁöÑPass@K‰ºòÂåñÔºàSimKOÔºâÊó®Âú®Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâÊâÄÂ∏¶Êù•ÁöÑËøáÂ∫¶ÈõÜ‰∏≠ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑRLVRÊñπÊ≥ïÂæÄÂæÄÂÅèÂêë‰∫éÂà©Áî®ËÄåÈùûÊé¢Á¥¢ÔºåÂØºËá¥Âú®pass@1Ë°®Áé∞ËâØÂ•Ω‰ΩÜÂú®pass@KÔºàK>1ÔºâË°®Áé∞‰∏ç‰Ω≥„ÄÇÈÄöËøáÂàÜÊûêËÆ≠ÁªÉÂä®ÊÄÅÔºåÊàë‰ª¨ÂèëÁé∞Ê¶ÇÁéáÈõÜ‰∏≠ÊïàÂ∫î‰ΩøÂæóÊéíÂêçÁ¨¨‰∏ÄÁöÑÂÄôÈÄâÈ°πÊ¶ÇÁéá‰∏çÊñ≠Â¢ûÂä†ÔºåÊäëÂà∂‰∫ÜÂÖ∂‰ªñÂÄôÈÄâÈ°πÁöÑÊ¶ÇÁéá„ÄÇSimKOÈÄöËøá‰∏çÂØπÁß∞Ë∞ÉÊï¥ÂÄôÈÄâÈ°πÁöÑÊ¶ÇÁéáÔºåÂ¢ûÂº∫‰∫ÜÊé¢Á¥¢ËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´ò‰∫Üpass@KÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14300",
            "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
            "url": "https://huggingface.co/papers/2510.14300",
            "abstract": "AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
            "score": 8,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "7c55fed8dcc26cc0",
            "authors": [
                "Weijie Shen",
                "Yitian Liu",
                "Yuhao Wu",
                "Zhixuan Liang",
                "Sijia Gu",
                "Dehui Wang",
                "Tian Nian",
                "Lei Xu",
                "Yusen Qin",
                "Jiangmiao Pang",
                "Xinping Guan",
                "Xiaokang Yang",
                "Yao Mu"
            ],
            "affiliations": [
                "D-Robotics",
                "Key Laboratory of System Control and Information Processing, Ministry of Education of China",
                "MoE key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
                "School of Automation and Intelligent Sensing, Shanghai Jiao Tong University",
                "School of Computer Science, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory",
                "Shanghai Key Laboratory of Integrated Administration Technologies for Information Security",
                "The University of Hong Kong",
                "Tongji University",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14300.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#agi",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏",
                    "desc": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AdaMoE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ Mixture-of-Experts –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –∏—Ö –≤–µ—Å–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ, –∞ –Ω–µ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´–ø–æ–±–µ–¥–∏—Ç–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –≤—Å—ë¬ª. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 1.8% –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ LIBERO, 9.3% –≤ RoboTwin –∏ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ 21.5% –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ü–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–µ—Ñ–∏—Ü–∏—Ç–∞ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ VLA –º–æ–¥–µ–ª–µ–π —Å –Ω—É–ª—è."
                },
                "en": {
                    "title": "Collaborative Expertise for Enhanced Robotic Manipulation",
                    "desc": "AdaMoE is a Mixture-of-Experts architecture designed to enhance Vision-Language-Action (VLA) models for robotic manipulation. It effectively utilizes pretrained weights from dense VLA models to improve computational efficiency and performance. By implementing a decoupling technique, AdaMoE allows for collaborative expert utilization, where multiple experts can contribute to decision-making without competing against each other. This innovative approach leads to significant performance improvements in real-world robotic tasks, demonstrating its practical effectiveness."
                },
                "zh": {
                    "title": "AdaMoEÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊû∂ÊûÑ",
                    "desc": "AdaMoEÊòØ‰∏ÄÁßç‰∏ìÂÆ∂Ê∑∑ÂêàÊû∂ÊûÑÔºåÊó®Âú®ÈÄöËøáÂà©Áî®È¢ÑËÆ≠ÁªÉÊùÉÈáçÂíåÊèêÈ´òËÆ°ÁÆóÊïàÁéáÊù•Â¢ûÂº∫ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫Ü‰ªéÂ§¥ËÆ≠ÁªÉVLAÊ®°ÂûãÊâÄÈúÄÁöÑÈ´òËÆ°ÁÆóËµÑÊ∫êÂíåÊï∞ÊçÆÈõÜÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇAdaMoEÈÄöËøáÂ∞ÜÂâçÈ¶àÂ±ÇÊõøÊç¢‰∏∫Á®ÄÁñèÊøÄÊ¥ªÁöÑ‰∏ìÂÆ∂Â±ÇÔºå‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÂÆπÈáè‰∏éËÆ°ÁÆóÊïàÁéáÁöÑÂπ≥Ë°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdaMoEÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞‰∫Ü21.5%ÁöÑÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13054",
            "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
            "url": "https://huggingface.co/papers/2510.13054",
            "abstract": "",
            "score": 8,
            "issue_id": 6468,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "6612b26b86cfae4d",
            "authors": [
                "Ankit Goyal",
                "Hugo Hadfield",
                "Xuning Yang",
                "Valts Blukis",
                "Fabio Ramos"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13054.jpg",
            "data": {
                "categories": [],
                "emoji": "üéØ",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –í–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤, –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏. –ü–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ LLM –∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "ÊèêÂçáÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÂáÜÁ°ÆÊÄß‰∏éÊïàÁéá",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂíåÁâπÂæÅÈÄâÊã©Êù•Â¢ûÂº∫Â≠¶‰π†ËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊäÄÊúØ„ÄÇÊúÄÁªàÔºåËøôÈ°πÁ†îÁ©∂‰∏∫Êú∫Âô®Â≠¶‰π†È¢ÜÂüüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÂíåÂ∑•ÂÖ∑„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12764",
            "title": "AnyUp: Universal Feature Upsampling",
            "url": "https://huggingface.co/papers/2510.12764",
            "abstract": "AnyUp is a feature-agnostic upsampling method that generalizes across different vision features and resolutions without requiring re-training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.",
            "score": 8,
            "issue_id": 6481,
            "pub_date": "2025-10-14",
            "pub_date_card": {
                "ru": "14 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 14",
                "zh": "10Êúà14Êó•"
            },
            "hash": "c797c8b259a33385",
            "authors": [
                "Thomas Wimmer",
                "Prune Truong",
                "Marie-Julie Rakotosaona",
                "Michael Oechsle",
                "Federico Tombari",
                "Bernt Schiele",
                "Jan Eric Lenssen"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google",
                "Max Planck Institute for Informatics",
                "TU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12764.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#inference"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥ –¥–ª—è –ª—é–±—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è",
                    "desc": "AnyUp ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ñ–∏—á–µ–π (DINO, CLIP –∏ –¥—Ä—É–≥–∏–º–∏) –∏ –ª—é–±—ã–º–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è–º–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ feature extractor, AnyUp –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —è–≤–ª—è–µ—Ç—Å—è feature-agnostic, —Ç–æ –µ—Å—Ç—å –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ downstream –∑–∞–¥–∞—á–∏."
                },
                "en": {
                    "title": "AnyUp: Universal Upsampling for Vision Features Without Retraining",
                    "desc": "AnyUp is a novel upsampling method designed to enhance vision features without the need for retraining on specific encoders. Unlike traditional learning-based upsamplers that require separate training for each feature extractor, AnyUp operates in a feature-agnostic manner, allowing it to generalize across various feature types and resolutions. This approach not only improves the quality of upsampled features but also maintains the semantic integrity of the original features. Our experiments demonstrate that AnyUp achieves state-of-the-art performance while being efficient and applicable to a diverse set of downstream tasks."
                },
                "zh": {
                    "title": "AnyUpÔºöÊó†ÁâπÂæÅ‰æùËµñÁöÑÈ´òÊïà‰∏äÈááÊ†∑ÊñπÊ≥ï",
                    "desc": "AnyUpÊòØ‰∏ÄÁßçÁâπÂæÅÊó†ÂÖ≥ÁöÑ‰∏äÈááÊ†∑ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂêåÁöÑËßÜËßâÁâπÂæÅÂíåÂàÜËæ®Áéá‰∏ã‰ΩøÁî®ÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÂ≠¶‰π†ÁöÑÁâπÂæÅ‰∏äÈááÊ†∑Âô®ÔºåÂ¶ÇDINOÊàñCLIPÔºåÈúÄË¶ÅÈíàÂØπÊØè‰∏™ÁâπÂæÅÊèêÂèñÂô®ËøõË°åÈáçÊñ∞ËÆ≠ÁªÉÔºåÂõ†Ê≠§Âú®Êé®ÁêÜÊó∂Êó†Ê≥ïÊ≥õÂåñÂà∞‰∏çÂêåÁöÑÁâπÂæÅÁ±ªÂûã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊé®ÁêÜÊó∂ÁöÑÁâπÂæÅÊó†ÂÖ≥‰∏äÈááÊ†∑Êû∂ÊûÑÔºå‰ª•ÁºìËß£Ëøô‰∏ÄÈôêÂà∂Âπ∂ÊèêÈ´ò‰∏äÈááÊ†∑Ë¥®Èáè„ÄÇÂú®ÂÆûÈ™å‰∏≠ÔºåAnyUpÂú®‰∏äÈááÊ†∑ÁâπÂæÅÊñπÈù¢ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåËÉΩÂ§üÊ≥õÂåñÂà∞‰∏çÂêåÁöÑÁâπÂæÅÁ±ªÂûãÔºåÂπ∂Âú®È´òÊïàÁöÑÂêåÊó∂‰øùÊåÅÁâπÂæÅËØ≠‰πâÔºåÊòì‰∫éÂ∫îÁî®‰∫éÂπøÊ≥õÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14974",
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "url": "https://huggingface.co/papers/2510.14974",
            "abstract": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models (pi-Flow). pi-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard ell_2 flow matching loss. By simply mimicking the teacher's behavior, pi-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256^2, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
            "score": 6,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "8bb31ac0ea75d5e3",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "affiliations": [
                "Adobe Research",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14974.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç pi-Flow ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ flow-based –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä—è–º–æ–≥–æ –ø—É—Ç–∏ –∫ –∏—Ç–æ–≥–æ–≤–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç policy (—Å—Ç—Ä–∞—Ç–µ–≥–∏—é), –∫–æ—Ç–æ—Ä–∞—è –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–∫–æ—Ä–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–∞—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç FID 2.85 –Ω–∞ ImageNet –ø—Ä–∏ –æ–¥–Ω–æ–π –æ—Ü–µ–Ω–∫–µ —Å–µ—Ç–∏ –∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç–∏–ø–∞ FLUX.1 –∏ Qwen-Image."
                },
                "en": {
                    "title": "Dynamic Policies for Enhanced Image Generation",
                    "desc": "This paper introduces policy-based flow models (pi-Flow) that enhance image generation by effectively transferring knowledge from teacher models to student models. By modifying the output layer of the student model to predict a network-free policy, pi-Flow generates dynamic flow velocities that improve both the speed and accuracy of the image generation process. The authors propose a novel imitation distillation method that aligns the student's policy with the teacher's trajectory, thus avoiding the common trade-off between quality and diversity in generated images. The results demonstrate that pi-Flow achieves superior performance on benchmark datasets, outperforming existing methods in both image quality and diversity."
                },
                "zh": {
                    "title": "Âü∫‰∫éÁ≠ñÁï•ÁöÑÊµÅÊ®°ÂûãÔºöÊèêÂçáÂõæÂÉèÁîüÊàêË¥®Èáè‰∏éÂ§öÊ†∑ÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁ≠ñÁï•ÁöÑÊµÅÊ®°ÂûãÔºàpi-FlowÔºâÔºåÊó®Âú®ÊèêÈ´òÂõæÂÉèÁîüÊàêÁöÑÊïàÁéáÂíåË¥®Èáè„ÄÇÈÄöËøáÂ∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜÊèêÁÇºÂà∞Â≠¶ÁîüÊ®°Âûã‰∏≠Ôºåpi-FlowËÉΩÂ§üÂä®ÊÄÅÈ¢ÑÊµãÊµÅÈÄüÔºå‰ªéËÄåÊîπÂñÑÁîüÊàêÂõæÂÉèÁöÑÂ§öÊ†∑ÊÄßÂíåË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰øÆÊîπÂ≠¶ÁîüÊ®°ÂûãÁöÑËæìÂá∫Â±ÇÔºå‰ΩøÂÖ∂Âú®‰∏Ä‰∏™Êó∂Èó¥Ê≠•ÈïøÂÜÖÈ¢ÑÊµãÊó†ÁΩëÁªúÁ≠ñÁï•ÔºåÂπ∂Âú®Êú™Êù•ÁöÑÂ≠êÊ≠•È™§‰∏≠ÁîüÊàêÂä®ÊÄÅÊµÅÈÄü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåpi-FlowÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂá†Ê≠•ÁîüÊàêÊñπÊ≥ïÔºåÂêåÊó∂ÈÅøÂÖç‰∫ÜË¥®Èáè‰∏éÂ§öÊ†∑ÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14955",
            "title": "RealDPO: Real or Not Real, that is the Preference",
            "url": "https://huggingface.co/papers/2510.14955",
            "abstract": "RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
            "score": 6,
            "issue_id": 6472,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "9d501f9fad1593d2",
            "authors": [
                "Guo Cheng",
                "Danni Yang",
                "Ziqi Huang",
                "Jianlou Si",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14955.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–µ–ª–∞–µ—Ç AI-–¥–≤–∏–∂–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏",
                    "desc": "RealDPO ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ –¥–≤–∏–∂–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ supervised fine-tuning, –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç Direct Preference Optimization (DPO), —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ —Å –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç RealAction-5K —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ª—é–¥–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏–π, –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "Enhancing Motion Realism in Video Generation with RealDPO",
                    "desc": "RealDPO is a new method for improving how video generative models create realistic motions by using real-world data for preference learning. It addresses the challenge of generating smooth and contextually accurate movements, which has been a limitation in current models. By using Direct Preference Optimization (DPO) and a special loss function, RealDPO allows the model to learn from real videos and correct its mistakes iteratively. The introduction of the RealAction-5K dataset provides high-quality examples of human activities, further enhancing the model's ability to produce lifelike motion in videos."
                },
                "zh": {
                    "title": "ÁúüÂÆûÊï∞ÊçÆÈ©±Âä®ÁöÑËøêÂä®ÂêàÊàê‰ºòÂåñ",
                    "desc": "RealDPOÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÅèÂ•ΩÂ≠¶‰π†ËåÉÂºèÔºåÂà©Áî®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÊù•Â¢ûÂº∫ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠ÁöÑËøêÂä®ÁúüÂÆûÊÑü„ÄÇÂÆÉÈÄöËøáÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÂíåËø≠‰ª£Ëá™Êàë‰øÆÊ≠£ÁöÑÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫ÜÁîüÊàêÂ§çÊùÇËøêÂä®Êó∂ÁöÑÊåëÊàò„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâ‰∏çÂêåÔºåRealDPO‰ΩøÁî®ÂÆöÂà∂ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÊèê‰æõÊõ¥ÊúâÊïàÁöÑÂèçÈ¶àÔºå‰ªéËÄåÊèêÈ´òËøêÂä®ÂêàÊàêÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáÂØπÊØîÁúüÂÆûËßÜÈ¢ëÂíåÊ®°ÂûãËæìÂá∫ÁöÑÈîôËØØÔºåRealDPOËÉΩÂ§üÈÄêÊ≠•ÊîπËøõËøêÂä®Ë¥®ÈáèÔºåÊòæËëóÊèêÂçáËßÜÈ¢ëË¥®ÈáèÂíåËøêÂä®ÁúüÂÆûÊÑü„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14211",
            "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
            "url": "https://huggingface.co/papers/2510.14211",
            "abstract": "LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
            "score": 6,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "2fca134be90f8d31",
            "authors": [
                "Beomseok Kang",
                "Jiwon Song",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14211.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#training",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ—ë–≤",
                    "desc": "LiteStage ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –ø—Ä–æ–ø—É—Å–∫–∞ —Å–ª–æ—ë–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –°–∏—Å—Ç–µ–º–∞ —É—á–∏—Ç—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ —ç—Ç–∞–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–ø—É—Å–∫—É —Å–ª–æ—ë–≤, –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–Ω–Ω–µ–≥–æ –≤—ã—Ö–æ–¥–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–æ 1.70 —Ä–∞–∑ –ø—Ä–∏ –ø–æ—Ç–µ—Ä–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–Ω–µ–µ 4%, —á—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –ø—Ä–æ–ø—É—Å–∫–∞ —Å–ª–æ—ë–≤ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Speed Up Multi-Stage Reasoning with LiteStage!",
                    "desc": "LiteStage is a framework designed to improve the efficiency of multi-stage reasoning in small language models by optimizing how layers are used. It addresses the challenges of varying sensitivity to layer skipping and the issue of generating unnecessary output tokens. By implementing a two-part approach that includes an offline search for optimal layer budgets and an online method for early exits based on confidence, LiteStage minimizes latency while maintaining accuracy. Experiments demonstrate that it can significantly speed up processing times with only a small decrease in accuracy compared to previous methods."
                },
                "zh": {
                    "title": "LiteStageÔºöÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶ÁöÑÊô∫ËÉΩÂ±ÇË∑≥ËøáÊ°ÜÊû∂",
                    "desc": "LiteStageÊòØ‰∏Ä‰∏™ÂÖ≥Ê≥®Âª∂ËøüÁöÑÂ±ÇË∑≥ËøáÊ°ÜÊû∂ÔºåÊó®Âú®‰ºòÂåñÂ§öÈò∂ÊÆµÊé®ÁêÜÁöÑÊïàÁéá„ÄÇÂÆÉÈÄöËøáÂàÜÈÖçÊúÄ‰Ω≥Â±ÇÈ¢ÑÁÆóÂíåÊäëÂà∂ÂÜó‰ΩôËæìÂá∫Ê†áËÆ∞ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÂ∞èÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÈò∂ÊÆµÊÄßÁ¶ªÁ∫øÊêúÁ¥¢ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂú®Á∫øÁîüÊàêÊó©ÊúüÈÄÄÂá∫Á≠ñÁï•Ôºå‰ª•Ëß£ÂÜ≥Áé∞ÊúâÊäÄÊúØÂú®ÊïàÁéáÂíåÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLiteStageÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ1.70ÂÄçÁöÑÂä†ÈÄüÔºå‰∏îÂáÜÁ°ÆÊÄßÊçüÂ§±‰Ωé‰∫é4.0%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14978",
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "url": "https://huggingface.co/papers/2510.14978",
            "abstract": "A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "f8c41e0883a370dc",
            "authors": [
                "Nupur Kumari",
                "Sheng-Yu Wang",
                "Nanxuan Zhao",
                "Yotam Nitzan",
                "Yuheng Li",
                "Krishna Kumar Singh",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu",
                "Xun Huang"
            ],
            "affiliations": [
                "Adobe",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14978.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#synthetic",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "‚úÇÔ∏è",
                "ru": {
                    "title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç VLM",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø–∞—Ä—ã –≤—Ö–æ–¥–Ω–æ–µ-—Ü–µ–ª–µ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ). –ú–µ—Ç–æ–¥ –Ω–∞–ø—Ä—è–º—É—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç diffusion –º–æ–¥–µ–ª—å —Å –º–∞–ª—ã–º —á–∏—Å–ª–æ–º —à–∞–≥–æ–≤, —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—è –µ—ë –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç vision-language –º–æ–¥–µ–ª–µ–π (VLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è distribution matching loss (DMD), –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–µ–º, –∏–∑—É—á–µ–Ω–Ω—ã–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ë–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å supervised –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç RL-–ø–æ–¥—Ö–æ–¥—ã –≤—Ä–æ–¥–µ Flow-GRPO."
                },
                "en": {
                    "title": "Unpaired Image Editing: A New Era with Diffusion Models and Vision-Language Feedback",
                    "desc": "This paper introduces a novel training method for image editing models that eliminates the need for paired data, which is often difficult to obtain. The approach utilizes unrolled diffusion models and incorporates feedback from vision-language models (VLMs) to optimize the editing process. By evaluating edits based on natural language instructions, the VLM provides direct gradients for training, ensuring that the edits are both accurate and visually coherent. The proposed method achieves performance comparable to traditional supervised models while avoiding the pitfalls of synthetic training pairs, demonstrating its effectiveness through rigorous benchmarking and ablation studies."
                },
                "zh": {
                    "title": "Êó†ÈÖçÂØπÊï∞ÊçÆÁöÑÂõæÂÉèÁºñËæëÊñ∞ËåÉÂºè",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂÉèÁºñËæëÊ®°ÂûãËÆ≠ÁªÉËåÉÂºèÔºåÂà©Áî®Â±ïÂºÄÁöÑÊâ©Êï£Ê®°ÂûãÂíåËßÜËßâ-ËØ≠Ë®ÄÂèçÈ¶àÔºåËÉΩÂ§üÂú®Ê≤°ÊúâÈÖçÂØπÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∏éÁõëÁù£Ê®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÂõæÂÉèÁºñËæëÊ®°Âûã‰æùËµñ‰∫éÂ§ßÈáèËæìÂÖ•-ÁõÆÊ†áÂØπÁöÑÁõëÁù£ÂæÆË∞ÉÔºåËøôÂú®Êï∞ÊçÆÊî∂ÈõÜ‰∏äÂ≠òÂú®Áì∂È¢à„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÁõ¥Êé•‰ºòÂåñÂá†Ê≠•Êâ©Êï£Ê®°ÂûãÔºåÂπ∂Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèçÈ¶àÔºåÊ∂àÈô§‰∫ÜÂØπÈÖçÂØπÊï∞ÊçÆÁöÑÈúÄÊ±Ç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Ê≤°ÊúâÈÖçÂØπÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩ‰∏é‰ΩøÁî®Â§ßÈáèÈÖçÂØπÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂõæÂÉèÁºñËæëÊâ©Êï£Ê®°ÂûãÁõ∏ÂΩì„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14969",
            "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
            "url": "https://huggingface.co/papers/2510.14969",
            "abstract": "",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "95b0c7bd0991a16b",
            "authors": [
                "Yiming Wang",
                "Da Yin",
                "Yuedong Cui",
                "Ruichen Zheng",
                "Zhiqian Li",
                "Zongyu Lin",
                "Di Wu",
                "Xueqing Wu",
                "Chenchen Ye",
                "Yu Zhou",
                "Kai-Wei Chang"
            ],
            "affiliations": [
                "Harvard University",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14969.jpg",
            "data": {
                "categories": [],
                "emoji": "ü§ù",
                "ru": {
                    "title": "–ö–æ–≥–¥–∞ AI –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî –ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏—Ç—å —á–µ–ª–æ–≤–µ–∫–∞",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ –∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –ø–æ–º–æ—â–∏ —á–µ–ª–æ–≤–µ–∫–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –æ—Ü–µ–Ω–∏—Ç—å —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å. –ö–æ–≥–¥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç –Ω–∏–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞, –º–æ–¥–µ–ª—å –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∑–∞ –ø–æ–º–æ—â—å—é –∫ —á–µ–ª–æ–≤–µ–∫—É –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç AI-—Å–∏—Å—Ç–µ–º—ã –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "ÊèêÂçáÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÁöÑÂàõÊñ∞ÁÆóÊ≥ï",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÁâπÂæÅÈÄâÊã©ÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂáèÂ∞ëÊï∞ÊçÆÁª¥Â∫¶ÔºåÂêåÊó∂‰øùÁïôÈáçË¶Å‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇÈÄöËøá‰ºòÂåñÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåÁ†îÁ©∂ËÄÖÂ∏åÊúõÊé®Âä®Êú∫Âô®Â≠¶‰π†Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14949",
            "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
            "url": "https://huggingface.co/papers/2510.14949",
            "abstract": "A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.  \t\t\t\t\tAI-generated summary \t\t\t\t Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
            "score": 5,
            "issue_id": 6471,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "23527b0261302314",
            "authors": [
                "Yu Zhou",
                "Sohyun An",
                "Haikang Deng",
                "Da Yin",
                "Clark Peng",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Nanyun Peng"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#low_resource",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–∏–∞–ª–µ–∫—Ç—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —à–µ—Å—Ç–∏ –¥–∏–∞–ª–µ–∫—Ç–∞—Ö –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ 4200 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–∞–¥–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 32-48% –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∏–∞–ª–µ–∫—Ç–Ω—ã—Ö —Å–ª–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ fine-tuning –∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤, –¥–∞—é—Ç –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –∏ –º–æ–≥—É—Ç —É—Ö—É–¥—à–∏—Ç—å —Ä–∞–±–æ—Ç—É —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª–∏–ª–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–¥–Ω—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –ø—è—Ç–∏ –¥–∏–∞–ª–µ–∫—Ç–∞—Ö –¥–æ —É—Ä–æ–≤–Ω—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ (+34.4%) –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω—ë–º."
                },
                "en": {
                    "title": "Bridging Dialects: Enhancing Generative Models Without Compromise",
                    "desc": "This paper addresses the challenge of improving multimodal generative models' performance when processing dialectal textual input without harming their effectiveness on Standard American English (SAE). The authors introduce a new benchmark that includes a diverse set of prompts from six English dialects, revealing significant performance drops in existing models when dialect words are used. They propose an innovative encoder-based mitigation strategy that enables models to learn dialect features while maintaining SAE performance. Experimental results demonstrate that their approach can enhance dialect performance significantly, achieving parity with SAE without degrading its quality."
                },
                "zh": {
                    "title": "ÊèêÂçáÊñπË®ÄÁîüÊàêËÉΩÂäõÔºå‰øùÁïôÊ†áÂáÜËã±ËØ≠Ë°®Áé∞",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÂíåÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÁºìËß£Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÂ§öÊ®°ÊÄÅÁîüÊàêÊ®°ÂûãÂú®ÊñπË®ÄÊñáÊú¨ËæìÂÖ•‰∏äÁöÑË°®Áé∞ÔºåÂêåÊó∂‰∏çÂΩ±ÂìçÊ†áÂáÜÁæéÂºèËã±ËØ≠ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê∂µÁõñÂÖ≠ÁßçÂ∏∏ËßÅËã±ËØ≠ÊñπË®ÄÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜÔºåÂπ∂Êî∂ÈõÜ‰∫Ü4200Â§ö‰∏™Áã¨ÁâπÁöÑÊèêÁ§∫ËøõË°åËØÑ‰º∞„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêÊ®°ÂûãÂú®‰ΩøÁî®ÊñπË®ÄËØçÊ±áÊó∂ÔºåÊÄßËÉΩ‰∏ãÈôçÂπÖÂ∫¶ÂèØËææ32.26%Ëá≥48.17%„ÄÇÊàë‰ª¨ÁöÑÁºñÁ†ÅÂô®Á≠ñÁï•ËÉΩÂ§üËÆ©Ê®°ÂûãËØÜÂà´Êñ∞ÁöÑÊñπË®ÄÁâπÂæÅÔºåÂêåÊó∂‰øùÊåÅÊ†áÂáÜÁæéÂºèËã±ËØ≠ÁöÑÊÄßËÉΩÂá†‰πé‰∏çÂèóÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13996",
            "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
            "url": "https://huggingface.co/papers/2510.13996",
            "abstract": "The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible.",
            "score": 5,
            "issue_id": 6471,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "83d28be82371955f",
            "authors": [
                "Lukas Gienapp",
                "Christopher Schr√∂der",
                "Stefan Schweter",
                "Christopher Akiki",
                "Ferdinand Schlatt",
                "Arden Zimmermann",
                "Phillipe Gen√™t",
                "Martin Potthast"
            ],
            "affiliations": [
                "Friedrich-Schiller-Universit√§t Jena",
                "German National Library",
                "Independent Researcher",
                "InfAI and ScaDS.AI",
                "Leipzig University and ScaDS.AI",
                "University of Kassel, hessian.AI, and ScaDS.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13996.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "üá©üá™",
                "ru": {
                    "title": "–ù–µ–º–µ—Ü–∫–∏–µ –æ–±—â–∏–Ω—ã: –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ German Commons ‚Äî –∫—Ä—É–ø–Ω–µ–π—à—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –ª–∏—Ü–µ–Ω–∑–∏—è–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç 154,56 –º–∏–ª–ª–∏–∞—Ä–¥–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ 41 –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Å–µ–º—å –¥–æ–º–µ–Ω–æ–≤: —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ, –Ω–∞—É—á–Ω—ã–µ, –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ, –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ, –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏ –≤–µ–±-—Ç–µ–∫—Å—Ç—ã. –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –ª–∏—Ü–µ–Ω–∑–∏–∏ CC-BY-SA 4.0 –∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–µ, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª–µ–≥–∞–ª—å–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—É–±–ª–∏–∫—É—é—Ç –∫–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –¥–µ–ª–∞—è German Commons –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–º."
                },
                "en": {
                    "title": "Empowering German Language Models with Open Data",
                    "desc": "The German Commons is a comprehensive dataset designed to support the training of German language models by providing openly licensed text. It includes 154.56 billion tokens sourced from 41 different domains, ensuring a diverse range of topics such as legal, scientific, and cultural content. The dataset is meticulously processed to maintain high quality through filtering and deduplication, making it suitable for machine learning applications. By offering legally compliant data, the German Commons fills a significant gap in the availability of German language resources for AI development."
                },
                "zh": {
                    "title": "Âæ∑ÂõΩÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÔºöÂºÄÊîæÂæ∑ËØ≠Ê®°ÂûãÁöÑÂÖ≥ÈîÆ",
                    "desc": "Âæ∑ÂõΩÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÊèê‰æõ‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÂºÄÊîæËÆ∏ÂèØÁöÑÂæ∑ËØ≠ÊñáÊú¨Êï∞ÊçÆÈõÜÔºåÊó®Âú®Ëß£ÂÜ≥Âæ∑ËØ≠ËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇËØ•Êï∞ÊçÆÈõÜÊ±áÈõÜ‰∫ÜÊù•Ëá™41‰∏™Êù•Ê∫êÁöÑÊñáÊú¨ÔºåÊ∂µÁõñÊ≥ïÂæã„ÄÅÁßëÂ≠¶„ÄÅÊñáÂåñ„ÄÅÊîøÊ≤ª„ÄÅÊñ∞Èóª„ÄÅÁªèÊµéÂíåÁΩëÁªúÁ≠â‰∏É‰∏™È¢ÜÂüüÔºåÂÖ±ËÆ°1545.6‰∫ø‰∏™È´òË¥®ÈáèÊ†áËÆ∞„ÄÇÈÄöËøáÁ≥ªÁªüÂåñÁöÑÊï∞ÊçÆÊù•Ê∫êÂíå‰∏•Ê†ºÁöÑË¥®ÈáèËøáÊª§ÔºåËØ•Êï∞ÊçÆÈõÜÁ°Æ‰øù‰∫ÜÊñáÊú¨ÁöÑ‰∏ÄËá¥ÊÄßÂíåÂêàÊ≥ïÊÄßÔºåÈÄÇÂêàÁî®‰∫éËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÂÜçÂàÜÂèë„ÄÇÂæ∑ÂõΩÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÁöÑÂèëÂ∏É‰∏∫ÂºÄÊîæËÆ∏ÂèØÁöÑÂæ∑ËØ≠È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÂ°´Ë°•‰∫ÜÈáçË¶ÅÁ©∫ÁôΩÔºå‰øÉËøõ‰∫ÜÁúüÊ≠£ÂºÄÊîæÁöÑÂæ∑ËØ≠ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13454",
            "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
            "url": "https://huggingface.co/papers/2510.13454",
            "abstract": "VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
            "score": 5,
            "issue_id": 6467,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "b26ad07b318d2b9a",
            "authors": [
                "Hyojun Go",
                "Dominik Narnhofer",
                "Goutam Bhat",
                "Prune Truong",
                "Federico Tombari",
                "Konrad Schindler"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13454.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#alignment",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ 3D —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ: —Å—à–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ü–µ–Ω",
                    "desc": "VIST3A ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç latent text-to-video –º–æ–¥–µ–ª–∏ —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–µ—Ö–Ω–∏–∫—É model stitching, —á—Ç–æ–±—ã —Å–æ–µ–¥–∏–Ω–∏—Ç—å —Å–ª–æ–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –≤–∏–¥–µ–æ –∏ 3D-–¥–µ–∫–æ–¥–µ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –∑–Ω–∞–Ω–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è direct reward finetuning, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é 3D-—Å—Ü–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ text-to-3D –º–µ—Ç–æ–¥–∞–º–∏, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é Gaussian splats –∏ pointmap."
                },
                "en": {
                    "title": "Transforming Text into Stunning 3D Scenes with VIST3A!",
                    "desc": "VIST3A is a novel framework that integrates latent text-to-video models with 3D reconstruction systems to create detailed 3D scenes from textual descriptions. It addresses the challenge of effectively combining these two components while preserving their learned knowledge through a process called model stitching. Additionally, VIST3A ensures that the outputs from the text-to-video generator are compatible with the 3D decoder using a technique known as direct reward finetuning. The results demonstrate significant improvements over previous text-to-3D methods, allowing for the generation of high-quality 3D representations and pointmaps."
                },
                "zh": {
                    "title": "VIST3AÔºöÊñáÊú¨ÁîüÊàêÈ´òË¥®Èáè3DÂú∫ÊôØÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "VIST3AÊòØ‰∏Ä‰∏™ÁªìÂêàÊΩúÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°ÂûãÂíå3DÈáçÂª∫Á≥ªÁªüÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÊñáÊú¨ÁîüÊàêÈ´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ®°ÂûãÊãºÊé•ÊäÄÊúØÔºåÂ∞ÜÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÂô®‰∏é3DËß£Á†ÅÂô®ÊúâÊïàÁªìÂêàÔºå‰øùÁïô‰∫ÜÂêÑËá™ÁöÑÁü•ËØÜ„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÁîüÊàêÁöÑÊΩúÂú®Ë°®Á§∫ÂèØ‰ª•Ëß£Á†Å‰∏∫‰∏ÄËá¥ÁöÑ3DÂá†‰ΩïÂΩ¢Áä∂ÔºåVIST3AËøòÈááÁî®‰∫ÜÁõ¥Êé•Â•ñÂä±ÂæÆË∞ÉÊäÄÊúØËøõË°åÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVIST3AÂú®‰∏çÂêåÁöÑËßÜÈ¢ëÁîüÊàêÂô®Âíå3DÈáçÂª∫Ê®°Âûã‰∏äÂùáÊòæËëó‰ºò‰∫é‰πãÂâçÁöÑÊñáÊú¨Âà∞3DÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10472",
            "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the\n  Importance of Exploration Breadth",
            "url": "https://huggingface.co/papers/2510.10472",
            "abstract": "FML-bench evaluates automatic machine learning research agents on diverse fundamental problems using a unified framework with multiple metrics, highlighting the importance of broad exploration strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at https://github.com/qrzou/FML-bench.",
            "score": 5,
            "issue_id": 6482,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 12",
                "zh": "10Êúà12Êó•"
            },
            "hash": "3b1ff47aec462986",
            "authors": [
                "Qiran Zou",
                "Hou Hei Lam",
                "Wenhao Zhao",
                "Yiming Tang",
                "Tingting Chen",
                "Samson Yu",
                "Tianyi Zhang",
                "Chang Liu",
                "Xiangyang Ji",
                "Dianbo Liu"
            ],
            "affiliations": [
                "National University of Singapore",
                "Tsinghua University",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10472.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#survey",
                    "#optimization",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–®–∏—Ä–æ—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω–µ–µ –≥–ª—É–±–∏–Ω—ã –¥–ª—è AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FML-bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –∏–¥–µ–∏ –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 8 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á ML –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Å –ø—è—Ç—å—é –º–µ—Ç—Ä–∏–∫–∞–º–∏, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∞ –Ω–µ –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∞–≥–µ–Ω—Ç—ã —Å —à–∏—Ä–æ–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ, —á–µ–º —Ç–µ, —á—Ç–æ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —É–∑–∫–æ–π, –Ω–æ –≥–ª—É–±–æ–∫–æ–π –ø—Ä–æ—Ä–∞–±–æ—Ç–∫–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
                },
                "en": {
                    "title": "FML-bench: Broad Exploration for Better ML Research",
                    "desc": "FML-bench is a new benchmark designed to evaluate automatic machine learning research agents on a variety of fundamental problems. It aims to address the limitations of existing benchmarks by focusing on diverse tasks and reducing the coding burden for researchers. The framework includes multiple metrics to assess the performance of these agents comprehensively. The findings indicate that agents using broad exploration strategies achieve better research outcomes compared to those that concentrate on narrow, incremental improvements."
                },
                "zh": {
                    "title": "ÂπøÊ≥õÊé¢Á¥¢ÔºåÊèêÂçáËá™Âä®Êú∫Âô®Â≠¶‰π†Á†îÁ©∂ÁöÑÊïàÊûú",
                    "desc": "FML-benchÊòØ‰∏Ä‰∏™ËØÑ‰º∞Ëá™Âä®Êú∫Âô®Â≠¶‰π†Á†îÁ©∂‰ª£ÁêÜÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÂÖ´‰∏™Â§öÊ†∑ÂåñÁöÑÂü∫Á°ÄÊú∫Âô®Â≠¶‰π†Á†îÁ©∂ÈóÆÈ¢ò„ÄÇÂÆÉÊó®Âú®ÂáèÂ∞ëÁºñÁ†ÅË¥üÊãÖÔºåÂº∫Ë∞ÉÂü∫Á°ÄÈóÆÈ¢òËÄåÈùûÁâπÂÆöÂ∫îÁî®Ê°à‰æãÔºåÂπ∂Êèê‰æõÈ´ò‰ªªÂä°Â§öÊ†∑ÊÄß„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂Âíå‰∫î‰∏™‰∫íË°•ÁöÑÊåáÊ†áÔºåFML-benchÂÖ®Èù¢ËØÑ‰º∞‰ª£ÁêÜÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈááÁî®ÂπøÊ≥õÊé¢Á¥¢Á≠ñÁï•ÁöÑ‰ª£ÁêÜÂú®Á†îÁ©∂ÊàêÊûú‰∏ä‰ºò‰∫é‰∏ìÊ≥®‰∫éÁã≠Á™Ñ‰ΩÜÊ∑±ÂÖ•Êé¢Á¥¢ÁöÑ‰ª£ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14961",
            "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
            "url": "https://huggingface.co/papers/2510.14961",
            "abstract": "A new diffusion forcing sampler accelerates token generation in recurrent-depth language models, offering a 5x speedup without tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
            "score": 4,
            "issue_id": 6478,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "ca9713b9a228bba8",
            "authors": [
                "Jonas Geiping",
                "Xinyu Yang",
                "Guinan Su"
            ],
            "affiliations": [
                "ELLIS Institute T√ºbingen",
                "Electrical and Computer Engineering Carnegie Mellon University",
                "Max-Planck Institute for Intelligent Systems, T√ºbingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14961.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Å—ç–º–ø–ª–∏–Ω–≥",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π (–≥–¥–µ —Å–ª–æ–∏ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π —Å–≤—è–∑–∏ –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Å—ç–º–ø–ª–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ö–æ–¥–µ –º–æ–¥–µ–ª–∏, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É—Ç–æ—á–Ω—è—è —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Å–∏—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–æ 5-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–º–µ—Ä–æ–º 3.5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ –∫–∞–∫–æ–π-–ª–∏–±–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –†–∞–±–æ—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∫–∞—É–∑–∞–ª—å–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ LLM —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Accelerating Token Generation with Diffusion Forcing Sampler",
                    "desc": "This paper introduces a new method called the diffusion forcing sampler, which significantly speeds up the process of generating tokens in recurrent-depth language models. By leveraging the similarities between recurrent-depth and diffusion models, the sampler allows for the generation of new tokens during each forward pass while refining previous tokens in parallel. This approach results in a fivefold increase in speed without requiring any adjustments to the existing model architecture. The findings suggest that recurrent-depth models can be effectively treated as advanced diffusion language models, enhancing their efficiency in language tasks."
                },
                "zh": {
                    "title": "Âä†ÈÄüÈÄíÂΩíÊ∑±Â∫¶Ê®°ÂûãÁöÑÊâ©Êï£ÈááÊ†∑Âô®",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊâ©Êï£Âº∫Âà∂ÈááÊ†∑Âô®ÔºåËÉΩÂ§üÂä†ÈÄüÈÄíÂΩíÊ∑±Â∫¶ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊ†áËÆ∞ÁîüÊàêÔºåÈÄüÂ∫¶ÊèêÂçáÂèØËææ5ÂÄçÔºåÊó†ÈúÄË∞É‰ºò„ÄÇÈÄíÂΩíÊ∑±Â∫¶Ê®°ÂûãÈÄöËøáÈáçÂ§çÂ±ÇÁöÑËÆ°ÁÆóËÉΩÂäõÊù•Â¢ûÂº∫ÂÖ∂ÊÄßËÉΩÔºåÈÄÇÁî®‰∫éÁé∞‰ª£ËØ≠Ë®ÄÂª∫Ê®°‰ªªÂä°„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫ÜÈÄíÂΩíÊ∑±Â∫¶Ê®°Âûã‰∏éÊâ©Êï£ËØ≠Ë®ÄÊ®°Âûã‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂Âü∫‰∫éÊ≠§ÂºÄÂèë‰∫ÜÊñ∞ÁöÑÈááÊ†∑Âô®„ÄÇËØ•ÈááÊ†∑Âô®Âú®ÊØèÊ¨°ÂâçÂêë‰º†ÈÄí‰∏≠Ëß£Á†ÅÊñ∞Ê†áËÆ∞ÔºåÂêåÊó∂ÈÄöËøáÈÄíÂΩíÂπ∂Ë°åÂú∞Ëøõ‰∏ÄÊ≠•‰ºòÂåñËøô‰∫õÊ†áËÆ∞ÁöÑÊΩúÂú®Áä∂ÊÄÅ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14913",
            "title": "Budget-aware Test-time Scaling via Discriminative Verification",
            "url": "https://huggingface.co/papers/2510.14913",
            "abstract": "A hybrid approach combining discriminative verification with self-consistency outperforms generative verification in test-time scaling for large language models, achieving higher accuracy within a fixed compute budget.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
            "score": 4,
            "issue_id": 6486,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "7b1d93574c553153",
            "authors": [
                "Kyle Montgomery",
                "Sijun Tan",
                "Yuqi Chen",
                "Siyuan Zhuang",
                "Tianjun Zhang",
                "Raluca Ada Popa",
                "Chenguang Wang"
            ],
            "affiliations": [
                "UC Berkeley",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14913.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "‚öñÔ∏è",
                "ru": {
                    "title": "–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–±–µ–∂–¥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –±—é–¥–∂–µ—Ç–µ",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ test-time scaling. –í–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å self-consistency. –ü—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–º –±—é–¥–∂–µ—Ç–µ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ 15.3% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ AIME2025 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –æ–∫–∞–∑–∞–ª—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º, –Ω–æ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –±–ª–∞–≥–æ–¥–∞—Ä—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤."
                },
                "en": {
                    "title": "Boosting Accuracy with Budget-Friendly Hybrid Verification",
                    "desc": "This paper presents a hybrid approach that combines discriminative verification with self-consistency to improve the performance of large language models during test-time scaling. Traditional methods often rely on generative verification, which can be computationally expensive and impractical. The authors show that while discriminative verifiers alone may not perform as well, their combination with self-consistency leads to significant improvements in accuracy without exceeding a fixed compute budget. The results indicate that this hybrid method can outperform generative verification by up to 15.3% on the AIME2025 benchmark, making it a more efficient choice for real-world applications."
                },
                "zh": {
                    "title": "Ê∑∑ÂêàÈ™åËØÅÔºöÈ´òÊïàÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂà©Âô®",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÊñπÊ≥ïÔºåÂ∞ÜÂà§Âà´È™åËØÅ‰∏éËá™‰∏ÄËá¥ÊÄßÁõ∏ÁªìÂêàÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÊµãËØïÊó∂ÁöÑÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÁîüÊàêÈ™åËØÅÊñπÊ≥ïËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨È´òÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÁî®ÊÄß„ÄÇÈÄöËøáÂÆûËØÅÂàÜÊûêÔºåÊàë‰ª¨ÂèëÁé∞Âà§Âà´È™åËØÅÂú®ÂçïÁã¨‰ΩøÁî®Êó∂ÂèØËÉΩË°®Áé∞‰∏ç‰Ω≥Ôºå‰ΩÜ‰∏éËá™‰∏ÄËá¥ÊÄßÁªìÂêàÂêéÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÊµãËØïÊó∂Êâ©Â±ïÊú∫Âà∂„ÄÇËØ•Ê∑∑ÂêàÊñπÊ≥ïÂú®Âõ∫ÂÆöËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÔºåÂáÜÁ°ÆÁéáÊØîÊúÄÂÖàËøõÁöÑÁîüÊàêÈ™åËØÅÊñπÊ≥ïÈ´òÂá∫15.3%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13697",
            "title": "On Pretraining for Project-Level Code Completion",
            "url": "https://huggingface.co/papers/2510.13697",
            "abstract": "Extending the context window and adapting to a new rotary positional embedding scaling parameter improve repository-level code completion in OpenCoder, achieving performance comparable to larger models with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t Repository-level pretraining is commonly used to enable large language models for code to leverage codebase-wide context. This enhances their ability to generate accurate and context-aware code completions. In this work, we investigate how different repository-processing strategies affect in-context learning in OpenCoder, a 1.5B-parameter model. We extend its context window from 4,096 to 16,384 tokens by training on additional 1B tokens of curated repository-level data. Despite relying on a smaller dataset than competing models (which often use hundreds of billions of tokens), our model achieves comparable performance on the Long Code Arena benchmark. We find that various repository-processing techniques yield similarly strong results, with the primary gain coming from adapting to a new rotary positional embedding (RoPE) scaling parameter. Finally, we show that a simpler file-level training approach at the original sequence length remains highly effective, opening up repository-level code completion research to settings with more constrained data and compute resources.",
            "score": 4,
            "issue_id": 6476,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "d964acf6c0110803",
            "authors": [
                "Maksim Sapronov",
                "Evgeniy Glukhov"
            ],
            "affiliations": [
                "JetBrains Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13697.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#long_context",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#data",
                    "#architecture"
                ],
                "emoji": "üìÅ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –º–æ–¥–µ–ª—å OpenCoder –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, —Ä–∞—Å—à–∏—Ä–∏–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ —Å 4,096 –¥–æ 16,384 —Ç–æ–∫–µ–Ω–æ–≤. –ö–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–∫–∞–∑–∞–ª–∞—Å—å –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–º—É –ø–∞—Ä–∞–º–µ—Ç—Ä—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è rotary positional embedding (RoPE). –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - –≤—Å–µ–≥–æ 1 –º–∏–ª–ª–∏–∞—Ä–¥ —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ —Å–æ—Ç–µ–Ω –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤. –¢–∞–∫–∂–µ –±—ã–ª–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∞–π–ª–æ–≤ –æ—Å—Ç–∞—ë—Ç—Å—è –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —Ç–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö."
                },
                "en": {
                    "title": "Enhancing Code Completion with Extended Context and Efficient Training",
                    "desc": "This paper discusses improvements in code completion using the OpenCoder model by extending its context window and adapting a new rotary positional embedding scaling parameter. By increasing the context window from 4,096 to 16,384 tokens and training on an additional 1 billion tokens, the model can leverage more codebase-wide context for better performance. Despite using less data than larger models, OpenCoder achieves results comparable to those models on the Long Code Arena benchmark. The findings suggest that simpler training methods can also be effective, making advanced code completion techniques accessible even with limited resources."
                },
                "zh": {
                    "title": "Êâ©Â±ï‰∏ä‰∏ãÊñáÔºåÊèêÂçá‰ª£Á†ÅË°•ÂÖ®ÊÄßËÉΩ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÊâ©Â±ï‰∏ä‰∏ãÊñáÁ™óÂè£ÂíåË∞ÉÊï¥ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÁöÑÁº©ÊîæÂèÇÊï∞Êù•ÊèêÈ´òOpenCoderÁöÑ‰ª£Á†ÅË°•ÂÖ®ÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞Ü‰∏ä‰∏ãÊñáÁ™óÂè£‰ªé4096‰∏™Ê†áËÆ∞Êâ©Â±ïÂà∞16384‰∏™Ê†áËÆ∞ÔºåÂπ∂Âú®È¢ùÂ§ñÁöÑ10‰∫ø‰∏™Ê†áËÆ∞ÁöÑÁ≤æÂøÉÁ≠ñÂàíÁöÑ‰ª£Á†ÅÂ∫ìÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇÂ∞ΩÁÆ°‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜÊØîÁ´û‰∫âÊ®°ÂûãÂ∞èÔºå‰ΩÜÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Long Code ArenaÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éÊõ¥Â§ßÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈááÁî®‰∏çÂêåÁöÑ‰ª£Á†ÅÂ∫ìÂ§ÑÁêÜÁ≠ñÁï•ÂèØ‰ª•Ëé∑ÂæóÂº∫Â§ßÁöÑÁªìÊûúÔºåÂ∞§ÂÖ∂ÊòØÈÄÇÂ∫îÊñ∞ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•Áº©ÊîæÂèÇÊï∞Â∏¶Êù•‰∫Ü‰∏ªË¶ÅÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14976",
            "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
            "url": "https://huggingface.co/papers/2510.14976",
            "abstract": "Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
            "score": 3,
            "issue_id": 6470,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "dea3c35c6b10157d",
            "authors": [
                "Shaowei Liu",
                "Chuan Guo",
                "Bing Zhou",
                "Jian Wang"
            ],
            "affiliations": [
                "Snap Inc.",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14976.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ü§ù",
                "ru": {
                    "title": "–ê–Ω–∏–º–∞—Ü–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –ø–æ–∑—ã –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞",
                    "desc": "Ponimator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–≤—É—Ö –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑ –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞ –∏–∑ motion capture –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ conditional diffusion models: –æ–¥–∏–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–∑, –≤—Ç–æ—Ä–æ–π —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Å–∞–º–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–∑—ã –∏–∑ –æ–¥–∏–Ω–æ—á–Ω–æ–π –ø–æ–∑—ã, —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏. –ü–æ–¥—Ö–æ–¥ –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ —Å–∏–ª—å–Ω—ã–µ prior'—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–∏–Ω–∞–º–∏–∫—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ –ø–æ–∑–∞–º –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏: –∞–Ω–∏–º–∞—Ü–∏—é –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–∞–∫—Ü–∏–π –∏ —Å–∏–Ω—Ç–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å—è—è –∑–Ω–∞–Ω–∏—è –∏–∑ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö mocap –¥–∞–Ω–Ω—ã—Ö –≤ –æ—Ç–∫—Ä—ã—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏."
                },
                "en": {
                    "title": "Transforming Motion Capture into Interactive Animation",
                    "desc": "Ponimator is a framework that utilizes conditional diffusion models to create and synthesize interactive poses based on motion capture data. It focuses on close-contact human interactions, allowing for the generation of dynamic motion sequences and interactive poses. The framework consists of two main components: a pose animator for generating motion sequences and a pose generator for synthesizing poses from various inputs. Through empirical testing, Ponimator shows its ability to effectively transfer interaction knowledge to various animation tasks, enhancing versatility in animation applications."
                },
                "zh": {
                    "title": "PonimatorÔºö‰∫íÂä®Âä®ÁîªÁöÑÊô∫ËÉΩÁîüÊàê",
                    "desc": "Ponimator ÊòØ‰∏Ä‰∏™Âü∫‰∫éÊù°‰ª∂Êâ©Êï£Ê®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫é‰ªéÂä®‰ΩúÊçïÊçâÊï∞ÊçÆÁîüÊàêÂíåÂêàÊàê‰∫íÂä®ÂßøÂäø„ÄÇËØ•Ê®°ÂûãÂà©Áî®ËøëË∑ùÁ¶ª‰∫∫ÈôÖ‰∫íÂä®ÂßøÂäøÔºåËÉΩÂ§üÊçïÊçâ‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂ∏ÆÂä©‰∫∫Á±ªÁõ¥ËßÇÊé®Êµã‰∫íÂä®ÁöÑÂä®ÊÄÅ„ÄÇPonimator ÈÄöËøá‰∏§‰∏™Êù°‰ª∂Êâ©Êï£Ê®°ÂûãÊù•ÂÆûÁé∞Ôºö‰∏Ä‰∏™Áî®‰∫éÁîüÊàêÂä®ÊÄÅËøêÂä®Â∫èÂàóÔºåÂè¶‰∏Ä‰∏™Áî®‰∫é‰ªéÂçï‰∏ÄÂßøÂäøÊàñÊñáÊú¨ÂêàÊàê‰∫íÂä®ÂßøÂäø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPonimator Âú®Â§öÁßçÊï∞ÊçÆÈõÜÂíåÂ∫îÁî®‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÈÄöÁî®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14919",
            "title": "Predicting Task Performance with Context-aware Scaling Laws",
            "url": "https://huggingface.co/papers/2510.14919",
            "abstract": "A framework models downstream performance of large language models as a function of training compute and context, offering insights into efficient design for long-context tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
            "score": 3,
            "issue_id": 6486,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "05f9185d4f9b9de7",
            "authors": [
                "Kyle Montgomery",
                "David Park",
                "Jianhong Tu",
                "Michael Bendersky",
                "Beliz Gunel",
                "Dawn Song",
                "Chenguang Wang"
            ],
            "affiliations": [
                "Databricks",
                "Google DeepMind",
                "UC Berkeley",
                "UC Santa Cruz",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14919.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#reasoning",
                    "#machine_translation",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üìè",
                "ru": {
                    "title": "–ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ LLM",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é framework –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ downstream –∑–∞–¥–∞—á–∞—Ö –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏—é –æ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ upstream –º–µ—Ç—Ä–∏–∫–∞—Ö –≤—Ä–æ–¥–µ cross-entropy loss, –Ω–æ –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª–∏ –≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. Framework –±—ã–ª –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö Llama-2-7B –∏ Llama-2-13B –Ω–∞ 65,500 –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–∑ —Ç—Ä—ë—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á: –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±—ä—ë–º—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ long-context LLM."
                },
                "en": {
                    "title": "Optimizing Large Language Models: Context and Compute Unleashed!",
                    "desc": "This paper introduces a new framework that helps understand how the performance of large language models (LLMs) on specific tasks depends on the amount of training compute and the context provided during training. Traditional scaling laws focus on metrics like loss but do not adequately explain how well models perform on real-world tasks, especially when context length varies. The authors validate their framework using data from Llama-2 models across various tasks, showing that it can predict performance accurately as context increases. This research provides insights that can help design more efficient LLMs for tasks requiring long contexts."
                },
                "zh": {
                    "title": "È´òÊïàËÆæËÆ°Èïø‰∏ä‰∏ãÊñáËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê°ÜÊû∂ÔºåÁî®‰∫éÂª∫Ê®°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåËÄÉËôë‰∫ÜËÆ≠ÁªÉËÆ°ÁÆóËµÑÊ∫êÂíå‰∏ä‰∏ãÊñáÁöÑÂΩ±Âìç„ÄÇ‰º†ÁªüÁöÑÁº©ÊîæÊ≥ïÂàôÊó†Ê≥ïÊúâÊïàÊçïÊçâ‰∏ä‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩÔºåËÄåÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂÆûËØÅÈ™åËØÅÔºåÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêå‰∏ä‰∏ãÊñáÊù°‰ª∂‰∏ãÁöÑÊ®°ÂûãË°®Áé∞„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™‰ªªÂä°‰∏äÂØπLlama-2-7BÂíåLlama-2-13BËøõË°å‰∫ÜÊµãËØïÔºåÁªìÊûúË°®ÊòéËØ•Ê°ÜÊû∂ËÉΩÂ§üÂáÜÁ°ÆÈ¢ÑÊµãÊ®°ÂûãÊÄßËÉΩÔºåÂπ∂Âú®ËÆ≠ÁªÉËÆ°ÁÆóËµÑÊ∫êÁöÑÂèòÂåñ‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Á†îÁ©∂‰∏∫ËÆæËÆ°È´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14252",
            "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
            "url": "https://huggingface.co/papers/2510.14252",
            "abstract": "The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
            "score": 2,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "b6535f904cb7a0c8",
            "authors": [
                "Jihao Zhao",
                "Zhiyuan Ji",
                "Simin Niu",
                "Hanyu Wang",
                "Feiyu Xiong",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "School of Information, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14252.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#multimodal",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MoM, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—ã, –ø—Ä–µ–≤—Ä–∞—â–∞—è –ø–∞—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –≤ –∞–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ \"–≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è\" –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å–æ–∑–¥–∞–≤–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ö–µ–º—ã –∏ –∏–∑–≤–ª–µ–∫–∞—è –∫–ª—é—á–µ–≤–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å —É—á—ë—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –¥–æ–º–µ–Ω–∞. –ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –æ–±—É—á–∞—é—Ç—Å—è —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É —á—Ç–µ–Ω–∏—é —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –¢—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º retrieval –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Transforming Text Processing: From Passive Chunking to Proactive Understanding",
                    "desc": "The MoM framework improves the Retrieval-Augmented Generation (RAG) approach by shifting from passive text chunking to an active understanding of documents. This new method allows large language models (LLMs) to create structured document memories, enhancing their reasoning and knowledge retention. Additionally, small language models (SLMs) are trained to develop reading skills similar to humans through proactive exploration of content. The framework employs advanced techniques like multi-path sampling and reverse reasoning to optimize document memory retrieval and improve text processing capabilities."
                },
                "zh": {
                    "title": "‰∏ªÂä®ÁêÜËß£ÔºåÊûÑÂª∫ÊñáÊ°£ËÆ∞ÂøÜÁöÑÊú™Êù•",
                    "desc": "MoMÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÊñáÊú¨Â§ÑÁêÜ‰ªéË¢´Âä®ÂàÜÂùóËΩ¨Âèò‰∏∫‰∏ªÂä®ÁêÜËß£ÔºåÂ¢ûÂº∫‰∫ÜRAGÁöÑËÉΩÂäõÔºå‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üÁîüÊàêÁªìÊûÑÂåñÁöÑÊñáÊ°£ËÆ∞ÂøÜÔºåÂπ∂‰ΩøÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂèëÂ±ïÂá∫Á±ª‰∫∫ÈòÖËØªËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑRAGËåÉÂºèÂú®ÁêÜËß£Áõ∏ÂÖ≥ÊñáÊú¨ÂùóÊó∂Â≠òÂú®Â±ÄÈôêÔºåÈôêÂà∂‰∫ÜÁü•ËØÜÂÜÖÂåñÁöÑÊ∑±Â∫¶ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÂÆö‰πâ‰∫ÜÊñáÊ°£ËÆ∞ÂøÜÊèêÂèñÁöÑËøáÁ®ãÔºåÊó®Âú®Ê®°Êãü‰∫∫Á±ªÂú®ÈòÖËØªÊó∂ÁöÑËÆ§Áü•ËøáÁ®ã„ÄÇÈÄöËøáÂ§öË∑ØÂæÑÈááÊ†∑ÂíåÂ§öËßíÂ∫¶ËØÑ‰º∞Êú∫Âà∂ÔºåMoMÊ°ÜÊû∂ÊúâÊïàÂ§ÑÁêÜÂ§öÈ¢ÜÂüüÊñáÊ°£ÔºåÂ∏ÆÂä©SLMs‰∏ªÂä®Êé¢Á¥¢ÂíåÊûÑÂª∫ÊñáÊ°£ËÆ∞ÂøÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13913",
            "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty\n  Enhancement Mechanisms",
            "url": "https://huggingface.co/papers/2510.13913",
            "abstract": "A two-pronged data synthesis pipeline generates complex question-answer pairs, enabling the training of more effective web-based research agents with higher diversity in tool use.  \t\t\t\t\tAI-generated summary \t\t\t\t Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.",
            "score": 2,
            "issue_id": 6482,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "19ea6eddc0efed68",
            "authors": [
                "Shrey Pandit",
                "Xuan-Phi Nguyen",
                "Yifei Ming",
                "Austin Xu",
                "Jiayu Wang",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13913.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#long_context",
                    "#synthetic",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É—Å–ª–æ–∂–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –≤–µ–±-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –±–∞–∑–æ–≤—ã–π –∞–≥–µ–Ω—Ç –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç –æ—à–∏–±–∞—Ç—å—Å—è, –ø—Ä–∏ —ç—Ç–æ–º —Å–∞–º –∞–≥–µ–Ω—Ç —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏. –û–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∏–∑–±–µ–≥–∞—é—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤."
                },
                "en": {
                    "title": "Enhancing Web Agents with Complex Question-Answer Synthesis",
                    "desc": "This paper presents a novel two-pronged data synthesis pipeline designed to create complex question-answer pairs for training web-based research agents. The approach focuses on progressively increasing the complexity of tasks until a baseline agent fails, ensuring that the generated data captures the necessary intricacies for long-horizon reasoning. By validating factuality and exploring alternative answers, the pipeline enhances the quality and diversity of the training data. Experiments demonstrate that agents trained on this synthesized dataset outperform those trained on existing datasets, achieving greater tool-use diversity and improved performance."
                },
                "zh": {
                    "title": "ÂèåÁÆ°ÈΩê‰∏ãÔºåÊèêÂçáÁΩëÁªúÁ†îÁ©∂‰ª£ÁêÜÁöÑËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÁÆ°ÈΩê‰∏ãÁöÑÊï∞ÊçÆÂêàÊàêÊµÅÁ®ãÔºåÁî®‰∫éÁîüÊàêÂ§çÊùÇÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºå‰ª•ËÆ≠ÁªÉÊõ¥ÊúâÊïàÁöÑÁΩëÁªúÁ†îÁ©∂‰ª£ÁêÜ„ÄÇËØ•ÊµÅÁ®ãÈÄöËøáÈÄêÊ≠•Â¢ûÂä†‰ªªÂä°Â§çÊùÇÊÄßÔºåÁõ¥Âà∞Âü∫Á∫ø‰ª£ÁêÜÂ§±Ë¥•Ôºå‰ªéËÄåÁ°Æ‰øùÁîüÊàêÁöÑÊï∞ÊçÆËÉΩÂ§üÊçïÊçâÂà∞ÈïøÊó∂Èó¥Êé®ÁêÜÊâÄÈúÄÁöÑÂ§çÊùÇÊÄß„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊéßÂà∂Êï∞ÊçÆÁöÑÈöæÂ∫¶ÂíåË¥®ÈáèÔºåÈÅøÂÖç‰∫ÜÊï∞ÊçÆÂíåËÆ≠ÁªÉÊïàÊûúÊ∑∑Ê∑ÜÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Êï∞ÊçÆÈõÜËæÉÂ∞èÔºå‰ΩÜÊàë‰ª¨ÁöÑÊñπÊ≥ïËÆ≠ÁªÉÂá∫ÁöÑÁΩëÁªú‰ª£ÁêÜÂú®Â∑•ÂÖ∑‰ΩøÁî®ÁöÑÂ§öÊ†∑ÊÄßÂíåÊÄßËÉΩ‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊï∞ÊçÆÈõÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06694",
            "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
            "url": "https://huggingface.co/papers/2510.06694",
            "abstract": "SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
            "score": 2,
            "issue_id": 6472,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 8",
                "zh": "10Êúà8Êó•"
            },
            "hash": "d1a64ad898f0ebc3",
            "authors": [
                "Jipeng Lyu",
                "Jiahua Dong",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06694.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–ë—ã—Å—Ç—Ä–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é",
                    "desc": "SCas4D ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 3D Gaussian Splatting. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≥–¥–µ –≥—Ä—É–ø–ø—ã —Ç–æ—á–µ–∫ –¥–≤–∏–≥–∞—é—Ç—Å—è –ø–æ—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º. –ú–µ—Ç–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É—Ç–æ—á–Ω—è–µ—Ç –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ç –≥—Ä—É–±–æ–≥–æ —É—Ä–æ–≤–Ω—è —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–∞ –¥–æ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –∑–∞ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ –∫–∞–¥—Ä. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞—Ä—Ç–∏–∫—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–æ—á–µ–∫, —Ä–∞–±–æ—Ç–∞—è –≤ 20 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤."
                },
                "en": {
                    "title": "Efficient Dynamic Scene Modeling with SCas4D",
                    "desc": "SCas4D is a novel framework designed for efficiently modeling dynamic scenes using 3D Gaussian Splatting. It focuses on capturing hierarchical deformation patterns, which allows for faster convergence and high-quality outputs. By refining deformations from a coarse to a fine level, SCas4D can achieve results comparable to existing methods while significantly reducing training iterations. This approach is particularly effective in tasks such as self-supervised articulated object segmentation and novel view synthesis."
                },
                "zh": {
                    "title": "È´òÊïàÂä®ÊÄÅÂú∫ÊôØÂª∫Ê®°ÁöÑÂ±ÇÊ¨°Âåñ‰ºòÂåñÊ°ÜÊû∂",
                    "desc": "SCas4DÊòØ‰∏ÄÁßçÁ∫ßËÅî‰ºòÂåñÊ°ÜÊû∂ÔºåÂà©Áî®3DÈ´òÊñØÁÇπ‰∫ëÊúâÊïàÂª∫Ê®°Âä®ÊÄÅÂú∫ÊôØ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ±ÇÊ¨°ÂåñÂèòÂΩ¢Ê®°ÂºèÔºåÂø´ÈÄüÊî∂ÊïõÂπ∂Âú®Â§öÁßç‰ªªÂä°‰∏≠ÂÆûÁé∞È´òË¥®ÈáèÁªìÊûú„ÄÇSCas4DÁöÑÂÖ≥ÈîÆÂú®‰∫éÁúüÂÆû‰∏ñÁïåÁöÑÂèòÂΩ¢ÈÄöÂ∏∏ÂëàÁé∞Â±ÇÊ¨°ÂåñÊ®°ÂºèÔºåÂ§ö‰∏™È´òÊñØÂÖ±‰∫´Áõ∏‰ººÁöÑÂèòÊç¢„ÄÇÈÄöËøá‰ªéÁ≤óÂà∞ÁªÜÈÄêÊ≠•‰ºòÂåñÂèòÂΩ¢ÔºåSCas4DÂú®ÊØè‰∏™Êó∂Èó¥Â∏ßÂÜÖ‰ªÖÈúÄ100Ê¨°Ëø≠‰ª£Âç≥ÂèØÊî∂ÊïõÔºå‰∏îËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞‰ªÖ‰∏∫Áé∞ÊúâÊñπÊ≥ïÁöÑ‰∫îÂàÜ‰πã‰∏Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14942",
            "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
            "url": "https://huggingface.co/papers/2510.14942",
            "abstract": "GroundedPRM uses Monte Carlo Tree Search and external validation to improve multi-step reasoning in LLMs with fewer, higher-quality annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
            "score": 1,
            "issue_id": 6481,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "49ee4eff0ffba5f0",
            "authors": [
                "Yao Zhang",
                "Yu Wu",
                "Haowei Zhang",
                "Weiguo Li",
                "Haokun Chen",
                "Jingpei Wu",
                "Guohao Li",
                "Zhen Han",
                "Volker Tresp"
            ],
            "affiliations": [
                "Fudan University",
                "LMU Munich",
                "Munich Center for Machine Learning",
                "Technical University of Munich",
                "University Heidelberg",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14942.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "üå≥",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é GroundedPRM",
                    "desc": "GroundedPRM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM, —Å–Ω–∏–∂–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –°–∏—Å—Ç–µ–º–∞ —Å—Ç—Ä–æ–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—ã–π —à–∞–≥ —Å –ø–æ–º–æ—â—å—é –≤–Ω–µ—à–Ω–µ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞–≥—Ä–∞–¥, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ GroundedPRM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å –º–∞–ª—É—é —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Multi-Step Reasoning with GroundedPRM",
                    "desc": "GroundedPRM is a novel framework designed to enhance multi-step reasoning in Large Language Models (LLMs) by utilizing Monte Carlo Tree Search (MCTS) and external validation methods. It addresses the challenges of noisy rewards and misalignment in existing Process Reward Models (PRMs) by constructing structured reasoning paths and validating intermediate steps with external tools. This approach allows for fine-grained credit assignment and reduces hallucination in supervision, leading to higher factual fidelity. GroundedPRM demonstrates significant performance improvements with fewer annotations, making it a scalable solution for effective process-level reasoning in LLMs."
                },
                "zh": {
                    "title": "GroundedPRMÔºöÈ´òÊïàÁöÑÂ§öÊ≠•Êé®ÁêÜÊ°ÜÊû∂",
                    "desc": "GroundedPRMÊòØ‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÂíåÂ§ñÈÉ®È™åËØÅÊù•ÊîπÂñÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§öÊ≠•Êé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂåñÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÂáèÂ∞ëÂ•ñÂä±Âô™Â£∞ÔºåÂπ∂ÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑ‰ø°Áî®ÂàÜÈÖç„ÄÇÂÆÉËøòÈÄöËøáÂ§ñÈÉ®Â∑•ÂÖ∑È™åËØÅÊØè‰∏™‰∏≠Èó¥Ê≠•È™§ÔºåÊ∂àÈô§ËôöÂÅáÁõëÁù£Ôºå‰ªéËÄåÊèê‰æõÂü∫‰∫éÊâßË°åÁöÑÊ≠£Á°ÆÊÄß‰ø°Âè∑„ÄÇÊúÄÁªàÔºåGroundedPRMÂú®‰ªÖ‰ΩøÁî®40KËá™Âä®Ê†áÊ≥®Ê†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÈ´òÊïà‰∏îÂèØÈ™åËØÅÁöÑËøáÁ®ãÁ∫ßÊé®ÁêÜËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14351",
            "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
            "url": "https://huggingface.co/papers/2510.14351",
            "abstract": "Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
            "score": 1,
            "issue_id": 6467,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 16",
                "zh": "10Êúà16Êó•"
            },
            "hash": "cc92d03746a5b325",
            "authors": [
                "Perapard Ngokpol",
                "Kun Kerdthaisong",
                "Pasin Buakhaw",
                "Pitikorn Khlaisamniang",
                "Supasate Vorathammathorn",
                "Piyalitt Ittichaiwong",
                "Nutchanon Yongsatianchot"
            ],
            "affiliations": [
                "Artificial Intelligence Association of Thailand",
                "Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University",
                "School of Biomedical Engineering & Imaging Sciences, Kings College London",
                "Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University",
                "Thammasat School of Engineering, Thammasat University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14351.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#multimodal",
                    "#alignment",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "ü¶∏",
                "ru": {
                    "title": "–°—É–ø–µ—Ä–≥–µ—Ä–æ–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤—Å–µ–ª–µ–Ω–Ω—ã—Ö: –ø—Ä–æ–≤–µ—Ä–∫–∞ LLM –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–ª–µ–≤–æ–π –∏–≥—Ä—ã",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ Beyond One World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏–≥—Ä–∞—Ç—å —Ä–æ–ª–∏ —Å—É–ø–µ—Ä–≥–µ—Ä–æ–µ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –≤–µ—Ä—Å–∏–π (–∫–æ–º–∏–∫—Å—ã, —Ñ–∏–ª—å–º—ã). Benchmark –≤–∫–ª—é—á–∞–µ—Ç 30 –≥–µ—Ä–æ–µ–≤ –≤ 90 –≤–µ—Ä—Å–∏—è—Ö –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –¥–≤–∞ –∑–∞–¥–∞–Ω–∏—è: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏–∑ –±–∏–æ–≥—Ä–∞—Ñ–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏ —Ä–µ—à–µ–Ω–∏–µ –º–æ—Ä–∞–ª—å–Ω—ã—Ö –¥–∏–ª–µ–º–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ç—Ä–∏ –ø—Ä–æ–±–ª–µ–º—ã: chain-of-thought –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ —É–ª—É—á—à–∞–µ—Ç —Å–≤—è–∑–Ω–æ—Å—Ç—å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —É —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —É —Å–∏–ª—å–Ω—ã—Ö; –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ –æ–±–æ–±—â–∞—é—Ç –∑–Ω–∞–Ω–∏—è –º–µ–∂–¥—É –≤–µ—Ä—Å–∏—è–º–∏ –æ–¥–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞; –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–∏ –∏ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∏ –≤ –¥–µ–π—Å—Ç–≤–∏—è—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Think-Act Matching –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è–º–∏ –∏ —Ä–µ—à–µ–Ω–∏—è–º–∏, —Å–ª—É–∂–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Evaluating LLMs in Superhero Roleplay Across Canons",
                    "desc": "The paper introduces the Beyond One World benchmark, which assesses large language models (LLMs) on their ability to accurately portray superheroes from different canons, such as Marvel and DC. It consists of two main tasks: Canon Events, which tests the models' factual recall of significant character events, and Moral Dilemmas, which evaluates their ethical reasoning in complex scenarios. The study highlights the challenges LLMs face in maintaining consistency across various character versions and proposes a new metric called Think-Act Matching to measure the alignment between a model's reasoning and its actions. The findings reveal that while some models can think or act well, achieving proficiency in both remains a significant challenge."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ë∂ÖÁ∫ßËã±ÈõÑËßíËâ≤ÊâÆÊºî‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß‰∏éÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫\"Beyond One World\"ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∏çÂêåËÉåÊôØ‰∏ãÂáÜÁ°ÆÊèèÁªòÁâπÂÆöÁâàÊú¨Ë∂ÖÁ∫ßËã±ÈõÑÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫Ü30‰∏™Ê†áÂøóÊÄßËã±ÈõÑÂíå90‰∏™ÁâπÂÆöÁâàÊú¨ÔºåÂåÖÂê´‰∏§‰∏™‰∏ªË¶Å‰ªªÂä°Ôºö‰∫ãÂÆûÂõûÂøÜÂíåÈÅìÂæ∑Âõ∞Â¢É„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÂèØ‰ª•ÊèêÈ´òËæÉÂº±Ê®°ÂûãÁöÑÂèô‰∫ãËøûË¥ØÊÄßÔºå‰ΩÜÂèØËÉΩÈôç‰ΩéÂº∫Ê®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÂú®ÊÄùËÄÉÂíåË°åÂä®ÊñπÈù¢ÂæÄÂæÄË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂæàÂ∞ëÂêåÊó∂ÂÖºÈ°æ‰∏§ËÄÖ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14095",
            "title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
            "url": "https://huggingface.co/papers/2510.14095",
            "abstract": "Transformer networks are enhanced with four architectural mechanisms to improve out-of-distribution generalization and algorithmic reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.",
            "score": 1,
            "issue_id": 6483,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "7b973056587ce859",
            "authors": [
                "Awni Altabaa",
                "Siyu Chen",
                "John Lafferty",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "Department of Statistics & Data Science, Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14095.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#architecture",
                    "#training"
                ],
                "emoji": "üîß",
                "ru": {
                    "title": "–ß–µ—Ç—ã—Ä–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —á–µ—Ç—ã—Ä–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Transformer-—Å–µ—Ç–µ–π –æ–±–æ–±—â–∞—Ç—å –∑–Ω–∞–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã–µ –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ú–µ—Ö–∞–Ω–∏–∑–º—ã –≤–∫–ª—é—á–∞—é—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å, –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º, –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏ —è–≤–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫. –ü–æ–¥—Ö–æ–¥ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏ –≤ —Å—Ç–∏–ª–µ GSM8K —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã."
                },
                "en": {
                    "title": "Enhancing Transformers for Better Out-of-Distribution Reasoning",
                    "desc": "This paper focuses on improving the ability of Transformer networks to generalize beyond their training data, particularly in reasoning tasks. It introduces four new architectural mechanisms: input-adaptive recurrence, algorithmic supervision, anchored latent representations, and an error-correction mechanism. These enhancements aim to enable better out-of-distribution (OOD) generalization and improve the reasoning capabilities of language models. The authors also provide an analysis to explain how these mechanisms contribute to the improved performance in OOD scenarios."
                },
                "zh": {
                    "title": "Â¢ûÂº∫ÂèòÊç¢Âô®ÁΩëÁªúÁöÑÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÂõõÁßçÊû∂ÊûÑÊú∫Âà∂Êù•Â¢ûÂº∫ÂèòÊç¢Âô®ÁΩëÁªúÁöÑÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõÂíåÁÆóÊ≥ïÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂‰ΩøÁî®GSM8KÈ£éÊ†ºÁöÑÊ®°ÂùóÂåñÁÆóÊúØ‰ªªÂä°‰Ωú‰∏∫ÊµãËØïÂπ≥Âè∞ÔºåÂàÜÊûê‰∫ÜÂèòÊç¢Âô®ÁΩëÁªúÂú®ÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÊ≥õÂåñÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÊèêÂá∫ÁöÑÂõõÁßçÊú∫Âà∂ÂåÖÊã¨ÔºöËæìÂÖ•Ëá™ÈÄÇÂ∫îÈÄíÂΩí„ÄÅÁÆóÊ≥ïÁõëÁù£„ÄÅÈÄöËøáÁ¶ªÊï£Áì∂È¢àÈîöÂÆöÁöÑÊΩúÂú®Ë°®Á§∫Ôºå‰ª•ÂèäÊòæÂºèÁöÑÈîôËØØ‰øÆÊ≠£Êú∫Âà∂„ÄÇËøô‰∫õÊú∫Âà∂ÂÖ±ÂêåÊûÑÊàê‰∫Ü‰∏ÄÁßçÊû∂ÊûÑÊñπÊ≥ïÔºå‰ΩøÂèòÊç¢Âô®ÁΩëÁªúËÉΩÂ§üÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÁ®≥ÂÅ•ÁöÑÁÆóÊ≥ïÊé®ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13910",
            "title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval\n  Augmented Generation Systems",
            "url": "https://huggingface.co/papers/2510.13910",
            "abstract": "RAGCap-Bench evaluates intermediate tasks in agentic RAG workflows, highlighting the importance of enhancing these capabilities for better end-to-end performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that \"slow-thinking\" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.",
            "score": 1,
            "issue_id": 6473,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "fcc3e047868a25f7",
            "authors": [
                "Jingru Lin",
                "Chen Zhang",
                "Stephen Y. Liu",
                "Haizhou Li"
            ],
            "affiliations": [
                "National University of Singapore, Singapore",
                "The Chinese University of Hong Kong, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#benchmark",
                    "#rag",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RAGCap-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º–∞—Ö. RAG (Retrieval-Augmented Generation) –ø–æ–º–æ–≥–∞–µ—Ç LLM –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –ø—É—Ç—ë–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≤—ã—è–≤–∏–ª–∏ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ LLM –∏ —Å–æ–∑–¥–∞–ª–∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Ä–∞–±–æ—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ RAGCap –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª—É—á—à–µ–π –∏—Ç–æ–≥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π."
                },
                "en": {
                    "title": "Enhancing Agentic RAG Workflows for Superior Performance",
                    "desc": "The paper introduces RAGCap-Bench, a benchmark designed to evaluate the intermediate tasks in agentic Retrieval-Augmented Generation (RAG) workflows. It highlights the need for improving the reasoning capabilities of Large Language Models (LLMs) to enhance their performance on complex queries. By analyzing outputs from advanced systems, the authors identify essential tasks and common errors, creating a taxonomy for better evaluation. The findings suggest that models that take a more deliberate approach to reasoning perform better overall, validating the importance of focusing on intermediate capabilities."
                },
                "zh": {
                    "title": "ÊèêÂçá‰ª£ÁêÜ RAG ËÉΩÂäõÔºå‰ºòÂåñÊï¥‰ΩìÊÄßËÉΩ",
                    "desc": "RAGCap-Bench ÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞‰ª£ÁêÜ RAG Â∑•‰ΩúÊµÅ‰∏≠Èó¥‰ªªÂä°ÁöÑÂü∫ÂáÜÔºåÂº∫Ë∞ÉÊèêÂçáËøô‰∫õËÉΩÂäõÂØπÊï¥‰ΩìÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÈÄöËøáÂä®ÊÄÅÊ£ÄÁ¥¢Â§ñÈÉ®‰ø°ÊÅØÊù•ÁºìËß£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂÖ≥ÈîÆÈôêÂà∂ÔºåÂ¶Ç‰∫ãÂÆûÈîôËØØÂíåËøáÊó∂Áü•ËØÜ„ÄÇÂ∞ΩÁÆ°‰ª£ÁêÜ RAG Á≥ªÁªüÂú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢Êó∂Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Â§öË∑≥ÈóÆÈ¢òÂíå‰∏≠Èó¥Êé®ÁêÜËÉΩÂäõÊñπÈù¢‰ªçÂ≠òÂú®ÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂÖ∑ÊúâÊõ¥Âº∫ RAGCap ÊÄßËÉΩÁöÑ‚ÄúÊÖ¢ÊÄùËÄÉ‚ÄùÊ®°ÂûãÂú®Êï¥‰ΩìÁªìÊûú‰∏äË°®Áé∞Êõ¥‰Ω≥ÔºåÈ™åËØÅ‰∫ÜÂü∫ÂáÜÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10390",
            "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
            "url": "https://huggingface.co/papers/2510.10390",
            "abstract": "RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting either dangerous overconfidence or overcaution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarks -- RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) -- and our complete generation framework to enable continued, dynamic evaluation of this critical capability.",
            "score": 1,
            "issue_id": 6469,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 12",
                "zh": "10Êúà12Êó•"
            },
            "hash": "0545dfda4fd8c317",
            "authors": [
                "Aashiq Muhamed",
                "Leonardo F. R. Ribeiro",
                "Markus Dreyer",
                "Virginia Smith",
                "Mona T. Diab"
            ],
            "affiliations": [
                "Amazon AGI",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10390.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#hallucinations",
                    "#alignment",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "üö´",
                "ru": {
                    "title": "–ö–æ–≥–¥–∞ AI –¥–æ–ª–∂–µ–Ω —Å–∫–∞–∑–∞—Ç—å ¬´–Ω–µ –∑–Ω–∞—é¬ª: —Ç–µ—Å—Ç–∏—Ä—É–µ–º —É–º–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç–≤–µ—á–∞—Ç—å",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö: —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –æ—Ç–∫–∞–∑–æ–º –æ—Ç –æ—Ç–≤–µ—Ç–∞, –∫–æ–≥–¥–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –î–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–∫–∞–∑–∞ –Ω–∏–∂–µ 50% –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –ø—Ä–æ—è–≤–ª—è—è –ª–∏–±–æ –æ–ø–∞—Å–Ω—É—é —Å–∞–º–æ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –ª–∏–±–æ –∏–∑–ª–∏—à–Ω—é—é –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω RefusalBench ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å 176 —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º—É—â–µ–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É–º–µ–Ω–∏–µ –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–∂–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ alignment, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM."
                },
                "en": {
                    "title": "RefusalBench: Enhancing Selective Refusal in Language Models",
                    "desc": "This paper introduces RefusalBench, a tool designed to assess how well language models in Retrieval-Augmented Generation (RAG) systems can refuse to answer questions based on incorrect context. The study shows that even advanced models often fail to refuse appropriately, with accuracy dropping below 50% in complex tasks. It highlights that traditional benchmarks are inadequate, as models can exploit specific dataset features rather than genuinely understanding refusal. By using a systematic approach with various perturbation strategies, the authors provide insights into the failure patterns and suggest that selective refusal can be improved through targeted training."
                },
                "zh": {
                    "title": "ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄâÊã©ÊÄßÊãíÁªùËÉΩÂäõ",
                    "desc": "RefusalBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÂú® RAG Á≥ªÁªü‰∏≠ÈÄâÊã©ÊÄßÊãíÁªùËÉΩÂäõÁöÑÂ∑•ÂÖ∑Ôºå‰ΩøÁî®Á®ãÂ∫èÁîüÊàêÁöÑÊµãËØïÊ°à‰æãÊù•Êè≠Á§∫Á≥ªÁªüÊÄßÂ§±Ë¥•Ê®°ÂºèÂπ∂Êèê‰æõÊîπËøõË∑ØÂæÑ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Â§öÊñáÊ°£‰ªªÂä°‰∏≠ÁöÑÊãíÁªùÂáÜÁ°ÆÁéá‰πü‰Ωé‰∫é 50%ÔºåÂπ∂‰∏îË°®Áé∞Âá∫ËøáÂ∫¶Ëá™‰ø°ÊàñËøáÂ∫¶Ë∞®ÊÖéÁöÑÂÄæÂêë„ÄÇÈùôÊÄÅÂü∫ÂáÜÊµãËØïÊó†Ê≥ïÂèØÈù†ËØÑ‰º∞Ëøô‰∏ÄËÉΩÂäõÔºåÂõ†‰∏∫Ê®°Âûã‰ºöÂà©Áî®Êï∞ÊçÆÈõÜÁâπÂÆöÁöÑ‰º™ÂΩ±Âπ∂ËÆ∞ÂøÜÊµãËØïÂÆû‰æã„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑ RefusalBench ÈááÁî®ÁîüÊàêÊÄßÊñπÊ≥ïÔºåÈÄöËøáÊéßÂà∂ËØ≠Ë®ÄÊâ∞Âä®Á®ãÂ∫èÂåñÂàõÂª∫ËØäÊñ≠ÊµãËØïÊ°à‰æãÔºåÊè≠Á§∫‰∫ÜÊãíÁªùËÉΩÂäõÁöÑÂèØËÆ≠ÁªÉÊÄßÂíåÂØπÈΩêÊïèÊÑüÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13161",
            "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM\n  Inference",
            "url": "https://huggingface.co/papers/2510.13161",
            "abstract": "Mirror Speculative Decoding accelerates large language model inference by parallelizing speculative execution across heterogeneous accelerators and using multi-token speculative streaming to reduce draft latency without compromising acceptance rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding accelerates LLM inference by using a draft model to look ahead, but gains are capped by the cost of autoregressive draft generation: increasing draft size elevates acceptance rates but introduces additional latency overhead exacerbating the speed-accuracy tradeoff. Prior methods (Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade acceptance or introduce overheads that limit scaling. We present Mirror Speculative Decoding (Mirror-SD), an inference algorithm that breaks the latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from early-exit signals in parallel with the target model's suffix and explicitly maps computation across heterogeneous accelerators (GPU and NPU) to exploit cross-device parallelism. The draft speculates forward continuations for the target to verify, while the target simultaneously speculates correction paths for the draft, converting speculation into two complementary execution pipelines. To further cut draft latency without weakening acceptance semantics, we add speculative streaming so the draft emits multiple tokens per step. This dual strategy of parallel heterogeneous execution plus multi-token speculative streaming pushes speculative decoding toward its ideal regime of high acceptance with low overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving 2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative improvement over the strongest baseline, EAGLE3.",
            "score": 0,
            "issue_id": 6481,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "db6a1a4635cfeda2",
            "authors": [
                "Nikhil Bhendawade",
                "Kumari Nishu",
                "Arnav Kundu",
                "Chris Bartels",
                "Minsik Cho",
                "Irina Belousova"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13161.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ü™û",
                "ru": {
                    "title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–µ–µ –∑–µ—Ä–∫–∞–ª–æ —É—Å–∫–æ—Ä—è–µ—Ç LLM —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ",
                    "desc": "Mirror Speculative Decoding - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ LLM. –ú–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤–µ—Ç–≤–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö —É—Å–∫–æ—Ä–∏—Ç–µ–ª—è—Ö (GPU –∏ NPU), –≥–¥–µ draft-–º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ, –∞ —Ü–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —ç—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è multi-token speculative streaming –ø–æ–∑–≤–æ–ª—è–µ—Ç draft-–º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —à–∞–≥, —á—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –ù–∞ –º–æ–¥–µ–ª—è—Ö –æ—Ç 14B –¥–æ 66B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 2.8-5.8 —Ä–∞–∑–∞ –∏ –Ω–∞ 30% –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π baseline EAGLE3."
                },
                "en": {
                    "title": "Accelerating Inference with Mirror-SD: Speed Meets Accuracy!",
                    "desc": "Mirror Speculative Decoding (Mirror-SD) is a novel inference algorithm designed to enhance the performance of large language models (LLMs) by optimizing the speculative decoding process. It achieves this by parallelizing execution across different types of hardware accelerators, such as GPUs and NPUs, and implementing multi-token speculative streaming to minimize latency. This approach allows the draft model to generate multiple tokens at once while maintaining high acceptance rates, effectively breaking the traditional latency-acceptance tradeoff. As a result, Mirror-SD significantly accelerates inference times, achieving impressive speedups on large-scale models without compromising accuracy."
                },
                "zh": {
                    "title": "ÈïúÂÉèÊé®ÊµãËß£Á†ÅÔºöÂä†ÈÄüÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "ÈïúÂÉèÊé®ÊµãËß£Á†ÅÔºàMirror-SDÔºâÊòØ‰∏ÄÁßçÂä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÊñ∞ÁÆóÊ≥ïÔºåÈÄöËøáÂú®ÂºÇÊûÑÂä†ÈÄüÂô®‰∏äÂπ∂Ë°åÊâßË°åÊé®ÊµãÔºåÊòæËëóÈôç‰ΩéËçâÁ®øÂª∂ËøüËÄå‰∏çÂΩ±ÂìçÊé•ÂèóÁéá„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Êó©ÊúüÈÄÄÂá∫‰ø°Âè∑Âπ∂Ë°åÂêØÂä®ÂÆåÊï¥ÁöÑÂàÜÊîØÂõûÊªöÔºåÂêåÊó∂Âú®ÁõÆÊ†áÊ®°ÂûãÁöÑÂêéÁºÄ‰∏äËøõË°åËÆ°ÁÆóÔºåÂÖÖÂàÜÂà©Áî®Ë∑®ËÆæÂ§áÁöÑÂπ∂Ë°åÊÄß„ÄÇÈïúÂÉèÊé®ÊµãËß£Á†ÅËøòÂºïÂÖ•‰∫ÜÂ§öÊ†áËÆ∞Êé®ÊµãÊµÅÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÂáèÂ∞ëËçâÁ®øÂª∂ËøüÔºåÂêåÊó∂‰øùÊåÅÈ´òÊé•ÂèóÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMirror-SDÂú®Â§öÁßç‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü2.8Âà∞5.8ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÂπ∂Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂº∫Âü∫Á∫øEAGLE3„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-10-17.html",
    "link_next": "2025-10-21.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10Êúà17Êó•"
    },
    "short_date_next": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10Êúà21Êó•"
    },
    "categories": {
        "#dataset": 15,
        "#data": 7,
        "#benchmark": 21,
        "#agents": 10,
        "#cv": 7,
        "#rl": 7,
        "#rlhf": 5,
        "#rag": 4,
        "#plp": 1,
        "#inference": 7,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 10,
        "#healthcare": 0,
        "#training": 23,
        "#robotics": 1,
        "#agi": 4,
        "#games": 3,
        "#interpretability": 6,
        "#reasoning": 18,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 1,
        "#optimization": 28,
        "#survey": 2,
        "#diffusion": 6,
        "#alignment": 7,
        "#story_generation": 2,
        "#hallucinations": 4,
        "#long_context": 7,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 3,
        "#science": 3,
        "#low_resource": 7
    }
}