{
    "date": {
        "ru": "20 октября",
        "en": "October 20",
        "zh": "10月20日"
    },
    "time_utc": "2025-10-20 03:43",
    "weekday": 0,
    "issue_id": 6499,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.15870",
            "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
            "url": "https://huggingface.co/papers/2510.15870",
            "abstract": "OmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
            "score": 22,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "05a0d602f6ebe4fa",
            "authors": [
                "Hanrong Ye",
                "Chao-Han Huck Yang",
                "Arushi Goel",
                "Wei Huang",
                "Ligeng Zhu",
                "Yuanhang Su",
                "Sean Lin",
                "An-Chieh Cheng",
                "Zhen Wan",
                "Jinchuan Tian",
                "Yuming Lou",
                "Dong Yang",
                "Zhijian Liu",
                "Yukang Chen",
                "Ambrish Dantrey",
                "Ehsan Jahangiri",
                "Sreyan Ghosh",
                "Daguang Xu",
                "Ehsan Hosseini-Asl",
                "Danial Mohseni Taheri",
                "Vidya Murali",
                "Sifei Liu",
                "Jason Lu",
                "Oluwatobi Olabiyi",
                "Frank Wang",
                "Rafael Valle",
                "Bryan Catanzaro",
                "Andrew Tao",
                "Song Han",
                "Jan Kautz",
                "Hongxu Yin",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15870.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#robotics",
                    "#reasoning",
                    "#healthcare",
                    "#multimodal",
                    "#data",
                    "#alignment"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Видеть, слышать и понимать: омни-модальный AI с минимальными затратами",
                    "desc": "OmniVinci — это open-source омни-модальная LLM, которая воспринимает и понимает мир через несколько модальностей: аудио, видео и изображения. Архитектура модели включает три ключевых инновации: OmniAlignNet для выравнивания эмбеддингов разных модальностей, Temporal Embedding Grouping для захвата относительной временной синхронизации между видео и аудио, и Constrained Rotary Time Embedding для кодирования абсолютной временной информации. Модель обучена на 24 миллионах синтезированных диалогов и использует всего 0.2T токенов (в 6 раз меньше конкурентов), при этом превосходит Qwen2.5-Omni на 19 пунктов в кросс-модальном понимании. OmniVinci демонстрирует преимущества омни-модального подхода в робототехнике, медицинском AI и умных фабриках."
                },
                "en": {
                    "title": "OmniVinci: Bridging Modalities for Enhanced AI Understanding",
                    "desc": "OmniVinci is an open-source omni-modal large language model (LLM) designed to improve understanding and performance across different types of data, such as audio, vision, and robotics. The model features innovative architecture, including OmniAlignNet for better alignment of audio and visual data, and Temporal Embedding Grouping to capture timing relationships between these modalities. It also introduces a new data curation pipeline that creates a vast dataset of conversations, enhancing the model's training efficiency. OmniVinci shows significant performance improvements over existing models, demonstrating its effectiveness in various applications like robotics and medical AI."
                },
                "zh": {
                    "title": "OmniVinci：跨模态理解的新突破",
                    "desc": "OmniVinci是一种开源的全模态大语言模型，旨在增强音频、视觉和机器人应用中的跨模态理解和性能。该模型通过创新的架构和高效的数据整理，提升了不同模态之间的对齐能力。我们提出了三项关键创新，包括OmniAlignNet、时间嵌入分组和约束旋转时间嵌入，以捕捉模态间的相对和绝对时间信息。实验结果表明，OmniVinci在多个任务上超越了现有模型，展示了其在机器人、医疗AI和智能工厂等下游应用中的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15869",
            "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
            "url": "https://huggingface.co/papers/2510.15869",
            "abstract": "Skyfall-GS creates large-scale, high-quality 3D urban scenes using satellite imagery and diffusion models, offering real-time exploration and improved geometry and texture consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose Skyfall-GS, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/",
            "score": 13,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "dd05f6fe39cf9ebd",
            "authors": [
                "Jie-Ying Lee",
                "Yi-Ruei Liu",
                "Shr-Ruei Tsai",
                "Wei-Cheng Chang",
                "Chung-Ho Wu",
                "Jiewen Chan",
                "Zhenjun Zhao",
                "Chieh Hubert Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University",
                "UC Merced",
                "UIUC",
                "University of Zaragoza"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15869.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Городские 3D-сцены из спутниковых снимков и диффузии",
                    "desc": "Skyfall-GS — это первая система для создания городских 3D-сцен масштаба целого квартала без дорогостоящей 3D-разметки. Метод использует спутниковые снимки для получения грубой геометрии и diffusion-модели для генерации реалистичных текстур вблизи. Авторы предлагают стратегию итеративного улучшения с постепенным усложнением задачи для повышения геометрической полноты и фотореалистичности. Система обеспечивает согласованность между видами и позволяет исследовать 3D-сцены в реальном времени."
                },
                "en": {
                    "title": "Revolutionizing 3D Urban Scene Generation with Skyfall-GS",
                    "desc": "Skyfall-GS is a framework designed to generate large-scale, high-quality 3D urban scenes using satellite images and diffusion models. It addresses the challenge of creating realistic 3D environments without the need for expensive 3D scans by leveraging available satellite imagery for basic geometry. The framework employs a curriculum-driven iterative refinement strategy to enhance both the geometric accuracy and the photorealistic quality of textures in the generated scenes. Experiments show that Skyfall-GS outperforms existing methods in terms of geometry consistency and texture realism, enabling real-time exploration of the 3D environments."
                },
                "zh": {
                    "title": "Skyfall-GS：实时生成高质量3D城市场景的创新方法",
                    "desc": "Skyfall-GS 是一种利用卫星图像和扩散模型创建大规模高质量3D城市场景的方法。该方法解决了缺乏大规模高质量3D扫描数据的问题，能够生成可实时探索的3D场景。通过结合卫星图像提供的粗略几何形状和扩散模型生成的高质量细节，Skyfall-GS 实现了几何完整性和真实感纹理的逐步提升。实验结果表明，Skyfall-GS 在几何一致性和纹理真实感方面优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15868",
            "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
            "url": "https://huggingface.co/papers/2510.15868",
            "abstract": "LightsOut enhances Single Image Flare Removal by reconstructing off-frame light sources using a diffusion-based outpainting framework, improving performance across challenging scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
            "score": 11,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "64e85d772c36be6b",
            "authors": [
                "Shr-Ruei Tsai",
                "Wei-Cheng Chang",
                "Jie-Ying Lee",
                "Chih-Hai Su",
                "Yu-Lun Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.15868.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#training"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Гасим свет: реконструкция источников бликов за пределами кадра",
                    "desc": "Блики от объективов серьёзно ухудшают качество изображений и мешают работе систем компьютерного зрения, особенно в автономном вождении. Существующие методы удаления бликов с одиночных изображений плохо справляются, когда источники света находятся за пределами кадра. LightsOut использует diffusion-модель с outpainting для реконструкции этих внекадровых источников света, применяя мультизадачный регрессионный модуль и LoRA fine-tuning для физически корректных результатов. Метод работает как универсальное plug-and-play решение, улучшая существующие алгоритмы удаления бликов без дополнительного переобучения."
                },
                "en": {
                    "title": "Illuminate Your Images: LightsOut for Flare-Free Vision",
                    "desc": "LightsOut is a novel approach that improves Single Image Flare Removal (SIFR) by reconstructing light sources that are not visible in the frame. It uses a diffusion-based outpainting framework to fill in these off-frame light sources, which helps to enhance image quality significantly. The method incorporates a multitask regression module and a fine-tuned diffusion model to produce realistic and consistent results. Experiments show that LightsOut enhances the performance of existing SIFR techniques in difficult scenarios without needing additional training, making it a versatile preprocessing tool."
                },
                "zh": {
                    "title": "LightsOut：提升单图像眩光去除的智能解决方案",
                    "desc": "LightsOut 是一种增强单图像眩光去除的方法，它通过扩展框架重建画面外的光源来提高性能。该方法使用基于扩散的外绘框架，能够在光源不完整或缺失的情况下有效处理眩光问题。LightsOut 结合了多任务回归模块和经过 LoRA 微调的扩散模型，确保生成的外绘结果既真实又符合物理规律。实验结果表明，LightsOut 在各种挑战性场景中都能显著提升现有单图像眩光去除方法的性能，且无需额外的再训练，成为一种通用的预处理解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15301",
            "title": "Latent Diffusion Model without Variational Autoencoder",
            "url": "https://huggingface.co/papers/2510.15301",
            "abstract": "SVG, a novel latent diffusion model without VAEs, uses self-supervised representations to enable efficient training, few-step sampling, and high-quality visual generation with semantic and discriminative capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations.",
            "score": 10,
            "issue_id": 6499,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "1fd61790278a702f",
            "authors": [
                "Minglei Shi",
                "Haolin Wang",
                "Wenzhao Zheng",
                "Ziyang Yuan",
                "Xiaoshi Wu",
                "Xintao Wang",
                "Pengfei Wan",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15301.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Генерация изображений через self-supervised представления без VAE",
                    "desc": "Авторы представляют SVG — новую модель latent diffusion, которая отказывается от использования VAE (variational autoencoder) в пользу self-supervised представлений. Вместо VAE используются замороженные DINO-признаки, которые обеспечивают семантическую разделимость, а лёгкая residual-ветка добавляет детали для качественной реконструкции. Diffusion-модель обучается напрямую в этом семантически структурированном латентном пространстве, что ускоряет обучение и позволяет генерировать изображения за меньшее число шагов. При этом сохраняются семантические и дискриминативные свойства исходных представлений, что открывает путь к универсальным визуальным представлениям для разных задач."
                },
                "en": {
                    "title": "SVG: Revolutionizing Visual Generation Without VAEs",
                    "desc": "SVG is a new type of latent diffusion model that does not use variational autoencoders (VAEs), which are commonly used in visual generation. By utilizing self-supervised representations, SVG achieves efficient training and high-quality image generation with better semantic understanding. The model creates a feature space that clearly separates different concepts, allowing for faster learning and fewer steps needed for sampling. Overall, SVG enhances the generative process while maintaining strong performance across various vision tasks."
                },
                "zh": {
                    "title": "SVG：高效的潜在扩散模型",
                    "desc": "SVG是一种新型的潜在扩散模型，不依赖变分自编码器（VAE），通过自监督表示实现高效训练和高质量视觉生成。该模型解决了传统VAE+扩散模型在训练效率、推理速度和迁移能力方面的局限性。SVG利用冻结的DINO特征构建了具有清晰语义可分性的特征空间，同时通过轻量级残差分支捕捉细节，实现高保真重建。实验结果表明，SVG在保持自监督表示的语义和判别能力的同时，提供了一种通用的高质量视觉表示的有效路径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14265",
            "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
            "url": "https://huggingface.co/papers/2510.14265",
            "abstract": "MorphoBench is a benchmark that evaluates large models' reasoning capabilities using multidisciplinary questions, adaptive difficulty, and simulation-generated questions.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.",
            "score": 10,
            "issue_id": 6498,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 октября",
                "en": "October 16",
                "zh": "10月16日"
            },
            "hash": "d2723fbb55ae5010",
            "authors": [
                "Xukai Wang",
                "Xuanbo Liu",
                "Mingrui Chen",
                "Haitian Zhong",
                "Xuanlin Yang",
                "Bohan Zeng",
                "Jinbo Hu",
                "Hao Liang",
                "Junbo Niu",
                "Xuchen Li",
                "Ruitao Wu",
                "Ruichuan An",
                "Yang Shi",
                "Liu Liu",
                "Xu-Yao Zhang",
                "Qiang Liu",
                "Zhouchen Lin",
                "Wentao Zhang",
                "Bin Dong"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14265.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Адаптивный бенчмарк с эволюционирующей сложностью для оценки рассуждений LLM",
                    "desc": "MorphoBench — это новый бенчмарк для оценки способностей к рассуждению у больших языковых моделей, который использует мультидисциплинарные вопросы из различных областей знаний, включая олимпиадные задачи. Ключевая особенность — адаптивная сложность вопросов, которая автоматически подстраивается под возможности продвинутых моделей вроде o3 и GPT-5. Бенчмарк включает более 1300 тестовых вопросов и использует симуляционное ПО для динамической генерации новых задач с минимальными ресурсными затратами. Такой подход позволяет более комплексно и надёжно оценивать reasoning-способности LLM и их научную обоснованность."
                },
                "en": {
                    "title": "MorphoBench: Adaptive Benchmarking for AI Reasoning Skills",
                    "desc": "MorphoBench is a new benchmark designed to evaluate the reasoning capabilities of large AI models through a diverse set of multidisciplinary questions. It addresses the limitations of existing benchmarks by allowing for adaptive difficulty, which means the questions can change based on the model's reasoning skills. The benchmark includes complex questions sourced from high-level competitions and uses simulation-generated questions to dynamically adjust the challenge level. With over 1,300 test questions, MorphoBench aims to provide a comprehensive and valid assessment of model reasoning, helping to enhance the performance and reliability of advanced AI systems."
                },
                "zh": {
                    "title": "MorphoBench：动态评估大型模型推理能力的基准测试",
                    "desc": "MorphoBench是一个评估大型模型推理能力的基准测试，使用多学科问题、适应性难度和模拟生成的问题。它解决了现有基准测试在评估大型模型推理能力时的局限性，能够根据模型的推理能力动态调整问题的难度。MorphoBench收集了来自奥林匹克级别竞赛等来源的复杂推理问题，并利用模型推理过程中的关键陈述来修改问题的分析挑战。通过这种方式，MorphoBench提高了模型推理评估的全面性和有效性，为提升大型模型的推理能力和科学稳健性提供了可靠的指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15742",
            "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
            "url": "https://huggingface.co/papers/2510.15742",
            "abstract": "Ditto framework addresses data scarcity in instruction-based video editing by generating a large dataset and using a curriculum learning strategy to train Editto, achieving superior instruction-following ability.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.",
            "score": 9,
            "issue_id": 6499,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "12113856c30298fe",
            "authors": [
                "Qingyan Bai",
                "Qiuyu Wang",
                "Hao Ouyang",
                "Yue Yu",
                "Hanlin Wang",
                "Wen Wang",
                "Ka Leong Cheng",
                "Shuailei Ma",
                "Yanhong Zeng",
                "Zichen Liu",
                "Yinghao Xu",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group",
                "HKUST",
                "Northeastern University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15742.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#agents",
                    "#synthetic",
                    "#training",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Миллион примеров для обучения редактированию видео по текстовым инструкциям",
                    "desc": "Фреймворк Ditto решает проблему нехватки данных для редактирования видео по текстовым инструкциям. Авторы создали pipeline для генерации датасета, объединяющий возможности редактора изображений и генератора видео, с использованием эффективной дистиллированной модели и интеллектуального агента для контроля качества. В результате был создан датасет Ditto-1M из миллиона высококачественных примеров редактирования видео, на создание которого потратили более 12000 GPU-дней. Модель Editto, обученная на этом датасете с применением curriculum learning, достигла state-of-the-art результатов в следовании инструкциям при редактировании видео."
                },
                "en": {
                    "title": "Empowering Video Editing with Abundant Data and Smart Learning",
                    "desc": "The Ditto framework addresses the challenge of limited training data in instruction-based video editing by generating a large dataset of one million high-quality examples. It combines a creative image editor with an in-context video generator to enhance data diversity and quality. The framework employs a distilled model architecture and a temporal enhancer to optimize performance while minimizing computational costs. By utilizing a curriculum learning strategy during training, the model Editto achieves exceptional instruction-following capabilities, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "Ditto框架：解决视频编辑数据稀缺的创新之路",
                    "desc": "Ditto框架旨在解决基于指令的视频编辑中的数据稀缺问题，通过生成一个大型数据集并使用课程学习策略来训练Editto模型，从而实现更优的指令跟随能力。该框架的核心是一个新颖的数据生成管道，结合了领先图像编辑器的创意多样性和上下文视频生成器，克服了现有模型的局限性。为了降低成本和提高质量，Ditto采用了一种高效的精简模型架构，并通过时间增强器来减少计算开销和改善时间一致性。最终，Ditto通过智能代理生成多样化的指令并严格过滤输出，确保了大规模的质量控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14438",
            "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
            "url": "https://huggingface.co/papers/2510.14438",
            "abstract": "A new paradigm, Explore to Evolve, is proposed to enhance web agents' information aggregation by constructing a large dataset and developing foundation models that outperform existing models on a challenging benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
            "score": 7,
            "issue_id": 6498,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 октября",
                "en": "October 16",
                "zh": "10月16日"
            },
            "hash": "514e4a9f0daa79b9",
            "authors": [
                "Rui Wang",
                "Ce Zhang",
                "Jun-Yu Ma",
                "Jianshu Zhang",
                "Hongru Wang",
                "Yi Chen",
                "Boyang Xue",
                "Tianqing Fang",
                "Zhisong Zhang",
                "Hongming Zhang",
                "Haitao Mi",
                "Dong Yu",
                "Kam-Fai Wong"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14438.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#science",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🕷️",
                "ru": {
                    "title": "Учимся не просто искать, а думать: веб-агенты, которые анализируют информацию",
                    "desc": "Исследователи предложили новую парадигму Explore to Evolve для обучения веб-агентов не только находить информацию в интернете, но и глубоко её анализировать и агрегировать. Они создали датасет WebAggregatorQA из 10 тысяч примеров с 50 тысяч сайтов, где агент сам исследует веб и формирует логические цепочки для синтеза проверяемых вопросов-ответов. На основе этих данных обучили модели WebAggregator, где версия на 32B параметров превосходит GPT-4.1 более чем на 10% на бенчмарке GAIA-text. Существующие LLM показывают слабые результаты на новом бенчмарке (Claude-3.7-sonnet достигает лишь 28%), что подчеркивает важность развития навыков агрегации информации у AI-агентов."
                },
                "en": {
                    "title": "Explore to Evolve: Revolutionizing Information Aggregation for Web Agents",
                    "desc": "The paper introduces a new approach called Explore to Evolve, aimed at improving how web agents gather and analyze information. It emphasizes the importance of not just finding data but also effectively aggregating it for deeper insights. By creating a large dataset, WebAggregatorQA, the authors develop advanced foundation models that significantly outperform existing ones on a challenging benchmark. This work highlights the necessity of enhancing information aggregation capabilities in web agents to support comprehensive research."
                },
                "zh": {
                    "title": "探索以进化：提升网络代理的信息聚合能力",
                    "desc": "本文提出了一种新的范式——探索以进化，旨在通过构建大型数据集和开发基础模型来增强网络代理的信息聚合能力。这些深度研究网络代理不仅从多种来源检索信息，还需要严格分析和聚合知识，以支持深入研究。现有的开源深度研究代理主要关注信息检索能力，而忽视了信息聚合的必要性。通过主动在线探索，代理能够收集真实网络中的信息，并自我进化出聚合程序，从而生成可验证的问答对。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15857",
            "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
            "url": "https://huggingface.co/papers/2510.15857",
            "abstract": "BLIP3o-NEXT, a unified text-to-image generation and image editing model, uses an Autoregressive + Diffusion architecture to achieve superior performance and realism.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.",
            "score": 4,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "2f0d627fa6567a03",
            "authors": [
                "Jiuhai Chen",
                "Le Xue",
                "Zhiyang Xu",
                "Xichen Pan",
                "Shusheng Yang",
                "Can Qin",
                "An Yan",
                "Honglu Zhou",
                "Zeyuan Chen",
                "Lifu Huang",
                "Tianyi Zhou",
                "Junnan Li",
                "Silvio Savarese",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15857.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Объединение авторегрессии и диффузии для реалистичной генерации изображений",
                    "desc": "BLIP3o-NEXT — это полностью open-source модель, объединяющая генерацию изображений из текста и редактирование изображений в единой архитектуре. Модель использует комбинированный подход Autoregressive + Diffusion: сначала авторегрессионная модель генерирует дискретные токены изображения, затем диффузионная модель создаёт высококачественное изображение на основе этих токенов. Авторы применили reinforcement learning для улучшения качества генерации и использовали data engine для повышения способности следовать инструкциям при редактировании. Модель демонстрирует превосходные результаты на бенчмарках по генерации и редактированию изображений, достигая нового уровня реалистичности."
                },
                "en": {
                    "title": "BLIP3o-NEXT: Unifying Text-to-Image Generation and Editing with Advanced Architecture",
                    "desc": "BLIP3o-NEXT is a cutting-edge model that combines text-to-image generation and image editing into one framework using an Autoregressive + Diffusion architecture. This model excels in producing high-quality images by first generating image tokens with an autoregressive model, which are then refined by a diffusion model for enhanced realism. Key insights from the research highlight the importance of efficient architecture, the role of reinforcement learning, and the impact of data quality on performance. Overall, BLIP3o-NEXT sets a new standard in the field by achieving superior results in both generating and editing images."
                },
                "zh": {
                    "title": "统一文本到图像生成与编辑的创新模型",
                    "desc": "BLIP3o-NEXT 是一种统一的文本到图像生成和图像编辑模型，采用自回归与扩散架构，展现出卓越的性能和真实感。该模型结合了文本生成和图像编辑的能力，能够在单一架构中实现高质量的图像生成。研究表明，架构的选择、强化学习的应用、图像编辑的挑战以及数据质量和规模都是影响模型性能的关键因素。通过自回归和扩散模型的结合，BLIP3o-NEXT 实现了更高水平的连贯性和真实感。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15564",
            "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
            "url": "https://huggingface.co/papers/2510.15564",
            "abstract": "A vision-guided 3D layout generation system uses an image generation model and scene graphs to produce rich and coherent 3D scenes from prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium.",
            "score": 3,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "2a261aeaa3efed28",
            "authors": [
                "Xiaoming Zhu",
                "Xu Huang",
                "Qinghongbing Xie",
                "Zhi Deng",
                "Junsheng Yu",
                "Yirui Guan",
                "Zhongyuan Liu",
                "Lin Zhu",
                "Qijun Zhao",
                "Ligang Liu",
                "Long Zeng"
            ],
            "affiliations": [
                "Southeast University, China",
                "Tencent, China",
                "Tsinghua University, China",
                "University of Science and Technology of China, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15564.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#graphs",
                    "#games",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Создание богатых 3D сцен с помощью визуально управляемой генерации",
                    "desc": "В статье представлена новая система генерации 3D макетов, управляемая визуальными данными, которая использует модель генерации изображений и графы сцен для создания богатых и согласованных 3D сцен из текстовых подсказок. Традиционные методы оптимизации часто ограничены сложными ручными правилами, а глубокие генеративные модели сталкиваются с трудностями в создании разнообразного контента. Для решения этих проблем авторы разработали модуль анализа изображений, который восстанавливает 3D макет сцен на основе визуальной семантики и геометрической информации. Тестирование показало, что их алгоритм значительно превосходит существующие методы по богатству и качеству макетов."
                },
                "en": {
                    "title": "Transforming Prompts into Rich 3D Scenes with Vision-Guided Generation",
                    "desc": "This paper introduces a vision-guided system for generating 3D layouts that combines image generation models with scene graphs. It addresses limitations of traditional methods and deep generative models by creating a comprehensive asset library and fine-tuning an image generation model to enhance prompt representations. The system includes a robust image parsing module that extracts 3D layouts from images, ensuring accurate spatial relationships. User testing shows that this approach significantly improves the richness and quality of generated layouts compared to existing techniques."
                },
                "zh": {
                    "title": "视觉引导的3D布局生成新方法",
                    "desc": "本文提出了一种基于视觉引导的3D布局生成系统，旨在从提示中生成丰富且连贯的3D场景。该系统首先构建了一个高质量的资产库，包含2037个场景资产和147个3D场景布局。接着，利用图像生成模型将提示表示扩展为图像，并进行微调以与资产库对齐。最后，通过场景图和整体视觉语义优化场景布局，确保逻辑一致性和与图像的对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15232",
            "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in\n  Finance Domain",
            "url": "https://huggingface.co/papers/2510.15232",
            "abstract": "FinTrust is a benchmark designed to evaluate the trustworthiness of LLMs in finance applications, focusing on alignment issues and revealing gaps in legal awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent LLMs have demonstrated promising ability in solving finance related problems. However, applying LLMs in real-world finance application remains challenging due to its high risk and high stakes property. This paper introduces FinTrust, a comprehensive benchmark specifically designed for evaluating the trustworthiness of LLMs in finance applications. Our benchmark focuses on a wide range of alignment issues based on practical context and features fine-grained tasks for each dimension of trustworthiness evaluation. We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini outperforms in most tasks such as safety while open-source models like DeepSeek-V3 have advantage in specific areas like industry-level fairness. For challenging task like fiduciary alignment and disclosure, all LLMs fall short, showing a significant gap in legal awareness. We believe that FinTrust can be a valuable benchmark for LLMs' trustworthiness evaluation in finance domain.",
            "score": 3,
            "issue_id": 6499,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "8cf91a55955e8ba4",
            "authors": [
                "Tiansheng Hu",
                "Tongyan Hu",
                "Liuyang Bai",
                "Yilun Zhao",
                "Arman Cohan",
                "Chen Zhao"
            ],
            "affiliations": [
                "Center for Data Science, New York University",
                "NYU Shanghai",
                "National University of Singapore",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15232.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "🏦",
                "ru": {
                    "title": "FinTrust: проверка AI на надёжность в финансах",
                    "desc": "Исследователи представили FinTrust - benchmark для оценки надёжности LLM в финансовых приложениях. Benchmark фокусируется на проблемах alignment и включает детальные задачи для каждого аспекта trustworthiness. Тестирование одиннадцати моделей показало, что проприетарные модели вроде o4-mini лучше справляются с безопасностью, а open-source модели типа DeepSeek-V3 сильнее в специфических областях. Все модели показали слабые результаты в задачах, связанных с юридической осведомлённостью и фидуциарными обязательствами."
                },
                "en": {
                    "title": "Evaluating Trustworthiness of LLMs in Finance with FinTrust",
                    "desc": "FinTrust is a new benchmark created to assess how trustworthy large language models (LLMs) are when used in finance. It highlights important issues related to alignment, which means how well the models' outputs match the expectations and needs of users in financial contexts. The benchmark includes detailed tasks that evaluate different aspects of trustworthiness, such as safety and fairness. The study shows that while some proprietary models perform better overall, there are still significant gaps in legal awareness across all models, especially in complex areas like fiduciary alignment."
                },
                "zh": {
                    "title": "FinTrust：评估金融领域LLMs可信度的新基准",
                    "desc": "FinTrust是一个专门用于评估大型语言模型（LLMs）在金融应用中可信度的基准。该基准关注对齐问题，并揭示法律意识的不足。研究表明，尽管一些专有模型在安全性等任务中表现优异，但在信托对齐和信息披露等复杂任务上，所有模型都存在明显的不足。FinTrust为金融领域的LLMs可信度评估提供了一个有价值的工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15831",
            "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
            "url": "https://huggingface.co/papers/2510.15831",
            "abstract": "VISTA, a multi-agent system, iteratively refines user prompts to enhance video quality and alignment with user intent, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
            "score": 2,
            "issue_id": 6499,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "21496ed87b0f95aa",
            "authors": [
                "Do Xuan Long",
                "Xingchen Wan",
                "Hootan Nakhost",
                "Chen-Yu Lee",
                "Tomas Pfister",
                "Sercan Ö. Arık"
            ],
            "affiliations": [
                "Google",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15831.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#video",
                    "#optimization",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Мультиагентная система для автоматического улучшения промптов в генерации видео",
                    "desc": "В работе представлена VISTA — мультиагентная система, которая итеративно улучшает качество генерируемого видео путём автоматической оптимизации промптов. Система разбивает задачу пользователя на временной план, генерирует варианты видео, выбирает лучший через турнирное сравнение, а затем три специализированных агента анализируют визуальные, аудио и контекстные аспекты. На основе этой обратной связи reasoning-агент переписывает промпт для следующей итерации генерации. Эксперименты показывают, что VISTA превосходит существующие baseline методы с частотой выигрыша до 60% в парных сравнениях, а человеческие оценщики предпочитают результаты VISTA в 66.4% случаев."
                },
                "en": {
                    "title": "VISTA: Iterative Prompt Refinement for Superior Video Generation",
                    "desc": "VISTA is a multi-agent system designed to improve the quality of AI-generated videos by refining user prompts through an iterative process. It breaks down user ideas into a structured plan and generates videos based on these prompts. After generating the videos, VISTA uses a pairwise tournament to select the best output, which is then evaluated by specialized agents for visual, audio, and contextual quality. The feedback from these agents is used to enhance the original prompt, leading to consistently better video quality and alignment with user intent compared to existing methods."
                },
                "zh": {
                    "title": "VISTA：迭代优化视频生成的智能系统",
                    "desc": "VISTA是一种多智能体系统，通过迭代优化用户提示来提升视频质量和与用户意图的一致性。该系统首先将用户的想法分解为结构化的时间计划，然后生成视频，并通过强有力的对比赛选出最佳视频。接着，三个专门的智能体对视频的视觉、音频和上下文准确性进行评估，最后一个推理智能体根据反馈重新编写提示，以便在下一个生成周期中进行改进。实验结果表明，VISTA在视频质量和用户意图对齐方面表现优于现有方法，赢得了高达60%的对比胜率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15262",
            "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
            "url": "https://huggingface.co/papers/2510.15262",
            "abstract": "A new weight-decay scaling rule for AdamW is introduced to preserve sublayer gain across widths in modern scale-invariant architectures, enabling zero-shot transfer of learning rate and weight decay.  \t\t\t\t\tAI-generated summary \t\t\t\t Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximal-update parameterization (muP) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizer-governed steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading muP transfer. We address this by introducing a weight-decay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as eta/lambda with an approximately invariant shape; under width scaling d, we observe that the top singular value scales approximately as eta/lambdacdot d^{0.75}. Combining this observation with the muP learning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an empirical weight-decay scaling rule lambda_2propto d that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields zero-shot transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in a minimal synthetic setting, and we provide a simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend muP beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW.",
            "score": 2,
            "issue_id": 6499,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "3f4b8d1b04131eb0",
            "authors": [
                "Zhiyuan Fan",
                "Yifeng Liu",
                "Qingyue Zhao",
                "Angela Yuan",
                "Quanquan Gu"
            ],
            "affiliations": [
                "Department of Computer Science, UCLA, CA, USA",
                "Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15262.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#transfer_learning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Масштабирование weight decay для переноса гиперпараметров между разными размерами моделей",
                    "desc": "Исследователи решили проблему переноса learning rate и weight decay между нейросетями разной ширины в современных архитектурах с нормализацией. Они обнаружили, что в установившемся режиме тренировки сингулярные значения матриц параметров масштабируются как d^0.75, где d — ширина модели. На основе этого наблюдения они вывели правило масштабирования weight decay пропорционально ширине модели (λ∝d), которое сохраняет усиление в подслоях инвариантным. Новый метод позволяет переносить оба гиперпараметра от маленьких моделей к большим без дополнительного подбора, что проверено на Transformer-архитектурах в стиле LLaMA."
                },
                "en": {
                    "title": "Achieving Width-Invariant Training with New AdamW Scaling Rules",
                    "desc": "This paper presents a new scaling rule for weight decay in the AdamW optimizer, aimed at maintaining consistent performance across different model widths in scale-invariant architectures. The authors highlight that traditional training methods can lead to a dependency of the effective learning rate on the model width, which can hinder the transfer of learning rates and weight decay parameters. By introducing a weight-decay scaling rule that aligns with the observed singular value behavior of matrix parameters, they ensure that sublayer gains remain invariant across varying widths. The proposed method allows for zero-shot transfer of hyperparameters, simplifying the training process and enhancing the robustness of models like LLaMA-style Transformers."
                },
                "zh": {
                    "title": "实现宽度不变的学习率与权重衰减转移",
                    "desc": "本文提出了一种新的AdamW权重衰减缩放规则，以保持现代尺度不变架构中子层增益的宽度不变性。这种方法使得学习率和权重衰减能够在不同宽度之间进行零-shot转移。通过观察矩阵参数的奇异值谱，我们发现其在宽度缩放下的顶级奇异值呈现出特定的缩放规律。最终，我们在LLaMA风格的Transformer上验证了这一规则，提供了一种实用的超参数转移方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15110",
            "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.15110",
            "abstract": "DLER, a reinforcement learning training recipe, improves the accuracy-efficiency trade-off in reasoning language models by addressing challenges in advantage estimation, entropy collapse, and sparse reward signals, leading to shorter outputs and better test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve strong performance via extended chains of thought but often generate unnecessarily long outputs. Maximizing intelligence per token--accuracy relative to response length--remains an open problem. We revisit reinforcement learning (RL) with the simplest length penalty--truncation--and show that accuracy degradation arises not from the lack of sophisticated penalties but from inadequate RL optimization. We identify three key challenges: (i) large bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward signal. We address them with Doing Length pEnalty Right (DLER), a training recipe combining batch-wise reward normalization, higher clipping, dynamic sampling, and a simple truncation length penalty. DLER achieves state-of-the-art accuracy--efficiency trade-offs, cutting output length by over 70 percent while surpassing all previous baseline accuracy. It also improves test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple concise responses in parallel with 28 percent higher accuracy and lower latency. We further introduce Difficulty-Aware DLER, which adaptively tightens truncation on easier questions for additional efficiency gains. We also propose an update-selective merging method that preserves baseline accuracy while retaining the concise reasoning ability of the DLER model, which is useful for scenarios where RL training data is scarce.",
            "score": 2,
            "issue_id": 6499,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 октября",
                "en": "October 16",
                "zh": "10月16日"
            },
            "hash": "c79ed5b395aa6db2",
            "authors": [
                "Shih-Yang Liu",
                "Xin Dong",
                "Ximing Lu",
                "Shizhe Diao",
                "Mingjie Liu",
                "Min-Hung Chen",
                "Hongxu Yin",
                "Yu-Chiang Frank Wang",
                "Kwang-Ting Cheng",
                "Yejin Choi",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "HKUST",
                "NVIDIA Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15110.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Короче и точнее: как научить LLM рассуждать эффективно",
                    "desc": "Статья представляет DLER — метод обучения языковых моделей с reinforcement learning, который решает проблему избыточной длины рассуждений в моделях типа OpenAI-o1 и DeepSeek-R1. Авторы выявили три ключевые проблемы RL-оптимизации: смещение в оценке advantage, коллапс энтропии и разреженный reward signal. DLER использует batch-wise нормализацию наград, динамическую выборку и простой truncation penalty, сокращая длину выходных данных на 70% при улучшении точности. Метод также включает адаптивную версию для разных уровней сложности задач и технику слияния моделей для сохранения базовой точности."
                },
                "en": {
                    "title": "Maximizing Intelligence per Token with DLER",
                    "desc": "The paper introduces DLER, a new reinforcement learning training method that enhances the balance between accuracy and efficiency in reasoning language models. It tackles significant issues like bias in advantage estimation, entropy collapse, and sparse reward signals, which often lead to longer and less efficient outputs. By implementing techniques such as batch-wise reward normalization and dynamic sampling, DLER reduces output length by over 70% while achieving superior accuracy compared to previous models. Additionally, the paper presents Difficulty-Aware DLER, which optimizes response length based on question difficulty, further improving efficiency and performance."
                },
                "zh": {
                    "title": "DLER：提升推理模型的效率与准确性",
                    "desc": "DLER是一种强化学习训练方法，旨在改善推理语言模型的准确性与效率之间的平衡。它解决了优势估计偏差、熵崩溃和稀疏奖励信号等挑战，从而生成更短的输出并提高测试时的扩展性。通过结合批量奖励归一化、更高的剪切、动态采样和简单的截断长度惩罚，DLER实现了最先进的准确性和效率权衡。该方法在生成多个简洁响应时，准确性提高了28%，并且延迟更低，适用于强化学习数据稀缺的场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15842",
            "title": "Paper2Web: Let's Make Your Paper Alive!",
            "url": "https://huggingface.co/papers/2510.15842",
            "abstract": "Paper2Web is a benchmark and evaluation framework for academic webpage generation, featuring PWAgent, an autonomous pipeline that enhances content and layout through MCP tools, outperforming end-to-end baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
            "score": 1,
            "issue_id": 6498,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "660d9e11e8072a57",
            "authors": [
                "Yuhang Chen",
                "Tianpeng Lv",
                "Siyi Zhang",
                "Yixiang Yin",
                "Yao Wan",
                "Philip S. Yu",
                "Dongping Chen"
            ],
            "affiliations": [
                "ONE Lab, Huazhong University of Science and Technology",
                "University of Illinois Chicago",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15842.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Превращаем научные статьи в красивые интерактивные сайты с помощью AI-агента",
                    "desc": "Статья представляет Paper2Web — бенчмарк и фреймворк для оценки генерации академических веб-страниц из научных статей. Авторы разработали PWAgent — автономный pipeline, который преобразует научные работы в интерактивные мультимедийные сайты, итеративно улучшая контент и layout с помощью MCP-инструментов. Оценка включает rule-based метрики (связность, полнота), LLM-as-a-Judge для проверки интерактивности и эстетики, а также PaperQuiz для измерения сохранности знаний из статьи. Эксперименты показали, что PWAgent значительно превосходит end-to-end baseline подходы, включая template-based решения и версии arXiv, достигая оптимального баланса качества и стоимости."
                },
                "en": {
                    "title": "Transforming Academic Papers into Engaging Web Experiences",
                    "desc": "The paper introduces Paper2Web, a new benchmark and evaluation framework designed for generating academic webpages. It highlights the limitations of existing methods like direct LLM generation and templates, which often fail to create interactive and well-structured sites. The framework includes various metrics for assessing webpage quality, such as Connectivity and Completeness, along with a unique evaluation tool called PaperQuiz for knowledge retention. Additionally, the paper presents PWAgent, an autonomous system that enhances the content and layout of academic webpages, demonstrating superior performance compared to traditional methods."
                },
                "zh": {
                    "title": "Paper2Web：提升学术网页生成的智能化解决方案",
                    "desc": "Paper2Web是一个用于学术网页生成的基准和评估框架，旨在提高研究成果的传播效果。它引入了PWAgent，一个自主的管道，利用多种工具优化内容和布局，显著超越了传统的端到端基线方法。该框架包括多维度的评估指标，如连通性、完整性，以及人类验证的LLM评估，确保生成网页的互动性、美观性和信息量。实验结果表明，PWAgent在生成学术网页时，能够以低成本实现高质量的内容展示。"
                }
            }
        }
    ],
    "link_prev": "2025-10-17.html",
    "link_next": "2025-10-21.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10月17日"
    },
    "short_date_next": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10月21日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}