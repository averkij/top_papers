{
    "date": {
        "ru": "12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 12",
        "zh": "8æœˆ12æ—¥"
    },
    "time_utc": "2025-08-12 03:44",
    "weekday": 1,
    "issue_id": 5295,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.07999",
            "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
            "url": "https://huggingface.co/papers/2508.07999",
            "abstract": "WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/",
            "score": 44,
            "issue_id": 5295,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 11",
                "zh": "8æœˆ11æ—¥"
            },
            "hash": "45f13eb4f1110e39",
            "authors": [
                "Ryan Wong",
                "Jiawei Wang",
                "Junjie Zhao",
                "Li Chen",
                "Yan Gao",
                "Long Zhang",
                "Xuan Zhou",
                "Zuo Wang",
                "Kai Xiang",
                "Ge Zhang",
                "Wenhao Huang",
                "Yang Wang",
                "Ke Wang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07999.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#agents",
                    "#dataset",
                    "#survey"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸",
                    "desc": "WideSearch - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ±Ğ¾Ñ€Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 200 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 15 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 10 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 0%, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 5%. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "WideSearch: Evaluating the Reliability of Automated Search Agents",
                    "desc": "WideSearch is a benchmark designed to assess the reliability of automated search agents in large-scale information collection tasks. It highlights significant shortcomings in current systems, which struggle to perform effectively in wide-context searches. The benchmark includes 200 curated questions across various domains, requiring agents to gather and organize information that can be objectively verified. Results show that most tested systems have low success rates, indicating a need for improvement in agentic search capabilities."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨æœç´¢ä»£ç†çš„å¯é æ€§",
                    "desc": "WideSearchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨æœç´¢ä»£ç†åœ¨å¤§è§„æ¨¡ä¿¡æ¯æ”¶é›†ä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚å½“å‰çš„ç³»ç»Ÿåœ¨æ‰§è¡Œè¿™äº›ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼ŒæˆåŠŸç‡æ™®éæ¥è¿‘0%ã€‚è¯¥åŸºå‡†åŒ…å«200ä¸ªæ‰‹åŠ¨ç­–åˆ’çš„é—®é¢˜ï¼Œæ¶µç›–15ä¸ªä¸åŒé¢†åŸŸï¼Œæ—¨åœ¨æµ‹è¯•ä»£ç†æ”¶é›†å’Œç»„ç»‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æœç´¢ä»£ç†åœ¨å¤§è§„æ¨¡ä¿¡æ¯è·å–æ–¹é¢äºŸéœ€æ”¹è¿›ï¼Œæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æ–¹å‘åº”é›†ä¸­åœ¨æå‡å…¶æ€§èƒ½ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05614",
            "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
            "url": "https://huggingface.co/papers/2508.05614",
            "abstract": "OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.",
            "score": 11,
            "issue_id": 5294,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "d9c4313e65f213fe",
            "authors": [
                "Zixuan Wang",
                "Dingming Li",
                "Hongxing Li",
                "Shuo Chen",
                "Yuchen Yan",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05614.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "OmniEAR: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸",
                    "desc": "OmniEAR - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ² 1500 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‡ĞµĞ¼ Ñ‚Ğµ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Evaluating Language Models in Real-World Reasoning Tasks",
                    "desc": "OmniEAR is a framework designed to assess how well language models can reason in real-world scenarios involving physical interactions, tool usage, and teamwork. It highlights that while these models perform well with clear instructions, their performance significantly drops when faced with constraints or when they need to figure things out on their own. The study shows that even with complete information about the environment, models struggle to coordinate effectively, revealing limitations in their architecture. Overall, OmniEAR serves as a new benchmark to push the boundaries of embodied AI capabilities."
                },
                "zh": {
                    "title": "OmniEARï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹çš„ä½“ç°æ¨ç†èƒ½åŠ›",
                    "desc": "OmniEARæ˜¯ä¸€ä¸ªè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†äº¤äº’ã€å·¥å…·ä½¿ç”¨å’Œå¤šæ™ºèƒ½ä½“åè°ƒä¸­çš„ä½“ç°æ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚ç ”ç©¶å‘ç°ï¼Œå½“æ¨¡å‹åœ¨çº¦æŸæ¡ä»¶ä¸‹è¿›è¡Œæ¨ç†æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å·¥å…·æ¨ç†å’Œéšæ€§åä½œä»»åŠ¡ä¸­ã€‚å°½ç®¡åœ¨æ˜ç¡®æŒ‡ä»¤ä¸‹æ¨¡å‹çš„æˆåŠŸç‡é«˜è¾¾85-96%ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­å¤±è´¥ç‡è¶…è¿‡50%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä½“ç°æ¨ç†æ—¶é¢ä¸´æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼ŒOmniEARä¸ºè¯„ä¼°å’Œæ¨åŠ¨ä½“ç°äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸¥æ ¼çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.07629",
            "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization",
            "url": "https://huggingface.co/papers/2508.07629",
            "abstract": "Klear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\% on AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.",
            "score": 9,
            "issue_id": 5295,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 11",
                "zh": "8æœˆ11æ—¥"
            },
            "hash": "a361b2d0bb1f93c2",
            "authors": [
                "Zhenpeng Su",
                "Leiyu Pan",
                "Xue Bai",
                "Dening Liu",
                "Guanting Dong",
                "Jiaming Huang",
                "Wenping Hu",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Klear Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07629.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization",
                    "#math",
                    "#plp",
                    "#rl",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Klear-Reasoner: ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Klear-Reasoner - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Klear-Reasoner Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… AIME Ğ¸ LiveCodeBench."
                },
                "en": {
                    "title": "Klear-Reasoner: Mastering Long Reasoning with Enhanced Learning Techniques",
                    "desc": "Klear-Reasoner is a machine learning model designed for long reasoning tasks, showcasing its ability to solve complex problems effectively. It utilizes a comprehensive post-training workflow that includes long Chain-of-Thought supervised fine-tuning and a novel reinforcement learning approach called Gradient-Preserving clipping Policy Optimization. The model demonstrates that using a few high-quality data sources can be more beneficial than many diverse ones, and it addresses issues in current reinforcement learning methods that hinder exploration. Klear-Reasoner achieves impressive performance on various benchmarks, particularly in mathematics and programming tasks."
                },
                "zh": {
                    "title": "Klear-Reasonerï¼šé•¿æ¨ç†èƒ½åŠ›çš„çªç ´",
                    "desc": "Klear-Reasoneræ˜¯ä¸€ç§å…·æœ‰é•¿æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§£å†³é—®é¢˜æ—¶è¿›è¡Œç»†è‡´çš„æ€è€ƒï¼Œè¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡è¯¦ç»†çš„åè®­ç»ƒå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´çš„ç›‘ç£å¾®è°ƒå’Œæ¢¯åº¦ä¿ç•™å‰ªåˆ‡ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¾¾åˆ°äº†é«˜æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°‘é‡é«˜è´¨é‡çš„æ•°æ®æºæ¯”å¤§é‡å¤šæ ·åŒ–çš„æ•°æ®æºæ›´æœ‰æ•ˆï¼Œä¸”å›°éš¾æ ·æœ¬åœ¨æ²¡æœ‰å‡†ç¡®æ€§è¿‡æ»¤çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼ŒKlear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œå–å¾—äº†å¤šä¸ªåŸºå‡†æµ‹è¯•çš„é«˜åˆ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06600",
            "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of\n  Deep-Research Agent",
            "url": "https://huggingface.co/papers/2508.06600",
            "abstract": "BrowseComp-Plus, a curated benchmark, enables controlled evaluation of deep research agents and retrieval methods, providing insights into their performance and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.",
            "score": 9,
            "issue_id": 5295,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "bbe851198d9c0416",
            "authors": [
                "Zijian Chen",
                "Xueguang Ma",
                "Shengyao Zhuang",
                "Ping Nie",
                "Kai Zou",
                "Andrew Liu",
                "Joshua Green",
                "Kshama Patel",
                "Ruoxi Meng",
                "Mingyi Su",
                "Sahel Sharifymoghaddam",
                "Yanxi Li",
                "Haoran Hong",
                "Xinyu Shi",
                "Xuye Liu",
                "Nandan Thakur",
                "Crystina Zhang",
                "Luyu Gao",
                "Wenhu Chen",
                "Jimmy Lin"
            ],
            "affiliations": [
                "CSIRO",
                "Carnegie Mellon University",
                "Independent",
                "The University of Queensland",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06600.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#ethics",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "BrowseComp-Plus: ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "BrowseComp-Plus - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹, Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹. Ğ¢ĞµÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ GPT-5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 55.9%, Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ¼ Qwen3-Embedding-8B Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ğ´Ğ¾ 70.1%. BrowseComp-Plus ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ± ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "BrowseComp-Plus: A Fair Benchmark for Deep Research Evaluation",
                    "desc": "BrowseComp-Plus is a new benchmark designed to evaluate deep research agents and retrieval methods in a controlled manner. It addresses limitations of existing benchmarks by using a fixed, curated document corpus, allowing for fair comparisons and reproducibility. The benchmark includes human-verified documents and challenging negatives for each query, enabling detailed analysis of the performance of different deep research systems. Results show significant improvements in accuracy when using advanced models like GPT-5, highlighting the effectiveness of this new evaluation framework."
                },
                "zh": {
                    "title": "BrowseComp-Plusï¼šæ·±åº¦ç ”ç©¶çš„å…¬å¹³è¯„ä¼°å·¥å…·",
                    "desc": "BrowseComp-Plusæ˜¯ä¸€ä¸ªç»è¿‡ç²¾å¿ƒç­–åˆ’çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¯¹æ·±åº¦ç ”ç©¶ä»£ç†å’Œæ£€ç´¢æ–¹æ³•è¿›è¡Œæ§åˆ¶è¯„ä¼°ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨å…¬å¹³æ€§å’Œé€æ˜æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡ä½¿ç”¨å›ºå®šçš„æ–‡æ¡£åº“æ¥ç¡®ä¿å®éªŒçš„å¯æ§æ€§ã€‚æ¯ä¸ªæŸ¥è¯¢éƒ½åŒ…å«ç»è¿‡äººå·¥éªŒè¯çš„æ”¯æŒæ–‡æ¡£å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è´Ÿæ ·æœ¬ï¼Œä»è€Œä½¿å¾—å®éªŒç»“æœæ›´å…·å¯ä¿¡åº¦ã€‚è¯¥åŸºå‡†æµ‹è¯•æœ‰æ•ˆåŒºåˆ†äº†ä¸åŒæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„æ€§èƒ½ï¼Œæä¾›äº†å¯¹æ£€ç´¢æœ‰æ•ˆæ€§å’Œå¼•ç”¨å‡†ç¡®æ€§çš„æ·±å…¥åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.08134",
            "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
            "url": "https://huggingface.co/papers/2508.08134",
            "abstract": "Follow-Your-Shape framework uses a Trajectory Divergence Map and Scheduled KV Injection to enable precise and controllable shape editing in images while preserving non-target content.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.",
            "score": 5,
            "issue_id": 5295,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 11",
                "zh": "8æœˆ11æ—¥"
            },
            "hash": "46943e7de387bfc9",
            "authors": [
                "Zeqian Long",
                "Mingzhe Zheng",
                "Kunyu Feng",
                "Xinhua Zhang",
                "Hongyu Liu",
                "Harry Yang",
                "Linfeng Zhang",
                "Qifeng Chen",
                "Yue Ma"
            ],
            "affiliations": [
                "HKUST",
                "Shanghai Jiao Tong University",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.08134.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#games"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¾Ğ½Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Follow-Your-Shape Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñƒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ (Trajectory Divergence Map) Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ (Scheduled KV Injection) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ReShapeBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼."
                },
                "en": {
                    "title": "Precision in Shape Editing with Follow-Your-Shape",
                    "desc": "The Follow-Your-Shape framework introduces a novel approach for shape editing in images, focusing on maintaining the quality of non-target content. It utilizes a Trajectory Divergence Map (TDM) to analyze differences in editing paths, allowing for precise localization of areas that can be modified. Additionally, the Scheduled KV Injection mechanism ensures that the editing process remains stable and accurate. This method outperforms existing models, especially in complex scenarios involving significant shape transformations, while also introducing a new benchmark for evaluating shape-aware editing."
                },
                "zh": {
                    "title": "ç²¾ç¡®å¯æ§çš„å½¢çŠ¶ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "Follow-Your-Shapeæ¡†æ¶åˆ©ç”¨è½¨è¿¹å‘æ•£å›¾å’Œè°ƒåº¦KVæ³¨å…¥æŠ€æœ¯ï¼Œå®ç°äº†å›¾åƒä¸­å½¢çŠ¶çš„ç²¾ç¡®å’Œå¯æ§ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒéç›®æ ‡å†…å®¹çš„å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰åŸºäºæµçš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å¤§è§„æ¨¡å½¢çŠ¶å˜æ¢ä¸­çš„ä¸è¶³ï¼Œé¿å…äº†å¯¹éç›®æ ‡åŒºåŸŸçš„æ„å¤–ä¿®æ”¹ã€‚é€šè¿‡è®¡ç®—åæ¼”å’Œå»å™ªè·¯å¾„ä¹‹é—´çš„é€Ÿåº¦å·®å¼‚ï¼ŒFollow-Your-Shapeèƒ½å¤Ÿç²¾ç¡®å®šä½å¯ç¼–è¾‘åŒºåŸŸã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ReShapeBenchåŸºå‡†ï¼Œä¸“é—¨ç”¨äºå½¢çŠ¶æ„ŸçŸ¥ç¼–è¾‘çš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡å½¢çŠ¶æ›¿æ¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.07917",
            "title": "MolmoAct: Action Reasoning Models that can Reason in Space",
            "url": "https://huggingface.co/papers/2508.07917",
            "abstract": "Action Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact",
            "score": 2,
            "issue_id": 5294,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 11",
                "zh": "8æœˆ11æ—¥"
            },
            "hash": "fa9d385dde41879a",
            "authors": [
                "Jason Lee",
                "Jiafei Duan",
                "Haoquan Fang",
                "Yuquan Deng",
                "Shuo Liu",
                "Boyang Li",
                "Bohan Fang",
                "Jieyu Zhang",
                "Yi Ru Wang",
                "Sangho Lee",
                "Winson Han",
                "Wilbert Pumacay",
                "Angelica Wu",
                "Rose Hendrix",
                "Karen Farley",
                "Eli VanderBilt",
                "Ali Farhadi",
                "Dieter Fox",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07917.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#robotics",
                    "#agents",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MolmoAct: Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Action Reasoning Models (ARM) Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². MolmoAct, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ARM, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MolmoAct Dataset, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 10000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Transforming Perception into Purposeful Action with ARMs",
                    "desc": "Action Reasoning Models (ARMs) are designed to enhance robotic behavior by combining perception, planning, and control in a structured way. The proposed model, MolmoAct, processes observations and instructions into depth-aware tokens, creates editable spatial plans, and predicts specific actions, making the robot's behavior more explainable and adaptable. MolmoAct demonstrates impressive performance in both simulated and real-world tasks, outperforming existing models in accuracy and generalization. Additionally, the introduction of the MolmoAct Dataset provides valuable training data, further improving the model's capabilities and establishing a new standard in robotics."
                },
                "zh": {
                    "title": "è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼šå°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„çš„è¡ŒåŠ¨",
                    "desc": "è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰ç»“åˆäº†æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶ï¼Œèƒ½å¤Ÿå®ç°çµæ´»å’Œå¯è§£é‡Šçš„æœºå™¨äººè¡Œä¸ºï¼Œæå‡äº†åœ¨å„ç§ä»»åŠ¡å’Œç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºçš„MolmoActæ¨¡å‹é€šè¿‡ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰é˜¶æ®µæµç¨‹ï¼Œå°†è§‚å¯Ÿå’ŒæŒ‡ä»¤ç¼–ç ä¸ºæ·±åº¦æ„ŸçŸ¥æ ‡è®°ï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„ä¸­çº§ç©ºé—´è®¡åˆ’ï¼Œå¹¶é¢„æµ‹ç²¾ç¡®çš„ä½çº§åŠ¨ä½œã€‚è¯¥æ¨¡å‹åœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é¦–æ¬¡å‘å¸ƒäº†MolmoActæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10,000æ¡é«˜è´¨é‡æœºå™¨äººè½¨è¿¹ï¼Œè®­ç»ƒè¯¥æ•°æ®é›†å¯æ˜¾è‘—æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06601",
            "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant\n  Safeguards into Open-Weight LLMs",
            "url": "https://huggingface.co/papers/2508.06601",
            "abstract": "Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.",
            "score": 0,
            "issue_id": 5294,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "4d2d99e849d503f3",
            "authors": [
                "Kyle O'Brien",
                "Stephen Casper",
                "Quentin Anthony",
                "Tomek Korbak",
                "Robert Kirk",
                "Xander Davies",
                "Ishan Mishra",
                "Geoffrey Irving",
                "Yarin Gal",
                "Stella Biderman"
            ],
            "affiliations": [
                "EleutherAI",
                "OATML, University of Oxford",
                "UK AI Security Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06601.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ±Ğ¸Ğ¾ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°Ñ… Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ°ÑĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ."
                },
                "en": {
                    "title": "Data Filtering: A Shield for Open-Weight AI Systems",
                    "desc": "This paper explores how filtering training data can improve the resilience of large language models (LLMs) against adversarial fine-tuning attacks. By removing text related to dual-use topics, the authors create a multi-stage data filtering pipeline that enhances the models' resistance without harming their unrelated capabilities. The study shows that pretrained models can withstand significant adversarial attacks, outperforming existing methods by a large margin. However, the research also highlights that while these models do not retain dangerous knowledge, they can still respond to such information when presented in context, indicating the need for comprehensive defense strategies."
                },
                "zh": {
                    "title": "æ•°æ®è¿‡æ»¤æå‡LLMå¯¹æŠ—æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µè¿›è¡Œæ•°æ®è¿‡æ»¤å¦‚ä½•å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹å¯¹æŠ—æ€§å¾®è°ƒæ”»å‡»çš„æŠµæŠ—åŠ›ï¼ŒåŒæ—¶ä¸å½±å“å…¶å…¶ä»–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è¿‡æ»¤ä¸åŒé‡ç”¨é€”ç›¸å…³çš„æ–‡æœ¬ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢æ¨¡å‹å­¦ä¹ ä¸å¿…è¦çš„èƒ½åŠ›ï¼Œä»è€Œæä¾›æ›´å¼ºçš„é˜²æŠ¤æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µçš„æ•°æ®è¿‡æ»¤æµç¨‹ï¼Œç»è¿‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠµå¾¡å¯¹æŠ—æ€§å¾®è°ƒæ”»å‡»æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æœªè§‚å¯Ÿåˆ°å¯¹æ— å…³èƒ½åŠ›çš„é™çº§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›å‘ç°ä¸ºå¼€æ”¾æƒé‡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå»ºç«‹äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„é˜²å¾¡å±‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-11.html",
    "link_next": "2025-08-13.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "11.08",
        "en": "08/11",
        "zh": "8æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.08",
        "en": "08/13",
        "zh": "8æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}