{
    "date": {
        "ru": "11 –∞–≤–≥—É—Å—Ç–∞",
        "en": "August 11",
        "zh": "8Êúà11Êó•"
    },
    "time_utc": "2025-08-11 23:11",
    "weekday": 0,
    "issue_id": 5292,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.06471",
            "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
            "url": "https://huggingface.co/papers/2508.06471",
            "abstract": "GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.",
            "score": 71,
            "issue_id": 5273,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 8",
                "zh": "8Êúà8Êó•"
            },
            "hash": "fdf71e495fcd6efa",
            "authors": [
                "GLM-4. 5 Team",
                ":",
                "Aohan Zeng",
                "Xin Lv",
                "Qinkai Zheng",
                "Zhenyu Hou",
                "Bin Chen",
                "Chengxing Xie",
                "Cunxiang Wang",
                "Da Yin",
                "Hao Zeng",
                "Jiajie Zhang",
                "Kedong Wang",
                "Lucen Zhong",
                "Mingdao Liu",
                "Rui Lu",
                "Shulin Cao",
                "Xiaohan Zhang",
                "Xuancheng Huang",
                "Yao Wei",
                "Yean Cheng",
                "Yifan An",
                "Yilin Niu",
                "Yuanhao Wen",
                "Yushi Bai",
                "Zhengxiao Du",
                "Zihan Wang",
                "Zilin Zhu",
                "Bohan Zhang",
                "Bosi Wen",
                "Bowen Wu",
                "Bowen Xu",
                "Can Huang",
                "Casey Zhao",
                "Changpeng Cai",
                "Chao Yu",
                "Chen Li",
                "Chendi Ge",
                "Chenghua Huang",
                "Chenhui Zhang",
                "Chenxi Xu",
                "Chenzheng Zhu",
                "Chuang Li",
                "Congfeng Yin",
                "Daoyan Lin",
                "Dayong Yang",
                "Dazhi Jiang",
                "Ding Ai",
                "Erle Zhu",
                "Fei Wang",
                "Gengzheng Pan",
                "Guo Wang",
                "Hailong Sun",
                "Haitao Li",
                "Haiyang Li",
                "Haiyi Hu",
                "Hanyu Zhang",
                "Hao Peng",
                "Hao Tai",
                "Haoke Zhang",
                "Haoran Wang",
                "Haoyu Yang",
                "He Liu",
                "He Zhao",
                "Hongwei Liu",
                "Hongxi Yan",
                "Huan Liu",
                "Huilong Chen",
                "Ji Li",
                "Jiajing Zhao",
                "Jiamin Ren",
                "Jian Jiao",
                "Jiani Zhao",
                "Jianyang Yan",
                "Jiaqi Wang",
                "Jiayi Gui",
                "Jiayue Zhao",
                "Jie Liu",
                "Jijie Li",
                "Jing Li",
                "Jing Lu",
                "Jingsen Wang",
                "Jingwei Yuan",
                "Jingxuan Li",
                "Jingzhao Du",
                "Jinhua Du",
                "Jinxin Liu",
                "Junkai Zhi",
                "Junli Gao",
                "Ke Wang",
                "Lekang Yang",
                "Liang Xu",
                "Lin Fan",
                "Lindong Wu",
                "Lintao Ding",
                "Lu Wang",
                "Man Zhang",
                "Minghao Li",
                "Minghuan Xu",
                "Mingming Zhao",
                "Mingshu Zhai",
                "Pengfan Du",
                "Qian Dong",
                "Shangde Lei",
                "Shangqing Tu",
                "Shangtong Yang",
                "Shaoyou Lu",
                "Shijie Li",
                "Shuang Li",
                "Shuang-Li",
                "Shuxun Yang",
                "Sibo Yi",
                "Tianshu Yu",
                "Wei Tian",
                "Weihan Wang",
                "Wenbo Yu",
                "Weng Lam Tam",
                "Wenjie Liang",
                "Wentao Liu",
                "Xiao Wang",
                "Xiaohan Jia",
                "Xiaotao Gu",
                "Xiaoying Ling",
                "Xin Wang",
                "Xing Fan",
                "Xingru Pan",
                "Xinyuan Zhang",
                "Xinze Zhang",
                "Xiuqing Fu",
                "Xunkai Zhang",
                "Yabo Xu",
                "Yandong Wu",
                "Yida Lu",
                "Yidong Wang",
                "Yilin Zhou",
                "Yiming Pan",
                "Ying Zhang",
                "Yingli Wang",
                "Yingru Li",
                "Yinpei Su",
                "Yipeng Geng",
                "Yitong Zhu",
                "Yongkun Yang",
                "Yuhang Li",
                "Yuhao Wu",
                "Yujiang Li",
                "Yunan Liu",
                "Yunqing Wang",
                "Yuntao Li",
                "Yuxuan Zhang",
                "Zezhen Liu",
                "Zhen Yang",
                "Zhengda Zhou",
                "Zhongpei Qiao",
                "Zhuoer Feng",
                "Zhuorui Liu",
                "Zichen Zhang",
                "Zihan Wang",
                "Zijun Yao",
                "Zikang Wang",
                "Ziqiang Liu",
                "Ziwei Chai",
                "Zixuan Li",
                "Zuodong Zhao",
                "Wenguang Chen",
                "Jidong Zhai",
                "Bin Xu",
                "Minlie Huang",
                "Hongning Wang",
                "Juanzi Li",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06471.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#architecture",
                    "#agi",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "GLM-4.5: –ú–æ—â–Ω–∞—è MoE-–º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò",
                    "desc": "GLM-4.5 - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è 355 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 23 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. GLM-4.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –º–Ω–æ–≥–∏–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–ø—É—Å—Ç–∏–ª–∏ –∫–∞–∫ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é GLM-4.5, —Ç–∞–∫ –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é GLM-4.5-Air –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö –∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."
                },
                "en": {
                    "title": "GLM-4.5: Powerful Reasoning with Fewer Parameters",
                    "desc": "GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors."
                },
                "zh": {
                    "title": "GLM-4.5ÔºöÂº∫Â§ßÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "GLM-4.5ÊòØ‰∏ÄÁßçÂÖ∑Êúâ3550‰∫øÂèÇÊï∞ÁöÑÊ∑∑Âêà‰∏ìÂÆ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåË°®Áé∞Âá∫Ëâ≤ÔºåÁâπÂà´ÊòØÂú®‰ª£ÁêÜ„ÄÅÊé®ÁêÜÂíåÁºñÁ†Å‰ªªÂä°‰∏ä„ÄÇÂÆÉÈááÁî®Â§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÁªèËøá23‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑËÆ≠ÁªÉÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇGLM-4.5Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÁª©ÔºåÂ∞§ÂÖ∂Âú®‰ª£ÁêÜÂü∫ÂáÜ‰∏≠ÊéíÂêçÁ¨¨‰∫å„ÄÇËØ•Ê®°ÂûãÁöÑÂºÄÊ∫êÁâàÊú¨ÂíåÁ¥ßÂáëÁâàÔºàGLM-4.5-AirÔºâÈÉΩÂ∑≤ÂèëÂ∏ÉÔºåÊó®Âú®Êé®Âä®Êé®ÁêÜÂíå‰ª£ÁêÜ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04825",
            "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
            "url": "https://huggingface.co/papers/2508.04825",
            "abstract": "Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.",
            "score": 33,
            "issue_id": 5272,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 6",
                "zh": "8Êúà6Êó•"
            },
            "hash": "085e45094380ed54",
            "authors": [
                "Seungyong Lee",
                "Jeong-gi Kwak"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.04825.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#inference",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üëö",
                "ru": {
                    "title": "Voost: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–µ –æ–¥–µ–∂–¥—ã",
                    "desc": "Voost - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–µ –æ–¥–µ–∂–¥—ã –∏ –µ–µ —Å–Ω—è—Ç–∏—é. –û–Ω–∞ —É–ª—É—á—à–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –æ–¥–µ–∂–¥–æ–π –∏ —Ç–µ–ª–æ–º —á–µ–ª–æ–≤–µ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏. Voost –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –æ–¥–µ–∂–¥—ã –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ—Ç–µ–π. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É."
                },
                "en": {
                    "title": "Voost: Revolutionizing Virtual Try-On with Unified Learning",
                    "desc": "Voost is a new framework that uses a diffusion transformer to improve virtual try-on and try-off tasks in fashion technology. It learns to create realistic images of people wearing clothes by understanding how garments fit the body, even when poses and appearances change. By training both tasks together, Voost enhances the relationship between garments and bodies without needing extra networks or labels. The framework also includes innovative techniques to improve performance during image generation, leading to top results in accuracy and visual quality across various benchmarks."
                },
                "zh": {
                    "title": "VoostÔºöËôöÊãüËØïÁ©ø‰∏éËØïËÑ±ÁöÑÁªü‰∏ÄÊ°ÜÊû∂",
                    "desc": "VoostÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊâ©Êï£ÂèòÊç¢Âô®Ê°ÜÊû∂ÔºåËÉΩÂ§üÂêåÊó∂Â≠¶‰π†ËôöÊãüËØïÁ©øÂíåËØïËÑ±„ÄÇÂÆÉÈÄöËøáËÅîÂêàÂª∫Ê®°Ëøô‰∏§‰∏™‰ªªÂä°ÔºåÂ¢ûÂº∫‰∫ÜÊúçË£Ö‰∏éË∫´‰Ωì‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåËß£ÂÜ≥‰∫ÜÂú®ÂßøÂäøÂíåÂ§ñËßÇÂèòÂåñ‰∏ãÁöÑÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÁÅµÊ¥ªÁöÑÁîüÊàêÊñπÂêëÂíåÊúçË£ÖÁ±ªÂà´ÔºåÈÅøÂÖç‰∫ÜÁâπÂÆö‰ªªÂä°ÁöÑÁΩëÁªúÂíåÈ¢ùÂ§ñÊ†áÁ≠æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVoostÂú®ËØïÁ©øÂíåËØïËÑ±ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊàêÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05731",
            "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
            "url": "https://huggingface.co/papers/2508.05731",
            "abstract": "Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.",
            "score": 17,
            "issue_id": 5272,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 7",
                "zh": "8Êúà7Êó•"
            },
            "hash": "31f17f5fb142d3e8",
            "authors": [
                "Yuhang Liu",
                "Zeyu Liu",
                "Shuanghe Zhu",
                "Pengxiang Li",
                "Congkai Xie",
                "Jiasheng Wang",
                "Xueyu Hu",
                "Xiaotian Han",
                "Jianbo Yuan",
                "Xinyao Wang",
                "Shengyu Zhang",
                "Hongxia Yang",
                "Fei Wu"
            ],
            "affiliations": [
                "Amazon",
                "InfiX.ai",
                "The Hong Kong Polytechnic University",
                "The University of Chicago",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05731.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "AEPO: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (AEPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM) –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. AEPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç–µ–º –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (AER). –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é AEPO, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –¥–æ 9.0% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º –º–µ—Ç–æ–¥–æ–º RLVR –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –æ–±–æ–±—â–µ–Ω–∏–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–∞—Ö."
                },
                "en": {
                    "title": "Enhancing GUI Interaction with Adaptive Exploration in MLLMs",
                    "desc": "Adaptive Exploration Policy Optimization (AEPO) is a novel framework designed to enhance semantic alignment in Multimodal Large Language Models (MLLMs) for effective interaction with Graphical User Interfaces (GUIs). The paper identifies that while existing methods like Reinforcement Learning with Verifiable Rewards (RLVR) improve spatial alignment, they struggle with semantic alignment due to inefficient exploration strategies. AEPO addresses this by implementing a multi-answer generation approach that encourages broader exploration, guided by an Adaptive Exploration Reward (AER) function. As a result, models trained with AEPO demonstrate significant performance improvements, achieving state-of-the-art results on various GUI grounding benchmarks."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÊé¢Á¥¢ÔºåÊèêÂçáËØ≠‰πâÂØπÈΩêÔºÅ",
                    "desc": "Ëá™ÈÄÇÂ∫îÊé¢Á¥¢Á≠ñÁï•‰ºòÂåñÔºàAEPOÔºâÈÄöËøáÂ¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰∫§‰∫í‰∏≠ÁöÑËØ≠‰πâÂØπÈΩêÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§‰∏éUIÂÖÉÁ¥†‰πãÈó¥ÁöÑËØ≠‰πâÂØπÈΩêÈóÆÈ¢òÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Êé¢Á¥¢ÊïàÁéá‰∏äÁöÑÁì∂È¢à„ÄÇAEPOÈááÁî®Â§öÁ≠îÊ°àÁîüÊàêÁ≠ñÁï•ÔºåÁªìÂêàÁêÜËÆ∫Âü∫Á°ÄÁöÑËá™ÈÄÇÂ∫îÊé¢Á¥¢Â•ñÂä±ÂáΩÊï∞Ôºå‰øÉËøõ‰∫ÜÊõ¥ÂπøÊ≥õÁöÑÊé¢Á¥¢„ÄÇÁªèËøáAEPOËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§ö‰∏™GUIÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÈ´ò9.0%ÁöÑÁõ∏ÂØπÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËØ≠‰πâÁêÜËß£ÂíåÊ≥õÂåñËÉΩÂäõ‰∏äÁöÑ‰ºòÂäø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06433",
            "title": "Memp: Exploring Agent Procedural Memory",
            "url": "https://huggingface.co/papers/2508.06433",
            "abstract": "Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.",
            "score": 16,
            "issue_id": 5279,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 8",
                "zh": "8Êúà8Êó•"
            },
            "hash": "28c51ac0d694bc54",
            "authors": [
                "Runnan Fang",
                "Yuan Liang",
                "Xiaobin Wang",
                "Jialong Wu",
                "Shuofei Qiao",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06433.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agi",
                    "#agents"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Memp: —É–º–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Memp - –æ–±—É—á–∞–µ–º–∞—è –∏ –æ–±–Ω–æ–≤–ª—è–µ–º–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). Memp –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –æ–ø—ã—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ –∫–∞–∫ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –°–∏—Å—Ç–µ–º–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç, –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–≤–æ–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Memp –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á."
                },
                "en": {
                    "title": "Empowering Agents with Evolving Procedural Memory",
                    "desc": "This paper introduces Memp, a learnable and updatable procedural memory system for agents that enhances their performance and efficiency in various tasks. Memp allows agents to distill their past experiences into detailed instructions and higher-level abstractions, addressing the limitations of traditional static procedural memory. The authors explore different strategies for building, retrieving, and updating this memory, ensuring it evolves with new experiences. Empirical results demonstrate that agents using Memp show improved success rates and efficiency, even when transferring memory from a stronger model to a weaker one."
                },
                "zh": {
                    "title": "Êô∫ËÉΩ‰ΩìÁöÑÂèØÂ≠¶‰π†Á®ãÂ∫èËÆ∞ÂøÜÊèêÂçá‰ªªÂä°Ë°®Áé∞",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÂ≠¶‰π†ÂíåÂèØÊõ¥Êñ∞ÁöÑÁ®ãÂ∫èËÆ∞ÂøÜÁ≥ªÁªüMempÔºåÊó®Âú®ÊèêÈ´òÊô∫ËÉΩ‰ΩìÂú®‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÂíåÊïàÁéá„ÄÇMempÈÄöËøáÂ∞ÜËøáÂéªÁöÑÁªèÈ™åÊèêÁÇº‰∏∫ËØ¶ÁªÜÁöÑÊåá‰ª§ÂíåÊõ¥È´òÂ±ÇÊ¨°ÁöÑÊäΩË±°ÔºåÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÊõ¥Â•ΩÂú∞ÊâßË°å‰ªªÂä°„ÄÇÁ†îÁ©∂Êé¢ËÆ®‰∫ÜÁ®ãÂ∫èËÆ∞ÂøÜÁöÑÊûÑÂª∫„ÄÅÊ£ÄÁ¥¢ÂíåÊõ¥Êñ∞Á≠ñÁï•ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂä®ÊÄÅÊõ¥Êñ∞Êú∫Âà∂Â¶Ç‰Ωï‰ΩøËÆ∞ÂøÜÂ∫ìÈöèÁùÄÊñ∞ÁªèÈ™å‰∏çÊñ≠ÊºîÂèò„ÄÇÂÆûÈ™åËØÅÊòéÔºåÈöèÁùÄËÆ∞ÂøÜÂ∫ìÁöÑ‰ºòÂåñÔºåÊô∫ËÉΩ‰ΩìÂú®Á±ª‰ºº‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéáÂíåÊïàÁéáÊòæËëóÊèêÈ´ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05988",
            "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
            "url": "https://huggingface.co/papers/2508.05988",
            "abstract": "ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs.",
            "score": 12,
            "issue_id": 5272,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 8",
                "zh": "8Êúà8Êó•"
            },
            "hash": "82ad3c225d1615aa",
            "authors": [
                "Wenhao Zeng",
                "Yaoning Wang",
                "Chao Hu",
                "Yuling Shi",
                "Chengcheng Wan",
                "Hongyu Zhang",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Chongqing University",
                "East China Normal University",
                "Fudan University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05988.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#architecture",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º",
                    "desc": "ASAP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –∫–æ–¥–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –≤—ã–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —à–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç–∏ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. ASAP –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∂–∞—Ç—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ASAP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥."
                },
                "en": {
                    "title": "Efficient Code Reasoning with ASAP: Smart Compression for Better Performance",
                    "desc": "The paper introduces ASAP, a new framework designed to compress Chain-of-Thought (CoT) in code reasoning while maintaining essential logical structures. It addresses the challenges posed by long reasoning traces, which can increase training costs and slow down inference. ASAP uses anchor-guided pruning to focus on core reasoning elements and applies a novel first-token surprisal metric to identify critical reasoning steps. The results demonstrate that ASAP not only improves efficiency by reducing token generation and inference latency but also maintains high accuracy in code generation tasks."
                },
                "zh": {
                    "title": "ASAPÔºöÈ´òÊïàÁöÑ‰ª£Á†ÅÊé®ÁêÜÊÄùÁª¥ÈìæÂéãÁº©Ê°ÜÊû∂",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ASAPÁöÑÊñ∞ÁöÑÁ≤óÂà∞ÁªÜÊ°ÜÊû∂ÔºåÁî®‰∫éÂéãÁº©‰ª£Á†ÅÊé®ÁêÜ‰∏≠ÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÔºåÊó®Âú®‰øùÁïôÊ†∏ÂøÉÁªìÊûÑÂíåÂÖ≥ÈîÆÊ≠•È™§Ôºå‰ªéËÄåÈôç‰ΩéÊàêÊú¨Âπ∂ÊèêÈ´òÊïàÁéá„ÄÇASAPÈ¶ñÂÖàÈÄöËøáÈîöÁÇπÂºïÂØº‰øÆÂâ™Êù•‰øùÁïôÊ†∏ÂøÉÊé®ÁêÜÁªìÊûÑÔºåÂáèÂ∞ëÂêéÁª≠Â§ÑÁêÜÁöÑÊêúÁ¥¢Á©∫Èó¥„ÄÇÊé•ÁùÄÔºåÂÆÉÂü∫‰∫éÊñ∞È¢ñÁöÑÈ¶ñ‰∏™‰ª§ÁâåÊÉäËÆ∂Â∫¶ÊåáÊ†áÔºåÈÄâÊã©ÈÄªËæë‰∏äÈáçË¶ÅÁöÑÊé®ÁêÜÊ≠•È™§ËøõË°åÈÄªËæëÊÑüÁü•‰øÆÂâ™„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåASAPÂú®Â§ö‰∏™‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂ÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜÊàêÊú¨„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03616",
            "title": "Hidden Dynamics of Massive Activations in Transformer Training",
            "url": "https://huggingface.co/papers/2508.03616",
            "abstract": "The emergence of massive activations in transformer models follows predictable patterns that can be modeled and predicted using architectural specifications, impacting model stability, training duration, and optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Massive activations are scalar values in transformer hidden states that achieve values orders of magnitude larger than typical activations and have been shown to be critical for model functionality. While prior work has characterized these phenomena in fully trained models, the temporal dynamics of their emergence during training remain poorly understood. We present the first comprehensive analysis of massive activation development throughout transformer training, using the Pythia model family as our testbed. Through systematic analysis of various model sizes across multiple training checkpoints, we demonstrate that massive activation emergence follows predictable mathematical patterns that can be accurately modeled using an exponentially-modulated logarithmic function with five key parameters. We develop a machine learning framework to predict these mathematical parameters from architectural specifications alone, achieving high accuracy for steady-state behavior and moderate accuracy for emergence timing and magnitude. These findings enable architects to predict and potentially control key aspects of massive activation emergence through design choices, with significant implications for model stability, training cycle length, interpretability, and optimization. Our findings demonstrate that the emergence of massive activations is governed by model design and can be anticipated, and potentially controlled, before training begins.",
            "score": 9,
            "issue_id": 5291,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 5",
                "zh": "8Êúà5Êó•"
            },
            "hash": "b675755cf9022ccb",
            "authors": [
                "Jorge Gallego-Feliciano",
                "S. Aaron McClendon",
                "Juan Morinelli",
                "Stavros Zervoudakis",
                "Antonios Saravanos"
            ],
            "affiliations": [
                "Aimpoint Digital Labs, Atlanta, GA, USA",
                "New York University, New York, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03616.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "üìà",
                "ru": {
                    "title": "–ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Å–ª–µ–¥—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ-–º–æ–¥—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π —á–µ—Ä–µ–∑ –≤—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —á—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Predicting Massive Activations for Better Transformer Design",
                    "desc": "This paper investigates the phenomenon of massive activations in transformer models, which are significantly larger than typical activations and crucial for model performance. The authors analyze how these massive activations develop during the training process, revealing that their emergence follows predictable mathematical patterns. They introduce a framework that allows for the prediction of key parameters related to massive activations based solely on the model's architecture. This research provides insights that can help in designing transformer models for improved stability and efficiency during training."
                },
                "zh": {
                    "title": "È¢ÑÊµãÂ§ßËßÑÊ®°ÊøÄÊ¥ªÁöÑÂá∫Áé∞Ôºå‰ºòÂåñÊ®°ÂûãËÆæËÆ°",
                    "desc": "Âú®ÂèòÊç¢Âô®Ê®°Âûã‰∏≠ÔºåÂ§ßËßÑÊ®°ÊøÄÊ¥ªÊòØÈöêËóèÁä∂ÊÄÅ‰∏≠ÁöÑÊ†áÈáèÂÄºÔºåÂÖ∂ÂÄºËøúÂ§ß‰∫éÂÖ∏ÂûãÊøÄÊ¥ªÔºåÂØπÊ®°ÂûãÂäüËÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨È¶ñÊ¨°ÂÖ®Èù¢ÂàÜÊûê‰∫ÜÂ§ßËßÑÊ®°ÊøÄÊ¥ªÂú®ÂèòÊç¢Âô®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÂèëÂ±ïÔºå‰ΩøÁî®PythiaÊ®°ÂûãÁ≥ªÂàó‰Ωú‰∏∫ÊµãËØïÂü∫Á°Ä„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßËßÑÊ®°ÊøÄÊ¥ªÁöÑÂá∫Áé∞ÈÅµÂæ™ÂèØÈ¢ÑÊµãÁöÑÊï∞Â≠¶Ê®°ÂºèÔºåÂèØ‰ª•ÈÄöËøá‰∫î‰∏™ÂÖ≥ÈîÆÂèÇÊï∞ÁöÑÊåáÊï∞Ë∞ÉÂà∂ÂØπÊï∞ÂáΩÊï∞ËøõË°åÂª∫Ê®°„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûú‰ΩøÂæóÊ®°ÂûãËÆæËÆ°ËÄÖËÉΩÂ§üÂú®ËÆ≠ÁªÉÂâçÈ¢ÑÊµãÂπ∂ÊéßÂà∂Â§ßËßÑÊ®°ÊøÄÊ¥ªÁöÑÂÖ≥ÈîÆÁâπÊÄßÔºå‰ªéËÄåÂΩ±ÂìçÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄß„ÄÅËÆ≠ÁªÉÂë®Êúü„ÄÅÂèØËß£ÈáäÊÄßÂíå‰ºòÂåñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02831",
            "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
            "url": "https://huggingface.co/papers/2508.02831",
            "abstract": "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)",
            "score": 6,
            "issue_id": 5276,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 4",
                "zh": "8Êúà4Êó•"
            },
            "hash": "95f1bf82d6941705",
            "authors": [
                "Miko≈Çaj Zieli≈Ñski",
                "Krzysztof Byrski",
                "Tomasz Szczepanik",
                "Przemys≈Çaw Spurek"
            ],
            "affiliations": [
                "IDEAS Research Institute",
                "Jagiellonian University, Faculty of Mathematics and Computer Science, ≈Åojasiewicza 6, 30-348, Krakow, Poland",
                "Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02831.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "GENIE: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ NeRF –∏ Gaussian Splatting –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-—Å—Ü–µ–Ω",
                    "desc": "GENIE - —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ NeRF —Å —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º Gaussian Splatting. –í–º–µ—Å—Ç–æ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–∏—Ö –≥–∞—Ä–º–æ–Ω–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –∫–∞–∂–¥–æ–º—É –≥–∞—É—Å—Å–∏–∞–Ω—É –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –æ–±—É—á–∞–µ–º–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –≠—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è —Å–µ—Ç–∏ NeRF –Ω–∞ –æ—Å–Ω–æ–≤–µ k –±–ª–∏–∂–∞–π—à–∏—Ö –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤ –∫ –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å–æ —Å—Ü–µ–Ω–æ–π, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
                },
                "en": {
                    "title": "GENIE: Real-Time 3D Scene Editing with Photorealism and Intuition",
                    "desc": "GENIE is a new model that merges the strengths of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) for 3D scene rendering and editing. It allows for photorealistic images while also enabling real-time, interactive editing through an explicit representation of scenes using Gaussian primitives. By introducing a method called Ray-Traced Gaussian Proximity Search (RT-GPS), GENIE efficiently finds the nearest Gaussians for each point, making editing faster and more intuitive. This combination supports dynamic interactions and physical simulations, making it easier to manipulate 3D scenes in a realistic way."
                },
                "zh": {
                    "title": "GENIEÔºöÂÆûÊó∂ÂèØÁºñËæëÁöÑ3DÂú∫ÊôØÊ∏≤ÊüìÊñ∞ÊñπÊ≥ï",
                    "desc": "GENIEÊòØ‰∏ÄÁßçÁªìÂêà‰∫ÜNeRFÁöÑÈ´òË¥®ÈáèÂÖâÁ∫øÊ∏≤ÊüìÂíåGaussian SplattingÁöÑÂèØÁºñËæëÁªìÊûÑË°®Á§∫ÁöÑÊ∑∑ÂêàÊ®°Âûã„ÄÇÂÆÉÈÄöËøá‰∏∫ÊØè‰∏™È´òÊñØÂàÜÂ∏ÉÂàÜÈÖçÂèØËÆ≠ÁªÉÁöÑÁâπÂæÅÂµåÂÖ•ÔºåÊù•ÂÆûÁé∞ÂÆûÊó∂ÁöÑÂ±ÄÈÉ®ÊÑüÁü•ÁºñËæë„ÄÇGENIEËøòÂºïÂÖ•‰∫ÜÂü∫‰∫éÂÖâÁ∫øËøΩË∏™ÁöÑÈ´òÊñØÈÇªËøëÊêúÁ¥¢ÔºàRT-GPSÔºâÔºåÊèêÈ´ò‰∫ÜÈ´òÊñØÊêúÁ¥¢ÁöÑÊïàÁéá„ÄÇÈÄöËøáËøô‰∫õÂàõÊñ∞ÔºåGENIEÊîØÊåÅÁõ¥ËßÇÁöÑÂú∫ÊôØÊìç‰ΩúÂíå‰∏éÁâ©ÁêÜÊ®°ÊãüÁöÑÂÖºÂÆπÊÄßÔºåÂº•Âêà‰∫ÜÂá†‰ΩïÁºñËæë‰∏éÁ•ûÁªèÊ∏≤Êüì‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05547",
            "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2508.05547",
            "abstract": "A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.",
            "score": 5,
            "issue_id": 5274,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 7",
                "zh": "8Êúà7Êó•"
            },
            "hash": "9c2f43e8f72ea22d",
            "authors": [
                "Hao Dong",
                "Lijun Sheng",
                "Jian Liang",
                "Ran He",
                "Eleni Chatzi",
                "Olga Fink"
            ],
            "affiliations": [
                "EPFL, Switzerland",
                "ETH Zurich, Switzerland",
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05547.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#survey",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏",
                    "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–∑—Ä–µ–Ω–∏–µ + —è–∑—ã–∫) –±–µ–∑ —É—á–∏—Ç–µ–ª—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è—è —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã. –í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã. –¢–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏ –≤—ã–¥–µ–ª—è—é—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π."
                },
                "en": {
                    "title": "Unlocking Vision-Language Models: A Guide to Unsupervised Adaptation",
                    "desc": "This paper provides a detailed survey of unsupervised adaptation methods for Vision-Language Models (VLMs), which are designed to improve model performance without the need for labeled data. It categorizes these methods into four main types based on the availability of unlabeled visual data: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation. The authors analyze various methodologies and strategies within this framework, aiming to create a clearer understanding of how to effectively adapt VLMs to specific tasks. Furthermore, the paper discusses existing benchmarks and identifies future research opportunities in the field of unsupervised VLM adaptation."
                },
                "zh": {
                    "title": "Êó†ÁõëÁù£ÈÄÇÂ∫îÔºöÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂäõ",
                    "desc": "Êú¨ÊñáÂØπËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Êó†ÁõëÁù£ÈÄÇÂ∫îÊñπÊ≥ïÊñπÈù¢ËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑË∞ÉÊü•„ÄÇÁ†îÁ©∂Â∞ÜÁé∞ÊúâÊñπÊ≥ïÊ†πÊçÆÊó†Ê†áÁ≠æËßÜËßâÊï∞ÊçÆÁöÑÂèØÁî®ÊÄßËøõË°åÂàÜÁ±ªÔºåÊèêÂá∫‰∫ÜÂõõÁßç‰∏ªË¶ÅËåÉÂºèÔºöÊó†Êï∞ÊçÆËøÅÁßª„ÄÅÊó†ÁõëÁù£È¢ÜÂüüËøÅÁßª„ÄÅÊÉÖÊôØÊµãËØïÊó∂ÈÄÇÂ∫îÂíåÂú®Á∫øÊµãËØïÊó∂ÈÄÇÂ∫î„ÄÇÊñáÁ´†ÂàÜÊûê‰∫ÜÊØèÁßçËåÉÂºèÁöÑÊ†∏ÂøÉÊñπÊ≥ïÂíåÈÄÇÂ∫îÁ≠ñÁï•ÔºåÂπ∂ÂõûÈ°æ‰∫Ü‰∏çÂêåÂ∫îÁî®‰∏≠ÁöÑ‰ª£Ë°®ÊÄßÂü∫ÂáÜ„ÄÇÊúÄÂêéÔºåÊåáÂá∫‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂºÄÊîæÊåëÊàòÂíåÊúâÂâçÊôØÁöÑÊñπÂêë„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05502",
            "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
            "url": "https://huggingface.co/papers/2508.05502",
            "abstract": "MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce \"thin descriptions\", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing \"thick descriptions\". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
            "score": 4,
            "issue_id": 5272,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 7",
                "zh": "8Êúà7Êó•"
            },
            "hash": "b7b633c31ed6e35e",
            "authors": [
                "Yufei Gao",
                "Jiaying Fei",
                "Nuo Chen",
                "Ruirui Chen",
                "Guohang Yan",
                "Yunshi Lan",
                "Botian Shi"
            ],
            "affiliations": [
                "East China Normal University",
                "Institute of High Performance Computing, A*STAR",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05502.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "üåç",
                "ru": {
                    "title": "MELLA: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ MLLM –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤",
                    "desc": "–î–∞—Ç–∞—Å–µ—Ç MELLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤. MELLA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–æ–π–Ω–æ–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø–æ–¥–ø–∏—Å–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ MLLM, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –æ–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–æ—Å—å–º–∏ —è–∑—ã–∫–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö MLLM –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ MELLA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–∞–Ω—ã –∫–∞–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, —Ç–∞–∫ –∏ —Å –ø–æ–≤—ã—à–µ–Ω–∏–µ–º –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing MLLMs for Low-Resource Languages with MELLA",
                    "desc": "This paper introduces MELLA, a new dataset designed to improve Multimodal Large Language Models (MLLMs) for low-resource languages. It focuses on enhancing both linguistic capabilities and cultural groundedness by using native web alt-text and MLLM-generated captions. The study highlights the limitations of existing methods that rely solely on text or machine translation, which often fail to provide rich, informative content. By fine-tuning MLLMs on MELLA, the results show significant performance improvements across multiple languages, enabling models to generate more detailed and culturally aware descriptions."
                },
                "zh": {
                    "title": "ÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "MELLAÊòØ‰∏Ä‰∏™Â§öÊ®°ÊÄÅ„ÄÅÂ§öËØ≠Ë®ÄÁöÑÊï∞ÊçÆÈõÜÔºåÊó®Âú®ÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑËØ≠Ë®ÄËÉΩÂäõÂíåÊñáÂåñÂü∫Á°Ä„ÄÇËØ•Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÊ∫êÁ≠ñÁï•ÔºåÈÄöËøáÊî∂ÈõÜÊú¨Âú∞ÁΩëÈ°µÁöÑÊõø‰ª£ÊñáÊú¨ÂíåMLLMÁîüÊàêÁöÑÊ†áÈ¢òÔºåÊù•ÂÆûÁé∞ËØ≠Ë®ÄËÉΩÂäõÂíåÊñáÂåñÂü∫Á°ÄÁöÑÂèåÈáçÁõÆÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®MELLA‰∏äËøõË°åÂæÆË∞ÉÂêéÔºåÂÖ´ÁßçËØ≠Ë®ÄÁöÑÊ®°ÂûãÂú®Â§öÁßçMLLMÂü∫Á°Ä‰∏äÊôÆÈÅçÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåËÉΩÂ§üÁîüÊàêÊõ¥‰∏∞ÂØåÁöÑÊèèËø∞„ÄÇÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂº∫Ë∞É‰∫ÜÊñáÂåñÁü•ËØÜÂíåËØ≠Ë®ÄËÉΩÂäõÁöÑÂ¢ûÂº∫ÔºåÈÄÇÁî®‰∫é‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁî®Êà∑„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01242",
            "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
            "url": "https://huggingface.co/papers/2508.01242",
            "abstract": "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50 times larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.",
            "score": 3,
            "issue_id": 5274,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 2",
                "zh": "8Êúà2Êó•"
            },
            "hash": "425226c54ca88a3a",
            "authors": [
                "Shuangkang Fang",
                "I-Chao Shen",
                "Yufeng Wang",
                "Yi-Hsuan Tsai",
                "Yi Yang",
                "Shuchang Zhou",
                "Wenrui Ding",
                "Takeo Igarashi",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Atmanity Inc.",
                "Beihang University",
                "StepFun Inc.",
                "The University of Tokyo",
                "UC Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01242.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#optimization",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "üßä",
                "ru": {
                    "title": "MeshLLM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ 3D-–º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "MeshLLM - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Ç–µ—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π. MeshLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–º–∏—Ç–∏–≤—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 1500000+ –æ–±—Ä–∞–∑—Ü–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤—ã–≤–æ–¥–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≥—Ä–∞–Ω–µ–π –∏ –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π, —É–ª—É—á—à–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–ø–æ–ª–æ–≥–∏–µ–π –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏."
                },
                "en": {
                    "title": "Revolutionizing 3D Mesh Generation with Language Models",
                    "desc": "MeshLLM is a new framework that uses large language models to generate and interpret 3D meshes represented as text. It improves upon previous methods by introducing a Primitive-Mesh decomposition strategy, which breaks down 3D meshes into meaningful parts, allowing for a much larger dataset of over 1.5 million samples. This approach helps maintain important 3D structural information that is often lost in serialization. Additionally, MeshLLM enhances the model's understanding of mesh topology and spatial relationships through innovative training techniques, leading to superior performance in generating and understanding 3D shapes compared to existing models."
                },
                "zh": {
                    "title": "MeshLLMÔºöÈáçÂ°ë3DÁΩëÊ†ºÁîüÊàê‰∏éÁêÜËß£ÁöÑÊú™Êù•",
                    "desc": "MeshLLMÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊù•ÁêÜËß£ÂíåÁîüÊàêÊñáÊú¨Â∫èÂàóÂåñÁöÑ3DÁΩëÊ†º„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÂéüÂßãÁΩëÊ†ºÂàÜËß£Á≠ñÁï•ÔºåÂ∞Ü3DÁΩëÊ†ºÂàÜËß£‰∏∫ÁªìÊûÑ‰∏äÊúâÊÑè‰πâÁöÑÂ≠êÂçïÂÖÉÔºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®Êï∞ÊçÆÈõÜËßÑÊ®°Âíå3DÁªìÊûÑ‰ø°ÊÅØÊçüÂ§±ÊñπÈù¢ÁöÑÂÖ≥ÈîÆÈôêÂà∂„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Ë∂ÖËøá150‰∏áÊ†∑Êú¨ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂá†‰πéÊòØ‰πãÂâçÊñπÊ≥ïÁöÑ50ÂÄçÔºåÊõ¥Â•ΩÂú∞Á¨¶ÂêàLLMÁöÑÊâ©Â±ïÊ≥ïÂàô„ÄÇÊ≠§Â§ñÔºåMeshLLMÈÄöËøáÊé®Êñ≠È°∂ÁÇπ‰πãÈó¥ÁöÑÈù¢ËøûÊé•ÊÄßÂíåÂ±ÄÈÉ®ÁΩëÊ†ºÁªÑË£ÖËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜLLMÊçïÊçâÁΩëÊ†ºÊãìÊâëÂíåÁ©∫Èó¥ÁªìÊûÑÁöÑËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22025",
            "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
            "url": "https://huggingface.co/papers/2507.22025",
            "abstract": "UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a \"Simple Thinking\" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.",
            "score": 2,
            "issue_id": 5274,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 –∏—é–ª—è",
                "en": "July 29",
                "zh": "7Êúà29Êó•"
            },
            "hash": "f7ff8c2c5517f5de",
            "authors": [
                "Shuquan Lian",
                "Yuhang Wu",
                "Jia Ma",
                "Zihan Song",
                "Bingqi Chen",
                "Xiawu Zheng",
                "Hui Li"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22025.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "üñ•Ô∏è",
                "ru": {
                    "title": "UI-AGILE: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –∏ –≤—ã–≤–æ–¥–µ –∞–≥–µ–Ω—Ç–æ–≤ GUI",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ UI-AGILE, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –∞–≥–µ–Ω—Ç–æ–≤ GUI —Å –ø–æ–º–æ—â—å—é —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è —Ç—Ä–∏ –Ω–æ–≤—à–µ—Å—Ç–≤–∞: —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Å—Ç–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–µ–∑–∫–∏. –î–ª—è –≤—ã–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Ä–∞–∑–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è —Å –≤—ã–±–æ—Ä–æ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UI-AGILE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö ScreenSpot-Pro –∏ ScreenSpot-v2."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with UI-AGILE",
                    "desc": "UI-AGILE is a framework designed to improve the performance of Graphical User Interface (GUI) agents by enhancing their training and inference processes. It introduces a Continuous Reward function to encourage precise grounding, a Simple Thinking reward to optimize the balance between planning speed and accuracy, and a Cropping-based Resampling technique to address sparse rewards in complex tasks. For inference, it employs Decomposed Grounding with Selection, which enhances grounding accuracy by breaking down images into smaller sections for better processing. The framework has demonstrated state-of-the-art results on GUI benchmarks, significantly improving grounding accuracy compared to existing methods."
                },
                "zh": {
                    "title": "UI-AGILEÔºöÊèêÂçáGUI‰ª£ÁêÜÁöÑÊô∫ËÉΩËÆ≠ÁªÉ‰∏éÊé®ÁêÜ",
                    "desc": "UI-AGILEÊòØ‰∏Ä‰∏™Â¢ûÂº∫ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊîπËøõËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ãÊù•ÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇÂú®ËÆ≠ÁªÉÈò∂ÊÆµÔºåUI-AGILEÂºïÂÖ•‰∫ÜËøûÁª≠Â•ñÂä±ÂáΩÊï∞„ÄÅÁÆÄÂçïÊÄùÁª¥Â•ñÂä±ÂíåÂü∫‰∫éË£ÅÂâ™ÁöÑÈáçÈááÊ†∑Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÈ´òÁ≤æÂ∫¶ÁöÑÂü∫Á°ÄÂÆö‰ΩçËÉΩÂäõÂíåÂ≠¶‰π†Â§çÊùÇ‰ªªÂä°ÁöÑÊïàÊûú„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåÈááÁî®‰∫ÜÂàÜËß£Âü∫Á°ÄÂÆö‰Ωç‰∏éÈÄâÊã©ÁöÑÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈ´òÂàÜËæ®ÁéáÊòæÁ§∫Âô®‰∏äÁöÑÂü∫Á°ÄÂÆö‰ΩçÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUI-AGILEÂú®ScreenSpot-ProÂíåScreenSpot-v2Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06494",
            "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
            "url": "https://huggingface.co/papers/2508.06494",
            "abstract": "Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.",
            "score": 1,
            "issue_id": 5279,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 8",
                "zh": "8Êúà8Êó•"
            },
            "hash": "f69d154601aca623",
            "authors": [
                "Yehonathan Litman",
                "Fernando De la Torre",
                "Shubham Tulsiani"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06494.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "üí°",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–ª–∞–π—Ç–∏–Ω–≥–µ: Lightswitch –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ",
                    "desc": "Lightswitch - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 3D-—Ä–µ–ª–∞–π—Ç–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ —É—á–∏—Ç—ã–≤–∞—é—â–∞—è —Å–≤–æ–π—Å—Ç–≤–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –û–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ —Ü–µ–ª–µ–≤–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–Ω–Ω—ã–µ –æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö. Lightswitch –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã. –°–∏—Å—Ç–µ–º–∞ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ä–µ–ª–∞–π—Ç–∏–Ω–≥–∞ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏."
                },
                "en": {
                    "title": "Revolutionizing 3D Relighting with Lightswitch",
                    "desc": "Lightswitch is a new framework designed for improving 3D relighting by using both multi-view and material information. It addresses the limitations of previous methods that relied solely on 2D image relighting, which often failed to utilize the intrinsic properties of objects. By integrating these cues into a diffusion model, Lightswitch can efficiently relight multiple images to match a desired lighting condition. The results show that it not only enhances the quality of relighting but also does so faster than existing techniques."
                },
                "zh": {
                    "title": "LightswitchÔºöÈ´òÊïàÁöÑ3DÈáçÂÖâÊñ∞ÊñπÊ≥ï",
                    "desc": "LightswitchÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÊùêÊñôÈáçÂÖâÊ°ÜÊû∂ÔºåÈÄöËøáÊï¥ÂêàÂ§öËßÜËßíÂíåÊùêÊñôÁ∫øÁ¥¢ÔºåÊòæËëóÊèêÂçá‰∫Ü3DÈáçÂÖâÁöÑË¥®ÈáèÂíåÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Â∞Ü‰ªªÊÑèÊï∞ÈáèÁöÑËæìÂÖ•ÂõæÂÉèÈáçÂÖâÂà∞ÁõÆÊ†áÂÖâÁÖßÊù°‰ª∂ÔºåÂêåÊó∂ËÄÉËôëÂà∞Áâ©‰ΩìÁöÑÂÜÖÂú®ÁâπÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑ2DÂõæÂÉèÈáçÂÖâÊñπÊ≥ïÁõ∏ÊØîÔºåLightswitchÂú®Â§ÑÁêÜÂ§öËßÜËßíÊï∞ÊçÆÊó∂Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLightswitchÂú®ÂêàÊàêÂíåÁúüÂÆûÁâ©‰ΩìÁöÑÈáçÂÖâ‰ªªÂä°‰∏≠ÔºåÈÄüÂ∫¶Âø´‰∏îË¥®ÈáèÈ´òÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04482",
            "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
            "url": "https://huggingface.co/papers/2508.04482",
            "abstract": "The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.",
            "score": 1,
            "issue_id": 5284,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 6",
                "zh": "8Êúà6Êó•"
            },
            "hash": "4f7eaba2df530414",
            "authors": [
                "Xueyu Hu",
                "Tao Xiong",
                "Biao Yi",
                "Zishu Wei",
                "Ruixuan Xiao",
                "Yurun Chen",
                "Jiasheng Ye",
                "Meiling Tao",
                "Xiangxin Zhou",
                "Ziyu Zhao",
                "Yuhuai Li",
                "Shengze Xu",
                "Shenzhi Wang",
                "Xinchen Xu",
                "Shuofei Qiao",
                "Zhaokai Wang",
                "Kun Kuang",
                "Tieyong Zeng",
                "Liang Wang",
                "Jiwei Li",
                "Yuchen Eleanor Jiang",
                "Wangchunshu Zhou",
                "Guoyin Wang",
                "Keting Yin",
                "Zhou Zhao",
                "Hongxia Yang",
                "Fan Wu",
                "Shengyu Zhang",
                "Fei Wu"
            ],
            "affiliations": [
                "1.AI",
                "Fudan University",
                "Institute of Automation, Chinese Academy of Sciences",
                "OPPO AI Center",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong Polytechnic University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04482.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–û–°-–∞–≥–µ–Ω—Ç—ã: –®–∞–≥ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ —É—Ä–æ–≤–Ω—è –î.–ñ.–ê.–†.–í.–ò.–°.",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ (–û–°-–∞–≥–µ–Ω—Ç—ã). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –û–°-–∞–≥–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞—è —Å—Ä–µ–¥—É, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π, –∞ —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ. –í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –û–°-–∞–≥–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ü–µ–Ω–∫–∏ –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏, –∞ —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è —Ç–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –°—Ç–∞—Ç—å—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –û–°-–∞–≥–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫."
                },
                "en": {
                    "title": "Advancing AI Assistants: The Rise of OS Agents",
                    "desc": "This paper surveys the development of OS Agents, which are advanced AI assistants that utilize multi-modal large language models to automate tasks on computing devices. It discusses the fundamental components of these agents, including their environment, observation space, and action space, as well as their capabilities like understanding and planning. The authors also explore methodologies for building OS Agents, focusing on domain-specific models and frameworks, and review evaluation protocols to assess their performance. Finally, the paper identifies challenges and future research directions, emphasizing the importance of safety, privacy, and personalization in the evolution of these intelligent systems."
                },
                "zh": {
                    "title": "Êìç‰ΩúÁ≥ªÁªü‰ª£ÁêÜÔºöËøàÂêëÊô∫ËÉΩÂä©ÊâãÁöÑÊú™Êù•",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊìç‰ΩúÁ≥ªÁªü‰ª£ÁêÜÔºàOS AgentsÔºâÁöÑÂèëÂ±ïÔºåËøô‰∫õ‰ª£ÁêÜÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàM-LLMsÔºâÂú®ËÆ°ÁÆóËÆæÂ§á‰∏äËá™Âä®Âåñ‰ªªÂä°„ÄÇÊàë‰ª¨ËØ¶ÁªÜ‰ªãÁªç‰∫ÜOS‰ª£ÁêÜÁöÑÂü∫Êú¨ÁªÑÊàêÈÉ®ÂàÜÔºåÂåÖÊã¨ÁéØÂ¢É„ÄÅËßÇÂØüÁ©∫Èó¥ÂíåË°åÂä®Á©∫Èó¥Ôºå‰ª•ÂèäÁêÜËß£„ÄÅËßÑÂàíÂíåÂü∫Á°ÄËÉΩÂäõÁ≠âÂÖ≥ÈîÆËÉΩÂäõ„ÄÇÊñáÁ´†ËøòËÆ®ËÆ∫‰∫ÜÊûÑÂª∫OS‰ª£ÁêÜÁöÑÊñπÊ≥ïÔºåÈáçÁÇπÂÖ≥Ê≥®È¢ÜÂüüÁâπÂÆöÁöÑÂü∫Á°ÄÊ®°ÂûãÂíå‰ª£ÁêÜÊ°ÜÊû∂„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜËØÑ‰º∞ÂçèËÆÆÂíåÂü∫ÂáÜÊµãËØïÔºåÂπ∂ÊåáÂá∫ÂΩìÂâçÈù¢‰∏¥ÁöÑÊåëÊàòÂèäÊú™Êù•Á†îÁ©∂ÁöÑÊñπÂêë„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-08-08.html",
    "link_next": "2025-08-12.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "08.08",
        "en": "08/08",
        "zh": "8Êúà8Êó•"
    },
    "short_date_next": {
        "ru": "12.08",
        "en": "08/12",
        "zh": "8Êúà12Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 5,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}