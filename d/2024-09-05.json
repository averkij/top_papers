{
    "date": {
        "ru": "5 сентября",
        "en": "September 5",
        "zh": "9月5日"
    },
    "time_utc": "2024-09-05 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-05",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.02634",
            "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
            "url": "https://huggingface.co/papers/2409.02634",
            "abstract": "With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.",
            "score": 87,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "e19fafe357b7235c",
            "data": {
                "categories": [
                    "#video",
                    "#audio",
                    "#games",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Loopy: естественная генерация видео по аудио без ограничений",
                    "desc": "Статья представляет новый подход к генерации видео с человеком на основе аудио, называемый Loopy. Модель использует только аудиосигнал, без дополнительных пространственных шаблонов, для создания естественных движений. Авторы разработали специальные модули для анализа временной информации и преобразования аудио в латентное представление. Эксперименты показывают, что Loopy превосходит существующие методы по качеству и реалистичности генерируемых видео."
                },
                "en": {
                    "title": "Loopy: Revolutionizing Audio-Driven Video Generation",
                    "desc": "This paper presents Loopy, a novel audio-only conditioned video diffusion model that enhances the generation of human videos based on audio signals. It introduces two key modules: an inter- and intra-clip temporal module and an audio-to-latents module, which help the model understand and replicate natural motion patterns over time. By eliminating the reliance on auxiliary spatial signals, Loopy allows for more fluid and realistic movements in generated videos. Experimental results demonstrate that Loopy surpasses existing models in producing high-quality, lifelike video outputs driven solely by audio."
                },
                "zh": {
                    "title": "音频驱动，视频生成的新突破！",
                    "desc": "本文提出了一种名为Loopy的端到端音频条件视频扩散模型，专注于音频驱动的人类视频生成。我们设计了一个时序模块和音频到潜在空间模块，使模型能够利用数据中的长期运动信息，从而学习自然的运动模式。与现有方法不同，Loopy不再需要手动指定的空间运动模板，这样可以提高运动的自然性和自由度。实验结果表明，Loopy在多个场景中优于最新的音频驱动肖像扩散模型，生成更逼真和高质量的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02889",
            "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture",
            "url": "https://huggingface.co/papers/2409.02889",
            "abstract": "Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as degraded performance with more images and high computational costs. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model LongLLaVA~(Long-Context Large Language and Vision Assistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.",
            "score": 54,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "934263e7a6fa057a",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#long_context",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "LongLLaVA: Эффективное понимание длинного мультимодального контекста",
                    "desc": "Статья представляет LongLLaVA - гибридную мультимодальную языковую модель с длинным контекстом для понимания видео и изображений высокого разрешения. Модель использует архитектуру, сочетающую блоки Mamba и Transformer, что позволяет эффективно обрабатывать большие объемы визуальных данных. Авторы применили прогрессивную стратегию обучения и специальный подход к конструированию данных с учетом временных и пространственных зависимостей. LongLLaVA демонстрирует высокую производительность на различных бенчмарках, сохраняя при этом низкое потребление памяти."
                },
                "en": {
                    "title": "Unlocking Long-Context Understanding in Multi-modal AI",
                    "desc": "This paper focuses on enhancing the long-context capabilities of Multi-modal Large Language Models (MLLMs) for better understanding of videos and high-resolution images. The authors propose a hybrid model architecture that combines Mamba and Transformer blocks, optimizing data construction to account for both temporal and spatial dependencies. They also introduce a progressive training strategy to tackle issues like performance degradation and high computational costs. The resulting model, LongLLaVA, demonstrates improved efficiency and effectiveness, capable of processing nearly a thousand images on a single A100 80GB GPU while achieving competitive benchmark results."
                },
                "zh": {
                    "title": "提升多模态模型的长文本处理能力",
                    "desc": "这篇论文探讨了多模态大型语言模型（MLLMs）在处理长文本时的能力，特别是在视频理解和高分辨率图像理解方面。作者提出了一种新的模型架构，结合了Mamba和Transformer模块，以优化性能和计算效率。通过考虑多图像之间的时间和空间依赖关系，论文还改进了数据构建和训练策略。最终，发布的LongLLaVA模型在多个基准测试中表现出色，能够在单个A100 80GB GPU上处理近千张图像，展现了广泛的应用前景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02897",
            "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA",
            "url": "https://huggingface.co/papers/2409.02897",
            "abstract": "Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o.",
            "score": 44,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "9bf59def629c3864",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#hallucinations",
                    "#training",
                    "#rag",
                    "#benchmark",
                    "#alignment",
                    "#small_models"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Улучшение доверия к LLM через точное цитирование",
                    "desc": "Статья посвящена проблеме отсутствия цитирования в ответах больших языковых моделей (LLM) с длинным контекстом. Авторы разработали бенчмарк LongBench-Cite для оценки способности LLM отвечать на вопросы с цитированием. Предложен новый пайплайн CoF для создания датасета LongCite-45k с точными цитатами на уровне предложений. На основе этого датасета обучены модели LongCite-8B и LongCite-9B, превзошедшие передовые проприетарные модели в качестве цитирования."
                },
                "en": {
                    "title": "Enhancing Trust in LLMs with Citation-Driven Responses",
                    "desc": "This paper addresses the issue of trustworthiness in long-context large language models (LLMs) by introducing a method for generating responses with detailed sentence-level citations. The authors present LongBench-Cite, a benchmark designed to evaluate LLMs' performance in Long-Context Question Answering with Citations (LQAC). They propose a novel pipeline called CoF (Coarse to Fine) that creates a large-scale dataset, LongCite-45k, to train LLMs for generating accurate answers with citations. The results demonstrate that their models, LongCite-8B and LongCite-9B, outperform existing models in citation quality, enhancing the reliability of LLM outputs."
                },
                "zh": {
                    "title": "提升长文本模型的可信度与可验证性",
                    "desc": "当前的长文本大语言模型在回答用户问题时表现出色，但缺乏引用使得用户验证变得困难，导致对其可信度的担忧。本文旨在使长文本大语言模型生成带有细粒度句子级引用的响应，从而提高其可信性和可验证性。我们首先介绍了LongBench-Cite，这是一个用于评估当前大语言模型在长文本问答中引用性能的自动化基准，显示出显著的改进空间。接着，我们提出了CoF（从粗到细）这一新颖的流程，利用现成的大语言模型自动生成带有精确句子级引用的长文本问答实例，并构建了LongCite-45k这一大规模的SFT数据集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02813",
            "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark",
            "url": "https://huggingface.co/papers/2409.02813",
            "abstract": "This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.",
            "score": 28,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "218ac6737df1271e",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#graphs",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый бенчмарк для истинного мультимодального понимания",
                    "desc": "MMMU-Pro - это усовершенствованная версия бенчмарка MMMU для оценки мультимодальных моделей. Он включает трехэтапный процесс, фильтрующий вопросы, отвечаемые только текстовыми моделями, расширяющий варианты ответов и вводящий задачи с визуальным вводом. Результаты показывают значительное снижение производительности моделей на MMMU-Pro по сравнению с MMMU. Исследование также изучает влияние OCR-подсказок и рассуждений по цепочке мыслей на производительность моделей."
                },
                "en": {
                    "title": "MMMU-Pro: Elevating Multimodal AI Evaluation",
                    "desc": "This paper presents MMMU-Pro, an enhanced benchmark for evaluating multimodal models' understanding and reasoning abilities. It employs a three-step process to filter out simpler questions, augment answer options, and introduce a vision-only input format that requires models to integrate visual and textual information. The results indicate that models perform significantly worse on MMMU-Pro compared to the original MMMU benchmark, highlighting the increased difficulty. Additionally, the study examines the effects of Optical Character Recognition (OCR) prompts and Chain of Thought (CoT) reasoning, finding that while OCR prompts have little impact, CoT reasoning generally enhances model performance."
                },
                "zh": {
                    "title": "MMMU-Pro：多模态理解的新挑战",
                    "desc": "本文介绍了MMMU-Pro，这是一个增强版的多学科多模态理解与推理基准（MMMU）。MMMU-Pro通过三个步骤严格评估多模态模型的理解和推理能力：首先，过滤掉仅能通过文本回答的问题；其次，增强候选选项；最后，引入仅使用视觉输入的设置，在图像中嵌入问题。研究结果表明，模型在MMMU-Pro上的表现明显低于MMMU，表现范围在16.8%到26.9%之间，同时探讨了光学字符识别（OCR）提示和思维链（CoT）推理的影响，发现OCR提示影响较小，而CoT通常能提高性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01083",
            "title": "Affordance-based Robot Manipulation with Flow Matching",
            "url": "https://huggingface.co/papers/2409.01083",
            "abstract": "We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot trajectories guided by affordances in a supervised Flow Matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance with language prompter achieves competitive performance and even outperforms other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot trajectories with a single flow matching policy also leads to consistently better performance than alternative behavior cloning methods, especially given multimodal robot action distributions. Our framework seamlessly unifies affordance model learning and trajectory generation with flow matching for robot manipulation.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "0563b4e48d6ef1b6",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#synthetic",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Интеллектуальная робототехника: от понимания среды к эффективным действиям",
                    "desc": "Представлена система для ассистивной робототехники, решающая две ключевые задачи: эффективная адаптация крупномасштабных моделей для понимания возможностей взаимодействия с объектами и обучение траекториям робота на основе визуальной модели. Используется метод настройки подсказок для эффективного обучения модели пониманию возможностей манипуляций в многозадачных сценариях. Предложен метод обучения траекториям робота с использованием Flow Matching под руководством модели возможностей взаимодействия. Система протестирована на реальном наборе данных, включающем 10 задач из повседневной жизни."
                },
                "en": {
                    "title": "Empowering Robots with Smart Manipulation and Efficient Learning",
                    "desc": "This paper introduces a new framework for assistive robots that helps them understand how to interact with their environment. It addresses two main challenges: adapting large models for specific tasks and learning how robots should move based on visual cues. The authors use a method called prompt tuning to efficiently adjust a vision model for predicting how to manipulate objects in various daily tasks. Additionally, they propose a flow matching technique to guide robot movements, resulting in improved performance in real-world scenarios involving multiple tasks."
                },
                "zh": {
                    "title": "助理机器人操作的新框架：高效学习与轨迹生成",
                    "desc": "本文提出了一种助理机器人操作的框架，重点解决两个基本挑战：首先是如何高效地将大规模模型适应于下游场景的可操作性理解任务，尤其是在日常生活场景中，收集涉及人类的多任务数据非常困难；其次是如何通过将视觉可操作性模型与机器人轨迹学习相结合来有效学习机器人轨迹。我们采用了一种参数高效的提示调优方法，通过在冻结的视觉模型前添加可学习的文本提示来预测多任务场景中的操作可操作性。最后，我们引入了一个包含10个日常生活活动任务的真实世界数据集来测试我们的框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02326",
            "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
            "url": "https://huggingface.co/papers/2409.02326",
            "abstract": "Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of \"high-quality\" remains underexplored. Focusing on the code domain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. Our evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, we find that the key to high-quality data is its alignment with the distribution of downstream applications.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "7e195db7e6a11320",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#data",
                    "#plp",
                    "#optimization",
                    "#benchmark",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "❄️",
                "ru": {
                    "title": "Качество важнее количества: эффективное обучение языковых моделей для кода",
                    "desc": "Исследование представляет Arctic-SnowCoder-1.3B, языковую модель для кода, обученную на 555 миллиардах токенов в три этапа с постепенным улучшением качества данных. Модель достигает передовых результатов на бенчмарке BigCodeBench, превосходя более крупные модели, обученные на большем объеме данных. Ключевым фактором успеха стало использование высококачественных данных, соответствующих распределению целевых задач. Исследование подчеркивает важность качества данных для предобучения языковых моделей, особенно в области программирования."
                },
                "en": {
                    "title": "High-Quality Data Drives Code Model Success",
                    "desc": "This paper introduces Arctic-SnowCoder-1.3B, a language model specifically designed for code generation, emphasizing the importance of high-quality data in its pretraining process. The model is pretrained in three phases, starting with a large dataset of standard-quality code, followed by a refinement phase using a BERT-style annotator to select high-quality tokens, and finally enhanced with synthetic data. Despite being trained on a smaller dataset compared to other models, Arctic-SnowCoder demonstrates superior performance on coding benchmarks like BigCodeBench and HumanEval+. The study highlights that the effectiveness of the model is largely due to the alignment of the training data with the needs of real-world coding tasks."
                },
                "zh": {
                    "title": "高质量数据，提升代码模型性能的关键",
                    "desc": "最近的研究表明，高质量数据对语言模型的有效预训练至关重要。然而，什么是“高质量”的定义仍然没有深入探讨。我们介绍了Arctic-SnowCoder-1.3B，这是一个在代码领域中高效的数据基础模型，经过三个阶段的逐步精炼数据进行预训练。我们的研究发现，高质量数据的关键在于其与下游应用的分布一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02245",
            "title": "FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation",
            "url": "https://huggingface.co/papers/2409.02245",
            "abstract": "Diffusion-based voice conversion (VC) techniques such as VoiceGrad have attracted interest because of their high VC performance in terms of speech quality and speaker similarity. However, a notable limitation is the slow inference caused by the multi-step reverse diffusion. Therefore, we propose FastVoiceGrad, a novel one-step diffusion-based VC that reduces the number of iterations from dozens to one while inheriting the high VC performance of the multi-step diffusion-based VC. We obtain the model using adversarial conditional diffusion distillation (ACDD), leveraging the ability of generative adversarial networks and diffusion models while reconsidering the initial states in sampling. Evaluations of one-shot any-to-any VC demonstrate that FastVoiceGrad achieves VC performance superior to or comparable to that of previous multi-step diffusion-based VC while enhancing the inference speed. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastvoicegrad/.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "5ebbcec0a4893d90",
            "data": {
                "categories": [
                    "#audio",
                    "#security",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Ускоренное преобразование голоса с помощью однократной диффузии",
                    "desc": "FastVoiceGrad - это новый метод преобразования голоса, основанный на однократной диффузии. Он значительно ускоряет процесс преобразования по сравнению с многошаговыми методами, сохраняя при этом высокое качество речи и сходство голосов. Модель обучается с использованием условной дистилляции диффузии на основе состязательного подхода (ACDD). Эксперименты показывают, что FastVoiceGrad достигает сопоставимых или лучших результатов по сравнению с предыдущими многошаговыми методами при значительном увеличении скорости вывода."
                },
                "en": {
                    "title": "FastVoiceGrad: One-Step Voice Conversion for Speed and Quality",
                    "desc": "This paper introduces FastVoiceGrad, a new voice conversion technique that improves upon existing diffusion-based methods like VoiceGrad. The key innovation is reducing the inference time from multiple steps to just one, while still maintaining high speech quality and speaker similarity. This is achieved through a method called adversarial conditional diffusion distillation (ACDD), which combines the strengths of generative adversarial networks and diffusion models. The results show that FastVoiceGrad not only matches but often exceeds the performance of traditional multi-step approaches, making it faster and more efficient for voice conversion tasks."
                },
                "zh": {
                    "title": "快速语音转换，性能与速度兼得",
                    "desc": "本论文提出了一种新的语音转换技术，称为FastVoiceGrad。与传统的多步扩散方法相比，FastVoiceGrad将推理步骤从数十步减少到一步，同时保持高质量的语音转换性能。该方法利用对抗条件扩散蒸馏（ACDD），结合生成对抗网络和扩散模型的优势。实验结果表明，FastVoiceGrad在一对一语音转换任务中表现优于或可与之前的多步扩散方法相媲美，同时显著提高了推理速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02078",
            "title": "Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text",
            "url": "https://huggingface.co/papers/2409.02078",
            "abstract": "Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, we release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "514e3ab8fefbdf04",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#training",
                    "#transfer_learning",
                    "#open_source",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "🗳️",
                "ru": {
                    "title": "Эффективные открытые модели для zero-shot классификации политических текстов",
                    "desc": "Статья представляет новые языковые модели Political DEBATE для классификации политических документов без предварительного обучения или с малым количеством примеров. Эти модели показывают результаты не хуже, а часто лучше, чем современные большие языковые модели, при этом являясь более эффективными и полностью открытыми. Авторы демонстрируют, что обучение на небольшой выборке из 10-25 документов позволяет превзойти supervised классификаторы, обученные на сотнях или тысячах документов. Также представлен новый датасет PolNLI с более чем 200 000 политических документов для более чем 800 задач классификации."
                },
                "en": {
                    "title": "Efficient Political Document Classification with Open Source Models",
                    "desc": "This paper presents the Political DEBATE language models, which are designed for zero-shot and few-shot classification of political documents. These models demonstrate superior efficiency and performance compared to existing large language models, achieving high accuracy with significantly fewer training documents. By utilizing a small sample of 10-25 documents, they can outperform traditional supervised classifiers that require extensive datasets. Additionally, the authors provide the PolNLI dataset, a comprehensive resource of over 200,000 labeled political documents for further research and development."
                },
                "zh": {
                    "title": "政治文档分类的新选择：高效开源模型",
                    "desc": "社会科学家迅速采用大型语言模型，因为它们能够在没有监督训练的情况下对文档进行标注，这种能力被称为零-shot学习。然而，由于计算需求、成本和通常的专有性质，这些模型往往与复制和开放科学标准相悖。本文介绍了政治辩论（Political DEBATE）语言模型，用于政治文档的零-shot和少量-shot分类。这些模型不仅在零-shot和少量-shot分类方面与最先进的大型语言模型相当，甚至更好，而且效率高得多，完全开源。"
                }
            }
        }
    ],
    "link_prev": "2024-09-04.html",
    "link_next": "2024-09-06.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "04.09",
        "en": "09/04",
        "zh": "9月4日"
    },
    "short_date_next": {
        "ru": "06.09",
        "en": "09/06",
        "zh": "9月6日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 3,
        "#science": 2,
        "#low_resource": 0
    }
}