{
    "date": {
        "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 12",
        "zh": "12æœˆ12æ—¥"
    },
    "time_utc": "2025-12-12 18:34",
    "weekday": 4,
    "issue_id": 36,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.10430",
            "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
            "url": "https://huggingface.co/papers/2512.10430",
            "abstract": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
            "score": 60,
            "issue_id": 29,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "dbb8812b26215808",
            "authors": [
                "Dmitrii Stoianov",
                "Danil Taranets",
                "Olga Tsymboi",
                "Ramil Latypov",
                "Almaz Dautov",
                "Vladislav Kruglikov",
                "Nikita Surkov",
                "German Abramov",
                "Pavel Gein",
                "Dmitry Abulkhanov",
                "Mikhail Gashkov",
                "Viktor Zelenkovskiy",
                "Artem Batalov",
                "Aleksandr Medvedev",
                "Anatolii Potapov"
            ],
            "affiliations": [
                "T-Tech, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10430.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#low_resource",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ°Ñ LLM Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "T-pro 2.0 â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ñ‹. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ EAGLE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ T-Wix Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº T-Math, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "T-pro 2.0: Efficient Reasoning for Russian Language Models",
                    "desc": "T-pro 2.0 is a new open-weight Russian language model (LLM) designed for hybrid reasoning and efficient inference. It utilizes a Cyrillic-dense tokenizer and an EAGLE speculative-decoding pipeline to enhance performance and reduce response times. The model is accompanied by various resources, including the T-Wix 500k instruction corpus and the T-Math reasoning benchmark, which facilitate reproducible research. A public web demo showcases the model's capabilities in both reasoning and non-reasoning tasks, making it a valuable tool for developing Russian LLM applications."
                },
                "zh": {
                    "title": "T-pro 2.0ï¼šé«˜æ•ˆçš„ä¿„è¯­æ¨ç†ä¸æ¨æ–­ç³»ç»Ÿ",
                    "desc": "T-pro 2.0 æ˜¯ä¸€ä¸ªå¼€æ”¾æƒé‡çš„ä¿„è¯­å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°æ··åˆæ¨ç†å’Œé«˜æ•ˆæ¨æ–­ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å¯†é›†çš„è¥¿é‡Œå°”å­—ç¬¦æ ‡è®°å™¨å’Œæ”¹è¿›çš„ EAGLE é¢„æµ‹è§£ç ç®¡é“ï¼Œä»¥å‡å°‘å»¶è¿Ÿå¹¶æ”¯æŒç›´æ¥å›ç­”å’Œæ¨ç†è½¨è¿¹ç”Ÿæˆã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤å’Œå¯æ‰©å±•çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹æƒé‡ã€T-Wix 500k æŒ‡ä»¤è¯­æ–™åº“ã€T-Math æ¨ç†åŸºå‡†å’Œ EAGLE æƒé‡ã€‚è¿™äº›èµ„æºä½¿ç”¨æˆ·èƒ½å¤Ÿç ”ç©¶ä¿„è¯­æ¨ç†ï¼Œå¹¶æ‰©å±•æˆ–è°ƒæ•´æ¨¡å‹åŠæ¨æ–­ç®¡é“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10739",
            "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
            "url": "https://huggingface.co/papers/2512.10739",
            "abstract": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
            "score": 37,
            "issue_id": 22,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "6063188a9daac19c",
            "authors": [
                "Songyang Gao",
                "Yuzhe Gu",
                "Zijian Wu",
                "Lingkai Kong",
                "Wenwei Zhang",
                "Zhongrui Cai",
                "Fan Zheng",
                "Tianyou Ma",
                "Junhao Shen",
                "Haiteng Zhao",
                "Duanyang Zhang",
                "Huilun Zhang",
                "Kuikun Liu",
                "Chengqi Lyu",
                "Yanhui Duan",
                "Chiyu Chen",
                "Ningsheng Ma",
                "Jianfei Gao",
                "Han Lyu",
                "Dahua Lin",
                "Kai Chen"
            ],
            "affiliations": [
                "3MMLab, The Chinese University of Hong Kong",
                "ICMAT, Spanish National Research Council",
                "Ren Hui Academy of Beijing",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The High School Affiliated to Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10739.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Outcome-based Process Verifier (OPV) â€” Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ°Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ OPV Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Rejection Fine-Tuning, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OPV Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ OPV Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Verification in Long Reasoning with OPV",
                    "desc": "The paper introduces the Outcome-based Process Verifier (OPV), an innovative framework designed to enhance the verification of long reasoning chains in large language models (LLMs). OPV utilizes an iterative active learning approach combined with Rejection Fine-Tuning (RFT) to improve its verification capabilities while minimizing the need for extensive human annotations. By focusing on the most uncertain cases, OPV progressively refines its accuracy and efficiency in verifying complex reasoning tasks. Experimental results show that OPV achieves state-of-the-art performance, significantly outperforming larger models and improving accuracy in collaborative applications."
                },
                "zh": {
                    "title": "OPVï¼šæå‡é•¿æ¨ç†é“¾éªŒè¯çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "OPVæ˜¯ä¸€ç§è¿­ä»£ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†æ‹’ç»å¾®è°ƒæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ¨ç†é“¾éªŒè¯ä¸­çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ€»ç»“ç»“æœçš„è¿‡ç¨‹éªŒè¯ï¼Œè§£å†³äº†å½“å‰åŸºäºç»“æœçš„éªŒè¯å™¨æ— æ³•æ£€æŸ¥ä¸å¯é ä¸­é—´æ­¥éª¤çš„é—®é¢˜ã€‚OPVé€šè¿‡ä¸“å®¶æ³¨é‡Šé€æ­¥æ”¹è¿›éªŒè¯èƒ½åŠ›ï¼Œé™ä½äº†äººå·¥æ³¨é‡Šçš„æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPVåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10949",
            "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
            "url": "https://huggingface.co/papers/2512.10949",
            "abstract": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
            "score": 35,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "166eabf1b49f06d0",
            "authors": [
                "Yiwen Tang",
                "Zoey Guo",
                "Kaixin Zhu",
                "Ray Zhang",
                "Qizhi Chen",
                "Dongzhi Jiang",
                "Junli Liu",
                "Bohan Zeng",
                "Haoming Song",
                "Delin Qu",
                "Tianyi Bai",
                "Dan Xu",
                "Wentao Zhang",
                "Bin Zhao"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Peking University",
                "Shanghai AI Lab",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10949.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#3d",
                    "#optimization",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¾ Ğ¸Ğ·-Ğ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL (Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² GRPO), Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MME-3DR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Hi-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° AR3D-R1 - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Text-to-3D Generation",
                    "desc": "This paper explores the use of reinforcement learning (RL) to improve the generation of 3D objects from text descriptions. It highlights the importance of reward design and RL algorithms in achieving high-quality 3D outputs, given the complexity of 3D structures. The authors introduce AR3D-R1, the first RL-enhanced model specifically for text-to-3D generation, which incorporates hierarchical optimization techniques. Additionally, they present new benchmarks to better evaluate the reasoning capabilities of 3D generation models, aiming to advance the field of 3D content creation."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›æ–‡æœ¬åˆ°3Dç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬åˆ°3Dç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨å¥–åŠ±è®¾è®¡ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€åŸºå‡†æµ‹è¯•å’Œåˆ†å±‚ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†AR3D-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¢å¼ºå‹3Dç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¥–åŠ±è®¾è®¡ä¸äººç±»åå¥½çš„å¯¹é½è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿä¸º3Då±æ€§æä¾›å¼ºæœ‰åŠ›çš„ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†MME-3DRåŸºå‡†ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°3Dç”Ÿæˆæ¨¡å‹çš„éšæ€§æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10756",
            "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
            "url": "https://huggingface.co/papers/2512.10756",
            "abstract": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
            "score": 30,
            "issue_id": 22,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "ec500b8a6785628e",
            "authors": [
                "Zijian Wu",
                "Lingkai Kong",
                "Wenwei Zhang",
                "Songyang Gao",
                "Yuzhe Gu",
                "Zhongrui Cai",
                "Tianyou Ma",
                "Yuhong Liu",
                "Zhi Wang",
                "Runyuan Ma",
                "Guangyu Wang",
                "Wei Li",
                "Conghui He",
                "Dahua Lin",
                "Kai Chen"
            ],
            "affiliations": [
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10756.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#rlhf",
                    "#rl",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âœ“",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Outcome-based Process Verifier (OPV), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. OPV Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rejection Fine-Tuning Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OPV Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Verification in Language Models with OPV",
                    "desc": "The Outcome-based Process Verifier (OPV) enhances the verification of reasoning chains in large language models by integrating outcome-based and process-based verification methods. It utilizes iterative active learning and Rejection Fine-Tuning to improve its performance while minimizing the need for extensive human annotations. OPV effectively addresses the limitations of existing verifiers by accurately assessing both the outcomes and the processes involved in complex reasoning tasks. Experimental results show that OPV achieves state-of-the-art performance on various benchmarks, significantly outperforming larger models in accuracy and error detection."
                },
                "zh": {
                    "title": "æå‡æ¨ç†é“¾éªŒè¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éªŒè¯æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºç»“æœçš„è¿‡ç¨‹éªŒè¯å™¨ï¼ˆOPVï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†é“¾ä¸­çš„éªŒè¯èƒ½åŠ›ã€‚OPVç»“åˆäº†åŸºäºç»“æœå’ŒåŸºäºè¿‡ç¨‹çš„éªŒè¯æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨è¿­ä»£ä¸»åŠ¨å­¦ä¹ å’Œæ‹’ç»å¾®è°ƒæŠ€æœ¯ï¼Œä»¥å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„éªŒè¯ã€‚é€šè¿‡ä¸“å®¶æ³¨é‡Šçš„è¿­ä»£å­¦ä¹ ï¼ŒOPVèƒ½å¤Ÿåœ¨å‡å°‘æ³¨é‡Šæˆæœ¬çš„åŒæ—¶ï¼Œé€æ­¥æå‡å…¶éªŒè¯èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPVåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10534",
            "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.10534",
            "abstract": "InternGeometry, an LLM agent, surpasses human performance on IMO geometry problems using a heuristic-driven approach with iterative proposition verification and a dynamic memory mechanism, significantly outperforming AlphaGeometry 2 with limited training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
            "score": 24,
            "issue_id": 24,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "33eae7215e30750d",
            "authors": [
                "Haiteng Zhao",
                "Junhao Shen",
                "Yiming Zhang",
                "Songyang Gao",
                "Kuikun Liu",
                "Tianyou Ma",
                "Fan Zheng",
                "Dahua Lin",
                "Wenwei Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "ICMAT, Spanish National Research Council",
                "MMLab, The Chinese University of Hong Kong",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10534.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#math",
                    "#agents",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LLM Ğ°Ğ³ĞµĞ½Ñ‚, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½",
                    "desc": "InternGeometry â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ²ÑƒÑ…ÑĞ¾Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑƒĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ IMO 2000-2024 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ¸Ğ»Ğ° 44 Ğ¸Ğ· 50 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 13 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ğ² 7500 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡ĞµĞ¼ AlphaGeometry 2. ĞĞ³ĞµĞ½Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "InternGeometry: Redefining Geometry Problem Solving with LLMs",
                    "desc": "InternGeometry is a large language model (LLM) agent designed to solve complex geometry problems, specifically those found in the International Mathematical Olympiad (IMO). It uses a heuristic-driven approach that involves iteratively proposing and verifying geometric propositions with a symbolic engine, allowing it to learn from feedback effectively. This model significantly outperforms previous systems like AlphaGeometry 2, achieving a higher success rate with far less training data. Additionally, it introduces a novel training method called Complexity-Boosting Reinforcement Learning (CBRL) to enhance its problem-solving capabilities by gradually increasing problem complexity during training."
                },
                "zh": {
                    "title": "è¶…è¶Šäººç±»çš„å‡ ä½•è§£é¢˜èƒ½åŠ›",
                    "desc": "InternGeometry æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œé‡‡ç”¨å¯å‘å¼é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£éªŒè¯å‘½é¢˜å’ŒåŠ¨æ€è®°å¿†æœºåˆ¶ï¼Œè¶…è¶Šäº†äººç±»åœ¨å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰å‡ ä½•é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚å®ƒé€šè¿‡ä¸ç¬¦å·å¼•æ“è¿›è¡Œå¤šæ¬¡äº¤äº’ï¼Œå…‹æœäº†å‡ ä½•é—®é¢˜è§£å†³ä¸­çš„å¯å‘å¼é™åˆ¶ï¼Œå¹¶å¼•å…¥äº†å¤æ‚æ€§å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆCBRLï¼‰æ¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚InternGeometry åœ¨ä»…ä½¿ç”¨ 13000 ä¸ªè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸè§£å†³äº† 50 ä¸ª IMO å‡ ä½•é—®é¢˜ä¸­çš„ 44 ä¸ªï¼Œè¡¨ç°è¶…è¿‡äº†å¹³å‡é‡‘ç‰Œå¾—åˆ†ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿæå‡ºäººç±»è§£å†³æ–¹æ¡ˆä¸­æœªå‡ºç°çš„æ–°è¾…åŠ©æ„é€ ï¼Œå±•ç¤ºäº† LLM ä»£ç†åœ¨ä¸“å®¶çº§å‡ ä½•ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10881",
            "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
            "url": "https://huggingface.co/papers/2512.10881",
            "abstract": "MoCapAnything is a reference-guided framework that reconstructs rotation-based animations from monocular video for arbitrary rigged 3D assets, enabling cross-species retargeting and scalable 3D motion capture.  \t\t\t\t\tAI-generated summary \t\t\t\t Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
            "score": 19,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "ab741303202c0b9c",
            "authors": [
                "Kehong Gong",
                "Zhengyu Wen",
                "Weixia He",
                "Mingxi Xu",
                "Qi Wang",
                "Ning Zhang",
                "Zhengyu Li",
                "Dongze Lian",
                "Wei Zhao",
                "Xiaoyu He",
                "Mingyuan Zhang"
            ],
            "affiliations": [
                "Huawei Central Media Technology Institute",
                "Huawei Technologies Co., Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10881.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "MoCapAnything Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ğ½Ğ³Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ (ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°ÑÑĞµÑ‚-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒĞ³Ğ»Ñ‹ Ğ¿Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Truebones Zoo Ñ 1038 Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Motion Capture: Any Video, Any Character!",
                    "desc": "MoCapAnything is a framework designed to create animations from single videos for any 3D character model. It addresses the challenge of motion capture that typically requires specific templates by introducing Category-Agnostic Motion Capture (CAMoCap). The system uses a combination of three modules: one to understand the 3D structure of the character, another to analyze the video for motion details, and a final module to generate smooth animations. This approach allows for flexible and efficient motion capture that can adapt to different types of 3D models, making it easier to create animations across various species and styles."
                },
                "zh": {
                    "title": "è·¨ç‰©ç§çš„3Dè¿åŠ¨æ•æ‰æ–°æ–¹æ³•",
                    "desc": "MoCapAnythingæ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŸºäºæ—‹è½¬çš„åŠ¨ç”»ï¼Œé€‚ç”¨äºä»»æ„çš„ç»‘å®š3Dèµ„äº§ã€‚è¿™ä¸€æ–¹æ³•è¢«ç§°ä¸ºç±»åˆ«æ— å…³è¿åŠ¨æ•æ‰ï¼ˆCAMoCapï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å•ç›®è§†é¢‘å’Œä»»æ„3Dèµ„äº§çš„æç¤ºï¼Œç”Ÿæˆé©±åŠ¨ç‰¹å®šèµ„äº§çš„åŠ¨ç”»ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªå¯å­¦ä¹ æ¨¡å—å’Œä¸€ä¸ªè½»é‡çº§çš„é€†å‘è¿åŠ¨å­¦é˜¶æ®µï¼Œèƒ½å¤Ÿé¢„æµ‹3Då…³èŠ‚è½¨è¿¹å¹¶æ¢å¤èµ„äº§ç‰¹å®šçš„æ—‹è½¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoCapAnythingåœ¨ä¸åŒçš„è®¾å¤‡ä¸Šå®ç°äº†é«˜è´¨é‡çš„éª¨éª¼åŠ¨ç”»ï¼Œå¹¶æœ‰æ•ˆæ”¯æŒè·¨ç‰©ç§çš„é‡å®šå‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10867",
            "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.10867",
            "abstract": "A benchmark framework evaluates Vision-Language Models in understanding microscopic spatial relationships, showing potential but highlighting the need for domain-specific knowledge integration.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
            "score": 11,
            "issue_id": 24,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "78f6014ffd133209",
            "authors": [
                "Zongzhao Li",
                "Xiangzhe Kong",
                "Jiahui Su",
                "Zongyang Ma",
                "Mingze Li",
                "Songyou Li",
                "Yuelin Zhang",
                "Yu Rong",
                "Tingyang Xu",
                "Deli Zhao",
                "Wenbing Huang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group, Hangzhou, China",
                "Dept. of Comp. Sci. & Tech., Tsinghua University",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Hupan Lab, Hangzhou, China",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "SKL-ESPC & SEPKL-AERM, College of Environmental Sciences and Engineering, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10867.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (MiSI) â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MiSI-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 163 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ 587 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ AGI."
                },
                "en": {
                    "title": "Unlocking Microscopic Spatial Intelligence with Vision-Language Models",
                    "desc": "This paper presents a new area called Microscopic Spatial Intelligence (MiSI), which focuses on understanding the spatial relationships of tiny, often invisible entities. To evaluate how well Vision-Language Models (VLMs) can handle these tasks, the authors created a benchmark framework named MiSI-Bench, consisting of a large dataset with over 163,000 question-answer pairs and 587,000 images. The results show that while current VLMs struggle to match human performance, a specially fine-tuned model shows promise in certain tasks, particularly spatial transformations. However, the findings also indicate that for VLMs to excel in more complex scientific tasks, integrating specific domain knowledge is essential."
                },
                "zh": {
                    "title": "å¾®è§‚ç©ºé—´æ™ºèƒ½ï¼šç§‘å­¦å‘ç°çš„æ–°è§†è§’",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†å¾®è§‚ç©ºé—´æ™ºèƒ½ï¼ˆMiSIï¼‰çš„æ¦‚å¿µï¼Œå¼ºè°ƒç†è§£å¾®è§‚å®ä½“çš„ç©ºé—´å…³ç³»å¯¹ç§‘å­¦å‘ç°çš„é‡è¦æ€§ã€‚ä¸ºäº†è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿™ä¸€é¢†åŸŸçš„æ½œåŠ›ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†æ¡†æ¶MiSI-Benchã€‚è¯¥æ¡†æ¶åŒ…å«è¶…è¿‡163,000ä¸ªé—®ç­”å¯¹å’Œ587,000å¼ å›¾åƒï¼Œæ¶µç›–ä¹ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œè¯„ä¼°ä»åŸºæœ¬ç©ºé—´å˜æ¢åˆ°å¤æ‚å…³ç³»è¯†åˆ«çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„VLMåœ¨è¿™ä¸€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä½äºäººç±»æ°´å¹³ï¼Œä½†ç»è¿‡å¾®è°ƒçš„7Bæ¨¡å‹åœ¨ç©ºé—´å˜æ¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¿‡äººç±»ï¼Œè¡¨æ˜åœ¨ç§‘å­¦ä»»åŠ¡ä¸­æ•´åˆé¢†åŸŸçŸ¥è¯†çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23386",
            "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
            "url": "https://huggingface.co/papers/2511.23386",
            "abstract": "VQRAE, a Vector Quantization Representation AutoEncoder, unifies multimodal understanding, generation, and reconstruction using a unified tokenizer with continuous semantic features and discrete tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
            "score": 10,
            "issue_id": 24,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "b756fe25185d0e62",
            "authors": [
                "Sinan Du",
                "Jiahao Guo",
                "Bo Li",
                "Shuhao Cui",
                "Zhengzhuo Xu",
                "Yifu Luo",
                "Yongxian Wei",
                "Kun Gai",
                "Xinggang Wang",
                "Kai Wu",
                "Chun Yuan"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology (HUST)",
                "Kolors Team, Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2511.23386.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "VQRAE Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·ĞµÑ€, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ViT Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ VQ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 100% Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ 1536. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ."
                },
                "en": {
                    "title": "Unifying Multimodal Tasks with VQRAE",
                    "desc": "VQRAE, or Vector Quantization Representation AutoEncoder, introduces a novel approach to unify multimodal tasks such as understanding, generation, and reconstruction using a single tokenizer. This model leverages continuous semantic features for image comprehension and discrete tokens for visual generation, addressing challenges faced by previous dual encoder systems. By employing a two-stage training strategy, VQRAE optimizes a high-dimensional semantic VQ codebook, enhancing the model's ability to maintain semantic information while enabling effective multimodal interactions. The results demonstrate VQRAE's competitive performance across various benchmarks, showcasing its potential for scaling in autoregressive applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹",
                    "desc": "VQRAEæ˜¯ä¸€ç§å‘é‡é‡åŒ–è¡¨ç¤ºè‡ªç¼–ç å™¨ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„æ ‡è®°å™¨å®ç°å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºã€‚è¯¥æ¨¡å‹ç»“åˆäº†è¿ç»­çš„è¯­ä¹‰ç‰¹å¾å’Œç¦»æ•£çš„æ ‡è®°ï¼Œé¦–æ¬¡æ¢ç´¢äº†ç»Ÿä¸€è¡¨ç¤ºçš„å¯èƒ½æ€§ã€‚VQRAEé‡‡ç”¨å¯¹ç§°çš„ViTè§£ç å™¨ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVQRAEåœ¨è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºçš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10675",
            "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
            "url": "https://huggingface.co/papers/2512.10675",
            "abstract": "A generative evaluation system using a frontier video model (Veo) enables comprehensive policy evaluation in robotics, including nominal performance, out-of-distribution generalization, and safety checks.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
            "score": 8,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "29c98bbb243ee39f",
            "authors": [
                "Gemini Robotics Team",
                "Coline Devin",
                "Yilun Du",
                "Debidatta Dwibedi",
                "Ruiqi Gao",
                "Abhishek Jindal",
                "Thomas Kipf",
                "Sean Kirmani",
                "Fangchen Liu",
                "Anirudha Majumdar",
                "Andrew Marmon",
                "Carolina Parada",
                "Yulia Rubanova",
                "Dhruv Shah",
                "Vikas Sindhwani",
                "Jie Tan",
                "Fei Xia",
                "Ted Xiao",
                "Sherry Yang",
                "Wenhao Yu",
                "Allan Zhou"
            ],
            "affiliations": [
                "Gemini Robotics Team, Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10675.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Veo, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ñ€ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…, Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ„Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1600 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ²Ğ¾ÑÑŒĞ¼ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Gemini Robotics Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Revolutionizing Robotics Evaluation with Generative Video Models",
                    "desc": "This paper presents a generative evaluation system using a frontier video model (Veo) to enhance policy evaluation in robotics. It allows for comprehensive assessments of robot performance, including standard scenarios and those that are out-of-distribution (OOD). The system integrates generative image-editing and multi-view completion to create realistic variations of environments, enabling accurate simulations of robot interactions. Through extensive testing, the system demonstrates its ability to evaluate policies effectively while ensuring safety and generalization across diverse conditions."
                },
                "zh": {
                    "title": "å‰æ²¿è§†é¢‘æ¨¡å‹åŠ©åŠ›æœºå™¨äººæ”¿ç­–å…¨é¢è¯„ä¼°",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå‰æ²¿è§†é¢‘æ¨¡å‹ï¼ˆVeoï¼‰çš„ç”Ÿæˆè¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºæœºå™¨äººæ”¿ç­–çš„å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬æ­£å¸¸æ€§èƒ½ã€è¶…å‡ºåˆ†å¸ƒçš„æ³›åŒ–å’Œå®‰å…¨æ£€æŸ¥ã€‚ç”Ÿæˆä¸–ç•Œæ¨¡å‹åœ¨æ¨¡æ‹Ÿä¸è§†è§‰è¿åŠ¨æ”¿ç­–çš„äº¤äº’æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒä¸­ç”Ÿæˆé€¼çœŸçš„è§‚å¯Ÿå’Œç¯å¢ƒäº¤äº’ã€‚è¯¥ç³»ç»Ÿä¼˜åŒ–äº†æœºå™¨äººåŠ¨ä½œæ¡ä»¶å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œç»“åˆç”Ÿæˆå›¾åƒç¼–è¾‘å’Œå¤šè§†å›¾è¡¥å…¨ï¼Œåˆæˆç°å®åœºæ™¯çš„çœŸå®å˜åŒ–ã€‚é€šè¿‡å¯¹å…«ä¸ªGemini Roboticsæ”¿ç­–æ£€æŸ¥ç‚¹å’Œäº”ä¸ªåŒæ‰‹æ“ä½œä»»åŠ¡è¿›è¡Œ1600å¤šæ¬¡çœŸå®ä¸–ç•Œè¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨æ”¿ç­–è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08511",
            "title": "Thinking with Images via Self-Calling Agent",
            "url": "https://huggingface.co/papers/2512.08511",
            "abstract": "sCoT, a language-only CoT paradigm with self-calling subagents, enhances visual reasoning performance and efficiency through group-relative policy optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
            "score": 8,
            "issue_id": 28,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "97cc63059506f639",
            "authors": [
                "Wenxi Yang",
                "Yuzhong Zhao",
                "Fang Wan",
                "Qixiang Ye"
            ],
            "affiliations": [
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08511.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#agents",
                    "#open_source",
                    "#optimization",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°ÑÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ sCoT â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼. Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¿Ğ¸Ğ¸ ÑĞµĞ±Ñ (Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ğ· Ğ½Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ HR-Bench 4K sCoT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 1.9% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° 75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "sCoT: Efficient Visual Reasoning with Self-Calling Subagents",
                    "desc": "The paper introduces sCoT, a new approach to visual reasoning that simplifies the integration of language and visual information. By using self-calling subagents, sCoT breaks down complex tasks into smaller subtasks, allowing for more efficient problem-solving without needing to mix different types of data. This method enhances training effectiveness and reduces the computational resources required, achieving better performance with fewer GPU hours. The results demonstrate that sCoT outperforms existing methods in visual reasoning tasks while being more resource-efficient."
                },
                "zh": {
                    "title": "è‡ªè°ƒç”¨é“¾å¼æ€ç»´æå‡è§†è§‰æ¨ç†æ•ˆç‡",
                    "desc": "sCoTæ˜¯ä¸€ç§æ–°çš„è§†è§‰æ¨ç†èŒƒå¼ï¼Œå®ƒé€šè¿‡è‡ªè°ƒç”¨çš„å­ä»£ç†æ¥ä¼˜åŒ–è¯­è¨€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€‚è¯¥æ–¹æ³•å°†å¤æ‚çš„è§†è§‰æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºç®€å•çš„å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å…±äº«å‚æ•°çš„è™šæ‹Ÿå‰¯æœ¬åœ¨ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡ä¸­è§£å†³è¿™äº›ä»»åŠ¡ã€‚sCoTä¸éœ€è¦åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´è¿›è¡Œæ˜¾å¼äº¤é”™ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒsCoTåœ¨HR-Bench 4Kæ•°æ®é›†ä¸Šç›¸æ¯”äºå¼ºåŸºçº¿æ–¹æ³•ï¼Œæ¨ç†æ€§èƒ½æé«˜äº†1.9%ï¼ŒåŒæ—¶å‡å°‘äº†75%çš„GPUä½¿ç”¨æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10959",
            "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
            "url": "https://huggingface.co/papers/2512.10959",
            "abstract": "StereoSpace uses viewpoint-conditioned diffusion to generate stereo images without explicit depth or warping, achieving superior performance on various scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
            "score": 7,
            "issue_id": 31,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "361db0e40a82f919",
            "authors": [
                "Tjark Behrens",
                "Anton Obukhov",
                "Bingxin Ke",
                "Fabio Tosi",
                "Matteo Poggi",
                "Konrad Schindler"
            ],
            "affiliations": [
                "ETH Zurich",
                "HUAWEI Bayer Lab",
                "University of Bologna"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10959.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#diffusion",
                    "#leakage",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚ĞµÑ€ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ±ĞµĞ· ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ, ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "StereoSpace â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡ĞºĞµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¸ĞºÑĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ· ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ iSQoE Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ñ„Ğ¾Ñ€Ñ‚Ğ° Ğ¸ MEt3R Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ†ĞµĞ½Ğ°Ñ… ÑĞ¾ ÑĞ»Ğ¾Ğ¸ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ»Ğ°Ğ¼Ğ±ĞµÑ€Ñ‚Ğ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»Ğ°ĞºÑ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ."
                },
                "en": {
                    "title": "Depth-Free Stereo Image Generation with Viewpoint-Conditioned Diffusion",
                    "desc": "StereoSpace is a novel framework that uses viewpoint-conditioned diffusion to create stereo images from single images without needing explicit depth information or warping techniques. It leverages a canonical rectified space to guide the generator in inferring spatial correspondences and filling in gaps in the images. The framework introduces a new evaluation protocol that avoids using ground truth data, focusing instead on metrics that assess perceptual comfort and geometric consistency. By outperforming existing methods, StereoSpace demonstrates that viewpoint-conditioned diffusion can effectively generate high-quality stereo images in a scalable manner without relying on depth estimation."
                },
                "zh": {
                    "title": "è§†ç‚¹æ¡ä»¶æ‰©æ•£ï¼šæ— æ·±åº¦ç«‹ä½“ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "StereoSpace æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºå•ç›®åˆ°ç«‹ä½“å›¾åƒçš„åˆæˆã€‚å®ƒé€šè¿‡è§†ç‚¹æ¡ä»¶å»ºæ¨¡å‡ ä½•å½¢çŠ¶ï¼Œè€Œä¸éœ€è¦æ˜¾å¼çš„æ·±åº¦æˆ–æ‰­æ›²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ‡å‡†åŒ–çš„ç©ºé—´å’Œæ¡ä»¶å¼•å¯¼ç”Ÿæˆå™¨æ¨æ–­å¯¹åº”å…³ç³»ï¼Œå¹¶å¡«è¡¥ç¼ºå¤±åŒºåŸŸã€‚StereoSpace åœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ï¼Œè¯æ˜äº†è§†ç‚¹æ¡ä»¶æ‰©æ•£æ˜¯ä¸€ç§å¯æ‰©å±•çš„ã€æ— æ·±åº¦çš„ç«‹ä½“ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10938",
            "title": "Stronger Normalization-Free Transformers",
            "url": "https://huggingface.co/papers/2512.10938",
            "abstract": "Derf, a novel point-wise normalization function, outperforms existing alternatives across various domains, enhancing generalization without increased fitting capacity.  \t\t\t\t\tAI-generated summary \t\t\t\t Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x) = erf(Î±x + s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
            "score": 5,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "0cb5e67a81ff3aa6",
            "authors": [
                "Mingzhi Chen",
                "Taiming Lu",
                "Jiachen Zhu",
                "Mingjie Sun",
                "Zhuang Liu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "NYU",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10938.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Derf: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ LayerNorm Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Derf, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ“Ğ°ÑƒÑÑĞ° Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Derf Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ LayerNorm, RMSNorm Ğ¸ Dynamic Tanh Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ”ĞĞš. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞµÑ‘ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Derf: A New Era in Point-Wise Normalization for Better Generalization",
                    "desc": "This paper introduces Derf, a new point-wise normalization function that enhances model generalization without increasing fitting capacity. It builds on the concept of Dynamic Tanh (DyT) and explores how different point-wise functions affect training and performance. The authors conduct a comprehensive search for effective function designs and present Derf(x) = erf(Î±x + s) as the top performer. Derf outperforms existing normalization methods like LayerNorm and RMSNorm across various applications, demonstrating its potential for use in normalization-free Transformer architectures."
                },
                "zh": {
                    "title": "Derfï¼šæå‡æ³›åŒ–èƒ½åŠ›çš„å½’ä¸€åŒ–æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç‚¹å¯¹ç‚¹å½’ä¸€åŒ–å‡½æ•°Derfï¼Œå®ƒåœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å½’ä¸€åŒ–æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDerfçš„è®¾è®¡åŸºäºå¯¹ç‚¹å¯¹ç‚¹å‡½æ•°å†…åœ¨ç‰¹æ€§çš„æ·±å…¥åˆ†æï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ‹Ÿåˆèƒ½åŠ›çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚Derfçš„å…¬å¼ä¸ºDerf(x) = erf(Î±x + s)ï¼Œå…¶ä¸­erf(x)æ˜¯é‡æ ‡å®šçš„é«˜æ–¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDerfåœ¨è§†è§‰ã€è¯­éŸ³è¡¨ç¤ºå’ŒDNAåºåˆ—å»ºæ¨¡ç­‰å¤šä¸ªé¢†åŸŸå‡è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºæ— å½’ä¸€åŒ–Transformeræ¶æ„çš„å®ç”¨é€‰æ‹©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05439",
            "title": "BEAVER: An Efficient Deterministic LLM Verifier",
            "url": "https://huggingface.co/papers/2512.05439",
            "abstract": "BEAVER is a framework that provides deterministic and sound probability bounds for verifying constraints in large language models, achieving tighter bounds and identifying more high-risk instances than baseline methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
            "score": 4,
            "issue_id": 36,
            "pub_date": "2025-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "1e44c2beda5d156c",
            "authors": [
                "Tarun Suresh",
                "Nalin Wadhwa",
                "Debangshu Banerjee",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "University of Illinois, Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.05439.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "BEAVER â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (token trie Ğ¸ frontier), Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. BEAVER Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ² 3-4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ¸ÑĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ production-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "BEAVER: Ensuring Reliable Constraints in Language Models",
                    "desc": "BEAVER is a framework designed to provide reliable and sound probability bounds for verifying constraints in large language models (LLMs). Unlike traditional sampling methods, which lack guarantees, BEAVER offers deterministic bounds that ensure model outputs meet specified requirements. It utilizes innovative data structures like token tries and frontiers to systematically explore the generation space while maintaining soundness throughout the process. The framework has been shown to significantly outperform baseline methods, achieving tighter bounds and identifying more high-risk instances, thus enhancing the reliability of LLMs in practical applications."
                },
                "zh": {
                    "title": "BEAVERï¼šå¤§å‹è¯­è¨€æ¨¡å‹çº¦æŸéªŒè¯çš„å¯é æ¡†æ¶",
                    "desc": "BEAVERæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„çº¦æŸéªŒè¯æä¾›ç¡®å®šæ€§å’Œå¯é çš„æ¦‚ç‡ç•Œé™ã€‚å®ƒé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢ç”Ÿæˆç©ºé—´ï¼Œä½¿ç”¨æ–°é¢–çš„token trieå’Œå‰æ²¿æ•°æ®ç»“æ„ï¼Œç¡®ä¿åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¿æŒå¯è¯æ˜çš„å¯é ç•Œé™ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒBEAVERèƒ½å¤Ÿå®ç°6åˆ°8å€æ›´ç´§çš„æ¦‚ç‡ç•Œé™ï¼Œå¹¶è¯†åˆ«å‡º3åˆ°4å€æ›´å¤šçš„é«˜é£é™©å®ä¾‹ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æ­£ç¡®æ€§éªŒè¯ã€éšç§éªŒè¯å’Œå®‰å…¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10791",
            "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
            "url": "https://huggingface.co/papers/2512.10791",
            "abstract": "The FACTS Leaderboard evaluates language models' factual accuracy across different scenarios using four sub-leaderboards: image-based questions, closed-book factoid questions, information-seeking with search API, and document-grounded long-form responses.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
            "score": 3,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "9bc3b9b0601ed18f",
            "authors": [
                "Aileen Cheng",
                "Alon Jacovi",
                "Amir Globerson",
                "Ben Golan",
                "Charles Kwong",
                "Chris Alberti",
                "Connie Tao",
                "Eyal Ben-David",
                "Gaurav Singh Tomar",
                "Lukas Haas",
                "Yonatan Bitton",
                "Adam Bloniarz",
                "Aijun Bai",
                "Andrew Wang",
                "Anfal Siddiqui",
                "Arturo Bajuelos Castillo",
                "Aviel Atias",
                "Chang Liu",
                "Corey Fry",
                "Daniel Balle",
                "Deepanway Ghosal",
                "Doron Kukliansky",
                "Dror Marcus",
                "Elena Gribovskaya",
                "Eran Ofek",
                "Honglei Zhuang",
                "Itay Laish",
                "Jan Ackermann",
                "Lily Wang",
                "Meg Risdal",
                "Megan Barnes",
                "Michael Fink",
                "Mohamed Amin",
                "Moran Ambar",
                "Natan Potikha",
                "Nikita Gupta",
                "Nitzan Katz",
                "Noam Velan",
                "Ofir Roval",
                "Ori Ram",
                "Polina Zablotskaia",
                "Prathamesh Bang",
                "Priyanka Agrawal",
                "Rakesh Ghiya",
                "Sanjay Ganapathy",
                "Simon Baumgartner",
                "Sofia Erell",
                "Sushant Prakash",
                "Thibault Sellam",
                "Vikram Rao",
                "Xuanhui Wang",
                "Yaroslav Akulov",
                "Yulong Yang",
                "Zhen Yang",
                "Zhixin Lai",
                "Zhongru Wu",
                "Anca Dragan",
                "Avinatan Hassidim",
                "Fernando Pereira",
                "Slav Petrov",
                "Srinivasan Venkatachary",
                "Tulsee Doshi",
                "Yossi Matias",
                "Sasha Goldshtein",
                "Dipanjan Das"
            ],
            "affiliations": [
                "Google Cloud",
                "Google DeepMind",
                "Google Research",
                "Kaggle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10791.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "âœ…",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ°ĞºÑ‚Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ FACTS Leaderboard, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¿Ğ¾Ğ´Ñ‚ĞµÑÑ‚Ğ¾Ğ²: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑƒĞ´ÑŒĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±Ğ°Ğ»Ğ»Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, Ğ° Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼. Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ„Ğ°ĞºÑ‚Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Evaluating Factual Accuracy in Language Models",
                    "desc": "The FACTS Leaderboard is a new tool designed to evaluate how accurately language models can provide factual information in various contexts. It consists of four sub-leaderboards that test different aspects of factual accuracy: image-based questions, closed-book factoid questions, information-seeking with a search API, and document-grounded long-form responses. Each sub-leaderboard uses automated judge models to score the responses, ensuring a consistent and objective evaluation. The overall score reflects the model's performance across all scenarios, making it a comprehensive measure of factuality in language models."
                },
                "zh": {
                    "title": "FACTSæ’è¡Œæ¦œï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§",
                    "desc": "FACTS Leaderboard æ˜¯ä¸€ä¸ªåœ¨çº¿è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹ç”Ÿæˆäº‹å®å‡†ç¡®æ–‡æœ¬çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«å››ä¸ªå­æ’è¡Œæ¦œï¼Œåˆ†åˆ«è¯„ä¼°å›¾åƒé—®é¢˜ã€é—­å·äº‹å®é—®é¢˜ã€ä¿¡æ¯æ£€ç´¢åœºæ™¯å’ŒåŸºäºæ–‡æ¡£çš„é•¿æ–‡æœ¬å“åº”çš„å‡†ç¡®æ€§ã€‚æ¯ä¸ªå­æ’è¡Œæ¦œä½¿ç”¨è‡ªåŠ¨è¯„åˆ†æ¨¡å‹æ¥è¯„ä¼°æ¨¡å‹çš„å›ç­”ï¼Œæœ€ç»ˆå¾—åˆ†æ˜¯å››ä¸ªéƒ¨åˆ†çš„å¹³å‡å€¼ï¼Œæä¾›äº†æ¨¡å‹æ•´ä½“äº‹å®æ€§çš„ç¨³å¥è¯„ä¼°ã€‚è¯¥æ’è¡Œæ¦œå°†æŒç»­ç»´æŠ¤ï¼Œå¹¶å…è®¸å¤–éƒ¨å‚ä¸ä»¥ç¡®ä¿å…¶å®Œæ•´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10359",
            "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
            "url": "https://huggingface.co/papers/2512.10359",
            "abstract": "A spatiotemporal reasoning framework enhances multimodal large language models for video question answering by strategically scheduling tools to improve spatial and temporal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
            "score": 3,
            "issue_id": 22,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "1d63017a5076d30f",
            "authors": [
                "Sunqi Fan",
                "Jiashuo Cui",
                "Meng-Hao Guo",
                "Shuojin Yang"
            ],
            "affiliations": [
                "BNRist, Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10359.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#open_source",
                    "#long_context",
                    "#video",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (STAR) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ½Ğ° 8.2% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VideoMME Ğ¸ 4.6% Ğ½Ğ° LongVideoBench."
                },
                "en": {
                    "title": "Enhancing VideoQA with Spatiotemporal Reasoning",
                    "desc": "This paper presents a Spatiotemporal Reasoning Framework (STAR) designed to improve the performance of Multimodal Large Language Models (MLLMs) in Video Question Answering (VideoQA) tasks. The framework enhances the models' ability to understand both spatial relationships in video frames and the temporal dynamics of events over time. By strategically scheduling the use of various tools within a comprehensive Video Toolkit, STAR helps the models to better localize important areas in videos and reason about them effectively. The results show significant performance improvements on benchmark tasks, indicating the potential of this approach for developing advanced video analysis systems."
                },
                "zh": {
                    "title": "æ—¶ç©ºæ¨ç†æ¡†æ¶æå‡è§†é¢‘é—®ç­”èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ—¶ç©ºæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒæ—¶å»ºæ¨¡è§†é¢‘å¸§ä¸­çš„ç©ºé—´å…³ç³»å’Œç†è§£æ—¶é—´æ¼”å˜çš„å› æœåŠ¨æ€æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬ä¸ºè¿™äº›æ¨¡å‹é…å¤‡äº†ä¸€ä¸ªå…¨é¢ä¸”å¯æ‰©å±•çš„è§†é¢‘å·¥å…·åŒ…ï¼Œä»¥æå‡å…¶æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°è°ƒåº¦å·¥å…·ï¼Œæˆ‘ä»¬çš„æ—¶ç©ºæ¨ç†æ¡†æ¶ï¼ˆSTARï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½è§†é¢‘ä¸­çš„å…³é”®åŒºåŸŸï¼Œä»è€Œæé«˜è§†é¢‘åˆ†æçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09270",
            "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
            "url": "https://huggingface.co/papers/2512.09270",
            "abstract": "MoRel, a novel 4D Gaussian Splatting framework, addresses long-range motion and memory efficiency in dynamic video rendering by using Anchor Relay-based Bidirectional Blending and Feature-variance-guided Hierarchical Densification.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap_{LR}. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
            "score": 3,
            "issue_id": 26,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "d1a140883f19d07d",
            "authors": [
                "Sangwoon Kwak",
                "Weeyoung Kwon",
                "Jun Young Jeong",
                "Geonho Kim",
                "Won-Sik Cheong",
                "Jihyong Oh"
            ],
            "affiliations": [
                "Chung-Ang University",
                "Electronics and Telecommunications Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09270.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "MoRel Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ 4D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Anchor Relay-based Bidirectional Blending Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ¾Ñ€ĞµĞ¹. Ğ’Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Feature-variance-guided Hierarchical Densification ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SelfCap_LR Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficiently Rendering Dynamic Videos with MoRel",
                    "desc": "MoRel is a new framework for 4D Gaussian Splatting that improves the rendering of dynamic videos by efficiently managing long-range motion and memory usage. It introduces an Anchor Relay-based Bidirectional Blending mechanism to ensure that the rendered scenes are temporally consistent and free from flickering. The framework also employs a Feature-variance-guided Hierarchical Densification method to enhance the quality of the rendered images while controlling memory consumption. By creating a new dataset for testing, MoRel demonstrates its ability to handle complex dynamic scenes effectively."
                },
                "zh": {
                    "title": "é«˜æ•ˆåŠ¨æ€è§†é¢‘æ¸²æŸ“çš„æ–°çªç ´",
                    "desc": "MoRelæ˜¯ä¸€ç§æ–°é¢–çš„4Dé«˜æ–¯ç‚¹äº‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€è§†é¢‘æ¸²æŸ“ä¸­çš„é•¿è·ç¦»è¿åŠ¨å’Œå†…å­˜æ•ˆç‡é—®é¢˜ã€‚å®ƒé‡‡ç”¨åŸºäºé”šç‚¹ä¸­ç»§çš„åŒå‘æ··åˆæœºåˆ¶å’Œç‰¹å¾æ–¹å·®å¼•å¯¼çš„åˆ†å±‚ç¨ å¯†åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ—¶é—´ä¸€è‡´ä¸”å†…å­˜é«˜æ•ˆçš„åŠ¨æ€åœºæ™¯å»ºæ¨¡ã€‚é€šè¿‡åœ¨å…³é”®å¸§æ—¶é—´ç´¢å¼•å¤„é€æ­¥æ„å»ºå±€éƒ¨æ ‡å‡†é”šç‚¹ç©ºé—´ï¼ŒMoRelå¢å¼ºäº†æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„ä¸é€æ˜åº¦æ§åˆ¶å‡è½»äº†æ—¶é—´ä¸è¿ç»­æ€§å’Œé—ªçƒä¼ªå½±ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„é•¿è·ç¦»4Dè¿åŠ¨æ•°æ®é›†SelfCap_{LR}ï¼Œä»¥æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œé•¿è·ç¦»è¿åŠ¨æ–¹é¢çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10398",
            "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
            "url": "https://huggingface.co/papers/2512.10398",
            "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
            "score": 2,
            "issue_id": 21,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "c13dcf28484db27f",
            "authors": [
                "Zhaodong Wang",
                "Zhenting Qi",
                "Sherman Wong",
                "Nathan Hu",
                "Samuel Lin",
                "Jun Ge",
                "Erwin Gao",
                "Yining Yang",
                "Ben Maurer",
                "Wenlin Chen",
                "David Recordon",
                "Yilun Du",
                "Minlan Yu",
                "Ying Zhang"
            ],
            "affiliations": [
                "Harvard",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10398.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#plp",
                    "#agents",
                    "#open_source",
                    "#interpretability"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ AI Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Confucius Code Agent (CCA) â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ AI Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. CCA Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Confucius SDK, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµÑÑĞ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» build-test-improve Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. CCA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´ĞµĞ» (state-of-the-art) Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-Bench-Pro Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ Resolve@1 Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¹ 54.3%, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering AI Software Engineering at Scale with CCA",
                    "desc": "The paper introduces the Confucius Code Agent (CCA), an open-source AI software engineer designed to handle large-scale software engineering tasks. It utilizes the Confucius SDK, which focuses on enhancing Agent Experience (AX), User Experience (UX), and Developer Experience (DX) through features like hierarchical working memory and a persistent note-taking system. CCA automates the development process with a meta-agent that continuously improves agent configurations, allowing for efficient adaptation to new tasks and environments. The results show that CCA outperforms previous coding agents, achieving a Resolve@1 performance of 54.3% on the SWE-Bench-Pro benchmark, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "å­”å­ä»£ç ä»£ç†ï¼šå·¥ä¸šçº§AIè½¯ä»¶å·¥ç¨‹å¸ˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†å­”å­ä»£ç ä»£ç†ï¼ˆCCAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„äººå·¥æ™ºèƒ½è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œèƒ½å¤Ÿåœ¨å·¥ä¸šè§„æ¨¡ä¸Šè¿è¡Œã€‚CCAåŸºäºå­”å­SDKæ„å»ºï¼Œè¯¥å¹³å°ä¸“æ³¨äºä»£ç†ä½“éªŒã€ç”¨æˆ·ä½“éªŒå’Œå¼€å‘è€…ä½“éªŒä¸‰ä¸ªæ–¹é¢ã€‚å®ƒå¼•å…¥äº†ç»Ÿä¸€çš„åè°ƒå™¨å’Œå±‚æ¬¡åŒ–å·¥ä½œè®°å¿†ï¼Œä»¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶å…·å¤‡æŒç»­å­¦ä¹ çš„ç¬”è®°ç³»ç»Ÿå’Œæ¨¡å—åŒ–æ‰©å±•æ¨¡å—ã€‚é€šè¿‡è‡ªåŠ¨åŒ–çš„å…ƒä»£ç†ï¼ŒCCAèƒ½å¤Ÿå¿«é€Ÿå¼€å‘æ–°ä»»åŠ¡å’Œç¯å¢ƒä¸‹çš„ä»£ç†é…ç½®ï¼Œæ˜¾è‘—æé«˜äº†è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09924",
            "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
            "url": "https://huggingface.co/papers/2512.09924",
            "abstract": "The ReViSE framework integrates reasoning and visual editing in video models using a self-reflective reasoning mechanism, enhancing editing accuracy and visual fidelity through intrinsic feedback from an internal vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
            "score": 2,
            "issue_id": 30,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "bff29b7f86edab13",
            "authors": [
                "Xinyu Liu",
                "Hangjie Yuan",
                "Yujie Wei",
                "Jiazheng Xing",
                "Yujin Han",
                "Jiahao Pan",
                "Yanbiao Ma",
                "Chi-Min Chan",
                "Kang Zhao",
                "Shiwei Zhang",
                "Wenhan Luo",
                "Yike Guo"
            ],
            "affiliations": [
                "FDU",
                "HKU",
                "HKUST",
                "RUC",
                "Tongyi Lab",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09924.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ReViSE Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RVE-Bench Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚Ñ€Ğ¸Ğ½ÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑ„Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 32% Ğ¿Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ."
                },
                "en": {
                    "title": "Bridging Reasoning and Visual Editing in Video Models",
                    "desc": "The ReViSE framework enhances video editing by integrating reasoning with visual editing through a self-reflective reasoning mechanism. It addresses the limitations of existing video models that struggle with reason-informed editing due to inadequate datasets and a disconnect between reasoning and editing capabilities. By introducing the Reason-Informed Video Editing (RVE) task and the RVE-Bench benchmark, the framework allows for systematic evaluation of reasoning-aware video editing. The internal vision-language model provides intrinsic feedback, improving the model's editing accuracy and visual fidelity, resulting in a significant performance boost over previous methods."
                },
                "zh": {
                    "title": "æ¨ç†ä¸è§†è§‰ç¼–è¾‘çš„å®Œç¾ç»“åˆ",
                    "desc": "ReViSEæ¡†æ¶é€šè¿‡è‡ªåæ€æ¨ç†æœºåˆ¶ï¼Œå°†æ¨ç†ä¸è§†é¢‘æ¨¡å‹ä¸­çš„è§†è§‰ç¼–è¾‘ç›¸ç»“åˆï¼Œä»è€Œæé«˜ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰æ•°æ®é›†ä¸è¶³ä»¥æ”¯æŒæ¨ç†é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘ï¼Œå¹¶ä¸”æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸ç¼–è¾‘èƒ½åŠ›ä¹‹é—´å­˜åœ¨å†…åœ¨çš„è„±èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†Reason-Informed Video Editing (RVE)ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†RVE-BenchåŸºå‡†ï¼Œæ¶µç›–å¤šç§æ¨ç†ç»´åº¦å’ŒçœŸå®ç¼–è¾‘åœºæ™¯ã€‚é€šè¿‡å†…éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„åé¦ˆï¼ŒReViSEæ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–ç”Ÿæˆå™¨çš„æ¨ç†è¡Œä¸ºï¼Œæ˜¾è‘—æå‡äº†ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09406",
            "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
            "url": "https://huggingface.co/papers/2512.09406",
            "abstract": "A video-to-video translation framework converts human-object interaction videos into realistic robot manipulation videos using unpaired training data and a generative model.  \t\t\t\t\tAI-generated summary \t\t\t\t Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
            "score": 2,
            "issue_id": 22,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "7043517b0b1e7fe7",
            "authors": [
                "Hai Ci",
                "Xiaokang Liu",
                "Pei Yang",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09406.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ: Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº (Ğ¼Ğ°Ñ€ĞºĞµÑ€ Ğ¸ ÑÑ‚Ñ€ĞµĞ»ĞºĞ°), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² ÑÑ†ĞµĞ½Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Human Actions into Robot Skills",
                    "desc": "This paper presents a novel video-to-video translation framework that transforms human-object interaction videos into realistic robot manipulation videos. It utilizes unpaired training data, allowing robots to learn from everyday human videos without the need for extensive data collection. The method introduces a transferable representation that helps bridge the gap between human actions and robot capabilities by inpainting and overlaying visual cues. The results show that this approach produces more realistic robot motions, indicating a significant advancement in robot learning from unlabeled human videos."
                },
                "zh": {
                    "title": "æ— é…å¯¹æ•°æ®ä¸‹çš„æœºå™¨äººå­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†é¢‘åˆ°è§†é¢‘çš„è½¬æ¢æ¡†æ¶ï¼Œå¯ä»¥å°†äººç±»ä¸ç‰©ä½“äº’åŠ¨çš„è§†é¢‘è½¬æ¢ä¸ºé€¼çœŸçš„æœºå™¨äººæ“ä½œè§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ— é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé¿å…äº†ç¹ççš„æœºå™¨äººæ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚é€šè¿‡åœ¨è®­ç»ƒè§†é¢‘ä¸­å¯¹æœºå™¨äººæ‰‹è‡‚è¿›è¡Œä¿®å¤ï¼Œå¹¶æ·»åŠ ç®€å•çš„è§†è§‰æç¤ºï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸äººç±»åŠ¨ä½œä¸€è‡´çš„æœºå™¨äººè§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æœºå™¨äººåŠ¨ä½œæ¯”åŸºçº¿æ–¹æ³•æ›´çœŸå®ï¼Œå±•ç¤ºäº†ä»æœªæ ‡è®°çš„äººç±»è§†é¢‘ä¸­æ‰©å±•æœºå™¨äººå­¦ä¹ çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08870",
            "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
            "url": "https://huggingface.co/papers/2512.08870",
            "abstract": "Fed-SE, a Federated Self-Evolution framework, enhances LLM agents in privacy-constrained environments by local parameter-efficient fine-tuning and global aggregation in a low-rank subspace.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
            "score": 2,
            "issue_id": 21,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "cca0f38b3636fadf",
            "authors": [
                "Xiang Chen",
                "Yuling Shi",
                "Qizhen Lan",
                "Yuchao Qiu",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "UTHealth Houston",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08870.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Fed-SE â€” ÑÑ‚Ğ¾ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Fed-SE Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 18% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing LLM Agents with Federated Self-Evolution",
                    "desc": "Fed-SE is a framework designed to improve the performance of large language model (LLM) agents in environments where privacy is a concern. It uses a method called federated learning to allow agents to learn from their experiences without sharing sensitive data. The framework focuses on local fine-tuning of agent parameters based on high-reward experiences, which helps stabilize the learning process. By aggregating these updates in a low-rank subspace, Fed-SE minimizes conflicts and enhances knowledge transfer across different environments, leading to better overall task success rates."
                },
                "zh": {
                    "title": "è”é‚¦è‡ªæˆ‘è¿›åŒ–ï¼šæå‡éšç§ç¯å¢ƒä¸­çš„LLMä»£ç†æ€§èƒ½",
                    "desc": "Fed-SEæ˜¯ä¸€ä¸ªè”é‚¦è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åœ¨éšç§å—é™ç¯å¢ƒä¸­å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å±€éƒ¨è¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œå¹¶åœ¨ä½ç§©å­ç©ºé—´ä¸­è¿›è¡Œå…¨å±€èšåˆï¼Œè§£å†³äº†ä¼ ç»Ÿè”é‚¦å­¦ä¹ åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨æŒ‘æˆ˜ã€‚Fed-SEé€šè¿‡è¿‡æ»¤é«˜å›æŠ¥è½¨è¿¹å®ç°ç¨³å®šçš„æ¢¯åº¦æ›´æ–°ï¼Œå¹¶æœ‰æ•ˆå‡å°‘å®¢æˆ·ç«¯ä¹‹é—´çš„è´Ÿè¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFed-SEåœ¨äº”ä¸ªå¼‚æ„ç¯å¢ƒä¸­å¹³å‡æé«˜äº†çº¦18%çš„ä»»åŠ¡æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶åœ¨éšç§å—é™éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09756",
            "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
            "url": "https://huggingface.co/papers/2512.09756",
            "abstract": "MOA, a reinforcement-learning framework, optimizes multiple dimensions of role-playing agents using multi-objective alignment and thought-augmented rollout, outperforming baselines in diverse scenarios and complex conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
            "score": 1,
            "issue_id": 25,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "7511a9a005b861a0",
            "authors": [
                "Chonghua Liao",
                "Ke Wang",
                "Yuchuan Wu",
                "Fei Huang",
                "Yongbin Li"
            ],
            "affiliations": [
                "Tongyi Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09756.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#agents",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MOA â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ supervised fine-tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, MOA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° thought-augmented rollout Ñ off-policy guidance. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MOA, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ GPT-4o Ğ¸ Claude Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹."
                },
                "en": {
                    "title": "MOA: Mastering Multi-Dimensional Role-Playing Agents with Reinforcement Learning",
                    "desc": "The paper introduces MOA, a reinforcement-learning framework designed to enhance role-playing agents (RPAs) by optimizing multiple conflicting skills simultaneously. Unlike previous methods that either overfit to surface cues or fail to capture the complexity of RPAs, MOA employs a multi-objective optimization strategy that focuses on fine-grained rubrics. Additionally, it incorporates thought-augmented rollout with off-policy guidance to improve the diversity and quality of model outputs. Experimental results demonstrate that MOA significantly outperforms existing models like GPT-4o and Claude in various challenging scenarios, showcasing its effectiveness in developing advanced RPAs."
                },
                "zh": {
                    "title": "å¤šç›®æ ‡å¯¹é½ï¼Œæå‡è§’è‰²æ‰®æ¼”ä»£ç†çš„èƒ½åŠ›",
                    "desc": "MOAï¼ˆå¤šç›®æ ‡å¯¹é½ï¼‰æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è§’è‰²æ‰®æ¼”ä»£ç†ï¼ˆRPAï¼‰çš„å¤šä¸ªç»´åº¦ã€‚å®ƒé€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶åœ¨å¤šä¸ªç»†åŒ–æ ‡å‡†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜ä¼˜åŒ–æ€§èƒ½ã€‚MOAè¿˜é‡‡ç”¨äº†æ€ç»´å¢å¼ºå›æ»šå’Œç¦»çº¿ç­–ç•¥æŒ‡å¯¼ï¼Œä»¥è§£å†³æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§å’Œè´¨é‡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMOAåœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¸å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ç›¸åª²ç¾ï¼Œå±•ç¤ºäº†å…¶åœ¨æ„å»ºé«˜æ•ˆRPAæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04537",
            "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
            "url": "https://huggingface.co/papers/2512.04537",
            "abstract": "X-Humanoid uses a generative video editing approach to translate human actions into humanoid robot actions, creating a large dataset for training embodied AI models.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
            "score": 1,
            "issue_id": 35,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "6b41ec08a2212652",
            "authors": [
                "Pei Yang",
                "Hai Ci",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.04537.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#synthetic",
                    "#open_source",
                    "#video",
                    "#robotics",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ²",
                    "desc": "X-Humanoid Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan 2.2 Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ video-to-video Ğ¸ ĞµÑ‘ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ pipeline Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Unreal Engine, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 3.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Human Actions into Humanoid Robot Movements",
                    "desc": "X-Humanoid presents a novel generative video editing technique that translates human actions into humanoid robot actions, addressing the need for diverse training data in embodied AI. By adapting the Wan 2.2 model into a video-to-video framework, it effectively creates a large dataset of paired human-humanoid videos. This dataset, generated using Unreal Engine, consists of over 3.6 million 'robotized' video frames, significantly enhancing the training resources for Vision-Language-Action models. User studies demonstrate that X-Humanoid outperforms existing methods in terms of motion consistency and embodiment correctness, showcasing its potential for advancing humanoid robotics."
                },
                "zh": {
                    "title": "X-Humanoidï¼šå°†äººç±»åŠ¨ä½œè½¬åŒ–ä¸ºç±»äººæœºå™¨äººåŠ¨ä½œçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "X-Humanoid æ˜¯ä¸€ç§ç”Ÿæˆè§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨å°†äººç±»åŠ¨ä½œè½¬æ¢ä¸ºç±»äººæœºå™¨äººåŠ¨ä½œï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªç”¨äºè®­ç»ƒå…·èº«äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¤§å‹æ•°æ®é›†ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¼ºå¤§çš„ Wan 2.2 æ¨¡å‹é€‚é…ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç»“æ„ï¼Œå¹¶é’ˆå¯¹äººç±»åˆ°ç±»äººæœºå™¨äººçš„è½¬æ¢ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®åˆ›å»ºç®¡é“ï¼Œåˆ©ç”¨è™šå¹»å¼•æ“å°†ç¤¾åŒºèµ„äº§è½¬åŒ–ä¸ºè¶…è¿‡ 17 å°æ—¶çš„é…å¯¹åˆæˆè§†é¢‘ã€‚ç»è¿‡å®šé‡åˆ†æå’Œç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿åŠ¨ä¸€è‡´æ€§å’Œå…·èº«æ­£ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼Œ69% çš„ç”¨æˆ·è®¤ä¸ºå…¶åœ¨è¿åŠ¨ä¸€è‡´æ€§ä¸Šæœ€ä½³ï¼Œ62.1% çš„ç”¨æˆ·è®¤ä¸ºå…¶åœ¨å…·èº«æ­£ç¡®æ€§ä¸Šæœ€ä½³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-11.html",
    "link_next": "2025-12-15.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "15.12",
        "en": "12/15",
        "zh": "12æœˆ15æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 10,
        "#agents": 6,
        "#cv": 2,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 7,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 3,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 9,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}