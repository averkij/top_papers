
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. March 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-11.html">â¬…ï¸ <span id="prev-date">11.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-13.html">â¡ï¸ <span id="next-date">13.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'};
        let feedDateNext = {'ru': '13.03', 'en': '03/13', 'zh': '3æœˆ13æ—¥'};
        let feedDatePrev = {'ru': '11.03', 'en': '03/11', 'zh': '3æœˆ11æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.07920', 'title': 'Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia', 'url': 'https://huggingface.co/papers/2503.07920', 'abstract': 'Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.', 'score': 69, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '32c690b6ffb8b143', 'authors': ['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'Christian Simon', 'Lynnette Hui Xian Ng', "Richardy Lobo' Sapan", 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Flores', 'Kenneth Ko Han Chen', 'Anjela Gail Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M. Alif Al Hakim', "Muhammad Rizky Sya'ban", 'Kun Kerdthaisong', 'Lester James V. Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'BÃ¶rje F. Karlsson', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Allen AI', 'Ateneo de Manila University', 'Auburn University', 'Bandung Institute of Technology', 'Beijing Academy of Artificial Intelligence (BAAI)', 'Binus University', 'Brawijaya University', 'Brown University', 'Capital One', 'Carnegie Mellon University', 'Chulalongkorn University', 'Cohere', 'Dataxet:Sonar', 'Faculty of Medicine Siriraj Hospital, Mahidol University', 'Graphcore', 'Hanyang University', 'Independent', 'Indian Statistical Institute, Kolkata', 'IndoNLP', 'Institut Teknologi Sepuluh Nopember', 'Institute for Infocomm Research, Singapore', 'King Mongkuts University of Technology Thonburi', 'MBZUAI', 'MOH Office for Healthcare Transformation', 'Macau University of Science and Technology', 'Meta', 'Mila - Quebec AI Institute', 'Monash University, Indonesia', 'Nara Institute of Science and Technology', 'National University Philippines', 'National University of Singapore', 'New York University', 'Oracle', 'Polytechnique Montreal', 'SCB 10X', 'SEACrowd', 'Samsung R&D Institute Philippines', 'Seoul National University of Science and Technology', 'Singapore Polytechnic', 'Singapore University of Technology and Design', 'Sony Group Corporation', 'Srinakharinwirot University', 'Thammasat University', 'The University of Manchester', 'Tianjin University', 'Ton Duc Thang University', 'Universitas Gadjah Mada', 'Universitas Islam Indonesia', 'Universitas Pelita Harapan', 'University of Bath', 'University of Illiinois, Urbana-Champaign', 'University of Indonesia', 'University of New Haven', 'University of Toronto', 'University of the Philippines', 'Vidyasirimedhi Institute of Science and Technology', 'Works Applications', 'WrocÅ‚aw Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.07920.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEA-VL - Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ 1,28 Ğ¼Ğ»Ğ½ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging the Cultural Gap in AI with SEA-VL', 'desc': "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."}, 'zh': {'title': 'å¡«è¡¥ä¸œå—äºšæ–‡åŒ–åœ¨AIç ”ç©¶ä¸­çš„ç©ºç™½', 'desc': 'ä¸œå—äºšåœ°åŒºè¯­è¨€å’Œæ–‡åŒ–å¤šæ ·æ€§æä¸ºä¸°å¯Œï¼Œä½†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å´ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEA-VLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„é¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºä¸œå—äºšè¯­è¨€å¼€å‘é«˜è´¨é‡ã€æ–‡åŒ–ç›¸å…³çš„æ•°æ®ã€‚é€šè¿‡å¸å¼•æ¥è‡ªä¸œå—äºšå›½å®¶çš„è´¡çŒ®è€…ï¼ŒSEA-VLç¡®ä¿äº†æ›´å¥½çš„æ–‡åŒ–ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œä¿ƒè¿›äº†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å¯¹è¢«ä½ä¼°è¯­è¨€çš„åŒ…å®¹æ€§ã€‚æˆ‘ä»¬æ”¶é›†äº†128ä¸‡å¼ ä¸ä¸œå—äºšæ–‡åŒ–ç›¸å…³çš„å›¾åƒï¼Œè¿œè¶…ç°æœ‰æ•°æ®é›†çš„è§„æ¨¡ï¼Œæ—¨åœ¨ç¼©å°ä¸œå—äºšçš„ä»£è¡¨æ€§å·®è·ï¼Œæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL', 'url': 'https://huggingface.co/papers/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'score': 48, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '59c304598f64f1e6', 'authors': ['Yingzhe Peng', 'Gongrui Zhang', 'Miaosen Zhang', 'Zhiyuan You', 'Jie Liu', 'Qipeng Zhu', 'Kai Yang', 'Xingzhong Xu', 'Xin Geng', 'Xu Yang'], 'affiliations': ['Ant Group', 'Fudan University', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07536.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark', '#small_models', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ 3B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-Instruct-3B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Boosting Reasoning in Multimodal Models Efficiently', 'desc': 'This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data.'}, 'zh': {'title': 'å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­å¢å¼ºæ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°é‡ä¸º3Bçš„ç´§å‡‘æ¶æ„ä¸­ï¼Œè§†è§‰æ„ŸçŸ¥ä¸é€»è¾‘æ¨ç†ä¹‹é—´çš„å¤æ‚äº’åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º\textit{method}çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡åŸºç¡€æ¨ç†å¢å¼ºï¼ˆFREï¼‰å’Œå¤šæ¨¡æ€æ³›åŒ–è®­ç»ƒï¼ˆMGTï¼‰æ¥é€‚åº”å¤šæ¨¡æ€æ¨ç†ã€‚FREé˜¶æ®µåˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ–‡æœ¬æ•°æ®çš„æ¨ç†èƒ½åŠ›ï¼ŒMGTé˜¶æ®µåˆ™å°†è¿™äº›æ¨ç†èƒ½åŠ›æ¨å¹¿åˆ°å¤šæ¨¡æ€é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{method}åœ¨å¤šæ¨¡æ€å’Œæ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”åŸºçº¿æé«˜äº†4.83%å’Œ4.5%ï¼Œåœ¨å¤æ‚çš„è¶³çƒæ¯”èµ›ä»»åŠ¡ä¸­æé«˜äº†3.63%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08638', 'title': 'YuE: Scaling Open Foundation Models for Long-Form Music Generation', 'url': 'https://huggingface.co/papers/2503.08638', 'abstract': "We tackle the task of long-form music generation--particularly the challenging lyrics-to-song problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation", 'score': 43, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'eb1539380bf435c9', 'authors': ['Ruibin Yuan', 'Hanfeng Lin', 'Shuyue Guo', 'Ge Zhang', 'Jiahao Pan', 'Yongyi Zang', 'Haohe Liu', 'Yiming Liang', 'Wenye Ma', 'Xingjian Du', 'Xinrun Du', 'Zhen Ye', 'Tianyu Zheng', 'Yinghao Ma', 'Minghao Liu', 'Zeyue Tian', 'Ziya Zhou', 'Liumeng Xue', 'Xingwei Qu', 'Yizhi Li', 'Shangda Wu', 'Tianhao Shen', 'Ziyang Ma', 'Jun Zhan', 'Chunhui Wang', 'Yatian Wang', 'Xiaowei Chi', 'Xinyue Zhang', 'Zhenzhu Yang', 'Xiangzhou Wang', 'Shansong Liu', 'Lingrui Mei', 'Peng Li', 'Junjie Wang', 'Jianwei Yu', 'Guojian Pang', 'Xu Li', 'Zihao Wang', 'Xiaohuan Zhou', 'Lijun Yu', 'Emmanouil Benetos', 'Yong Chen', 'Chenghua Lin', 'Xie Chen', 'Gus Xia', 'Zhaoxiang Zhang', 'Chao Zhang', 'Wenhu Chen', 'Xinyu Zhou', 'Xipeng Qiu', 'Roger Dannenberg', 'Jiaheng Liu', 'Jian Yang', 'Wenhao Huang', 'Wei Xue', 'Xu Tan', 'Yike Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.08638.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#benchmark', '#low_resource', '#open_source', '#story_generation', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'YuE: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'YuE - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ LLaMA2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑĞ½Ğ¸, Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸ Ñ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼. YuE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ° ĞµÑ‘ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸.'}, 'en': {'title': 'Transforming Lyrics into Melodies with YuE!', 'desc': 'This paper presents YuE, a new family of open foundation models designed for long-form music generation, specifically focusing on the lyrics-to-song challenge. YuE utilizes the LLaMA2 architecture and can generate up to five minutes of music while ensuring that the lyrics align with the musical structure and melodies. Key innovations include track-decoupled next-token prediction for better signal clarity, structural progressive conditioning for lyrical alignment, and a multitask pre-training approach to enhance generalization. The model also supports versatile style transfer and performs well on music understanding tasks, achieving results that rival existing proprietary systems.'}, 'zh': {'title': 'YuEï¼šæ­Œè¯è½¬æ­Œæ›²çš„éŸ³ä¹ç”Ÿæˆæ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºYuEçš„å¼€æ”¾åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºé•¿ç¯‡éŸ³ä¹ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯æ­Œè¯è½¬æ­Œæ›²çš„é—®é¢˜ã€‚YuEåŸºäºLLaMA2æ¶æ„ï¼Œèƒ½å¤Ÿç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼ŒåŒæ—¶ä¿æŒæ­Œè¯çš„å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ—‹å¾‹ã€‚é€šè¿‡é‡‡ç”¨è§£è€¦çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€ç»“æ„æ€§æ¸è¿›æ¡ä»¶å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒç­‰æŠ€æœ¯ï¼ŒYuEåœ¨éŸ³ä¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚ç»è¿‡è¯„ä¼°ï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§æ–¹é¢ä¸ä¸€äº›ä¸“æœ‰ç³»ç»Ÿç›¸åŒ¹æ•Œï¼Œç”šè‡³è¶…è¶Šäº†å®ƒä»¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05978', 'title': 'MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice', 'url': 'https://huggingface.co/papers/2503.05978', 'abstract': "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for <PRE_TAG>identity preservation</POST_TAG>, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.", 'score': 26, 'issue_id': 2658, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '2111564f7e67ca23', 'authors': ['Hongwei Yi', 'Tian Ye', 'Shitong Shao', 'Xuancheng Yang', 'Jiantong Zhao', 'Hanzhong Guo', 'Terrance Wang', 'Qingyu Yin', 'Zeke Xie', 'Lei Zhu', 'Wei Li', 'Michael Lingelbach', 'Daquan Zhou'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Hedra Inc.', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05978.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#inference', '#diffusion', '#video', '#open_source'], 'emoji': 'ğŸ­', 'ru': {'title': 'MagicInfinite: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'MagicInfinite - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. MagicInfinite Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Portrait Animation with MagicInfinite', 'desc': 'MagicInfinite is a cutting-edge diffusion Transformer framework designed to enhance portrait animation by producing high-quality results for various character types, including realistic humans and stylized anime. It introduces innovative techniques such as 3D full-attention mechanisms and a sliding window denoising strategy, allowing for infinite video generation while maintaining visual coherence. The framework employs a two-stage curriculum learning approach that integrates audio and text inputs for improved lip synchronization and expressive dynamics, along with region-specific masks for precise control over animations. With a significant boost in efficiency, MagicInfinite can generate high-resolution videos rapidly, outperforming existing models in key areas like audio-lip synchronization and identity preservation.'}, 'zh': {'title': 'é­”æ³•æ— é™ï¼šçªç ´è‚–åƒåŠ¨ç”»çš„ç•Œé™', 'desc': 'MagicInfiniteæ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿè‚–åƒåŠ¨ç”»çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤šç§è§’è‰²ç±»å‹ä¸­æä¾›é«˜ä¿çœŸåº¦çš„ç»“æœï¼ŒåŒ…æ‹¬çœŸå®äººç±»ã€å…¨èº«äººç‰©å’Œé£æ ¼åŒ–çš„åŠ¨æ¼«è§’è‰²ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§é¢éƒ¨å§¿åŠ¿çš„åŠ¨ç”»ï¼ŒåŒ…æ‹¬èƒŒé¢è§†å›¾ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡è¾“å…¥æ©ç ç²¾ç¡®æŒ‡å®šå¤šè§’è‰²åœºæ™¯ä¸­çš„å‘è¨€è€…ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰é¡¹åˆ›æ–°æ¥è§£å†³å…³é”®æŒ‘æˆ˜ï¼š3Då…¨æ³¨æ„åŠ›æœºåˆ¶ä¸æ»‘åŠ¨çª—å£å»å™ªç­–ç•¥ï¼Œæ”¯æŒæ— é™è§†é¢‘ç”Ÿæˆå¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ï¼›ä»¥åŠåŒºåŸŸç‰¹å®šæ©ç ä¸è‡ªé€‚åº”æŸå¤±å‡½æ•°çš„ç»“åˆï¼Œå¹³è¡¡å…¨å±€æ–‡æœ¬æ§åˆ¶å’Œå±€éƒ¨éŸ³é¢‘æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒMagicInfiniteåœ¨éŸ³é¢‘ä¸å”‡åŒæ­¥ã€èº«ä»½ä¿ç•™å’ŒåŠ¨ä½œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08120', 'title': 'UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2503.08120', 'abstract': "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.", 'score': 25, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7c6e7c685b283d61', 'authors': ['Junzhe Li', 'Xuerui Qiu', 'Linrui Xu', 'Liya Guo', 'Delin Qu', 'Tingting Long', 'Chun Fan', 'Ming Li'], 'affiliations': ['Central South University', 'Computer Center, Peking University', 'Fudan University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Peking University', 'Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08120.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#multimodal', '#architecture', '#diffusion'], 'emoji': 'ğŸ§‘', 'ru': {'title': 'UniF^2ace: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†', 'desc': 'UniF^2ace - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (UMM), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 130 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ… Ğ»Ğ¸Ñ†Ğ°. Ğ’ UniF^2ace Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ²Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UniF^2ace Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ UMM Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†.'}, 'en': {'title': 'UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation', 'desc': 'This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes.'}, 'zh': {'title': 'ç»†ç²’åº¦é¢éƒ¨ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢éƒ¨é¢†åŸŸç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç²—ç•¥çš„é¢éƒ¨å±æ€§ç†è§£ä¸Šï¼Œç¼ºä¹å¤„ç†ç»†ç²’åº¦é¢éƒ¨å±æ€§çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æœªèƒ½è§£å†³ç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†UniF^2aceï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç»†ç²’åº¦é¢éƒ¨ç†è§£å’Œç”Ÿæˆçš„UMMã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«13ä¸‡å¼ å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤ç§äº’è¡¥çš„æ‰©æ•£æŠ€æœ¯å’ŒåŒå±‚ä¸“å®¶æ··åˆæ¶æ„ï¼ŒUniF^2aceåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08625', 'title': 'SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories', 'url': 'https://huggingface.co/papers/2503.08625', 'abstract': "While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLM's text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the model's intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMs' visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs.", 'score': 22, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '081b91105002410a', 'authors': ['Muzhi Zhu', 'Yuzhuo Tian', 'Hao Chen', 'Chunluan Zhou', 'Qingpei Guo', 'Yang Liu', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08625.jpg', 'data': {'categories': ['#agents', '#cv', '#rl', '#reasoning', '#optimization', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ (HLMAT), Ğ³Ğ´Ğµ MLLM Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SegAgent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. HLMAT Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ MLLM Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Pixel-Level Understanding in MLLMs with Human-Like Annotation', 'desc': "This paper addresses the limitations of Multimodal Language Models (MLLMs) in understanding images at the pixel level, which restricts their practical use. It critiques existing evaluation methods for being too broad and introduces the Human-Like Mask Annotation Task (HLMAT) to improve pixel comprehension. HLMAT allows MLLMs to simulate human-like annotation through interactive segmentation, modeled as a multi-step Markov Decision Process. The proposed SegAgent model, trained on this new task, achieves competitive performance while maintaining the MLLM's language capabilities and flexibility."}, 'zh': {'title': 'æå‡åƒç´ ç†è§£çš„åˆ›æ–°æ ‡æ³¨ä»»åŠ¡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åƒç´ çº§ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦è¯„ä¼°ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”äººç±»æ ·æœ¬æ ‡æ³¨ä»»åŠ¡ï¼ˆHLMATï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æ ‡æ³¨è€…çš„æ–¹å¼ï¼Œä½¿ç”¨äº¤äº’å¼åˆ†å‰²å·¥å…·æ¥æé«˜æ¨¡å‹çš„åƒç´ ç†è§£èƒ½åŠ›ã€‚HLMATå°†åˆ†å‰²å»ºæ¨¡ä¸ºå¤šæ­¥éª¤çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œä½¿å¾—MLLMsèƒ½å¤Ÿè¿­ä»£ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç‚¹å‡»ç‚¹ï¼Œä»è€Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„æˆ–ä½¿ç”¨éšå¼æ ‡è®°ã€‚é€šè¿‡è¿™ä¸€æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†SegAgentæ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå¹¶æ”¯æŒæ©è†œç»†åŒ–å’Œæ ‡æ³¨è¿‡æ»¤ç­‰é¢å¤–ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07703', 'title': 'Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model', 'url': 'https://huggingface.co/papers/2503.07703', 'abstract': 'Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.', 'score': 22, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '00cd4369f3c531f9', 'authors': ['Lixue Gong', 'Xiaoxia Hou', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Fei Liu', 'Liyang Liu', 'Wei Liu', 'Wei Lu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Linjie Yang', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': ['Seed Vision Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.07703.jpg', 'data': {'categories': ['#cv', '#training', '#dataset', '#optimization', '#rlhf', '#alignment', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Seedream 2.0: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Seedream 2.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Glyph-Aligned ByT5 Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Scaled ROPE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT Ğ¸ RLHF, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Seedream 2.0: Bridging Cultures in Image Generation', 'desc': 'This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality.'}, 'zh': {'title': 'åŒè¯­å›¾åƒç”Ÿæˆçš„æœªæ¥ï¼šSeedream 2.0', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†Seedream 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­çš„å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“å’Œæ–‡åŒ–ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸­æ–‡å’Œè‹±æ–‡çš„æ–‡æœ¬æç¤ºï¼Œæ”¯æŒåŒè¯­å›¾åƒç”Ÿæˆï¼Œå¹¶é€šè¿‡å¼ºå¤§çš„æ•°æ®ç³»ç»Ÿå’Œæè¿°ç³»ç»Ÿæå‡å›¾åƒæè¿°çš„å‡†ç¡®æ€§å’Œä¸°å¯Œæ€§ã€‚Seedream 2.0ç»“åˆäº†è‡ªç ”çš„åŒè¯­å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æµ·é‡æ•°æ®ä¸­ç›´æ¥å­¦ä¹ æœ¬åœŸçŸ¥è¯†ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå‡†ç¡®è¡¨è¾¾æ–‡åŒ–ç»†èŠ‚å’Œç¾å­¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šé˜¶æ®µçš„åè®­ç»ƒä¼˜åŒ–ï¼ŒSeedream 2.0åœ¨å¤šä¸ªæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬éµå¾ªæç¤ºã€å®¡ç¾ã€æ–‡æœ¬æ¸²æŸ“å’Œç»“æ„æ­£ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07891', 'title': 'Gemini Embedding: Generalizable Embeddings from Gemini', 'url': 'https://huggingface.co/papers/2503.07891', 'abstract': "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.", 'score': 18, 'issue_id': 2655, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '149f8fc31808d626', 'authors': ['Jinhyuk Lee', 'Feiyang Chen', 'Sahil Dua', 'Daniel Cer', 'Madhuri Shanbhogue', 'Iftekhar Naim', 'Gustavo HernÃ¡ndez Ãbrego', 'Zhe Li', 'Kaifeng Chen', 'Henrique Schechter Vera', 'Xiaoqi Ren', 'Shanfeng Zhang', 'Daniel Salz', 'Michael Boratko', 'Jay Han', 'Blair Chen', 'Shuo Huang', 'Vikram Rao', 'Paul Suganthan', 'Feng Han', 'Andreas Doumanoglou', 'Nithi Gupta', 'Fedor Moiseev', 'Cathy Yip', 'Aashi Jain', 'Simon Baumgartner', 'Shahrokh Shahi', 'Frank Palma Gomez', 'Sandeep Mariserla', 'Min Choi', 'Parashar Shah', 'Sonam Goenka', 'Ke Chen', 'Ye Xia', 'Koert Chen', 'Sai Meher Karthik Duddu', 'Yichang Chen', 'Trevor Walker', 'Wenlei Zhou', 'Rakesh Ghiya', 'Zach Gleicher', 'Karan Gill', 'Zhe Dong', 'Mojtaba Seyedhosseini', 'Yunhsuan Sung', 'Raphael Hoffmann', 'Tom Duerig'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2503.07891.jpg', 'data': {'categories': ['#multimodal', '#multilingual', '#dataset', '#transfer_learning', '#benchmark', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Gemini Embedding: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Gemini Embedding - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini Ğ¾Ñ‚ Google. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Gemini Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Gemini Embedding Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMTEB, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 250+ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unifying Multilingual Understanding with Gemini Embedding', 'desc': "Gemini Embedding is a cutting-edge embedding model that utilizes Google's advanced Gemini large language model. It excels in generating versatile embeddings for text in multiple languages and formats, thanks to its multilingual and code comprehension abilities. These embeddings can be precomputed and effectively used in various machine learning tasks such as classification, similarity, clustering, ranking, and retrieval. In evaluations on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding significantly outperformed previous models, showcasing its superior embedding quality across a wide range of languages and tasks."}, 'zh': {'title': 'Gemini Embeddingï¼šå¤šè¯­è¨€åµŒå…¥çš„æ–°æ ‡æ†', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†Gemini Embeddingï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„åµŒå…¥æ¨¡å‹ï¼Œåˆ©ç”¨äº†è°·æ­Œæœ€å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹Geminiçš„èƒ½åŠ›ã€‚Gemini Embeddingèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€šç”¨çš„æ–‡æœ¬åµŒå…¥ï¼Œé€‚ç”¨äºå¤šç§è¯­è¨€å’Œæ–‡æœ¬å½¢å¼ï¼Œå……åˆ†åˆ©ç”¨äº†Geminiçš„å¤šè¯­è¨€å’Œä»£ç ç†è§£èƒ½åŠ›ã€‚ç”Ÿæˆçš„è¡¨ç¤ºå¯ä»¥é¢„å…ˆè®¡ç®—ï¼Œå¹¶åº”ç”¨äºåˆ†ç±»ã€ç›¸ä¼¼æ€§ã€èšç±»ã€æ’åºå’Œæ£€ç´¢ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMMTEBï¼‰ä¸­ï¼ŒGemini Embeddingæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†åµŒå…¥è´¨é‡çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07860', 'title': 'Video Action Differencing', 'url': 'https://huggingface.co/papers/2503.07860', 'abstract': 'How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.', 'score': 17, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '76b8b9c677de83cd', 'authors': ['James Burgess', 'Xiaohan Wang', 'Yuhui Zhang', 'Anita Rau', 'Alejandro Lozano', 'Lisa Dunlap', 'Trevor Darrell', 'Serena Yeung-Levy'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.07860.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#dataset', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Video Action Differencing (VidDiff), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VidDiffBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 549 Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ VidDiff, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling Subtle Differences in Action Videos with VidDiff', 'desc': 'This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance.'}, 'zh': {'title': 'è§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼šè¯†åˆ«ç»†å¾®å·®åˆ«çš„æ–°æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼ˆVidDiffï¼‰ï¼Œæ—¨åœ¨è¯†åˆ«åŒä¸€åŠ¨ä½œè§†é¢‘ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†VidDiffBenchï¼Œä¸€ä¸ªåŒ…å«549å¯¹è§†é¢‘çš„åŸºå‡†æ•°æ®é›†ï¼Œæ ‡æ³¨äº†4469ä¸ªç»†ç²’åº¦åŠ¨ä½œå·®å¼‚å’Œ2075ä¸ªæ—¶é—´æˆ³ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVidDiffBenchå¯¹ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å±€éƒ¨åŒ–ç›¸å…³å­åŠ¨ä½œå’Œç»†ç²’åº¦å¸§æ¯”è¾ƒæ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VidDiffæ–¹æ³•ï¼Œå°†ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåŠ¨ä½œå·®å¼‚æè®®ã€å…³é”®å¸§å®šä½å’Œå¸§å·®å¼‚åŒ–ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½åˆ©ç”¨äº†ä¸“é—¨çš„åŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07604', 'title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'url': 'https://huggingface.co/papers/2503.07604', 'abstract': "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.", 'score': 17, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '313594788f663498', 'authors': ['Tianhe Lin', 'Jian Xie', 'Siyu Yuan', 'Deqing Yang'], 'affiliations': ['School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07604.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#math', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞµÑĞ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: ÑĞ¸Ğ»Ğ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµÑ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑÑ…Ğ¾Ğ¶ĞµĞ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°, Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Implicit Reasoning in Language Models', 'desc': 'This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities.'}, 'zh': {'title': 'éšå¼æ¨ç†çš„æ·å¾„ä¸å±€é™æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨æµ‹è¯•æ—¶è®¡ç®—ä¸­ï¼Œéšå¼æ¨ç†ä¸æ˜¾å¼æ¨ç†çš„æ•ˆç‡å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡éšå¼æ¨ç†å®ç°é€æ­¥æ¨ç†å¹¶åœ¨å¤šæ­¥ä»»åŠ¡ä¸­å–å¾—é«˜å‡†ç¡®ç‡ã€‚ç›¸åï¼Œåœ¨éå›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„éšå¼æ¨ç†èƒ½åŠ›å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æ€»çš„æ¥è¯´ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹ä¸åŒæ¨¡å¼æ—¶è¡¨ç°ä¸ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'score': 17, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '49ca445bc3322f0d', 'authors': ['Yuxiao Qu', 'Matthew Y. R. Yang', 'Amrith Setlur', 'Lewis Tunstall', 'Edward Emanuel Beeching', 'Ruslan Salakhutdinov', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2503.07572.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Meta Reinforcement Fine-Tuning (MRT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° 'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ' Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. MRT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Optimizing Test-Time Compute for Enhanced LLM Reasoning', 'desc': "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."}, 'zh': {'title': 'ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—çš„é—®é¢˜å½¢å¼åŒ–ä¸ºå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è§†è§’æ¥æ”¯é…æµ‹è¯•æ—¶è®¡ç®—ã€‚é€šè¿‡å°†LLMçš„è¾“å‡ºæµè§†ä¸ºå¤šä¸ªæµ‹è¯•æ—¶çš„å›åˆï¼Œå¹¶ä½¿ç”¨ç´¯ç§¯é—æ†¾çš„æ¦‚å¿µæ¥è¡¡é‡æµ‹è¯•æ—¶è®¡ç®—çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºå…ƒå¼ºåŒ–å¾®è°ƒï¼ˆMRTï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿçš„ç»“æœå¥–åŠ±RLæ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†2-3å€ï¼Œä»¤ä»¤ç‰Œæ•ˆç‡æé«˜äº†çº¦1.5å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08619', 'title': 'LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2503.08619', 'abstract': 'Recent advances in text-to-image generation have primarily relied on extensive datasets and parameter-heavy architectures. These requirements severely limit accessibility for researchers and practitioners who lack substantial computational resources. In this paper, we introduce \\model, an efficient training paradigm for image generation models that uses knowledge distillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration from the success of data KD techniques widely adopted in Multi-Modal Large Language Models (MLLMs), LightGen distills knowledge from state-of-the-art (SOTA) text-to-image models into a compact Masked Autoregressive (MAR) architecture with only 0.7B parameters. Using a compact <PRE_TAG>synthetic dataset</POST_TAG> of just 2M high-quality images generated from varied captions, we demonstrate that data diversity significantly outweighs data volume in determining model performance. This strategy dramatically reduces computational demands and reduces pre-training time from potentially thousands of GPU-days to merely 88 GPU-days. Furthermore, to address the inherent shortcomings of synthetic data, particularly poor high-frequency details and spatial inaccuracies, we integrate the DPO technique that refines image fidelity and positional accuracy. Comprehensive experiments confirm that LightGen achieves image generation quality comparable to SOTA models while significantly reducing computational resources and expanding accessibility for resource-constrained environments. Code is available at https://github.com/XianfengWu01/LightGen', 'score': 15, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '1645a4236e6d91b6', 'authors': ['Xianfeng Wu', 'Yajing Bai', 'Haoze Zheng', 'Harold Haodong Chen', 'Yexin Liu', 'Zihao Wang', 'Xuran Ma', 'Wen-Jie Shu', 'Xianzu Wu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'The Hong Kong University of Science and Technology', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.08619.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#architecture', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LightGen - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Direct Preference Optimization. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ² 0.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 88 GPU-Ğ´Ğ½ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LightGen Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': 'Efficient Image Generation with LightGen: Less is More!', 'desc': 'This paper presents LightGen, a new approach to text-to-image generation that focuses on efficiency and accessibility. It utilizes knowledge distillation (KD) and Direct Preference Optimization (DPO) to create a compact model with only 0.7 billion parameters, making it less demanding on computational resources. By training on a small but diverse synthetic dataset of 2 million images, the authors show that the variety of data is more important than sheer volume for model performance. The results indicate that LightGen can produce high-quality images comparable to state-of-the-art models while significantly reducing training time and resource requirements.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒç”Ÿæˆï¼Œèµ„æºå‹å¥½ï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLightGençš„é«˜æ•ˆå›¾åƒç”Ÿæˆæ¨¡å‹è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚è¯¥æ–¹æ³•ä»æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œæ„å»ºäº†ä¸€ä¸ªä»…æœ‰0.7äº¿å‚æ•°çš„ç´§å‡‘å‹è‡ªå›å½’æ¶æ„ã€‚é€šè¿‡ä½¿ç”¨ä»…200ä¸‡å¼ é«˜è´¨é‡åˆæˆå›¾åƒçš„æ•°æ®é›†ï¼Œç ”ç©¶è¡¨æ˜æ•°æ®çš„å¤šæ ·æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“è¿œå¤§äºæ•°æ®çš„æ•°é‡ã€‚LightGenæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œå°†é¢„è®­ç»ƒæ—¶é—´ä»æ•°åƒä¸ªGPUå¤©ç¼©çŸ­è‡³ä»…88ä¸ªGPUå¤©ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08605', 'title': 'Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling', 'url': 'https://huggingface.co/papers/2503.08605', 'abstract': 'While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.', 'score': 15, 'issue_id': 2653, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '6f7bf7b6c171af43', 'authors': ['Subin Kim', 'Seoung Wug Oh', 'Jui-Hsien Wang', 'Joon-Young Lee', 'Jinwoo Shin'], 'affiliations': ['Adobe Research', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.08605.jpg', 'data': {'categories': ['#inference', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'SynCoS: Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Synchronized Coupled Sampling (SynCoS). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. SynCoS ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑˆÑƒĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SynCoS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Achieving Long-Range Coherence in Video Generation with SynCoS', 'desc': 'This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency.'}, 'zh': {'title': 'åŒæ­¥è€¦åˆé‡‡æ ·ï¼šæå‡é•¿è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œç§°ä¸ºåŒæ­¥è€¦åˆé‡‡æ ·ï¼ˆSynCoSï¼‰ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚é€šè¿‡åŒæ­¥å»å™ªè·¯å¾„ï¼ŒSynCoSç¡®ä¿äº†ç›¸é‚»å¸§å’Œè¿œç¨‹å¸§ä¹‹é—´çš„é•¿èŒƒå›´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åå‘é‡‡æ ·å’ŒåŸºäºä¼˜åŒ–çš„é‡‡æ ·ç­–ç•¥ï¼Œä»¥å®ç°å±€éƒ¨å¹³æ»‘è¿‡æ¸¡å’Œå…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynCoSåœ¨å¤šäº‹ä»¶é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¹³æ»‘çš„è¿‡æ¸¡å’Œæ›´å¥½çš„é•¿èŒƒå›´è¯­ä¹‰ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08686', 'title': 'OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models', 'url': 'https://huggingface.co/papers/2503.08686', 'abstract': "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba", 'score': 13, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7bb3cd796d69dc2c', 'authors': ['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08686.jpg', 'data': {'categories': ['#training', '#inference', '#multimodal', '#architecture', '#open_source', '#optimization'], 'emoji': 'ğŸ¦‹', 'ru': {'title': 'OmniMamba: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹', 'desc': 'OmniMamba - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Mamba-2, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ĞµĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ LoRA Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². OmniMamba Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² 1000 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'OmniMamba: Efficient Multimodal Generation with Linear Architecture', 'desc': 'OmniMamba is a groundbreaking multimodal generation model that efficiently creates both text and images using a linear architecture. It introduces a unified next-token prediction approach, which significantly reduces computational complexity and memory usage compared to traditional models. The model employs innovative techniques like decoupled vocabularies for specific modality guidance and task-specific LoRA for efficient parameter adaptation. With a unique two-stage training strategy, OmniMamba achieves impressive performance on benchmarks while requiring far less training data, demonstrating remarkable speed and memory efficiency during inference.'}, 'zh': {'title': 'OmniMambaï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆæ–°é€‰æ‹©', 'desc': 'OmniMambaæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨çº¿æ€§æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼ï¼Œå……åˆ†åˆ©ç”¨äº†Mamba-2çš„é«˜æ•ˆè®¡ç®—å’Œå†…å­˜æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼ŒOmniMambaå¼•å…¥äº†è§£è€¦è¯æ±‡å’Œä»»åŠ¡ç‰¹å®šçš„LoRAæŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æ¥è§£å†³ä»»åŠ¡é—´çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚æœ€ç»ˆï¼ŒOmniMambaåœ¨ç”Ÿæˆæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„Transformeræ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†119.2å€ï¼ŒGPUå†…å­˜å‡å°‘äº†63%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06940', 'title': 'CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing', 'url': 'https://huggingface.co/papers/2503.06940', 'abstract': "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain.", 'score': 11, 'issue_id': 2664, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '10b773e3c142acbe', 'authors': ['Jianxiong Gao', 'Yichang Liu', 'Baofeng Yang', 'Jianfeng Feng', 'Yanwei Fu'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06940.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#diffusion', '#dataset', '#video', '#audio'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºÑƒ', 'desc': 'CineBrain - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ­Ğ­Ğ“ Ğ¸ Ñ„ĞœĞ Ğ¢ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CineSync - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. CineSync ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ­Ğ­Ğ“ Ğ¸ Ñ„ĞœĞ Ğ¢, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CineSync Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'CineBrain: Uniting EEG and fMRI for Enhanced Audiovisual Reconstruction', 'desc': 'This paper presents CineBrain, a groundbreaking dataset that combines EEG and fMRI recordings while participants watch dynamic audiovisual content. By leveraging the fast response time of EEG and the detailed spatial information from fMRI, CineBrain offers a rich resource for studying brain activity during complex stimuli. The authors introduce CineSync, a novel framework that merges these two modalities to enhance the reconstruction of audiovisual experiences. Their results show that CineSync outperforms previous methods in reconstructing video and audio, marking a significant advancement in multimodal brain signal analysis.'}, 'zh': {'title': 'CineBrainï¼šå¤šæ¨¡æ€è§£ç çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CineBrainï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«åœ¨åŠ¨æ€è§†å¬åˆºæ¿€ä¸‹åŒæ—¶è®°å½•çš„EEGå’ŒfMRIæ•°æ®ã€‚CineBrainåˆ©ç”¨EEGçš„é«˜æ—¶é—´åˆ†è¾¨ç‡å’ŒfMRIçš„æ·±è„‘ç©ºé—´è¦†ç›–ä¼˜åŠ¿ï¼Œä¸ºå…­åå‚ä¸è€…æä¾›äº†çº¦å…­å°æ—¶çš„ã€Šç”Ÿæ´»å¤§çˆ†ç‚¸ã€‹å™äº‹å†…å®¹ã€‚åŸºäºè¿™ä¸€ç‹¬ç‰¹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†CineSyncï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡æ€è§£ç æ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€èåˆç¼–ç å™¨å’ŒåŸºäºæ‰©æ•£çš„ç¥ç»æ½œåœ¨è§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCineSyncåœ¨è§†é¢‘é‡å»ºæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒæˆåŠŸç»“åˆäº†fMRIå’ŒEEGæ¥é‡å»ºè§†é¢‘å’ŒéŸ³é¢‘åˆºæ¿€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution\n  Autonomous Driving VQA from Peru', 'url': 'https://huggingface.co/papers/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'score': 9, 'issue_id': 2667, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '274531da9b4c33d1', 'authors': ['Dunant Cusipuma', 'David Ortega', 'Victor Flores-Benites', 'Arturo Deza'], 'affiliations': ['Artificio', 'Universidad de Ingeneria Tecnologia (UTEC)'], 'pdf_title_img': 'assets/pdf/title_img/2503.07587.jpg', 'data': {'categories': ['#video', '#alignment', '#interpretability', '#multimodal', '#dataset'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº vs Ğ˜Ğ˜: ĞºÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğµ?', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞµ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ… Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Robusto-1 Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ĞŸĞµÑ€Ñƒ, ÑÑ‚Ñ€Ğ°Ğ½Ğµ Ñ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° (RSA), Ğ¾Ğ½Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Assessing Human-Like Responses in Self-Driving AI', 'desc': 'This paper investigates how well multimodal foundational models, specifically Visual Language Models (VLMs), perform in driving situations compared to humans, particularly in challenging environments like Peru. The authors introduce the Robusto-1 dataset, which includes dashcam footage from a region known for aggressive driving and unusual street objects. They employ a method called Visual Question Answering (VQA) and Representational Similarity Analysis (RSA) to assess cognitive alignment between humans and VLMs. The findings reveal that the alignment between human responses and VLMs varies significantly based on the types of questions posed, indicating a notable gap in their cognitive processing.'}, 'zh': {'title': 'æ¢ç´¢è‡ªåŠ¨é©¾é©¶ä¸­çš„äººæœºè®¤çŸ¥å·®è·', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬åˆ›å»ºäº†Robusto-1æ•°æ®é›†ï¼Œä½¿ç”¨æ¥è‡ªç§˜é²çš„è¡Œè½¦è®°å½•ä»ªè§†é¢‘æ•°æ®ï¼Œä»¥æµ‹è¯•åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸äººç±»åœ¨é©¾é©¶ä¸­çš„è®¤çŸ¥æ°´å¹³ã€‚é€šè¿‡å¤šæ¨¡æ€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ–¹æ³•ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†äººç±»å’Œæœºå™¨çš„ååº”ï¼Œå¹¶ä½¿ç”¨è¡¨å¾ç›¸ä¼¼æ€§åˆ†æï¼ˆRSAï¼‰æ¥è¯„ä¼°å®ƒä»¬çš„è®¤çŸ¥ä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒç±»å‹çš„é—®é¢˜ä¼šå¯¼è‡´VLMså’Œäººç±»åœ¨ååº”ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„è®¤çŸ¥å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08307', 'title': '^RFLAV: Rolling Flow matching for infinite Audio Video generation', 'url': 'https://huggingface.co/papers/2503.08307', 'abstract': 'Joint audio-video (AV) generation is still a significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present , a novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that  outperforms existing state-of-the-art models in multimodal AV generation tasks. Our code and checkpoints are available at https://github.com/ErgastiAlex/R-FLAV.', 'score': 7, 'issue_id': 2660, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '5b49ac0313e68c7e', 'authors': ['Alex Ergasti', 'Giuseppe Gabriele Tarollo', 'Filippo Botti', 'Tomaso Fontanini', 'Claudio Ferrari', 'Massimo Bertozzi', 'Andrea Prati'], 'affiliations': ['University of Parma, Department of Engineering and Architecture. Parma, Italy', 'University of Siena, Department of Information engineering and mathematics. Siena, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.08307.jpg', 'data': {'categories': ['#video', '#architecture', '#multimodal', '#audio'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ , Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ AV-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾  Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ SOTA-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ AV-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Audio-Video Generation with Efficient Alignment', 'desc': 'This paper addresses the challenges of joint audio-video generation in generative AI, focusing on quality, synchronization, and temporal coherence. The authors introduce a novel transformer-based architecture that effectively aligns audio and visual data. They explore three cross modality interaction modules, highlighting a lightweight temporal fusion module as the most efficient solution. Experimental results show that their approach surpasses existing state-of-the-art models in multimodal AV generation tasks.'}, 'zh': {'title': 'çªç ´éŸ³é¢‘-è§†é¢‘ç”Ÿæˆçš„å…³é”®æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è”åˆéŸ³é¢‘-è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€å¤šæ¨¡æ€çš„æ— ç¼åŒæ­¥å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å…³é”®é—®é¢˜ã€‚ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§ä¸åŒçš„è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œå…¶ä¸­è½»é‡çº§çš„æ—¶é—´èåˆæ¨¡å—è¢«è¯æ˜æ˜¯å¯¹é½éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„æœ€æœ‰æ•ˆå’Œè®¡ç®—é«˜æ•ˆçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€éŸ³é¢‘-è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08588', 'title': 'BiasEdit: Debiasing Stereotyped Language Models via Model Editing', 'url': 'https://huggingface.co/papers/2503.08588', 'abstract': "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.", 'score': 6, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '183584887772b6e7', 'authors': ['Xin Xu', 'Wei Xu', 'Ningyu Zhang', 'Julian McAuley'], 'affiliations': ['Georgia Institute of Technology', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08588.jpg', 'data': {'categories': ['#hallucinations', '#ethics', '#architecture', '#data', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BiasEdit Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ°, BiasEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BiasEdit Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'BiasEdit: Efficiently Editing Bias in Language Models', 'desc': "This paper introduces BiasEdit, a novel method designed to reduce stereotypical biases in language models. Unlike traditional debiasing techniques that often fail to effectively alter biased representations, BiasEdit utilizes lightweight networks to make targeted updates to the model's parameters. It incorporates a debiasing loss to guide these edits while ensuring that the model's language processing abilities remain intact through a retention loss. The results show that BiasEdit is not only effective in reducing bias but also maintains the overall performance of the language models across various tasks."}, 'zh': {'title': 'é«˜æ•ˆå»åè§ï¼Œæå‡è¯­è¨€æ¨¡å‹å…¬æ­£æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBiasEditçš„æ¨¡å‹ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è½»é‡çº§ç½‘ç»œå»é™¤è¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿åè§ã€‚ä¸ä¼ ç»Ÿçš„å»åè§ç­–ç•¥ç›¸æ¯”ï¼ŒBiasEditèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå±€éƒ¨å‚æ•°ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å»åè§æŸå¤±æŒ‡å¯¼ç¼–è¾‘ç½‘ç»œè¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é€šè¿‡ä¿ç•™æŸå¤±ç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ä¸å—å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBiasEditåœ¨æ¶ˆé™¤åè§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”å¯¹è¯­è¨€æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›å½±å“è¾ƒå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08685', 'title': '"Principal Components" Enable A New Language of Images', 'url': 'https://huggingface.co/papers/2503.08685', 'abstract': 'We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.', 'score': 5, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'a013cfdc2d1e9d7c', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Ismail Elezi', 'Jiankang Deng', 'Xiaojuan Qi'], 'affiliations': ['Imperial College London', 'Noahs Ark Lab', 'University of Edinburgh', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08685.jpg', 'data': {'categories': ['#training', '#cv', '#interpretability', '#diffusion', '#architecture', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ĞŸĞš-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼ÑƒÑ PCA-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ½ĞµĞ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒĞ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ²ÑĞ·Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Visual Tokenization with Structured Latent Spaces', 'desc': "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."}, 'zh': {'title': 'åˆ›æ–°è§†è§‰æ ‡è®°åŒ–ï¼Œæå‡å¯è§£é‡Šæ€§ä¸æ€§èƒ½', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æ ‡è®°åŒ–æ¡†æ¶ï¼Œå°†å¯è¯æ˜çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç»“æ„åµŒå…¥æ½œåœ¨æ ‡è®°ç©ºé—´ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°å™¨ä¸»è¦ä¼˜åŒ–é‡å»ºç²¾åº¦ï¼Œä½†å¾€å¾€å¿½è§†æ½œåœ¨ç©ºé—´çš„ç»“æ„ç‰¹æ€§ï¼Œè¿™å¯¹å¯è§£é‡Šæ€§å’Œä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå›¾åƒç”Ÿæˆä¸€ç»´å› æœæ ‡è®°åºåˆ—ï¼Œæ¯ä¸ªåç»­æ ‡è®°æä¾›ä¸é‡å çš„ä¿¡æ¯ï¼Œå¹¶ä¸”å…·æœ‰æ•°å­¦ä¸Šä¿è¯çš„é€’å‡è§£é‡Šæ–¹å·®ï¼Œç±»ä¼¼äºä¸»æˆåˆ†åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æé«˜äº†ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„å¯¹é½å¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08684', 'title': 'Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents', 'url': 'https://huggingface.co/papers/2503.08684', 'abstract': 'Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap.', 'score': 5, 'issue_id': 2663, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '13216e7922903487', 'authors': ['Haoyu Wang', 'Sunhao Dai', 'Haiyuan Zhao', 'Liang Pang', 'Xiao Zhang', 'Gang Wang', 'Zhenhua Dong', 'Jun Xu', 'Ji-Rong Wen'], 'affiliations': ['CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'Huawei Noahs Ark Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08684.jpg', 'data': {'categories': ['#ethics', '#data', '#interpretability', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº Ğ˜Ğ˜-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (PLM) Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PLM-Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸ĞµĞ¹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Causal Diagnosis and Correction (CDC).'}, 'en': {'title': 'Unraveling Source Bias in PLM Retrieval Models', 'desc': 'This paper investigates the issue of source bias in PLM-based retrieval models, where documents generated by large language models (LLMs) are favored over human-written content. The authors use a causal graph to explain how these models learn to estimate relevance based on perplexity features, leading to a preference for documents with lower perplexity scores. They identify a correlation between the gradients of loss functions in language modeling and retrieval tasks as a root cause of this bias. To address this, they propose a new method called Causal Diagnosis and Correction (CDC), which effectively debiases the relevance scores by isolating the bias introduced by perplexity.'}, 'zh': {'title': 'æ­ç¤ºæºåå·®ï¼Œæå‡ä¿¡æ¯æ£€ç´¢å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„æ£€ç´¢æ¨¡å‹ä¸­å­˜åœ¨çš„æºåå·®ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§åå·®ä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„å†…å®¹åˆ†é…æ›´é«˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼Œå³ä½¿è¿™äº›å†…å®¹çš„è¯­ä¹‰è´¨é‡ä¸äººç±»æ’°å†™çš„å†…å®¹ç›¸å½“ã€‚é€šè¿‡å› æœå›¾åˆ†æä¿¡æ¯æ£€ç´¢è¿‡ç¨‹ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç›¸å…³æ€§ä¼°è®¡ä¸­å­¦ä¹ åˆ°çš„å›°æƒ‘åº¦ç‰¹å¾å¯¼è‡´äº†æºåå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå› æœè¯Šæ–­ä¸ä¿®æ­£ï¼ˆCDCï¼‰çš„å»åæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»å›°æƒ‘åº¦çš„åå·®æ•ˆåº”ï¼Œæå‡æ£€ç´¢æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08644', 'title': 'Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval', 'url': 'https://huggingface.co/papers/2503.08644', 'abstract': 'Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.', 'score': 5, 'issue_id': 2666, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '4526f13d2d98134f', 'authors': ['Parishad BehnamGhader', 'Nicholas Meade', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.08644.jpg', 'data': {'categories': ['#multimodal', '#security', '#rag', '#ethics'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ Ğ² ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½ Ñ€Ğ¸ÑĞº, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ². Ğ”Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Uncovering the Risks of Powerful Retrievers in Malicious Queries', 'desc': 'This paper investigates the safety risks of instruction-following retrievers used with large language models (LLMs) in real-world applications. The authors analyze six prominent retrievers, revealing that they can often retrieve harmful content in response to malicious queries, with LLM2Vec achieving a 61.35% success rate. Additionally, the study highlights a concerning trend where instruction-following capabilities can inadvertently expose users to dangerous information. The findings emphasize the need for caution as the capabilities of retrievers grow, as even safety-aligned LLMs can inadvertently assist in fulfilling harmful requests.'}, 'zh': {'title': 'æ£€ç´¢å™¨èƒ½åŠ›æå‡å¸¦æ¥çš„å®‰å…¨éšæ‚£', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æŒ‡ä»¤è·Ÿéšæ£€ç´¢å™¨åœ¨å¤„ç†æ¶æ„æŸ¥è¯¢æ—¶çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬åˆ†æäº†å…­ç§é¢†å…ˆçš„æ£€ç´¢å™¨ï¼ŒåŒ…æ‹¬NV-Embedå’ŒLLM2Vecï¼Œå‘ç°å¤§å¤šæ•°æ£€ç´¢å™¨åœ¨è¶…è¿‡50%çš„æ¶æ„æŸ¥è¯¢ä¸­èƒ½å¤Ÿé€‰æ‹©ç›¸å…³çš„æœ‰å®³å†…å®¹ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLM2Vecåœ¨61.35%çš„æ¶æ„æŸ¥è¯¢ä¸­æ­£ç¡®é€‰æ‹©äº†æœ‰å®³æ®µè½ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯å®‰å…¨å¯¹é½çš„LLMï¼Œå¦‚Llama3ï¼Œåœ¨æä¾›æœ‰å®³æ£€ç´¢æ®µè½çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ»¡è¶³æ¶æ„è¯·æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07699', 'title': 'RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories', 'url': 'https://huggingface.co/papers/2503.07699', 'abstract': "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.", 'score': 5, 'issue_id': 2656, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '49ebba2cb63d91f8', 'authors': ['Huiyang Shao', 'Xin Xia', 'Yuhong Yang', 'Yuxi Ren', 'Xing Wang', 'Xuefeng Xiao'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.07699.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'RayFlow: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'RayFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑÑĞ¼Ğ¿Ğ» Ğ¿Ğ¾ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. RayFlow Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Time Sampler Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RayFlow Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RayFlow: Accelerating Diffusion Models Without Compromise', 'desc': 'This paper introduces RayFlow, a new diffusion model framework designed to improve the speed of image generation without sacrificing quality. Unlike traditional methods that often reduce the number of steps at the cost of sample diversity or stability, RayFlow customizes the sampling path for each instance, ensuring a more efficient and effective generation process. Additionally, the authors present Time Sampler, an importance sampling technique that optimizes training by prioritizing key timesteps. Experimental results show that RayFlow outperforms existing methods in generating high-quality images more quickly and with better control.'}, 'zh': {'title': 'RayFlowï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•è™½ç„¶æ—¨åœ¨å‡å°‘æ­¥éª¤ï¼Œä½†å¾€å¾€ä¼šå½±å“æ ·æœ¬è´¨é‡ã€å¯æ§æ€§æˆ–å¢åŠ è®­ç»ƒå¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RayFlowï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿè§£å†³è¿™äº›é™åˆ¶ã€‚RayFlowé€šè¿‡å¼•å¯¼æ¯ä¸ªæ ·æœ¬æ²¿ç€ç‹¬ç‰¹è·¯å¾„æœå‘ç‰¹å®šç›®æ ‡åˆ†å¸ƒï¼Œæœ€å°åŒ–é‡‡æ ·æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05860', 'title': 'Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol', 'url': 'https://huggingface.co/papers/2503.05860', 'abstract': "Benchmarks are essential for consistent evaluation and reproducibility. The integration of Artificial Intelligence into Software Engineering (AI4SE) has given rise to numerous benchmarks for tasks such as code generation and bug fixing. However, this surge presents challenges: (1) scattered benchmark knowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3) the absence of a uniform standard for benchmark development, and (4) limitations of existing benchmarks. In this paper, we review 173 studies and identify 204 AI4SE benchmarks. We classify these benchmarks, analyze their limitations, and expose gaps in practices. Based on our review, we created BenchScout, a semantic search tool to find relevant benchmarks, using automated clustering of the contexts from associated studies. We conducted a user study with 22 participants to evaluate BenchScout's usability, effectiveness, and intuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5. To advance benchmarking standards, we propose BenchFrame, a unified method to enhance benchmark quality. As a case study, we applied BenchFrame to the HumanEval benchmark and addressed its main limitations. This led to <PRE_TAG>HumanEvalNext</POST_TAG>, featuring (1) corrected errors, (2) improved language conversion, (3) expanded test coverage, and (4) increased difficulty. We then evaluated ten state-of-the-art code language models on HumanEval, HumanEvalPlus, and <PRE_TAG>HumanEvalNext</POST_TAG>. On <PRE_TAG>HumanEvalNext</POST_TAG>, models showed a pass@1 score reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus, respectively.", 'score': 5, 'issue_id': 2661, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'dee962dca6de084b', 'authors': ['Roham Koohestani', 'Philippe de Bekker', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.05860.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#survey'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² AI4SE: Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğº Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ (AI4SE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ»Ğ¸ 173 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ 204 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° AI4SE, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° BenchScout Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ BenchFrame Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºÑƒ HumanEval, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ HumanEvalNext.'}, 'en': {'title': 'Enhancing AI4SE Benchmarking with BenchScout and BenchFrame', 'desc': 'This paper addresses the challenges of evaluating benchmarks in the integration of Artificial Intelligence into Software Engineering (AI4SE). It reviews 173 studies to identify and classify 204 benchmarks, highlighting their limitations and gaps in current practices. The authors introduce BenchScout, a semantic search tool that helps users find relevant benchmarks through automated clustering. Additionally, they propose BenchFrame, a unified method to improve benchmark quality, demonstrated through the enhancement of the HumanEval benchmark into HumanEvalNext, which features significant improvements in error correction and test coverage.'}, 'zh': {'title': 'æå‡åŸºå‡†æµ‹è¯•è´¨é‡çš„ç»Ÿä¸€æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åŸºå‡†æµ‹è¯•ï¼ˆAI4SEï¼‰çš„é‡è¦æ€§ï¼Œåˆ†æäº†173é¡¹ç ”ç©¶å¹¶è¯†åˆ«å‡º204ä¸ªåŸºå‡†ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰åŸºå‡†å­˜åœ¨çŸ¥è¯†åˆ†æ•£ã€é€‰æ‹©å›°éš¾ã€ç¼ºä¹ç»Ÿä¸€æ ‡å‡†å’Œå±€é™æ€§ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†BenchScoutï¼Œä¸€ä¸ªè¯­ä¹‰æœç´¢å·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°ç›¸å…³åŸºå‡†ï¼Œå¹¶æå‡ºäº†BenchFrameï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ–¹æ³•æ¥æå‡åŸºå‡†è´¨é‡ã€‚é€šè¿‡å¯¹HumanEvalåŸºå‡†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†HumanEvalNextï¼Œä¿®æ­£äº†é”™è¯¯ã€æ”¹å–„äº†è¯­è¨€è½¬æ¢ã€æ‰©å±•äº†æµ‹è¯•è¦†ç›–ç‡å¹¶å¢åŠ äº†éš¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08689', 'title': 'QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension', 'url': 'https://huggingface.co/papers/2503.08689', 'abstract': 'Recent advances in long video understanding typically mitigate visual redundancy through visual token pruning based on attention distribution. However, while existing methods employ post-hoc low-response token pruning in decoder layers, they overlook the input-level semantic correlation between visual tokens and instructions (query). In this paper, we propose QuoTA, an ante-hoc training-free modular that extends existing large video-language models (LVLMs) for visual token assignment based on query-oriented frame-level importance assessment. The query-oriented token selection is crucial as it aligns visual processing with task-specific requirements, optimizing token budget utilization while preserving semantically relevant content. Specifically, (i) QuoTA strategically allocates frame-level importance scores based on query relevance, enabling one-time visual token assignment before cross-modal interactions in decoder layers, (ii) we decouple the query through Chain-of-Thoughts reasoning to facilitate more precise LVLM-based frame importance scoring, and (iii) QuoTA offers a plug-and-play functionality that extends to existing LVLMs. Extensive experimental results demonstrate that implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six benchmarks (including Video-MME and MLVU) while operating within an identical visual token budget as the baseline. Codes are open-sourced at https://github.com/MAC-AutoML/QuoTA.', 'score': 4, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7dbb5f0edfd69aad', 'authors': ['Yongdong Luo', 'Wang Chen', 'Xiawu Zheng', 'Weizhong Huang', 'Shukang Yin', 'Haojia Lin', 'Chaoyou Fu', 'Jinfa Huang', 'Jiayi Ji', 'Jiebo Luo', 'Rongrong Ji'], 'affiliations': ['Nanjing University', 'University of Rochester', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08689.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#long_context', '#architecture', '#video', '#benchmark', '#open_source', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuoTA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LVLM). QuoTA Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ QuoTA Ñ LLaVA-Video-7B Ğ´Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.2% Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Optimizing Visual Token Selection for Better Video Understanding', 'desc': 'This paper introduces QuoTA, a new method for improving long video understanding by optimizing how visual tokens are selected based on their relevance to specific queries. Unlike previous approaches that prune tokens after processing, QuoTA assesses the importance of visual tokens before they are used, ensuring that only the most relevant information is retained. By using Chain-of-Thoughts reasoning, QuoTA enhances the accuracy of frame importance scoring, allowing for better alignment between visual data and task requirements. The results show that QuoTA can significantly boost performance in existing large video-language models while maintaining the same token budget.'}, 'zh': {'title': 'ä¼˜åŒ–è§†è§‰æ ‡è®°åˆ†é…ï¼Œæå‡è§†é¢‘ç†è§£æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQuoTAçš„æ¨¡å—ï¼Œæ—¨åœ¨æ”¹è¿›é•¿è§†é¢‘ç†è§£ä¸­çš„è§†è§‰æ ‡è®°åˆ†é…ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒQuoTAåœ¨è¾“å…¥å±‚é¢ä¸Šè€ƒè™‘è§†è§‰æ ‡è®°ä¸æŸ¥è¯¢ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œä»è€Œä¼˜åŒ–æ ‡è®°çš„ä½¿ç”¨æ•ˆç‡ã€‚é€šè¿‡åŸºäºæŸ¥è¯¢ç›¸å…³æ€§çš„å¸§çº§é‡è¦æ€§è¯„ä¼°ï¼ŒQuoTAèƒ½å¤Ÿåœ¨è§£ç å™¨å±‚ä¹‹å‰è¿›è¡Œä¸€æ¬¡æ€§è§†è§‰æ ‡è®°åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuoTAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºçº¿ç›¸åŒçš„è§†è§‰æ ‡è®°é¢„ç®—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08507', 'title': 'Referring to Any Person', 'url': 'https://huggingface.co/papers/2503.08507', 'abstract': 'Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek', 'score': 4, 'issue_id': 2661, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '905fc0de3c82fe2e', 'authors': ['Qing Jiang', 'Lin Wu', 'Zhaoyang Zeng', 'Tianhe Ren', 'Yuda Xiong', 'Yihao Chen', 'Qin Liu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08507.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#optimization', '#interpretability', '#architecture', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ»ÑĞ´ĞµĞ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HumanRef, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ RexSeek, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RexSeek Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Person Detection with HumanRef and RexSeek', 'desc': 'This paper addresses the challenge of detecting individuals in computer vision using natural language descriptions, a task known as referring to any person. The authors highlight that existing models often fall short in real-world scenarios and current benchmarks are too narrow, focusing mainly on one-to-one referring. They propose a new dataset called HumanRef, which is designed to better represent the complexities of real-world applications. Additionally, they introduce a new model, RexSeek, which combines a multimodal large language model with an object detection framework, showing improved performance in both human and object referring tasks.'}, 'zh': {'title': 'äººç±»è¯†åˆ«çš„æ–°çªç ´ï¼šRexSeekæ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­äººç±»ä¸ªä½“è¯†åˆ«çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡å®šä¹‰â€”â€”æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°è¯†åˆ«ä¸ªä½“ã€‚ç°æœ‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ä¸€å¯¹ä¸€çš„è¯†åˆ«ä¸Šï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†HumanRefï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºRexSeekçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤šä¸ªä¸ªä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRexSeekåœ¨è¯†åˆ«äººç±»å’Œå¸¸è§ç‰©ä½“æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08417', 'title': 'AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2503.08417', 'abstract': "Despite recent advancements in learning-based motion in-betweening, a key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, a novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs a two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, a fine-tuning technique for video diffusion models. Additionally, we propose a ``motion-video mimicking'' optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to a wide range of motion in-betweening tasks.", 'score': 4, 'issue_id': 2662, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7610945506fac90e', 'authors': ['Kwan Yun', 'Seokhyeon Hong', 'Chaelin Kim', 'Junyong Noh'], 'affiliations': ['KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.08417.jpg', 'data': {'categories': ['#3d', '#training', '#diffusion', '#optimization', '#video', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': "AnyMoLe - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ ICAdapt Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 'motion-video mimicking' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ²."}, 'en': {'title': 'AnyMoLe: Motion Generation Without Character-Specific Data', 'desc': "This paper presents AnyMoLe, a new method for generating motion in-between frames for various characters without needing specific datasets. It utilizes video diffusion models in a two-stage frame generation process to improve the understanding of context in animations. The authors introduce ICAdapt, a technique that fine-tunes these models to better connect real-world and animated character movements. Additionally, a 'motion-video mimicking' optimization is proposed, allowing for fluid motion generation across different character joint structures, thus minimizing data dependency and enhancing the realism of transitions."}, 'zh': {'title': 'AnyMoLeï¼šæ— æ•°æ®ä¾èµ–çš„è§’è‰²è¿åŠ¨æ’å€¼æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•AnyMoLeï¼Œæ—¨åœ¨è§£å†³åŸºäºå­¦ä¹ çš„è¿åŠ¨æ’å€¼ä¸­å¯¹ç‰¹å®šè§’è‰²æ•°æ®é›†çš„ä¾èµ–é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆä»»æ„è§’è‰²çš„å¸§é—´è¿åŠ¨ï¼Œæ— éœ€å¤–éƒ¨æ•°æ®ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„å¸§ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼Œå¹¶å¼•å…¥äº†ICAdaptæŠ€æœ¯æ¥ç¼©å°çœŸå®ä¸–ç•Œä¸æ¸²æŸ“è§’è‰²åŠ¨ç”»ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè¿åŠ¨è§†é¢‘æ¨¡ä»¿â€ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿å¾—ä½¿ç”¨2Då’Œ3Dç‰¹å¾çš„ä»»æ„å…³èŠ‚ç»“æ„è§’è‰²çš„è¿åŠ¨ç”Ÿæˆå˜å¾—æ— ç¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06492', 'title': 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering', 'url': 'https://huggingface.co/papers/2503.06492', 'abstract': 'Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modality-specific modules. To bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset, <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules. The dataset is available at https://huggingface.co/datasets/WYLing/VisualSimpleQA.', 'score': 4, 'issue_id': 2659, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '5f5a7152d7773f38', 'authors': ['Yanling Wang', 'Yihan Zhao', 'Xiaodong Chen', 'Shasha Guo', 'Lixin Liu', 'Haoyang Li', 'Yong Xiao', 'Jing Zhang', 'Qi Li', 'Ke Xu'], 'affiliations': ['Renmin University of China', 'Tencent', 'Tsinghua University', 'Zhongguancun Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.06492.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#hallucinations', '#multimodal', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'VisualSimpleQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisualSimpleQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). Ğ’ Ğ½ĞµĞ¼ Ñ‚Ğ°ĞºĞ¶Ğµ ĞµÑÑ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ VisualSimpleQA-hard Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 60% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° VisualSimpleQA Ğ¸ 30% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸.'}, 'en': {'title': 'Enhancing Accuracy in Multimodal Fact-Seeking with VisualSimpleQA', 'desc': 'This paper addresses the issue of non-factual responses in large vision-language models (LVLMs) during fact-seeking question answering (QA). It introduces VisualSimpleQA, a new benchmark that allows for separate evaluation of visual and linguistic capabilities of these models. The benchmark includes specific difficulty levels to aid in human annotation and identifies a challenging subset called VisualSimpleQA-hard. Experiments reveal that even advanced models like GPT-4o struggle with accuracy, achieving only around 60% correctness overall and 30% on the harder subset, indicating significant room for improvement in both visual and linguistic processing.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€é—®ç­”çš„å‡†ç¡®æ€§', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨äº‹å®å¯»æ±‚é—®ç­”ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†ä»ç„¶å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®å›ç­”çš„é—®é¢˜ã€‚å½“å‰çš„å¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ä¸»è¦å…³æ³¨æ¨¡å‹è¾“å‡ºä¸çœŸå®ç­”æ¡ˆçš„æ¯”è¾ƒï¼Œç¼ºä¹å¯¹ç‰¹å®šæ¨¡æ€æ¨¡å—æ€§èƒ½çš„æ·±å…¥åˆ†æã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisualSimpleQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾çš„å¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ã€‚é€šè¿‡ç®€åŒ–å’Œè§£è€¦çš„è¯„ä¼°æ–¹å¼ï¼ŒVisualSimpleQAèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°LVLMsåœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼•å…¥æ˜ç¡®çš„éš¾åº¦æ ‡å‡†ä»¥æŒ‡å¯¼äººå·¥æ ‡æ³¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18858', 'title': 'Evaluating Intelligence via Trial and Error', 'url': 'https://huggingface.co/papers/2502.18858', 'abstract': "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 10^{7} times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence.", 'score': 4, 'issue_id': 2656, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '54797794006daa5e', 'authors': ['Jingtao Zhan', 'Jiahao Zhao', 'Jiayu Li', 'Yiqun Liu', 'Bo Zhang', 'Qingyao Ai', 'Jiaxin Mao', 'Hongning Wang', 'Min Zhang', 'Shaoping Ma'], 'affiliations': ['Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18858.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#agi', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ğ¿ÑƒÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Survival Game', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Survival Game Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€Ğ¾Ğ± Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ£Ñ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ£Ñ€Ğ¾Ğ²Ğ½Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµÑ†ĞµĞ»ĞµÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ˜Ğ˜.'}, 'en': {'title': 'Measuring Intelligence: The Survival Game Framework', 'desc': 'This paper introduces the Survival Game framework to measure intelligence based on the number of failed attempts in problem-solving. It defines the Autonomous Level of intelligence as the ability to find solutions with fewer failures, indicating a deeper understanding of tasks. The study evaluates existing AI systems, revealing that while they perform well on simple tasks, they struggle with complex ones like vision and language. The findings suggest that achieving a high Autonomous Level in AI would require an impractical scale of parameters, highlighting the limitations of current technologies and the complexity of human-like intelligence.'}, 'zh': {'title': 'ç”Ÿå­˜æ¸¸æˆï¼šè¯„ä¼°æ™ºèƒ½çš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç”Ÿå­˜æ¸¸æˆçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯•é”™è¿‡ç¨‹ä¸­å¤±è´¥æ¬¡æ•°çš„å¤šå°‘ã€‚å¤±è´¥æ¬¡æ•°è¶Šå°‘ï¼Œè¡¨ç¤ºæ™ºèƒ½æ°´å¹³è¶Šé«˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç®€å•ä»»åŠ¡ä¸­è¾¾åˆ°äº†è‡ªä¸»æ°´å¹³ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚è§†è§‰ã€æœç´¢ã€æ¨èå’Œè¯­è¨€å¤„ç†ï¼‰ä¸­ä»ç„¶è¿œæœªè¾¾åˆ°ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”Ÿå­˜æ¸¸æˆä¸ä»…å¯ä»¥æŒ‡å¯¼æœªæ¥çš„äººå·¥æ™ºèƒ½å‘å±•ï¼Œè¿˜èƒ½æ·±å…¥ç†è§£äººç±»æ™ºèƒ½çš„æœ¬è´¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06594', 'title': 'Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation', 'url': 'https://huggingface.co/papers/2503.06594', 'abstract': 'The field of neural machine translation (NMT) has changed with the advent of large language models (<PRE_TAG>LLMs)</POST_TAG>. Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained <PRE_TAG>Transformer decoder</POST_TAG>, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.', 'score': 3, 'issue_id': 2660, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'bea0df269ed71c2d', 'authors': ['Yingfeng Luo', 'Tong Zheng', 'Yongyu Mu', 'Bei Li', 'Qinghong Zhang', 'Yongqi Gao', 'Ziqiang Xu', 'Peinan Feng', 'Xiaoqian Liu', 'Tong Xiao', 'Jingbo Zhu'], 'affiliations': ['NLP Lab, Northeastern University, Shenyang, China', 'NiuTrans Research, Shenyang, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06594.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#inference', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ¸ LLM Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ NMT Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ (NMT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² NMT, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ NMT Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ NMT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ baseline-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 2.4-6.5 Ñ€Ğ°Ğ· Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ KV-ĞºÑÑˆĞ° Ğ½Ğ° 75%.'}, 'en': {'title': 'Revolutionizing Translation: Merging LLMs with NMT for Efficiency and Quality', 'desc': 'This paper discusses advancements in neural machine translation (NMT) by integrating large language models (LLMs) with traditional NMT architectures. The authors propose a novel approach that utilizes LLMs for encoding while maintaining the existing NMT decoder, enhancing efficiency and optimization. They introduce a new dataset to evaluate the generalization of their translation models across multiple tasks. Results indicate that their method not only improves translation quality but also significantly increases inference speed and reduces memory usage.'}, 'zh': {'title': 'ç»“åˆLLMsä¸NMTï¼Œæå‡ç¿»è¯‘æ•ˆç‡ä¸è´¨é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ç»“åˆï¼Œä»¥æé«˜ç¿»è¯‘æ¨¡å‹çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬é‡‡ç”¨LLMsè¿›è¡ŒNMTç¼–ç ï¼ŒåŒæ—¶ä¿æŒNMTè§£ç å™¨ä¸å˜ï¼Œå¹¶å¼€å‘äº†é€‚åº”LLMsä¸NMTè§£ç å™¨æ›´å¥½é…åˆçš„æ–¹æ³•ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«å¤šä»»åŠ¡çš„æ–°æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœºå™¨ç¿»è¯‘ç³»ç»Ÿåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¿»è¯‘è´¨é‡ä¸Šä¸å¤šç§åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†2.4åˆ°6.5å€ï¼ŒKVç¼“å­˜çš„å†…å­˜å ç”¨å‡å°‘äº†75%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08478', 'title': 'NullFace: Training-Free Localized Face Anonymization', 'url': 'https://huggingface.co/papers/2503.08478', 'abstract': "Privacy concerns around ever increasing number of cameras are increasing in today's digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce a training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes a pre-trained text-to-image diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/nullface .", 'score': 2, 'issue_id': 2659, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '150c9c8854f08130', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['National University of Singapore', 'University of Oulu', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.08478.jpg', 'data': {'categories': ['#ethics', '#cv', '#training', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ¾Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹, Ğ½Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Effortless Face Anonymization with Preserved Attributes', 'desc': 'This paper presents a novel method for face anonymization that does not require any training, making it efficient and practical. It leverages a pre-trained text-to-image diffusion model to anonymize faces while preserving important non-identity-related features. The method works by inverting the input image to retrieve its initial noise, which is then processed through a diffusion model that modifies identity embeddings. Additionally, it allows users to selectively anonymize specific facial regions, ensuring flexibility and control over the anonymization process.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„é¢éƒ¨åŒ¿ååŒ–æ–¹æ³•ï¼Œä¿æŠ¤éšç§ä¸å›¾åƒè´¨é‡å¹¶å­˜', 'desc': 'åœ¨æ•°å­—æ—¶ä»£ï¼Œè¶Šæ¥è¶Šå¤šçš„æ‘„åƒå¤´å¼•å‘äº†éšç§é—®é¢˜ã€‚ç°æœ‰çš„åŒ¿ååŒ–æ–¹æ³•è™½ç„¶å¯ä»¥æ¨¡ç³Šèº«ä»½ä¿¡æ¯ï¼Œä½†å¾€å¾€éš¾ä»¥ä¿æŒå›¾åƒçš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„é¢éƒ¨åŒ¿ååŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿä¿ç•™å…³é”®çš„éèº«ä»½ç›¸å…³å±æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡èº«ä»½æ¡ä»¶æ‰©æ•£è¿‡ç¨‹å»å™ªå£°ï¼Œç¡®ä¿åŒ¿ååŒ–çš„é¢å­”ä¸åŸå§‹èº«ä»½æ˜æ˜¾ä¸åŒï¼ŒåŒæ—¶æ”¯æŒå±€éƒ¨åŒ¿ååŒ–ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å“ªäº›é¢éƒ¨åŒºåŸŸè¢«åŒ¿ååŒ–æˆ–ä¿ç•™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08102', 'title': 'AI-native Memory 2.0: Second Me', 'url': 'https://huggingface.co/papers/2503.08102', 'abstract': 'Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.', 'score': 2, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '45b3f3aee8d173f9', 'authors': ['Jiale Wei', 'Xiang Ying', 'Tao Gao', 'Felix Tao', 'Jingbo Shang'], 'affiliations': ['Mindverse.ai'], 'pdf_title_img': 'assets/pdf/title_img/2503.08102.jpg', 'data': {'categories': ['#multimodal', '#agi', '#agents', '#optimization', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SECOND ME: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ°ÑˆĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'SECOND ME - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ½Ğ° Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. SECOND ME Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Memory Management with AI: Meet SECOND ME!', 'desc': 'The paper introduces SECOND ME, an AI-driven memory management system designed to enhance user interactions with digital environments. It addresses the issue of redundant information sharing by autonomously storing and organizing user-specific knowledge, allowing for context-aware responses and prefilled information. Unlike traditional memory solutions, SECOND ME utilizes large language models to enable dynamic knowledge retrieval and contextual reasoning. This innovative approach aims to reduce cognitive load and improve the efficiency of human-AI interactions in various applications.'}, 'zh': {'title': 'æ™ºèƒ½è®°å¿†ç®¡ç†ï¼Œæå‡äººæœºäº’åŠ¨ä½“éªŒ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSECOND MEçš„æ™ºèƒ½è®°å¿†ç®¡ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å‡å°‘ç”¨æˆ·åœ¨ä¸å¤–éƒ¨ä¸–ç•Œäº’åŠ¨æ—¶çš„å†—ä½™ä¿¡æ¯è¾“å…¥ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒSECOND MEèƒ½å¤Ÿæ™ºèƒ½åœ°å­˜å‚¨ã€ç»„ç»‡å’ŒåŠ¨æ€ä½¿ç”¨ç”¨æˆ·ç‰¹å®šçš„çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„è®°å¿†å­˜å‚¨è§£å†³æ–¹æ¡ˆä¸åŒï¼ŒSECOND MEä¸ä»…ä»…æ˜¯é™æ€æ•°æ®çš„ä¿ç•™ï¼Œè€Œæ˜¯é€šè¿‡ä¸Šä¸‹æ–‡æ¨ç†å’Œè‡ªé€‚åº”çŸ¥è¯†æ£€ç´¢æ¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚éšç€AIé©±åŠ¨çš„ä¸ªäººä»£ç†çš„æ™®åŠï¼ŒSECOND MEä»£è¡¨äº†å¢å¼ºäººç±»ä¸ä¸–ç•Œäº’åŠ¨çš„é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07639', 'title': 'Mixture of Experts Made Intrinsically Interpretable', 'url': 'https://huggingface.co/papers/2503.07639', 'abstract': 'Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.', 'score': 2, 'issue_id': 2653, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '7e9a13248a2692b5', 'authors': ['Xingyi Yang', 'Constantin Venhoff', 'Ashkan Khakzar', 'Christian Schroeder de Witt', 'Puneet K. Dokania', 'Adel Bibi', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.07639.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoE-X: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoE-X - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ Mixture-of-Experts. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². MoE-X Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¹ MoE ĞºĞ°Ğº ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoE-X Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'MoE-X: Enhancing Interpretability in Language Models with Sparse Activations', 'desc': 'This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods.'}, 'zh': {'title': 'MoE-Xï¼šå¯è§£é‡Šçš„æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç¥ç»å…ƒå¸¸å¸¸è¡¨ç°å‡ºå¤šä¹‰æ€§ï¼ŒåŒæ—¶ç¼–ç å¤šä¸ªæ— å…³çš„æ¦‚å¿µï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å·®ã€‚æˆ‘ä»¬æå‡ºäº†MoE-Xï¼Œè¿™æ˜¯ä¸€ç§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å†…åœ¨ä¸Šå…·æœ‰å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰ç¨€ç–æ¿€æ´»çš„å®½ç½‘ç»œæ›´æœ‰å¯èƒ½æ•æ‰å¯è§£é‡Šçš„å› ç´ ã€‚é€šè¿‡æ¿€æ´»ä»…ä¸€éƒ¨åˆ†ä¸“å®¶ï¼ŒMoEæ¶æ„æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»è€Œåœ¨ä¿æŒç¨€ç–æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆçš„éšè—å±‚è§„æ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08037', 'title': 'ObjectMover: Generative Object Movement with Video Prior', 'url': 'https://huggingface.co/papers/2503.08037', 'abstract': 'Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios.', 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '821b6b6ca9c0eb61', 'authors': ['Xin Yu', 'Tianyu Wang', 'Soo Ye Kim', 'Paul Guerrero', 'Xi Chen', 'Qing Liu', 'Zhe Lin', 'Xiaojuan Qi'], 'affiliations': ['Adobe Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08037.jpg', 'data': {'categories': ['#games', '#data', '#training', '#synthetic', '#video', '#optimization', '#dataset'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ObjectMover: Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ObjectMover - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ sequence-to-sequence Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ObjectMover ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Seamless Object Movement in Images with ObjectMover', 'desc': "This paper introduces ObjectMover, a generative model designed to move objects within images while addressing challenges like lighting harmonization and perspective adjustments. The authors treat the task as a sequence-to-sequence problem, utilizing a fine-tuned video generation model to ensure consistent object representation across frames. To overcome the lack of large-scale data for object movement, they create a data generation pipeline using a game engine to produce high-quality training pairs. Additionally, a multi-task learning strategy is proposed to enhance the model's performance on real-world video data, leading to impressive results in complex scenarios."}, 'zh': {'title': 'ObjectMoverï¼šæ™ºèƒ½ç‰©ä½“ç§»åŠ¨çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºObjectMoverçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å›¾åƒä¸­ç‰©ä½“ç§»åŠ¨çš„å¤æ‚ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å°†ç‰©ä½“ç§»åŠ¨è§†ä¸ºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œå¹¶å¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åˆ©ç”¨å…¶åœ¨è§†é¢‘å¸§é—´ä¸€è‡´ç”Ÿæˆç‰©ä½“çš„çŸ¥è¯†ã€‚ç”±äºç¼ºä¹å¤§è§„æ¨¡çš„ç‰©ä½“ç§»åŠ¨æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°ä»£æ¸¸æˆå¼•æ“æ„å»ºæ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆæˆé«˜è´¨é‡çš„æ•°æ®å¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œä»¥ä¾¿åœ¨çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05037', 'title': 'Collapse of Dense Retrievers: Short, Early, and Literal Biases\n  Outranking Factual Evidence', 'url': 'https://huggingface.co/papers/2503.05037', 'abstract': "Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid failures. In this work, by repurposing a relation extraction dataset (e.g. Re-DocRED), we design controlled experiments to quantify the impact of heuristic biases, such as favoring shorter documents, in retrievers like Dragon+ and Contriever. Our findings reveal significant vulnerabilities: retrievers often rely on superficial patterns like over-prioritizing document beginnings, shorter documents, repeated entities, and literal matches. Additionally, they tend to overlook whether the document contains the query's answer, lacking deep semantic understanding. Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 3% of cases over a biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than not providing any documents at all.", 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '862c1dc027b05f42', 'authors': ['Mohsen Fayyaz', 'Ali Modarressi', 'Hinrich Schuetze', 'Nanyun Peng'], 'affiliations': ['CIS, LMU Munich', 'Munich Center for Machine Learning', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.05037.jpg', 'data': {'categories': ['#benchmark', '#security', '#rag', '#interpretability', '#dataset'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ğ²ĞµÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Dragon+ Ğ¸ Contriever Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¼Ñ‹ÑĞ». Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº RAG.'}, 'en': {'title': 'Uncovering Biases in Dense Retrieval Models for Better Information Retrieval', 'desc': 'This paper investigates the weaknesses of dense retrieval models used in Information Retrieval, particularly in systems like Retrieval-Augmented Generation (RAG). By analyzing a relation extraction dataset, the authors conduct experiments to identify how heuristic biases, such as favoring shorter documents, affect the performance of retrievers like Dragon+ and Contriever. The results show that these models often depend on superficial patterns and fail to ensure that retrieved documents contain the relevant answers, leading to poor retrieval outcomes. The study highlights that when multiple biases are present, the performance can drastically decline, negatively impacting downstream applications that rely on accurate document retrieval.'}, 'zh': {'title': 'æ­ç¤ºå¯†é›†æ£€ç´¢æ¨¡å‹çš„è„†å¼±æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¯†é›†æ£€ç´¢æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢åº”ç”¨ä¸­çš„è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡é‡æ–°åˆ©ç”¨å…³ç³»æå–æ•°æ®é›†ï¼Œè®¾è®¡äº†æ§åˆ¶å®éªŒæ¥é‡åŒ–å¯å‘å¼åè§å¯¹æ£€ç´¢å™¨çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæ£€ç´¢å™¨å¾€å¾€ä¾èµ–è¡¨é¢æ¨¡å¼ï¼Œå¦‚è¿‡åº¦ä¼˜å…ˆè€ƒè™‘æ–‡æ¡£å¼€å¤´ã€è¾ƒçŸ­æ–‡æ¡£å’Œå­—é¢åŒ¹é…ï¼Œç¼ºä¹æ·±å±‚è¯­ä¹‰ç†è§£ã€‚å¤šä¸ªåè§ç»“åˆæ—¶ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´é€‰æ‹©åŒ…å«ç­”æ¡ˆçš„æ–‡æ¡£çš„æ¦‚ç‡ä½äº3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03734', 'title': 'OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction', 'url': 'https://huggingface.co/papers/2503.03734', 'abstract': 'Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.', 'score': 0, 'issue_id': 2668, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'a6a36002652e8be7', 'authors': ['Huang Huang', 'Fangchen Liu', 'Letian Fu', 'Tingfan Wu', 'Mustafa Mukadam', 'Jitendra Malik', 'Ken Goldberg', 'Pieter Abbeel'], 'affiliations': ['Meta AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.03734.jpg', 'data': {'categories': ['#agi', '#architecture', '#transfer_learning', '#video', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ¦¦', 'ru': {'title': 'OTTER: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'OTTER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. OTTER ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… OTTER Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLA, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğµ.'}, 'en': {'title': 'OTTER: Enhancing Robotic Actions with Smart Visual-Language Alignment', 'desc': 'The paper introduces OTTER, a new Vision-Language-Action (VLA) model designed to enhance robotic action prediction using visual inputs and language instructions. Unlike traditional methods that require fine-tuning of pre-trained vision-language models, OTTER maintains the integrity of these models by keeping them frozen and selectively extracting only the relevant visual features aligned with the language commands. This approach allows OTTER to leverage the rich semantic knowledge from large-scale pre-training, leading to improved performance in zero-shot scenarios. Experimental results show that OTTER outperforms existing VLA models in both simulated and real-world tasks, demonstrating its ability to generalize to new objects and environments without additional training.'}, 'zh': {'title': 'OTTERï¼šæå‡æœºå™¨äººåŠ¨ä½œé¢„æµ‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹OTTERï¼Œæ—¨åœ¨æ ¹æ®è§†è§‰è§‚å¯Ÿå’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹æœºå™¨äººåŠ¨ä½œã€‚OTTERé€šè¿‡æ˜¾å¼çš„æ–‡æœ¬æ„ŸçŸ¥è§†è§‰ç‰¹å¾æå–ï¼Œåˆ©ç”¨ç°æœ‰çš„è¯­ä¹‰å¯¹é½ï¼Œé¿å…äº†å¯¹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€‚è¯¥æ¨¡å‹é€‰æ‹©æ€§åœ°æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ç­–ç•¥å˜æ¢å™¨ï¼Œä»è€Œä¿æŒé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ä¸å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOTTERåœ¨æ–°ç‰©ä½“å’Œç¯å¢ƒä¸Šå±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„VLAæ¨¡å‹ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi (3)', '#alignment (3)', '#architecture (10)', '#audio (3)', '#benchmark (12)', '#cv (7)', '#data (5)', '#dataset (15)', '#diffusion (9)', '#ethics (4)', '#games (4)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (5)', '#interpretability (7)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (18)', '#open_source (7)', '#optimization (13)', '#plp', '#rag (2)', '#reasoning (5)', '#rl (3)', '#rlhf (1)', '#robotics', '#science', '#security (2)', '#small_models (1)', '#story_generation (1)', '#survey (1)', '#synthetic (4)', '#training (17)', '#transfer_learning (2)', '#video (10)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-12 18:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-12 18:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-12 18:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    