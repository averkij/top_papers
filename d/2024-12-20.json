{
    "date": {
        "ru": "20 декабря",
        "en": "December 20",
        "zh": "12月20日"
    },
    "time_utc": "2024-12-20 04:12",
    "weekday": 4,
    "issue_id": 1228,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.15115",
            "title": "Qwen2.5 Technical Report",
            "url": "https://huggingface.co/papers/2412.15115",
            "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
            "score": 91,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "578b15d8a263e387",
            "authors": [
                "Qwen",
                ":",
                "An Yang",
                "Baosong Yang",
                "Beichen Zhang",
                "Binyuan Hui",
                "Bo Zheng",
                "Bowen Yu",
                "Chengyuan Li",
                "Dayiheng Liu",
                "Fei Huang",
                "Haoran Wei",
                "Huan Lin",
                "Jian Yang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jianxin Yang",
                "Jiaxi Yang",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Keming Lu",
                "Keqin Bao",
                "Kexin Yang",
                "Le Yu",
                "Mei Li",
                "Mingfeng Xue",
                "Pei Zhang",
                "Qin Zhu",
                "Rui Men",
                "Runji Lin",
                "Tianhao Li",
                "Tingyu Xia",
                "Xingzhang Ren",
                "Xuancheng Ren",
                "Yang Fan",
                "Yang Su",
                "Yichang Zhang",
                "Yu Wan",
                "Yuqiong Liu",
                "Zeyu Cui",
                "Zhenru Zhang",
                "Zihan Qiu"
            ],
            "affiliations": [
                "Alibaba Cloud Model Studio",
                "Hugging Face Hub",
                "Kaggle",
                "ModelScope"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15115.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal",
                    "#architecture",
                    "#agi",
                    "#dataset",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Qwen2.5: Новое поколение языковых моделей с улучшенной эффективностью и разнообразием применений",
                    "desc": "Статья представляет серию больших языковых моделей Qwen2.5, разработанных для различных потребностей. Модели прошли значительные улучшения на этапах предобучения и постобучения, включая увеличение объема обучающих данных до 18 триллионов токенов. Применены техники тонкой настройки и многоэтапного обучения с подкреплением для улучшения генерации длинных текстов и следования инструкциям. Qwen2.5 демонстрирует высокую производительность в различных задачах, конкурируя с современными моделями, значительно превосходящими ее по размеру."
                },
                "en": {
                    "title": "Qwen2.5: Elevating Language Models with Unmatched Scale and Precision",
                    "desc": "Qwen2.5 is a new series of large language models (LLMs) that have been enhanced through extensive pre-training and post-training processes. The pre-training phase utilized a massive dataset of 18 trillion tokens, significantly improving the model's common sense and reasoning abilities. In the post-training phase, advanced techniques like supervised finetuning and reinforcement learning were applied to refine the model's performance on tasks such as long text generation and instruction following. Qwen2.5 models are available in various sizes and configurations, demonstrating top performance across multiple benchmarks and applications, including specialized models for math and coding."
                },
                "zh": {
                    "title": "Qwen2.5：满足多样化需求的大型语言模型",
                    "desc": "本文介绍了Qwen2.5，这是一个全面的大型语言模型系列，旨在满足多样化的需求。与之前的版本相比，Qwen2.5在预训练和后训练阶段都有显著改进，预训练数据集从7万亿个标记扩展到18万亿个标记，为常识、专家知识和推理能力提供了坚实基础。后训练方面，采用了超过100万样本的复杂监督微调和多阶段强化学习，显著提升了人类偏好和长文本生成能力。Qwen2.5在语言理解、推理、数学、编码等多个基准测试中表现出色，尤其是其旗舰模型Qwen2.5-72B-Instruct在性能上超越了许多开放和专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14835",
            "title": "Progressive Multimodal Reasoning via Active Retrieval",
            "url": "https://huggingface.co/papers/2412.14835",
            "abstract": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.",
            "score": 22,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "749dab304f766614",
            "authors": [
                "Guanting Dong",
                "Chenghao Zhang",
                "Mengjie Deng",
                "Yutao Zhu",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14835.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AR-MCTS: Новый подход к усилению мультимодальных рассуждений ИИ",
                    "desc": "Статья представляет AR-MCTS - универсальную систему для улучшения способностей мультимодальных языковых моделей (MLLM) в решении сложных задач рассуждения. Система использует активное извлечение (AR) и метод Монте-Карло для деревьев поиска (MCTS) для генерации пошаговых аннотаций и динамического извлечения ключевых идей на каждом этапе рассуждения. AR-MCTS также включает модель вознаграждения процесса для автоматической проверки мультимодальных рассуждений. Эксперименты показали эффективность AR-MCTS в повышении производительности различных мультимодальных моделей на трех сложных тестах мультимодальных рассуждений."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with AR-MCTS Framework",
                    "desc": "This paper addresses the challenges faced by multimodal large language models (MLLMs) in performing multi-step reasoning tasks. The authors introduce a new framework called AR-MCTS, which combines Active Retrieval (AR) and Monte Carlo Tree Search (MCTS) to enhance the reasoning capabilities of MLLMs. By utilizing a unified retrieval module, the framework retrieves essential insights from a hybrid-modal corpus to assist in solving complex problems. The approach not only improves the diversity and reliability of reasoning but also incorporates a process reward model for automatic verification of multimodal reasoning tasks, demonstrating significant performance improvements in experiments."
                },
                "zh": {
                    "title": "提升多模态推理能力的AR-MCTS框架",
                    "desc": "本文提出了一种名为AR-MCTS的通用框架，旨在通过主动检索和蒙特卡洛树搜索来逐步提升多模态大语言模型（MLLMs）的推理能力。该方法首先开发了一个统一的检索模块，从混合模态检索库中提取解决复杂推理问题的关键支持信息。为了弥补自动化多模态推理验证的不足，我们结合了MCTS算法和主动检索机制，实现了逐步注释的自动生成。实验结果表明，AR-MCTS框架在提升多模态模型性能方面具有显著效果，优化了采样的多样性和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14475",
            "title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval",
            "url": "https://huggingface.co/papers/2412.14475",
            "abstract": "Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70times more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field.",
            "score": 19,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "9cc225c1e0ce01c5",
            "authors": [
                "Junjie Zhou",
                "Zheng Liu",
                "Ze Liu",
                "Shitao Xiao",
                "Yueze Wang",
                "Bo Zhao",
                "Chen Jason Zhang",
                "Defu Lian",
                "Yongping Xiong"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Shanghai Jiaotong University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14475.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "MegaPairs: синтез данных для прорыва в мультимодальном поиске",
                    "desc": "Статья представляет MegaPairs - новый метод синтеза данных для мультимодального поиска, использующий модели компьютерного зрения и языка (VLM) и изображения из открытых источников. Исследователи создали массивный синтетический датасет, который позволяет значительно улучшить производительность мультимодальных ретриверов по сравнению с базовыми моделями. Модели, обученные на этих данных, достигают state-of-the-art результатов в zero-shot режиме на нескольких бенчмарках составного поиска изображений (CIR). Авторы планируют открыть доступ к датасету, обученным моделям и пайплайну синтеза данных для дальнейшего развития этой области."
                },
                "en": {
                    "title": "MegaPairs: Unlocking Multimodal Retrieval with Synthetic Data",
                    "desc": "This paper presents MegaPairs, a new method for creating training data for multimodal retrieval tasks, which combine images and text. By using vision language models (VLMs) and a large collection of open-domain images, MegaPairs generates a synthetic dataset that significantly enhances the training process. The results show that models trained with MegaPairs outperform those trained on much larger existing datasets, achieving state-of-the-art performance in various benchmarks. The authors plan to make their dataset and models publicly available to support further advancements in multimodal retrieval research."
                },
                "zh": {
                    "title": "MegaPairs：提升多模态检索的新方法",
                    "desc": "本论文介绍了一种名为MegaPairs的新数据合成方法，旨在解决多模态检索领域中训练数据不足的问题。该方法利用视觉语言模型（VLMs）和开放域图像，生成了一个大规模的合成数据集。实验结果表明，MegaPairs生成的数据质量高，使得多模态检索器的性能显著超过了基于现有数据集训练的基线模型。我们生成了超过2600万的训练实例，并训练了多种规模的模型，这些模型在多个基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15204",
            "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
            "url": "https://huggingface.co/papers/2412.15204",
            "abstract": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",
            "score": 10,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "6cf25f1f8b2e5710",
            "authors": [
                "Yushi Bai",
                "Shangqing Tu",
                "Jiajie Zhang",
                "Hao Peng",
                "Xiaozhi Wang",
                "Xin Lv",
                "Shulin Cao",
                "Jiazheng Xu",
                "Lei Hou",
                "Yuxiao Dong",
                "Jie Tang",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15204.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "LongBench v2: Испытание для ИИ в работе с длинными контекстами",
                    "desc": "LongBench v2 - это новый бенчмарк для оценки способности больших языковых моделей (LLM) работать с длинными контекстами. Он включает 503 сложных вопроса с множественным выбором в шести категориях задач, с контекстами от 8 тысяч до 2 миллионов слов. Данные собраны от почти 100 высококвалифицированных специалистов из разных областей, а качество и сложность обеспечиваются автоматическим и ручным рецензированием. Результаты показывают, что лучшая модель достигает точности 50.1% при прямом ответе на вопросы, в то время как модель o1-preview с более длительным рассуждением достигает 57.7%, превосходя человеческий базовый уровень на 4%."
                },
                "en": {
                    "title": "LongBench v2: Elevating LLMs' Long-Context Reasoning Skills",
                    "desc": "This paper presents LongBench v2, a benchmark for evaluating large language models (LLMs) on long-context tasks that require advanced reasoning and understanding. It includes 503 multiple-choice questions with contexts ranging from 8,000 to 2 million words, covering various tasks like question answering and dialogue understanding. The benchmark was developed using data from nearly 100 educated individuals to ensure quality and difficulty, with human experts achieving only 53.7% accuracy under time constraints. The findings show that while the best LLMs perform around 50.1% accuracy, a model with enhanced reasoning capabilities can exceed human performance, emphasizing the need for improved reasoning in LLMs for long-context challenges."
                },
                "zh": {
                    "title": "提升推理能力，挑战长上下文问题",
                    "desc": "本文介绍了LongBench v2，这是一个基准测试，旨在评估大型语言模型（LLM）处理长上下文问题的能力。这些问题需要深入理解和推理，涵盖了多个现实世界的任务。LongBench v2包含503个具有挑战性的多项选择题，文本长度从8000到200万字不等，涉及六个主要任务类别。评估结果显示，最佳模型在直接回答问题时的准确率仅为50.1%，而经过更长推理的o1-preview模型则达到了57.7%，超越了人类基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15213",
            "title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
            "url": "https://huggingface.co/papers/2412.15213",
            "abstract": "Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.",
            "score": 5,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "7e00a5665592fb4d",
            "authors": [
                "Qihao Liu",
                "Xi Yin",
                "Alan Yuille",
                "Andrew Brown",
                "Mannat Singh"
            ],
            "affiliations": [
                "GenAI, Meta",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15213.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "CrossFlow: Прямое отображение между модальностями без шума",
                    "desc": "Статья представляет новый подход к кросс-модальной генерации медиа, называемый CrossFlow. В отличие от традиционных диффузионных моделей, CrossFlow использует согласование потоков для прямого отображения распределения одной модальности в другую. Авторы применяют вариационные энкодеры к входным данным и вводят метод для бесклассовой направленности. CrossFlow показывает улучшенные результаты в задачах генерации изображений по тексту, описания изображений, оценки глубины и суперразрешения."
                },
                "en": {
                    "title": "Revolutionizing Cross-Modal Media Generation with CrossFlow",
                    "desc": "This paper introduces CrossFlow, a new framework for cross-modal flow matching that allows direct mapping between different media distributions without relying on Gaussian noise. Unlike traditional diffusion models, CrossFlow eliminates the need for a conditioning mechanism, simplifying the training process. The authors demonstrate that using Variational Encoders enhances the model's performance and enables Classifier-free guidance. Results show that CrossFlow not only outperforms standard flow matching in text-to-image tasks but also excels in various other cross-modal and intra-modal mapping tasks, indicating its broad applicability in media generation."
                },
                "zh": {
                    "title": "跨模态流匹配的新思路",
                    "desc": "扩散模型及其推广的流匹配在媒体生成领域产生了显著影响。传统方法是从简单的高斯噪声源分布学习到目标媒体分布的复杂映射。本文提出了一种新的思路，探索如何直接从一种模态的分布映射到另一种模态的分布，省去噪声分布和条件机制的需求。我们提出了CrossFlow框架，并展示了其在文本到图像生成等跨模态任务中的优越性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14462",
            "title": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion",
            "url": "https://huggingface.co/papers/2412.14462",
            "abstract": "As a common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to a more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explicitly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordance-aware-any.",
            "score": 4,
            "issue_id": 1228,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "d674ddd6732ab566",
            "authors": [
                "Jixuan He",
                "Wanhua Li",
                "Ye Liu",
                "Junsik Kim",
                "Donglai Wei",
                "Hanspeter Pfister"
            ],
            "affiliations": [
                "Boston College",
                "Cornell Tech",
                "Harvard University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14462.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#diffusion",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Умная вставка объектов в сцены с учетом их возможностей",
                    "desc": "Статья расширяет концепцию Affordance для задачи вставки объектов в сцены. Авторы создали датасет SAM-FB с более чем 3 миллионами примеров для обучения моделей. Предложена модель Mask-Aware Dual Diffusion (MADD), использующая двухпоточную архитектуру для одновременного шумоподавления RGB-изображения и маски вставки. Экспериментальные результаты показывают, что метод превосходит современные аналоги и демонстрирует хорошую обобщающую способность на реальных изображениях."
                },
                "en": {
                    "title": "Seamless Object Insertion through Affordance Awareness",
                    "desc": "This paper introduces a new approach to image composition by applying the concept of Affordance, which helps in understanding how objects can interact with their surroundings. It defines a novel task called affordance-aware object insertion, which allows for the seamless integration of objects into various scenes based on specific position prompts. To support this task, the authors created the SAM-FB dataset, featuring over 3 million examples from more than 3,000 object categories, addressing the challenge of limited data. The proposed Mask-Aware Dual Diffusion (MADD) model enhances the insertion process by using a dual-stream architecture to denoise both the image and the insertion mask, leading to improved performance over existing methods."
                },
                "zh": {
                    "title": "可供性驱动的图像合成新方法",
                    "desc": "本文探讨了图像合成中的前景物体与背景场景的复杂关系。我们引入了“可供性”这一概念，定义了可供性感知的物体插入任务，旨在将任意物体无缝插入任意场景。为了解决数据不足的问题，我们构建了SAM-FB数据集，包含超过300万例的3,000多个物体类别。此外，我们提出了Mask-Aware Dual Diffusion（MADD）模型，通过双流架构同时去噪RGB图像和插入掩码，从而有效促进可供性概念的实现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15200",
            "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
            "url": "https://huggingface.co/papers/2412.15200",
            "abstract": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.",
            "score": 3,
            "issue_id": 1228,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "7a7e2f117e332add",
            "authors": [
                "Wang Zhao",
                "Yan-Pei Cao",
                "Jiale Xu",
                "Yuejiang Dong",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15200.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#3d",
                    "#games"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DI-PCG: Эффективная обратная генерация процедурного контента с помощью диффузионных трансформеров",
                    "desc": "Статья представляет DI-PCG - новый эффективный метод обратной генерации процедурного контента (Inverse PCG) на основе изображений. В основе метода лежит легковесная модель диффузионного трансформера, которая напрямую генерирует параметры PCG, используя наблюдаемые изображения в качестве условий. DI-PCG демонстрирует превосходную производительность в точном восстановлении параметров и хорошо обобщается на реальные изображения. Метод предлагает перспективный подход к эффективной обратной PCG и представляет ценный шаг в исследовании пути генерации 3D-контента с использованием параметрических моделей."
                },
                "en": {
                    "title": "Efficient Inverse PCG with DI-PCG: Transforming Images into 3D Shapes",
                    "desc": "This paper introduces DI-PCG, a new method for Inverse Procedural Content Generation (PCG) that simplifies the process of generating 3D shapes from images. It utilizes a lightweight diffusion transformer model to directly link PCG parameters with observed images, making it easier to control the generation process. Unlike previous methods, DI-PCG requires fewer resources, with only 7.6 million parameters and 30 GPU hours for training, while still achieving high accuracy in parameter recovery. The results show that DI-PCG not only performs well in controlled settings but also generalizes effectively to real-world images, marking a significant advancement in 3D content creation."
                },
                "zh": {
                    "title": "高效逆程序内容生成的新方法",
                    "desc": "程序内容生成（PCG）在创建高质量3D内容方面非常强大，但控制其生成特定形状却很困难，通常需要大量的参数调优。逆程序内容生成旨在自动找到最佳参数以满足输入条件。现有的基于采样和神经网络的方法仍然面临许多样本迭代或可控性有限的问题。本文提出了一种新颖高效的逆PCG方法DI-PCG，利用轻量级扩散变换器模型，直接将PCG参数视为去噪目标，并将观察到的图像作为控制参数生成的条件。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14689",
            "title": "How to Synthesize Text Data without Model Collapse?",
            "url": "https://huggingface.co/papers/2412.14689",
            "abstract": "Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-{n} models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.",
            "score": 2,
            "issue_id": 1228,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "b10419cab812f04f",
            "authors": [
                "Xuekai Zhu",
                "Daixuan Cheng",
                "Hengli Li",
                "Kaiyan Zhang",
                "Ermo Hua",
                "Xingtai Lv",
                "Ning Ding",
                "Zhouhan Lin",
                "Zilong Zheng",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "LUMIA Lab, Shanghai Jiao Tong University",
                "Shanghai Artificial Intelligence Laboratory",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14689.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Токенное редактирование: ключ к качественным синтетическим данным для языковых моделей",
                    "desc": "Статья исследует влияние синтетических данных на обучение языковых моделей и способы их генерации без коллапса модели. Авторы обнаружили отрицательную корреляцию между долей синтетических данных и производительностью модели. Они предлагают метод редактирования токенов для получения полусинтетических данных, что теоретически предотвращает коллапс модели. Эксперименты подтверждают, что редактирование на уровне токенов улучшает качество данных и повышает производительность модели."
                },
                "en": {
                    "title": "Preventing Model Collapse with Smart Data Editing",
                    "desc": "This paper investigates the effects of using synthetic data in training language models, highlighting that too much synthetic data can lead to model collapse, where performance declines. The authors find a negative relationship between the amount of synthetic data and the model's effectiveness, indicating that relying solely on synthetic data is detrimental. They introduce a method called token editing on human-produced data to create semi-synthetic data, which helps maintain model performance. Their experiments confirm that this approach improves data quality and prevents the issues associated with model collapse."
                },
                "zh": {
                    "title": "合成数据与模型崩溃的挑战与解决方案",
                    "desc": "本论文探讨了合成数据对语言模型训练的影响，发现随着合成数据比例的增加，模型性能逐渐下降，出现模型崩溃现象。我们通过统计分析揭示了合成数据的分布偏移和n-gram特征的过度集中。为了解决这一问题，我们提出了对人类生成数据进行标记编辑，以获得半合成数据。实验结果验证了标记级编辑可以提高数据质量，从而提升模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14642",
            "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
            "url": "https://huggingface.co/papers/2412.14642",
            "abstract": "In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are available through https://github.com/phenixace/TOMG-Bench.",
            "score": 2,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "d6b4853faa2e7839",
            "authors": [
                "Jiatong Li",
                "Junxian Li",
                "Yunqing Liu",
                "Dongzhan Zhou",
                "Qing Li"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14642.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#science"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Новый бенчмарк раскрывает потенциал языковых моделей в генерации молекул",
                    "desc": "В статье представлен TOMG-Bench - первый бенчмарк для оценки способности языковых моделей (LLM) генерировать молекулы без ограничений. Бенчмарк включает три основные задачи: редактирование молекул, оптимизация молекул и генерация молекул по заданным параметрам. Авторы разработали автоматизированную систему оценки качества и точности сгенерированных молекул. Тестирование 25 языковых моделей выявило текущие ограничения и области для улучшения в области генерации молекул на основе текста."
                },
                "en": {
                    "title": "Benchmarking LLMs for Molecule Generation Excellence",
                    "desc": "This paper introduces TOMG-Bench, a benchmark designed to assess the ability of large language models (LLMs) in generating molecules. It includes three main tasks: molecule editing, optimization, and customized generation, each with multiple subtasks and a substantial dataset of test samples. An automated evaluation system is developed to measure the quality and accuracy of the generated molecules, highlighting the challenges in open-domain molecule generation. The study also shows that with the help of a specialized dataset for instruction tuning, Llama3.1-8B significantly outperforms other LLMs, indicating potential advancements in text-guided molecule discovery."
                },
                "zh": {
                    "title": "开放分子生成的新基准",
                    "desc": "本文提出了文本基础的开放分子生成基准（TOMG-Bench），这是第一个评估大型语言模型（LLM）在开放领域分子生成能力的基准。TOMG-Bench包含三个主要任务：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom），每个任务下又有三个子任务，每个子任务包含5000个测试样本。为了应对开放分子生成的复杂性，我们开发了一个自动评估系统，以测量生成分子的质量和准确性。我们的基准测试显示了25个LLM的当前局限性和潜在改进领域，并且通过使用OpenMolIns数据集，Llama3.1-8B在TOMG-Bench上超越了所有开源通用LLM，甚至比GPT-3.5-turbo高出46.5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15084",
            "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
            "url": "https://huggingface.co/papers/2412.15084",
            "abstract": "In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: https://research.nvidia.com/labs/adlr/acemath",
            "score": 1,
            "issue_id": 1228,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "b99bb71eb45dbc5a",
            "authors": [
                "Zihan Liu",
                "Yang Chen",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Wei Ping"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15084.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#math",
                    "#synthetic"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "AceMath: Прорыв в решении сложных математических задач с помощью ИИ",
                    "desc": "В этой статье представлена система AceMath - набор передовых математических моделей, способных решать сложные математические задачи. Авторы разработали эффективные модели вознаграждения для оценки генерируемых решений. Для создания инструктивно-настроенных математических моделей предложен процесс контролируемой тонкой настройки (SFT), который сначала достигает конкурентоспособной производительности в общих областях, а затем проводит целевую настройку для математической области. Результирующая модель AceMath-72B-Instruct значительно превосходит другие современные модели в решении математических задач."
                },
                "en": {
                    "title": "AceMath: Revolutionizing Math Problem Solving with Advanced Models",
                    "desc": "This paper presents AceMath, a collection of advanced mathematical models designed to solve complex math problems effectively. The authors introduce a supervised fine-tuning (SFT) process that enhances model performance in general domains before specializing in math through targeted training with curated prompts. The AceMath-72B-Instruct model significantly surpasses existing models like Qwen2.5-Math and GPT-4o in solving math problems. Additionally, the paper details the creation of AceMath-RewardBench, a benchmark for evaluating math reward models, leading to the development of AceMath-72B-RM, which outperforms other reward models in assessing solution accuracy."
                },
                "zh": {
                    "title": "AceMath：数学问题解决的前沿模型",
                    "desc": "本文介绍了AceMath，这是一个前沿数学模型套件，擅长解决复杂的数学问题，并配备了高效的奖励模型，能够评估生成的解决方案并可靠地识别正确答案。为了开发指令调优的数学模型，我们提出了一种监督微调(SFT)过程，首先在一般领域中实现竞争性表现，然后通过精心策划的提示和合成生成的响应进行针对数学领域的微调。最终模型AceMath-72B-Instruct在性能上大幅超越了Qwen2.5-Math-72B-Instruct、GPT-4o和Claude-3.5 Sonnet。我们还构建了AceMath-RewardBench，这是一个全面且强大的基准，用于评估数学奖励模型在不同问题和难度级别上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14233",
            "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
            "url": "https://huggingface.co/papers/2412.14233",
            "abstract": "Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption.   Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at https://github.com/syp2ysy/DCE.",
            "score": 1,
            "issue_id": 1228,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "007f47cd739c576c",
            "authors": [
                "Yanpeng Sun",
                "Jing Hao",
                "Ke Zhu",
                "Jiang-Jiang Liu",
                "Yuxiang Zhao",
                "Xiaofan Li",
                "Gang Zhang",
                "Zechao Li",
                "Jingdong Wang"
            ],
            "affiliations": [
                "Baidu VIS",
                "Nanjing University",
                "Nanjing University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14233.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Улучшение мультимодальных моделей с помощью специализированного визуального анализа",
                    "desc": "Статья представляет новый подход к обучению крупных мультимодальных моделей (LMM), используя специализированные визуальные модели для улучшения подписей к изображениям. Метод DCE исследует низкоуровневые и детальные атрибуты объектов, а также отношения между ними. Эксперименты показывают, что такой подход улучшает понимание визуальной информации и рассуждения на её основе. Авторы планируют опубликовать исходный код и pipeline для простой интеграции других визуальных специалистов."
                },
                "en": {
                    "title": "Enhancing Image Captions with Visual Specialists",
                    "desc": "This paper introduces a method called DCE that enhances image captions by utilizing existing visual specialists, which are models trained on annotated images for tasks other than captioning. DCE focuses on extracting low-level and fine-grained attributes of objects, such as depth and emotion, as well as their relationships, like location and interactions with humans. By integrating these detailed attributes into the captions, the method improves the performance of visual understanding and reasoning tasks. The authors plan to share their source code and datasets to facilitate the use of other visual specialists in this enhanced captioning process."
                },
                "zh": {
                    "title": "利用视觉专家提升图像描述质量",
                    "desc": "本文提出了一种新的方法，称为DCE，用于增强图像描述的质量。我们利用现成的视觉专家，这些专家最初是通过标注图像训练的，并不是专门用于图像描述。DCE方法探索了物体的低级和细粒度属性，以及物体之间的关系，并将这些属性结合到描述性标题中。实验表明，这种方法能够提高视觉理解任务的性能，并改善推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15216",
            "title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
            "url": "https://huggingface.co/papers/2412.15216",
            "abstract": "We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.",
            "score": 1,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "ee62a21bee761d14",
            "authors": [
                "Enis Simsar",
                "Alessio Tonioni",
                "Yongqin Xian",
                "Thomas Hofmann",
                "Federico Tombari"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google Switzerland",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15216.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Редактирование изображений без учителя: новый подход к обучению на неразмеченных данных",
                    "desc": "Авторы предлагают неконтролируемую модель для редактирования изображений на основе инструкций, которая устраняет необходимость в размеченных данных во время обучения. Метод вводит новый механизм редактирования под названием Cycle Edit Consistency (CEC), который применяет прямые и обратные правки в одном шаге обучения и обеспечивает согласованность в пространствах изображений и внимания. Это позволяет обучать модель на наборах данных, состоящих из пар изображение-подпись или триплетов изображение-подпись-правка, без необходимости в заранее отредактированных изображениях. Эмпирически показано, что предложенный неконтролируемый метод работает лучше для более широкого спектра правок с высокой точностью и достоверностью."
                },
                "en": {
                    "title": "Revolutionizing Image Editing: Unsupervised Learning Without Ground Truth",
                    "desc": "This paper presents an unsupervised model for instruction-based image editing that does not require ground-truth edited images for training. Traditional supervised methods rely on datasets with input images, edited images, and edit instructions, which can introduce biases and limit how well the model can generalize. The authors introduce a new editing mechanism called Cycle Edit Consistency (CEC), which allows for simultaneous forward and backward edits, ensuring consistency in both image and attention spaces. Their approach demonstrates improved performance across various edits, highlighting the potential for scaling instruction-based image editing without the constraints of existing datasets."
                },
                "zh": {
                    "title": "无监督图像编辑：打破传统限制",
                    "desc": "我们提出了一种无监督的基于指令的图像编辑模型，训练时不需要真实的编辑图像。现有的监督方法依赖于包含输入图像、编辑图像和编辑指令的三元组数据集，这些数据集可能引入偏差并限制了模型的泛化能力。我们的方法通过引入一种新的编辑机制——循环编辑一致性（CEC），在一个训练步骤中应用前向和后向编辑，从而在图像和注意力空间中强制一致性。我们的实验证明，这种无监督技术在更广泛的编辑任务中表现出更高的保真度和精确度，标志着基于指令的图像编辑的重大进展。"
                }
            }
        }
    ],
    "link_prev": "2024-12-19.html",
    "link_next": "2024-12-23.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12月19日"
    },
    "short_date_next": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12月23日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们每天都与电脑互动，无论是日常生活还是工作。随着大语言模型（LLMs）的改进，能够与环境互动并产生影响的AI代理也迅速发展。但是，AI代理在加速或自主执行工作任务方面的表现如何？这个问题的答案对希望在工作流程中采用AI的行业以及理解AI采用对劳动力市场影响的经济政策都有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的表现，我们介绍了TheAgentCompany，一个用于评估与数字工作者互动方式相似的AI代理的可扩展基准。我们构建了一个自包含的环境，模拟了一个小型软件公司，并创建了各种任务。测试结果显示，最具竞争力的代理可以自主完成24%的任务。这表明，在模拟真实工作环境中，较简单的任务可以被自动解决，但更复杂的长时间任务仍超出当前系统的能力。",
        "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
        "pinyin": "Wǒmen měitiān dōu yǔ diànnǎo hùdòng, wúlùn shì rìcháng shēnghuó háishì gōngzuò. Suízhe dà yǔyán móxíng (LLMs) de gǎijìn, nénggòu yǔ huánjìng hùdòng bìng chǎnshēng yǐngxiǎng de AI dàilǐ yě xùnsù fāzhǎn. Dànshì, AI dàilǐ zài jiāsù huò zìzhǔ zhíxíng gōngzuò rènwù fāngmiàn de biǎoxiàn rúhé? Zhè ge wèntí de dá'àn duì xīwàng zài gōngzuò liúchéng zhōng qǔyòng AI de hángyè yǐjiǎ lǐjiě AI qǔyòng duì láodònglì shìchǎng yǐngxiǎng de jīngjì zhèngcè dōu yǒu zhòngyào yìyì. Wèile héngliáng zhèxiē LLM dàilǐ zài zhíxíng xiànshì shìjiè zhuānyè rènwù fāngmiàn de biǎoxiàn, wǒmen jièshào le TheAgentCompany, yīgè yòngyú píngguā yǔ shùzì gōngzuòzhě hùdòng fāngshì xiāngsì de AI dàilǐ de kě kuòzhǎn jīzhǔn. Wǒmen gòuchéng le yīgè zì bāohán de huánjìng, mónǐ le yīgè xiǎo xíng ruǎnjiàn gōngsī, bìng chuàngjiàn le gèzhǒng rènwù. Cèshì jiéguǒ xiǎnshì, zuì jù yǐngzhàn lì de dàilǐ kěyǐ zìzhǔ wánchéng 24% de rènwù. Zhè biǎomíng, zài mónǐ zhēnshí gōngzuò huánjìng zhōng, jiào qiǎnjiàn de rènwù kěyǐ bèi zìdòng jiějué, dàn gèng fùzá de cháng shíjiān rènwù réng chāochū dāngqián xìtǒng de nénglì.",
        "vocab": "[\n    {\"word\": \"互动\", \"pinyin\": \"hùdòng\", \"trans\": \"interact\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔyán móxíng\", \"trans\": \"large language models\"},\n    {\"word\": \"代理\", \"pinyin\": \"dàilǐ\", \"trans\": \"agent\"},\n    {\"word\": \"迅速\", \"pinyin\": \"xùnsù\", \"trans\": \"rapidly\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomously\"},\n    {\"word\": \"执行\", \"pinyin\": \"zhíxíng\", \"trans\": \"execute\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"行业\", \"pinyin\": \"hángyè\", \"trans\": \"industry\"},\n    {\"word\": \"采用\", \"pinyin\": \"cǎiyòng\", \"trans\": \"adopt\"},\n    {\"word\": \"工作流程\", \"pinyin\": \"gōngzuò liúchéng\", \"trans\": \"workflow\"},\n    {\"word\": \"劳动力\", \"pinyin\": \"láodònglì\", \"trans\": \"labor force\"},\n    {\"word\": \"市场\", \"pinyin\": \"shìchǎng\", \"trans\": \"market\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐngxiǎng\", \"trans\": \"impact\"},\n    {\"word\": \"经济\", \"pinyin\": \"jīngjì\", \"trans\": \"economic\"},\n    {\"word\": \"政策\", \"pinyin\": \"zhèngcè\", \"trans\": \"policy\"},\n    {\"word\": \"衡量\", \"pinyin\": \"héngliáng\", \"trans\": \"measure\"},\n    {\"word\": \"现实世界\", \"pinyin\": \"xiànshí shìjiè\", \"trans\": \"real world\"},\n    {\"word\": \"专业\", \"pinyin\": \"zhuānyè\", \"trans\": \"professional\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"可扩展\", \"pinyin\": \"kě kuòzhǎn\", \"trans\": \"scalable\"},\n    {\"word\": \"数字工作者\", \"pinyin\": \"shùzì gōngzuòzhě\", \"trans\": \"digital worker\"},\n    {\"word\": \"自包含\", \"pinyin\": \"zì bāohán\", \"trans\": \"self-contained\"},\n    {\"word\": \"模拟\", \"pinyin\": \"mónǐ\", \"trans\": \"simulate\"},\n    {\"word\": \"软件公司\", \"pinyin\": \"ruǎnjiàn gōngsī\", \"trans\": \"software company\"},\n    {\"word\": \"竞争力\", \"pinyin\": \"jìngzhēnglì\", \"trans\": \"competitiveness\"},\n    {\"word\": \"完成\", \"pinyin\": \"wánchéng\", \"trans\": \"complete\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎomíng\", \"trans\": \"indicate\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiějué\", \"trans\": \"resolve\"},\n    {\"word\": \"超出\", \"pinyin\": \"chāochū\", \"trans\": \"exceed\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"capability\"},\n    {\"word\": \"系统\", \"pinyin\": \"xìtǒng\", \"trans\": \"system\"}\n]",
        "trans": "We interact with computers daily, whether it's in our daily lives or at work. As large language models (LLMs) improve, AI agents capable of interacting with the environment and generating impact are also rapidly developing. However, how well do these AI agents perform in accelerating or autonomously executing work tasks? The answer to this question is crucial for industries looking to adopt AI in their workflows, as well as for economic policies aiming to understand the impact of AI adoption on the labor market. To measure the performance of these LLM agents in executing real-world professional tasks, we introduce TheAgentCompany, a scalable benchmark for evaluating AI agents that interact in a manner similar to digital workers. We constructed a self-contained environment simulating a small software company and created a variety of tasks. Test results indicate that the most competitive agents can autonomously complete 24% of the tasks. This suggests that in a simulated real-world work environment, simpler tasks can be automated, but more complex, long-term tasks remain beyond the current capabilities of the systems.",
        "update_ts": "2024-12-19 09:11"
    }
}