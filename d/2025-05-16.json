{
    "date": {
        "ru": "16 мая",
        "en": "May 16",
        "zh": "5月16日"
    },
    "time_utc": "2025-05-16 02:30",
    "weekday": 4,
    "issue_id": 3791,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07782",
            "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
            "url": "https://huggingface.co/papers/2505.07782",
            "abstract": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.",
            "score": 3,
            "issue_id": 3791,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "6a556cb214d3d71d",
            "authors": [
                "Rushi Qiang",
                "Yuchen Zhuang",
                "Yinghao Li",
                "Dingu Sagar V K",
                "Rongzhi Zhang",
                "Changhao Li",
                "Ian Shu-Hei Wong",
                "Sherry Yang",
                "Percy Liang",
                "Chao Zhang",
                "Bo Dai"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07782.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#games",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "MLE-Dojo: Интерактивная среда для обучения автономных LLM-агентов",
                    "desc": "MLE-Dojo - это фреймворк для обучения, оценки и улучшения автономных агентов на основе больших языковых моделей (LLM) в итеративных рабочих процессах машинного обучения. В отличие от существующих бенчмарков, MLE-Dojo предоставляет интерактивную среду, позволяющую агентам экспериментировать, отлаживать и улучшать решения через структурированные циклы обратной связи. Фреймворк построен на более чем 200 реальных задачах Kaggle и охватывает разнообразные задачи машинного обучения, такие как обработка данных, поиск архитектуры, настройка гиперпараметров и отладка кода. MLE-Dojo поддерживает обучение агентов как с помощью контролируемой настройки, так и с помощью обучения с подкреплением."
                },
                "en": {
                    "title": "MLE-Dojo: Empowering Iterative Learning for LLMs",
                    "desc": "MLE-Dojo is a new framework designed for reinforcement learning and improving large language model (LLM) agents through iterative machine learning engineering (MLE) processes. It offers an interactive environment where agents can experiment and refine their solutions based on structured feedback, unlike traditional benchmarks that use static datasets. The framework is built on over 200 real-world Kaggle challenges, allowing for diverse MLE tasks such as data processing and hyperparameter tuning. Evaluations show that while LLMs can make iterative improvements, they still struggle with generating long-term solutions and solving complex issues, highlighting the need for further advancements in autonomous learning."
                },
                "zh": {
                    "title": "MLE-Dojo：推动自主学习模型的创新平台",
                    "desc": "MLE-Dojo是一个类似Gym的框架，旨在系统化地进行强化学习，评估和改进自主大型语言模型（LLM）代理。与现有的基准测试不同，MLE-Dojo提供了一个互动环境，使代理能够通过结构化反馈循环进行迭代实验、调试和优化解决方案。该框架基于200多个真实的Kaggle挑战，涵盖了多样化的开放式机器学习工程任务，反映了现实的工程场景。MLE-Dojo的可执行环境支持通过监督微调和强化学习进行全面的代理训练，促进了迭代实验和实时结果验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10527",
            "title": "WorldPM: Scaling Human Preference Modeling",
            "url": "https://huggingface.co/papers/2505.10527",
            "abstract": "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.",
            "score": 2,
            "issue_id": 3791,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "d2fe74535293635c",
            "authors": [
                "Binghai Wang",
                "Runji Lin",
                "Keming Lu",
                "Le Yu",
                "Zhenru Zhang",
                "Fei Huang",
                "Chujie Zheng",
                "Kai Dang",
                "Yang Fan",
                "Xingzhang Ren",
                "An Yang",
                "Binyuan Hui",
                "Dayiheng Liu",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang",
                "Yu-Gang Jiang",
                "Bowen Yu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Institute of Trustworthy Embodied Artificial Intelligence, Fudan University",
                "Qwen Team, Alibaba Group",
                "School of Computer Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10527.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Масштабирование моделей предпочтений: от данных к глобальному пониманию",
                    "desc": "Исследователи обнаружили, что законы масштабирования, аналогичные тем, что наблюдаются в языковом моделировании, существуют и в моделировании предпочтений. Они предложили концепцию World Preference Modeling (WorldPM) для изучения потенциала масштабирования в этой области. Используя 15 миллионов примеров данных и модели размером от 1.5 до 72 миллиардов параметров, они наблюдали различные паттерны масштабирования для разных метрик оценки. Эксперименты показали, что WorldPM эффективен как основа для дальнейшей настройки моделей предпочтений и улучшает обобщающую способность на различных наборах данных."
                },
                "en": {
                    "title": "Scaling Human Preferences with WorldPM",
                    "desc": "This paper introduces World Preference Modeling (WorldPM), which explores how human preferences can be effectively modeled and scaled in machine learning. The authors demonstrate that preference modeling follows similar scaling laws as language modeling, where larger models and datasets lead to improved performance. They collect diverse preference data and train models with varying sizes, revealing that adversarial and objective metrics improve with scale, while subjective metrics do not show consistent trends. The findings suggest that WorldPM enhances generalization across different human preference datasets and significantly boosts performance in reinforcement learning from human feedback (RLHF) applications."
                },
                "zh": {
                    "title": "世界偏好建模：提升人类偏好的新方法",
                    "desc": "本论文探讨了在偏好建模中存在的规模法则，类似于语言建模中的现象。我们提出了世界偏好建模（WorldPM），强调其在处理人类偏好时的统一表示能力。通过收集来自公共论坛的偏好数据，并在不同规模的模型上进行训练，我们发现不同评估指标的表现存在明显差异。实验结果表明，WorldPM在多种人类偏好数据集上显著提高了模型的泛化性能，尤其在关键子任务上提升超过5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10185",
            "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
            "url": "https://huggingface.co/papers/2505.10185",
            "abstract": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.",
            "score": 1,
            "issue_id": 3791,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "dc93377d68390280",
            "authors": [
                "Seongyun Lee",
                "Seungone Kim",
                "Minju Seo",
                "Yongrae Jo",
                "Dongyoung Go",
                "Hyeonbin Hwang",
                "Jinho Park",
                "Xiang Yue",
                "Sean Welleck",
                "Graham Neubig",
                "Moontae Lee",
                "Minjoon Seo"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Cornell University",
                "KAIST AI",
                "LG AI Research",
                "NAVER Search US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10185.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#training",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Расшифровка мышления ИИ: от цепочек к энциклопедии рассуждений",
                    "desc": "Статья представляет новый подход к анализу цепочек рассуждений (CoT) в больших языковых моделях. Авторы предлагают метод 'CoT Encyclopedia', который автоматически извлекает и кластеризует различные стратегии рассуждений из сгенерированных моделью цепочек. Этот метод позволяет более полно и интерпретируемо анализировать поведение моделей по сравнению с существующими подходами. Исследование также показывает, что формат обучающих данных оказывает большее влияние на стратегии рассуждений, чем их предметная область."
                },
                "en": {
                    "title": "Unlocking Model Reasoning with the CoT Encyclopedia",
                    "desc": "This paper presents the CoT Encyclopedia, a new framework for analyzing the reasoning strategies of large language models through their chain-of-thought (CoT) outputs. Unlike previous methods that rely on predefined categories, this approach uses a bottom-up technique to automatically extract and cluster diverse reasoning criteria from the models. The framework not only enhances the interpretability of model behaviors but also improves performance by predicting and guiding models towards more effective reasoning strategies. Additionally, the study highlights the significant influence of training data format on reasoning behavior, emphasizing the need for format-aware design in model training."
                },
                "zh": {
                    "title": "理解推理，提升模型表现",
                    "desc": "长链推理（CoT）是现代大型语言模型有效使用的重要组成部分，但我们对其推理策略的理解仍然有限。本文提出了CoT百科全书，这是一个自下而上的框架，用于分析和引导模型推理。我们的方法自动提取模型生成的CoT中的多样化推理标准，将其嵌入语义空间，聚类成代表性类别，并推导出对比性标准以解释推理行为。研究表明，这种框架比现有方法提供了更可解释和全面的分析，并且能够预测模型可能使用的策略，从而引导其朝向更有效的替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09926",
            "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
            "url": "https://huggingface.co/papers/2505.09926",
            "abstract": "Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.",
            "score": 1,
            "issue_id": 3791,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "25c3eaaddbc9d5ca",
            "authors": [
                "Bin-Bin Gao",
                "Yue Zhu",
                "Jiangtao Yan",
                "Yuezhi Cai",
                "Weixi Zhang",
                "Meng Wang",
                "Jun Liu",
                "Yong Liu",
                "Lei Wang",
                "Chengjie Wang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Siemens Corporate Research",
                "Technical University of Munich",
                "Tencent YouTu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09926.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#open_source",
                    "#transfer_learning",
                    "#healthcare"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "AdaptCLIP: Эффективное обнаружение аномалий с помощью адаптации CLIP",
                    "desc": "Статья представляет новый метод универсального обнаружения визуальных аномалий под названием AdaptCLIP. Этот метод основан на предобученной мультимодальной модели CLIP и использует три адаптера для улучшения ее производительности. AdaptCLIP обучается поочередно адаптивным визуальным и текстовым представлениям, а также использует как контекстные, так и выровненные остаточные признаки. Метод демонстрирует state-of-the-art результаты на 12 бенчмарках обнаружения аномалий в промышленных и медицинских доменах."
                },
                "en": {
                    "title": "AdaptCLIP: Simplifying Anomaly Detection with Adaptive Learning",
                    "desc": "This paper introduces AdaptCLIP, a novel approach for universal visual anomaly detection that operates without the need for additional fine-tuning. It leverages pre-trained vision-language models like CLIP to generalize effectively from just a few normal images. The method emphasizes learning adaptive visual and textual representations alternately and incorporates comparative learning that utilizes both contextual and aligned residual features. AdaptCLIP demonstrates superior performance on various anomaly detection benchmarks, showcasing its flexibility and efficiency in handling unseen vision domains."
                },
                "zh": {
                    "title": "AdaptCLIP：无微调的通用视觉异常检测",
                    "desc": "本论文提出了一种名为AdaptCLIP的通用视觉异常检测方法，旨在无需额外微调即可识别新颖或未见的视觉领域中的异常。研究表明，预训练的视觉-语言模型如CLIP在仅使用零或少量正常图像时展现出强大的泛化能力。AdaptCLIP通过交替学习自适应的视觉和文本表示，结合上下文和对齐的残差特征进行比较学习，从而提高了灵活性。该方法在12个工业和医疗领域的异常检测基准上实现了最先进的性能，显著超越了现有的竞争方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09265",
            "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
            "url": "https://huggingface.co/papers/2505.09265",
            "abstract": "Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.",
            "score": 1,
            "issue_id": 3791,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "62287f3647d6b6a7",
            "authors": [
                "Bin-Bin Gao"
            ],
            "affiliations": [
                "Tencent YouTu Lab, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09265.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Универсальная сегментация аномалий без языковых подсказок",
                    "desc": "Статья представляет новый подход к сегментации визуальных аномалий, основанный на чисто визуальной модели без использования языковых подсказок. Авторы предлагают парадигму, объединяющую сегментацию аномалий и сегментацию изменений, используя синтетические пары изображений для обучения. Разработан фреймворк MetaUAS, использующий мета-обучение и способный обобщаться на новые типы аномалий. Метод превосходит существующие решения для zero-shot, few-shot и full-shot сегментации аномалий."
                },
                "en": {
                    "title": "Revolutionizing Anomaly Segmentation with Pure Vision Models",
                    "desc": "This paper introduces a new approach for visual anomaly segmentation that does not depend on language, using a pure visual foundation model instead of traditional vision-language models. The authors propose a unified framework that treats anomaly segmentation as a form of change segmentation, allowing the use of synthetic image pairs to train the model. They develop a Meta-learning framework called MetaUAS, which can effectively identify unseen anomalies with just one normal image prompt. The method includes a soft feature alignment module to address geometric differences, achieving superior performance compared to existing anomaly segmentation techniques."
                },
                "zh": {
                    "title": "纯视觉模型实现通用异常分割",
                    "desc": "本文探讨了一种新的视觉异常分割方法，称为MetaUAS，旨在使用纯视觉模型而非传统的视觉-语言模型。该方法通过将异常分割统一为变化分割，利用大规模合成图像对进行训练，从而实现对未知异常的有效分割。我们提出的软特征对齐模块能够处理提示图像和查询图像之间的几何变化，提升了分割的准确性。MetaUAS在零-shot、少-shot和全-shot异常分割方法中表现优异，展示了其在实际应用中的广泛适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09264",
            "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
            "url": "https://huggingface.co/papers/2505.09264",
            "abstract": "Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.",
            "score": 1,
            "issue_id": 3791,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "d89f10b01e706c42",
            "authors": [
                "Bin-Bin Gao"
            ],
            "affiliations": [
                "Tencent YouTu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09264.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#survey",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "OneNIP: Обнаружение аномалий по одному нормальному изображению",
                    "desc": "Эта статья представляет новый метод OneNIP для обнаружения аномалий в изображениях с использованием всего одного эталонного нормального изображения. В отличие от предыдущих подходов, OneNIP способен реконструировать как нормальные, так и аномальные признаки, что повышает эффективность унифицированного обнаружения аномалий. Авторы также предлагают супервизорный уточнитель для улучшения сегментации аномалий на уровне пикселей. Метод OneNIP превзошел существующие решения на трех промышленных наборах данных для обнаружения аномалий."
                },
                "en": {
                    "title": "Revolutionizing Anomaly Detection with One Normal Image Prompt",
                    "desc": "This paper introduces a novel method called One Normal Image Prompt (OneNIP) for improving anomaly detection using self-attention transformers. Traditional models struggle with accurately identifying anomalies because they can reconstruct both normal and anomalous features too well, leading to confusion. OneNIP addresses this by allowing the model to reconstruct normal features while effectively restoring anomalies using just one normal image as a reference. Additionally, a supervised refiner is proposed to enhance pixel-level segmentation of anomalies, resulting in superior performance on multiple industry benchmarks."
                },
                "zh": {
                    "title": "用一个正常图像提示提升异常检测性能",
                    "desc": "本文提出了一种新的无监督重建网络方法，称为One Normal Image Prompt（OneNIP），用于多类异常检测。与传统方法不同，OneNIP只需一个正常图像提示即可重建或恢复异常特征，从而提高了异常检测的性能。该方法通过引入监督精炼器，利用真实正常图像和合成异常图像来回归重建误差，显著改善了像素级异常分割。实验结果表明，OneNIP在多个行业异常检测基准上超越了之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09263",
            "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
            "url": "https://huggingface.co/papers/2505.09263",
            "abstract": "Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.",
            "score": 1,
            "issue_id": 3791,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "1cde18cecb1ca918",
            "authors": [
                "Guan Gui",
                "Bin-Bin Gao",
                "Jun Liu",
                "Chengjie Wang",
                "Yunsheng Wu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent YouTu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09263.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#synthetic",
                    "#diffusion"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Генерация аномалий по нескольким примерам для улучшения их обнаружения",
                    "desc": "Статья представляет метод AnoGen для обнаружения аномалий в промышленной инспекции с использованием малого количества реальных аномальных образцов. Метод использует диффузионную модель для генерации реалистичных и разнообразных аномалий на основе нескольких реальных примеров. AnoGen включает три этапа: изучение распределения аномалий, генерация новых аномалий с помощью диффузионной модели и слабоконтролируемое обучение модели обнаружения аномалий. Эксперименты на наборе данных MVTec показали значительное улучшение производительности базовых моделей DRAEM и DesTSeg в задачах классификации и сегментации аномалий."
                },
                "en": {
                    "title": "Generating Realistic Anomalies for Better Detection",
                    "desc": "This paper presents a novel method called Few-shot Anomaly-driven Generation (AnoGen) for improving anomaly detection in industrial settings. The approach addresses the challenge of limited real anomaly samples by using a diffusion model to generate realistic anomalies based on a few existing examples. It consists of three stages: learning the anomaly distribution, guiding the generation of diverse anomalies, and training a weakly-supervised detection model. The results show significant improvements in both anomaly classification and segmentation tasks, demonstrating the effectiveness of the generated anomalies in enhancing model performance."
                },
                "zh": {
                    "title": "少样本驱动的异常生成，提升检测性能！",
                    "desc": "异常检测是一项实际且具有挑战性的任务，因为工业检测中异常样本稀缺。现有的一些异常检测方法通过合成噪声或外部数据来解决这个问题，但合成异常与真实异常之间存在较大的语义差距，导致检测性能较弱。为了解决这个问题，我们提出了一种少量样本驱动的异常生成方法（AnoGen），该方法利用少量真实异常指导扩散模型生成真实且多样的异常，从而有利于训练异常检测模型。我们的实验表明，生成的异常有效提高了模型在异常分类和分割任务上的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10320",
            "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.10320",
            "abstract": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.",
            "score": 0,
            "issue_id": 3791,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "61617ab29ad31cc7",
            "authors": [
                "Chenxi Whitehouse",
                "Tianlu Wang",
                "Ping Yu",
                "Xian Li",
                "Jason Weston",
                "Ilia Kulikov",
                "Swarnadeep Saha"
            ],
            "affiliations": [
                "FAIR at Meta",
                "GenAI at Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10320.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#hallucinations",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "J1: Революция в обучении моделей-судей через рассуждения",
                    "desc": "Статья представляет новый подход к обучению моделей-судей с использованием обучения с подкреплением, названный J1. Метод преобразует как проверяемые, так и непроверяемые запросы в задачи оценки с проверяемыми вознаграждениями, стимулирующими мышление и снижающими предвзятость суждений. J1 превосходит существующие модели размером 8B и 70B, включая модели, дистиллированные из DeepSeek-R1. Анализ показывает, что модели J1 улучшают суждения, формулируя критерии оценки, сравнивая с самостоятельно сгенерированными эталонными ответами и переоценивая правильность ответов модели."
                },
                "en": {
                    "title": "Empowering AI Judgment with J1: A Reinforcement Learning Breakthrough",
                    "desc": "This paper addresses the challenge of evaluating AI models by introducing J1, a reinforcement learning method designed to enhance the judgment capabilities of large language models (LLMs). J1 transforms both verifiable and non-verifiable prompts into judgment tasks that provide clear rewards, promoting better reasoning and reducing bias in evaluations. The results show that J1 outperforms existing models of similar sizes, demonstrating its effectiveness in training models to make more accurate judgments. The study also includes a detailed analysis of various training strategies and their impact on model performance, highlighting the importance of structured evaluation criteria and self-referential comparisons."
                },
                "zh": {
                    "title": "提升AI评估质量的关键：J1模型",
                    "desc": "本论文探讨了人工智能评估质量对AI进步的影响，并提出了一种名为J1的强化学习方法来训练大型语言模型（LLM）进行判断。J1通过将可验证和不可验证的提示转换为具有可验证奖励的判断任务，从而提高模型的思维能力并减少判断偏差。研究表明，J1在训练时表现优于现有的8B和70B模型，甚至在某些基准测试中超越了更大的模型。通过对比不同的训练策略和模型变体，发现J1模型能够更好地进行判断，学习评估标准并自我生成参考答案。"
                }
            }
        }
    ],
    "link_prev": "2025-05-15.html",
    "link_next": "2025-05-19.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "15.05",
        "en": "05/15",
        "zh": "5月15日"
    },
    "short_date_next": {
        "ru": "19.05",
        "en": "05/19",
        "zh": "5月19日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 4,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。",
        "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
        "pinyin": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。\n\nZhè piān wénzhāng tǎolùn le chuántǒng de shìjué yùcè rènwù shòuxiàn zài yùdìngyì lèibié, ér shìjué-yǔyán móxíng (VLMs) rú CLIP zài kāifàng cíhuì rènwù zhōng biǎoxiàn chū qiánlì. Rán'ér, CLIP zài mìjī yùcè zhōng biǎoxiàn bùjiā, yīnwèi qí júbù tèzhēng biǎoshì yǒuxiàn. Yánjiū fāxiàn, CLIP de túxiàng biāojiàn nán yǐ yǒuxiào jùhé kōngjiān huò yùyì xiāngguān qūyù de xìnxī. Wèi jiějué zhè yī wèntí, zuòzhě tíchū le DeCLIP kuàngjià, tōngguò jiěkǒu zìzhùyì mókuài huòdé “nèiróng” hé “shàngxìatú” tèzhēng, fēnbié tīgāo júbù biànbiéxìng hé kōngjiān yīzhìxìng. Shíyàn zhèngmíng, DeCLIP zài duōgè kāifàng cíhuì mìjī yùcè rènwù zhōng biǎoxiàn yōuyùn.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]",
        "trans": "This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains \"content\" and \"context\" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks.",
        "update_ts": "2025-05-15 09:12"
    }
}