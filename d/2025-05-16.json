{
    "date": {
        "ru": "15 Ğ¼Ğ°Ñ",
        "en": "May 15",
        "zh": "5æœˆ15æ—¥"
    },
    "time_utc": "2025-05-15 23:11",
    "weekday": 3,
    "issue_id": 3789,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 46,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… CLIP-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VAE-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP3-o, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¼ºè°ƒäº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„CLIPå›¾åƒç‰¹å¾ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å…ˆè¿›è¡Œå›¾åƒç†è§£è®­ç»ƒï¼Œå†è¿›è¡Œå›¾åƒç”Ÿæˆè®­ç»ƒçš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥ï¼Œä¿æŒäº†å›¾åƒç†è§£èƒ½åŠ›çš„åŒæ—¶å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†é«˜è´¨é‡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†BLIP3o-60kï¼Œä»¥æ”¯æŒå›¾åƒç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04410",
            "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
            "url": "https://huggingface.co/papers/2505.04410",
            "abstract": "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
            "score": 35,
            "issue_id": 3774,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ",
                "en": "May 7",
                "zh": "5æœˆ7æ—¥"
            },
            "hash": "24fee436fe24f861",
            "authors": [
                "Junjie Wang",
                "Bin Chen",
                "Yulin Li",
                "Bin Kang",
                "Yichi Chen",
                "Zhuotao Tian"
            ],
            "affiliations": [
                "International Research Institute for Artificial Intelligence, HIT, Shenzhen",
                "School of Computer Science and Technology, HIT, Shenzhen",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04410.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "DeCLIP: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ DeCLIP Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ CLIP Ğ½Ğ° 'ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ' Ğ¸ 'ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ' Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. DeCLIP Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Dense Visual Predictions with DeCLIP",
                    "desc": "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."
                },
                "zh": {
                    "title": "DeCLIPï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯†é›†é¢„æµ‹èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DeCLIPï¼Œæ—¨åœ¨æ”¹å–„è§†è§‰è¯­è¨€æ¨¡å‹CLIPåœ¨å¯†é›†è§†è§‰é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°CLIPçš„å›¾åƒæ ‡è®°åœ¨èšåˆç©ºé—´æˆ–è¯­ä¹‰ç›¸å…³åŒºåŸŸçš„ä¿¡æ¯æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç‰¹å¾ç¼ºä¹å±€éƒ¨å¯åŒºåˆ†æ€§å’Œç©ºé—´ä¸€è‡´æ€§ã€‚DeCLIPé€šè¿‡è§£è€¦è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ†åˆ«æå–â€œå†…å®¹â€å’Œâ€œä¸Šä¸‹æ–‡â€ç‰¹å¾ï¼Œä»è€Œæé«˜å±€éƒ¨å¯åŒºåˆ†æ€§å¹¶ä¿æŒç©ºé—´ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeCLIPåœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09343",
            "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
            "url": "https://huggingface.co/papers/2505.09343",
            "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
            "score": 24,
            "issue_id": 3773,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "3c249078ec32a334",
            "authors": [
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chong Ruan",
                "Damai Dai",
                "Huazuo Gao",
                "Jiashi Li",
                "Liyue Zhang",
                "Panpan Huang",
                "Shangyan Zhou",
                "Shirong Ma",
                "Wenfeng Liang",
                "Ying He",
                "Yuqing Wang",
                "Yuxuan Liu",
                "Y. X. Wei"
            ],
            "affiliations": [
                "DeepSeek-AI Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09343.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-V3/R1 Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ˜Ğ˜, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Multi-head Latent Attention (MLA) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Experts (MoE) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ FP8 Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ DeepSeek-V3 Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Innovating AI: Bridging Hardware and Model Design for Scalable Solutions",
                    "desc": "This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design."
                },
                "zh": {
                    "title": "ç¡¬ä»¶ä¸æ¨¡å‹å…±åŒè®¾è®¡ï¼Œæ¨åŠ¨AIåˆ›æ–°",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¡¬ä»¶æ¶æ„ä¸Šçš„é™åˆ¶ï¼ŒåŒ…æ‹¬å†…å­˜å®¹é‡ã€è®¡ç®—æ•ˆç‡å’Œäº’è¿å¸¦å®½ç­‰é—®é¢˜ã€‚DeepSeek-V3æ¨¡å‹åœ¨2048ä¸ªNVIDIA H800 GPUä¸Šè®­ç»ƒï¼Œå±•ç¤ºäº†ç¡¬ä»¶æ„ŸçŸ¥æ¨¡å‹å…±åŒè®¾è®¡å¦‚ä½•æœ‰æ•ˆè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è®ºæ–‡åˆ†æäº†DeepSeek-V3/R1æ¨¡å‹æ¶æ„åŠå…¶AIåŸºç¡€è®¾æ–½ï¼Œä»‹ç»äº†å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ã€ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„å’ŒFP8æ··åˆç²¾åº¦è®­ç»ƒç­‰åˆ›æ–°ã€‚æœ€åï¼Œä½œè€…ä¸å­¦æœ¯ç•Œå’Œå·¥ä¸šç•ŒåŒè¡Œæ¢è®¨äº†æœªæ¥ç¡¬ä»¶çš„å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†ç¡¬ä»¶ä¸æ¨¡å‹å…±åŒè®¾è®¡åœ¨æ»¡è¶³AIå·¥ä½œè´Ÿè½½éœ€æ±‚ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09358",
            "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\n  Image Analysis",
            "url": "https://huggingface.co/papers/2505.09358",
            "abstract": "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io",
            "score": 13,
            "issue_id": 3777,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "88afa5d56831ceb4",
            "authors": [
                "Bingxin Ke",
                "Kevin Qu",
                "Tianfu Wang",
                "Nando Metzger",
                "Shengyu Huang",
                "Bo Li",
                "Anton Obukhov",
                "Konrad Schindler"
            ],
            "affiliations": [
                "Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09358.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#synthetic",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸŒ¼",
                "ru": {
                    "title": "Marigold: Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Marigold - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Stable Diffusion. Marigold Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ½ĞµĞ¹. Marigold Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot."
                },
                "en": {
                    "title": "Unlocking Image Analysis with Pretrained Generative Models",
                    "desc": "This paper introduces Marigold, a set of conditional generative models designed to leverage pretrained latent diffusion models for various dense image analysis tasks. By fine-tuning these models, Marigold can effectively adapt to tasks like monocular depth estimation and surface normals prediction with minimal changes to the original architecture. The approach allows for training on small synthetic datasets, making it efficient and accessible for users with limited resources. Notably, Marigold achieves impressive zero-shot generalization, showcasing its potential in data-scarce environments."
                },
                "zh": {
                    "title": "Marigoldï¼šé«˜æ•ˆçš„å›¾åƒåˆ†æç”Ÿæˆæ¨¡å‹",
                    "desc": "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æˆåŠŸä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†å’Œå¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œè¿™äº›é¢„è®­ç»ƒæ¨¡å‹çš„è´¨é‡å¯¹æœ‰æ•ˆçš„è¿ç§»å­¦ä¹ è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å»å™ªæ‰©æ•£çš„æ½œåœ¨ç©ºé—´æ¨¡å‹ï¼Œå¼€åˆ›äº†ä¸€ç±»æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡å¸¦æ³¨é‡Šçš„å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡ä»‹ç»äº†Marigoldï¼Œä¸€ä¸ªæ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„å®¶æ—åŠå…¶å¾®è°ƒåè®®ï¼Œèƒ½å¤Ÿæå–é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶å°†å…¶é€‚åº”äºå¯†é›†å›¾åƒåˆ†æä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08787",
            "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations",
            "url": "https://huggingface.co/papers/2505.08787",
            "abstract": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.",
            "score": 12,
            "issue_id": 3779,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ",
                "en": "May 13",
                "zh": "5æœˆ13æ—¥"
            },
            "hash": "385e98801f0b0c4a",
            "authors": [
                "Hanjung Kim",
                "Jaehyun Kang",
                "Hyolim Kang",
                "Meedeum Cho",
                "Seon Joo Kim",
                "Youngwoon Lee"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.08787.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#transfer_learning",
                    "#dataset",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "UniSkill: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UniSkill - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. UniSkill Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging Human-Robot Learning with UniSkill",
                    "desc": "This paper introduces UniSkill, a framework designed to help robots learn skills by observing human actions without needing labeled data. It addresses the challenge of differences in appearance and capabilities between humans and robots by using large-scale video data that captures both. UniSkill creates skill representations that are not tied to any specific embodiment, allowing robots to apply learned skills from human videos to their own tasks. The results demonstrate that robots can effectively choose actions based on human video prompts, even when those prompts are new to them."
                },
                "zh": {
                    "title": "è·¨ä½“ç°æŠ€èƒ½å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "æ¨¡ä»¿æ˜¯äººç±»å­¦ä¹ æ–°ä»»åŠ¡çš„åŸºæœ¬æœºåˆ¶ï¼Œé€šè¿‡è§‚å¯Ÿå’Œæ¨¡ä»¿ä¸“å®¶æ¥å­¦ä¹ ã€‚ç„¶è€Œï¼Œå°†è¿™ç§èƒ½åŠ›åº”ç”¨äºæœºå™¨äººé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºäººç±»å’Œæœºå™¨äººçš„å¤–è§‚å’Œç‰©ç†èƒ½åŠ›å­˜åœ¨å›ºæœ‰å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†UniSkillï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å¤§è§„æ¨¡çš„è·¨ä½“ç°è§†é¢‘æ•°æ®ä¸­å­¦ä¹ ä¸ä½“ç°æ— å…³çš„æŠ€èƒ½è¡¨ç¤ºï¼Œè€Œæ— éœ€ä»»ä½•æ ‡ç­¾ï¼Œä»è€Œä½¿ä»äººç±»è§†é¢‘æç¤ºä¸­æå–çš„æŠ€èƒ½èƒ½å¤Ÿæœ‰æ•ˆè½¬ç§»åˆ°ä»…åœ¨æœºå™¨äººæ•°æ®ä¸Šè®­ç»ƒçš„æœºå™¨äººç­–ç•¥ä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›è·¨ä½“ç°æŠ€èƒ½èƒ½å¤ŸæˆåŠŸæŒ‡å¯¼æœºå™¨äººé€‰æ‹©åˆé€‚çš„åŠ¨ä½œï¼Œå³ä½¿åœ¨é¢å¯¹æœªè§è¿‡çš„è§†é¢‘æç¤ºæ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07849",
            "title": "SweRank: Software Issue Localization with Code Ranking",
            "url": "https://huggingface.co/papers/2505.07849",
            "abstract": "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.",
            "score": 6,
            "issue_id": 3777,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ",
                "en": "May 7",
                "zh": "5æœˆ7æ—¥"
            },
            "hash": "b9c609d9756c1056",
            "authors": [
                "Revanth Gangi Reddy",
                "Tarun Suresh",
                "JaeHyeok Doo",
                "Ye Liu",
                "Xuan Phi Nguyen",
                "Yingbo Zhou",
                "Semih Yavuz",
                "Caiming Xiong",
                "Heng Ji",
                "Shafiq Joty"
            ],
            "affiliations": [
                "KAIST AI",
                "Salesforce Research",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07849.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SweRank: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "SweRank - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SweLoc, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SweRank Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SweLoc Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "SweRank: Efficient Software Issue Localization with SweLoc Dataset",
                    "desc": "This paper presents SweRank, a new framework designed to improve software issue localization by efficiently retrieving and re-ranking code relevant to natural language issue descriptions. Unlike traditional models that struggle with the complexity of verbose queries, SweRank leverages a large-scale dataset called SweLoc, which contains real-world issue descriptions and their corresponding code changes. The empirical results indicate that SweRank outperforms existing models, including those based on costly closed-source large language models, in terms of accuracy and efficiency. This work not only introduces a novel approach to issue localization but also provides a valuable dataset for future research in the field."
                },
                "zh": {
                    "title": "é«˜æ•ˆè½¯ä»¶é—®é¢˜å®šä½çš„æ–°æ–¹æ³•",
                    "desc": "è½¯ä»¶é—®é¢˜å®šä½æ˜¯è¯†åˆ«ä¸è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼ˆå¦‚é”™è¯¯æŠ¥å‘Šã€åŠŸèƒ½è¯·æ±‚ï¼‰ç›¸å…³çš„ä»£ç ä½ç½®ï¼ˆæ–‡ä»¶ã€ç±»æˆ–å‡½æ•°ï¼‰çš„ä»»åŠ¡ï¼Œè¿™åœ¨è½¯ä»¶å¼€å‘ä¸­è‡³å…³é‡è¦ä½†è€—æ—¶ã€‚å°½ç®¡æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ–¹æ³•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†å’Œä¾èµ–äºå°é—­æºLLMï¼Œå¾€å¾€ä¼šå¯¼è‡´æ˜¾è‘—çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚ä¼ ç»Ÿçš„ä»£ç æ’åæ¨¡å‹é€šå¸¸é’ˆå¯¹æŸ¥è¯¢åˆ°ä»£ç æˆ–ä»£ç åˆ°ä»£ç çš„æ£€ç´¢è¿›è¡Œä¼˜åŒ–ï¼Œä½†åœ¨å¤„ç†å†—é•¿å’Œæè¿°æ€§å¤±è´¥çš„å®šä½æŸ¥è¯¢æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SweRankï¼Œä¸€ä¸ªé«˜æ•ˆä¸”æœ‰æ•ˆçš„è½¯ä»¶é—®é¢˜å®šä½æ£€ç´¢ä¸é‡æ’åæ¡†æ¶ï¼Œå¹¶æ„å»ºäº†SweLocï¼Œä¸€ä¸ªæ¥è‡ªå…¬å…±GitHubåº“çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®é—®é¢˜æè¿°åŠç›¸åº”çš„ä»£ç ä¿®æ”¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09558",
            "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
            "url": "https://huggingface.co/papers/2505.09558",
            "abstract": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.",
            "score": 5,
            "issue_id": 3783,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "74f9831e9fa216f6",
            "authors": [
                "Shengpeng Ji",
                "Tianle Liang",
                "Yangzhuo Li",
                "Jialong Zuo",
                "Minghui Fang",
                "Jinzheng He",
                "Yifu Chen",
                "Zhengqing Liu",
                "Ziyue Jiang",
                "Xize Cheng",
                "Siqi Zheng",
                "Jin Xu",
                "Junyang Lin",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09558.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#reasoning",
                    "#open_source",
                    "#rlhf",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "WavReward: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WavReward - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ²Ğ²Ğ¾Ğ´Ğ¾Ğ¼. WavReward Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ChatReward-30K, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "WavReward: Revolutionizing Evaluation for Spoken Dialogue Systems",
                    "desc": "This paper introduces WavReward, a novel evaluation model designed specifically for spoken dialogue systems. Unlike traditional text-based models, WavReward assesses both the intelligence (IQ) and emotional understanding (EQ) of dialogue systems using audio inputs. It employs a reinforcement learning approach with multi-sample feedback to enhance its evaluation capabilities. The model demonstrates significant improvements in accuracy and user preference over existing evaluation methods, showcasing its effectiveness in various dialogue scenarios."
                },
                "zh": {
                    "title": "WavRewardï¼šæå‡å¯¹è¯ç³»ç»Ÿè¯„ä¼°çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¨¡å‹WavRewardï¼Œç”¨äºè¯„ä¼°åŸºäºéŸ³é¢‘çš„å¯¹è¯ç³»ç»Ÿçš„è¡¨ç°ã€‚WavRewardç»“åˆäº†æ·±åº¦æ¨ç†è¿‡ç¨‹å’Œéçº¿æ€§å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶è¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„æ™ºå•†å’Œæƒ…å•†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ChatReward-30Kæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒWavRewardï¼Œæ¶µç›–äº†å¯¹è¯æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWavRewardåœ¨å¤šä¸ªå¯¹è¯åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„è¯„ä¼°æ¨¡å‹ï¼Œæå‡äº†å®¢è§‚å‡†ç¡®ç‡å’Œä¸»è§‚æµ‹è¯•çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12894",
            "title": "CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image",
            "url": "https://huggingface.co/papers/2502.12894",
            "abstract": "Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.",
            "score": 5,
            "issue_id": 3779,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "aeb3d023043e8a18",
            "authors": [
                "Kaixin Yao",
                "Longwen Zhang",
                "Xinhao Yan",
                "Yan Zeng",
                "Qixuan Zhang",
                "Wei Yang",
                "Lan Xu",
                "Jiayuan Gu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12894.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#graphs",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "CAST - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. CAST Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑÑ†ĞµĞ½Ğµ."
                },
                "en": {
                    "title": "CAST: Transforming 2D Images into Coherent 3D Scenes",
                    "desc": "The paper presents CAST, a new method for reconstructing high-quality 3D scenes from a single RGB image. It begins by extracting 2D segmentations and depth information, then uses a GPT-based model to analyze how objects relate spatially within the scene. CAST generates each object's geometry while addressing occlusions and partial data through a large-scale 3D generation model. Finally, it ensures physical consistency in the scene by optimizing object poses with a physics-aware correction step, making it useful for applications in robotics."
                },
                "zh": {
                    "title": "CASTï¼šä»å•å¼ RGBå›¾åƒé‡å»ºé«˜è´¨é‡3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCASTçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•å¼ RGBå›¾åƒä¸­é‡å»ºé«˜è´¨é‡çš„3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é¦–å…ˆæå–å¯¹è±¡çº§çš„2Dåˆ†å‰²å’Œç›¸å¯¹æ·±åº¦ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨åŸºäºGPTçš„æ¨¡å‹åˆ†æå¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä»¥å®ç°æ›´è¿è´¯çš„é‡å»ºã€‚CASTè¿˜é‡‡ç”¨äº†ä¸€ä¸ªè€ƒè™‘é®æŒ¡çš„å¤§è§„æ¨¡3Dç”Ÿæˆæ¨¡å‹ï¼Œç‹¬ç«‹ç”Ÿæˆæ¯ä¸ªå¯¹è±¡çš„å®Œæ•´å‡ ä½•å½¢çŠ¶ï¼Œå¹¶é€šè¿‡MAEå’Œç‚¹äº‘æ¡ä»¶æ¥å‡è½»é®æŒ¡å’Œéƒ¨åˆ†å¯¹è±¡ä¿¡æ¯çš„å½±å“ã€‚æœ€åï¼ŒCASTé€šè¿‡ç‰©ç†æ„ŸçŸ¥çš„æ ¡æ­£æ­¥éª¤ï¼Œç¡®ä¿ç”Ÿæˆçš„åœºæ™¯åœ¨ç‰©ç†ä¸Šä¿æŒä¸€è‡´ï¼Œé€‚ç”¨äºæœºå™¨äººç­‰é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09439",
            "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
            "url": "https://huggingface.co/papers/2505.09439",
            "abstract": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.",
            "score": 4,
            "issue_id": 3778,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "95b580ea2dcf9967",
            "authors": [
                "Andrew Rouditchenko",
                "Saurabhchand Bhati",
                "Edson Araujo",
                "Samuel Thomas",
                "Hilde Kuehne",
                "Rogerio Feris",
                "James Glass"
            ],
            "affiliations": [
                "Goethe University of Frankfurt",
                "IBM Research AI",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Tuebingen AI Center/University of Tuebingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09439.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ˜Ğ˜: Omni-R1 Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Omni-R1 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMAU Ğ¿Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ², Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Audio Understanding through Text-Based Reasoning",
                    "desc": "The paper introduces Omni-R1, a model that enhances the Qwen2.5-Omni multi-modal large language model (LLM) by fine-tuning it on an audio question answering dataset using the GRPO reinforcement learning method. This approach achieves state-of-the-art results on the MMAU benchmark, excelling in categories such as sounds, music, and speech. The authors found that the performance gains were largely due to improved text-based reasoning capabilities, even when audio data was not used during fine-tuning. Interestingly, they discovered that training on a text-only dataset could still boost the model's performance on audio tasks."
                },
                "zh": {
                    "title": "Omni-R1ï¼šéŸ³é¢‘é—®ç­”çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†Omni-R1ï¼Œå®ƒåœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šå¯¹æœ€æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹Qwen2.5-Omniè¿›è¡Œäº†å¾®è°ƒï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ æ–¹æ³•GRPOã€‚è¿™ä½¿å¾—Omni-R1åœ¨æœ€è¿‘çš„MMAUåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ•´ä½“å¹³å‡ç±»åˆ«ä¸Šéƒ½å–å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œæ— è®ºæ˜¯åœ¨Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²ä¸Šã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨æ²¡æœ‰éŸ³é¢‘çš„æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒï¼Œç«Ÿç„¶ä¹Ÿèƒ½æœ‰æ•ˆæé«˜éŸ³é¢‘ç›¸å…³çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜æ–‡æœ¬æ¨ç†èƒ½åŠ›çš„æå‡å¯¹æ•´ä½“è¡¨ç°æœ‰é‡è¦å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08455",
            "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
            "url": "https://huggingface.co/papers/2505.08455",
            "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.",
            "score": 4,
            "issue_id": 3773,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ",
                "en": "May 13",
                "zh": "5æœˆ13æ—¥"
            },
            "hash": "b7bc69bb40029690",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08455.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. VCRBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLM Ğ½Ğ° VCRBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Recognition-Reasoning Decomposition (RRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° VCRBench."
                },
                "en": {
                    "title": "Enhancing Video Causal Reasoning with RRD",
                    "desc": "This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘å› æœæ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "å°½ç®¡è§†é¢‘ç†è§£æŠ€æœ¯æœ‰æ‰€è¿›æ­¥ï¼Œä½†å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘åŸºç¡€çš„å› æœæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºè§†é¢‘åŸºç¡€çš„é•¿å½¢å¼å› æœæ¨ç†ï¼ˆVCRBenchï¼‰ï¼Œé€šè¿‡å¯¹æ—¥å¸¸æ´»åŠ¨çš„è§†é¢‘è¿›è¡Œå¤„ç†ï¼Œæµ‹è¯•LVLMsèƒ½å¦è¯†åˆ«ã€æ¨ç†å¹¶æ­£ç¡®æ’åºå®ç°ç‰¹å®šç›®æ ‡æ‰€éœ€çš„äº‹ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„æ–¹æ³•ï¼Œç§°ä¸ºè¯†åˆ«-æ¨ç†åˆ†è§£ï¼ˆRRDï¼‰ï¼Œå°†è§†é¢‘åŸºç¡€çš„å› æœæ¨ç†åˆ†ä¸ºè§†é¢‘è¯†åˆ«å’Œå› æœæ¨ç†ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLVLMsåœ¨å¤æ‚çš„é•¿å½¢å¼å› æœæ¨ç†ä»»åŠ¡ä¸­ä¸»è¦ä¾èµ–è¯­è¨€çŸ¥è¯†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09608",
            "title": "LightLab: Controlling Light Sources in Images with Diffusion Models",
            "url": "https://huggingface.co/papers/2505.09608",
            "abstract": "We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.",
            "score": 3,
            "issue_id": 3785,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "dbbc9a09c87e94d0",
            "authors": [
                "Nadav Magar",
                "Amir Hertz",
                "Eric Tabellion",
                "Yael Pritch",
                "Alex Rav-Acha",
                "Ariel Shamir",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "Google, Israel",
                "Google, United States of America",
                "Hebrew University of Jerusalem and Google, Israel",
                "Reichman University and Google, Israel",
                "Tel Aviv University and Google, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09608.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² ÑĞ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ†Ğ²ĞµÑ‚ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Mastering Light: Fine-Grained Control with Diffusion Models",
                    "desc": "This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques."
                },
                "zh": {
                    "title": "ç²¾ç¡®æ§åˆ¶å…‰æºçš„æ‰©æ•£é‡å…‰ç…§æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºå¯¹å›¾åƒä¸­çš„å…‰æºè¿›è¡Œç»†ç²’åº¦çš„å‚æ•°æ§åˆ¶ã€‚ç°æœ‰çš„é‡å…‰ç…§æ–¹æ³•è¦ä¹ˆä¾èµ–å¤šä¸ªè¾“å…¥è§†å›¾è¿›è¡Œé€†å‘æ¸²æŸ“ï¼Œè¦ä¹ˆæ— æ³•æä¾›å¯¹å…‰å˜åŒ–çš„æ˜ç¡®æ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€å°ç»„çœŸå®åŸå§‹ç…§ç‰‡å¯¹ä¸Šå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡åˆæˆæ¸²æŸ“å›¾åƒï¼Œæ¥å¼•å¯¼å…¶åœ¨é‡å…‰ç…§æ–¹é¢çš„çœŸå®æ„Ÿå…ˆéªŒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…‰ç¼–è¾‘ç»“æœä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†åŸºäºç”¨æˆ·åå¥½çš„ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04793",
            "title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\n  Recognition",
            "url": "https://huggingface.co/papers/2505.04793",
            "abstract": "Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/",
            "score": 2,
            "issue_id": 3778,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ",
                "en": "May 7",
                "zh": "5æœˆ7æ—¥"
            },
            "hash": "76d34246ac682dfd",
            "authors": [
                "Kailash A. Hambarde",
                "Nzakiese Mbongo",
                "Pavan Kumar MP",
                "Satish Mekewad",
                "Carolina Fernandes",
                "GÃ¶khan SilahtaroÄŸlu",
                "Alice Nithya",
                "Pawan Wasnik",
                "MD. Rashidunnabi",
                "Pranita Samale",
                "Hugo ProenÃ§a"
            ],
            "affiliations": [
                "Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal",
                "Istanbul Medipol University, Istanbul, Turkey",
                "J.N.N. College of Engineering, Shivamogga, Karnataka, India",
                "SRM Institute of Science and Technology, Kattankulathur, India",
                "School of Computational Sciences, SRTM University, Nanded, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04793.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "DetReIDX: Ğ¡Ñ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DetReIDX - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾-Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ´Ğ»Ñ 509 Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞµĞ¼Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¸Ñ… ĞºĞ°Ğ¼Ğ¿ÑƒÑĞ°Ñ… Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ĞµĞ½Ñ‚Ğ°Ñ…, Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ğ¾Ğ¹ ÑÑŠĞµĞ¼ĞºĞ¸ Ğ¾Ñ‚ 5,8 Ğ´Ğ¾ 120 Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². DetReIDX ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ ÑÑ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµÑÑĞ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ DetReIDX."
                },
                "en": {
                    "title": "DetReIDX: A Game-Changer for Real-World Person ReID Challenges",
                    "desc": "This paper presents DetReIDX, a new dataset designed to improve person reidentification (ReID) technology in real-world scenarios. It addresses the limitations of existing datasets by incorporating significant variability factors such as changes in clothing, viewpoint, and environmental conditions. DetReIDX includes over 13 million bounding boxes from 509 identities, collected across multiple sessions in diverse locations, making it a comprehensive resource for evaluating ReID systems. The authors demonstrate that current state-of-the-art methods struggle significantly under the conditions presented by DetReIDX, highlighting the need for robust solutions in person reidentification."
                },
                "zh": {
                    "title": "DetReIDXï¼šçœŸå®ä¸–ç•Œè¡Œäººé‡è¯†åˆ«çš„æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDetReIDXçš„å¤§è§„æ¨¡ç©ºä¸­-åœ°é¢è¡Œäººé‡è¯†åˆ«ï¼ˆReIDï¼‰æ•°æ®é›†ï¼Œæ—¨åœ¨æµ‹è¯•ReIDæŠ€æœ¯åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª509ä¸ªèº«ä»½çš„1300å¤šä¸‡ä¸ªè¾¹ç•Œæ¡†ï¼Œæ•°æ®æ”¶é›†è‡ªä¸‰ä¸ªå¤§æ´²çš„ä¸ƒä¸ªå¤§å­¦æ ¡å›­ï¼Œæ¶µç›–äº†ä¸åŒçš„æœè£…ã€å…‰ç…§å’Œä½ç½®å˜åŒ–ã€‚DetReIDXçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œå®ƒè®°å½•äº†è‡³å°‘ä¸¤ä¸ªä¸åŒæ—¥æœŸçš„ä¼šè¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°é•¿æœŸè¡Œäººé‡è¯†åˆ«çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨DetReIDXæ¡ä»¶ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ£€æµ‹å‡†ç¡®ç‡ä¸‹é™é«˜è¾¾80%ï¼ŒRank-1 ReIDä¸‹é™è¶…è¿‡70%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08084",
            "title": "Visually Interpretable Subtask Reasoning for Visual Question Answering",
            "url": "https://huggingface.co/papers/2505.08084",
            "abstract": "Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.",
            "score": 1,
            "issue_id": 3779,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "3bcc5517eeabe122",
            "authors": [
                "Yu Cheng",
                "Arushi Goel",
                "Hakan Bilen"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08084.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VISTAR: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VISTAR - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). VISTAR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ MLLM. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Subtask-of-Thought. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VISTAR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Visual Question Answering with VISTAR",
                    "desc": "This paper presents VISTAR, a new framework designed to improve the reasoning capabilities of multimodal large language models (MLLMs) when answering complex visual questions. VISTAR enhances interpretability by breaking down tasks into smaller subtasks and generating structured reasoning sequences, known as Subtask-of-Thought rationales. Unlike previous methods that relied on external models, VISTAR fine-tunes MLLMs directly, leading to better adaptation and accuracy on target data. Experimental results demonstrate that VISTAR not only boosts reasoning accuracy but also maintains a high level of interpretability in the model's outputs."
                },
                "zh": {
                    "title": "VISTARï¼šæå‡è§†è§‰æ¨ç†çš„å­ä»»åŠ¡é©±åŠ¨æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVISTARçš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚è§†è§‰é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚VISTARé€šè¿‡ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è§£é‡Šï¼Œé‡‡ç”¨å­ä»»åŠ¡é©±åŠ¨çš„è®­ç»ƒæ¡†æ¶ï¼Œå¢å¼ºäº†è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒVISTARå¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆç»“æ„åŒ–çš„æ€ç»´å­ä»»åŠ¡æ¨ç†åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVISTARåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06356",
            "title": "Understanding and Mitigating Toxicity in Image-Text Pretraining\n  Datasets: A Case Study on LLaVA",
            "url": "https://huggingface.co/papers/2505.06356",
            "abstract": "Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.",
            "score": 1,
            "issue_id": 3784,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ",
                "en": "May 9",
                "zh": "5æœˆ9æ—¥"
            },
            "hash": "159eecb990673138",
            "authors": [
                "Karthik Reddy Kanjula",
                "Surya Guthikonda",
                "Nahid Alam",
                "Shayekh Bin Islam"
            ],
            "affiliations": [
                "Bangladesh University of Engineering and Technology",
                "Cisco Meraki",
                "Cohere for AI Community",
                "Indiana University Bloomington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06356.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#ethics",
                    "#open_source",
                    "#data"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLaVA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾ ĞµĞ³Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 7500 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Building Safer Multimodal Models by Mitigating Toxicity",
                    "desc": "This paper addresses the issue of toxic content in pretraining datasets used for multimodal models, specifically focusing on the LLaVA image-text dataset. The authors analyze various categories of toxicity and how they appear in both images and text. They propose strategies to reduce this harmful content, resulting in a refined dataset that eliminates over 7,500 toxic image-text pairs. The study emphasizes the importance of filtering out toxic elements to create fairer and more responsible AI systems, and the new dataset is made available for further research."
                },
                "zh": {
                    "title": "æ„å»ºæ›´å®‰å…¨çš„å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†LLaVAå›¾åƒ-æ–‡æœ¬é¢„è®­ç»ƒæ•°æ®é›†ä¸­æœ‰æ¯’å†…å®¹çš„æ™®éæ€§ï¼Œåˆ†æäº†æœ‰å®³å†…å®¹åœ¨ä¸åŒæ¨¡æ€ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¯†åˆ«äº†å¸¸è§çš„æœ‰æ¯’ç±»åˆ«ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„å‡è½»ç­–ç•¥ï¼Œæœ€ç»ˆåˆ›å»ºäº†ä¸€ä¸ªç»è¿‡æ”¹è¿›çš„å»æ¯’æ€§æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ç§»é™¤äº†7531å¯¹æœ‰æ¯’çš„å›¾åƒ-æ–‡æœ¬é…å¯¹ï¼Œæä¾›äº†å®æ–½å¼ºå¤§æ¯’æ€§æ£€æµ‹ç®¡é“çš„æŒ‡å—ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä¸»åŠ¨è¯†åˆ«å’Œè¿‡æ»¤æœ‰æ¯’å†…å®¹çš„é‡è¦æ€§ï¼Œä»¥æ„å»ºæ›´è´Ÿè´£ä»»å’Œå…¬å¹³çš„å¤šæ¨¡æ€ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05587",
            "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2505.05587",
            "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
            "score": 1,
            "issue_id": 3784,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 Ğ¼Ğ°Ñ",
                "en": "May 8",
                "zh": "5æœˆ8æ—¥"
            },
            "hash": "c689f8e4175cc211",
            "authors": [
                "Peihao Wang",
                "Yuehao Wang",
                "Dilin Wang",
                "Sreyas Mohan",
                "Zhiwen Fan",
                "Lemeng Wu",
                "Ruisi Cai",
                "Yu-Ying Yeh",
                "Zhangyang Wang",
                "Qiang Liu",
                "Rakesh Ranjan"
            ],
            "affiliations": [
                "Meta Reality Labs",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05587.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 3D Gaussian Splatting: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ²Ñ‹ÑˆĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ 3D Gaussian Splatting (3DGS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ñ Ğ² 3DGS Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SteepGS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 50% ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ 3DGS, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing 3D Gaussian Splatting for Efficient Real-Time Rendering",
                    "desc": "This paper presents 3D Gaussian Splatting (3DGS), a technique for creating high-quality 3D views in real-time by using Gaussian primitives. It addresses the issue of excessive memory usage and slow performance caused by redundant point clouds during the densification process. The authors propose a theoretical framework to improve density control, which includes optimizing the number of Gaussian points and their opacity. The new method, SteepGS, effectively reduces the number of Gaussian points by about 50% while preserving rendering quality, making it more efficient for use on devices with limited resources."
                },
                "zh": {
                    "title": "é«˜æ•ˆç´§å‡‘çš„3Dé«˜æ–¯ç‚¹äº‘æ§åˆ¶",
                    "desc": "3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§ç”¨äºå®æ—¶é«˜åˆ†è¾¨ç‡æ–°è§†è§’åˆæˆçš„å¼ºå¤§æŠ€æœ¯ã€‚å®ƒé€šè¿‡å°†åœºæ™¯è¡¨ç¤ºä¸ºé«˜æ–¯åŸè¯­çš„æ··åˆä½“ï¼Œåˆ©ç”¨GPUå…‰æ …åŒ–ç®¡é“å®ç°é«˜æ•ˆæ¸²æŸ“å’Œé‡å»ºã€‚ä¸ºäº†ä¼˜åŒ–åœºæ™¯è¦†ç›–å’Œæ•æ‰ç»†èŠ‚ï¼Œ3DGSé‡‡ç”¨äº†å¯†åº¦åŒ–ç®—æ³•ç”Ÿæˆé¢å¤–ç‚¹ï¼Œä½†è¿™ä¼šå¯¼è‡´å†—ä½™ç‚¹äº‘ï¼Œå¢åŠ å†…å­˜ä½¿ç”¨å’Œå­˜å‚¨éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œæ”¹è¿›äº†3DGSä¸­çš„å¯†åº¦æ§åˆ¶ï¼Œæå‡ºäº†SteepGSæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†é«˜æ–¯ç‚¹æ•°é‡ï¼ŒåŒæ—¶ä¿æŒæ¸²æŸ“è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08910",
            "title": "Behind Maya: Building a Multilingual Vision Language Model",
            "url": "https://huggingface.co/papers/2505.08910",
            "abstract": "In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.",
            "score": 0,
            "issue_id": 3784,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ",
                "en": "May 13",
                "zh": "5æœˆ13æ—¥"
            },
            "hash": "617c510dcc5eefc7",
            "authors": [
                "Nahid Alam",
                "Karthik Reddy Kanjula",
                "Surya Guthikonda",
                "Timothy Chung",
                "Bala Krishna S Vegesna",
                "Abhipsha Das",
                "Anthony Susevski",
                "Ryan Sze-Yin Chan",
                "S M Iftekhar Uddin",
                "Shayekh Bin Islam",
                "Roshan Santhosh",
                "Snegha A",
                "Drishti Sharma",
                "Chen Liu",
                "Isha Chaturvedi",
                "Genta Indra Winata",
                "Ashvanth. S",
                "Snehanshu Mukherjee",
                "Alham Fikri Aji"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.08910.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Maya: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ VLM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Maya - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ²Ğ¾ÑĞµĞ¼ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞµÑ‘ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Maya ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº."
                },
                "en": {
                    "title": "Empowering Multilingual Understanding in Vision-Language Models",
                    "desc": "This paper presents Maya, a new open-source Multilingual Vision-Language Model (VLM) designed to improve performance in low-resource languages and diverse cultural contexts. It introduces a multilingual image-text pretraining dataset that includes eight languages, expanding upon the existing LLaVA dataset. The model aims to enhance understanding in vision-language tasks by incorporating cultural and linguistic nuances. By providing this resource, the authors hope to bridge the gap in VLM capabilities across different languages and cultures."
                },
                "zh": {
                    "title": "Mayaï¼šæå‡å¤šè¯­è¨€è§†è§‰ç†è§£çš„å¼€æºæ¨¡å‹",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿…é€Ÿå‘å±•ï¼Œå–å¾—äº†ä»¤äººç©ç›®çš„å­¦æœ¯æˆç»©ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€å’Œä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Mayaï¼Œä¸€ä¸ªå¼€æºçš„å¤šè¯­è¨€è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šåŸºäºLLaVAé¢„è®­ç»ƒæ•°æ®é›†çš„å…«ç§è¯­è¨€çš„å¤šè¯­è¨€å›¾åƒ-æ–‡æœ¬é¢„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥åŠæ”¯æŒè¿™äº›è¯­è¨€çš„å¤šè¯­è¨€å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œæå‡äº†è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„æ–‡åŒ–å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¼ ç»Ÿçš„è§†è§‰é¢„æµ‹ä»»åŠ¡å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œè€Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPåœ¨å¼€æ”¾è¯æ±‡ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒCLIPåœ¨å¯†é›†é¢„æµ‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå…¶å±€éƒ¨ç‰¹å¾è¡¨ç¤ºæœ‰é™ã€‚ç ”ç©¶å‘ç°ï¼ŒCLIPçš„å›¾åƒæ ‡è®°éš¾ä»¥æœ‰æ•ˆèšåˆç©ºé—´æˆ–è¯­ä¹‰ç›¸å…³åŒºåŸŸçš„ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†DeCLIPæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è‡ªæ³¨æ„æ¨¡å—è·å¾—â€œå†…å®¹â€å’Œâ€œä¸Šä¸‹æ–‡â€ç‰¹å¾ï¼Œåˆ†åˆ«æé«˜å±€éƒ¨è¾¨åˆ«æ€§å’Œç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDeCLIPåœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚",
        "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¼ ç»Ÿçš„è§†è§‰é¢„æµ‹ä»»åŠ¡å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œè€Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPåœ¨å¼€æ”¾è¯æ±‡ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒCLIPåœ¨å¯†é›†é¢„æµ‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå…¶å±€éƒ¨ç‰¹å¾è¡¨ç¤ºæœ‰é™ã€‚ç ”ç©¶å‘ç°ï¼ŒCLIPçš„å›¾åƒæ ‡è®°éš¾ä»¥æœ‰æ•ˆèšåˆç©ºé—´æˆ–è¯­ä¹‰ç›¸å…³åŒºåŸŸçš„ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†DeCLIPæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è‡ªæ³¨æ„æ¨¡å—è·å¾—â€œå†…å®¹â€å’Œâ€œä¸Šä¸‹æ–‡â€ç‰¹å¾ï¼Œåˆ†åˆ«æé«˜å±€éƒ¨è¾¨åˆ«æ€§å’Œç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDeCLIPåœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le chuÃ¡ntÇ’ng de shÃ¬juÃ© yÃ¹cÃ¨ rÃ¨nwÃ¹ shÃ²uxiÃ n zÃ i yÃ¹dÃ¬ngyÃ¬ lÃ¨ibiÃ©, Ã©r shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) rÃº CLIP zÃ i kÄifÃ ng cÃ­huÃ¬ rÃ¨nwÃ¹ zhÅng biÇoxiÃ n chÅ« qiÃ¡nlÃ¬. RÃ¡n'Ã©r, CLIP zÃ i mÃ¬jÄ« yÃ¹cÃ¨ zhÅng biÇoxiÃ n bÃ¹jiÄ, yÄ«nwÃ¨i qÃ­ jÃºbÃ¹ tÃ¨zhÄ“ng biÇoshÃ¬ yÇ’uxiÃ n. YÃ¡njiÅ« fÄxiÃ n, CLIP de tÃºxiÃ ng biÄojiÃ n nÃ¡n yÇ yÇ’uxiÃ o jÃ¹hÃ© kÅngjiÄn huÃ² yÃ¹yÃ¬ xiÄngguÄn qÅ«yÃ¹ de xÃ¬nxÄ«. WÃ¨i jiÄ›juÃ© zhÃ¨ yÄ« wÃ¨ntÃ­, zuÃ²zhÄ› tÃ­chÅ« le DeCLIP kuÃ ngjiÃ , tÅngguÃ² jiÄ›kÇ’u zÃ¬zhÃ¹yÃ¬ mÃ³kuÃ i huÃ²dÃ© â€œnÃ¨irÃ³ngâ€ hÃ© â€œshÃ ngxÃ¬atÃºâ€ tÃ¨zhÄ“ng, fÄ“nbiÃ© tÄ«gÄo jÃºbÃ¹ biÃ nbiÃ©xÃ¬ng hÃ© kÅngjiÄn yÄ«zhÃ¬xÃ¬ng. ShÃ­yÃ n zhÃ¨ngmÃ­ng, DeCLIP zÃ i duÅgÃ¨ kÄifÃ ng cÃ­huÃ¬ mÃ¬jÄ« yÃ¹cÃ¨ rÃ¨nwÃ¹ zhÅng biÇoxiÃ n yÅuyÃ¹n.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'predict'}, {'word': 'å—é™', 'pinyin': 'shÃ²u xiÃ n', 'trans': 'be limited'}, {'word': 'é¢„å®šä¹‰', 'pinyin': 'yÃ¹ dÃ¬ng yÃ¬', 'trans': 'predefined'}, {'word': 'ç±»åˆ«', 'pinyin': 'lÃ¨i biÃ©', 'trans': 'category'}, {'word': 'è§†è§‰-è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language model'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'perform'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'}, {'word': 'å¯†é›†', 'pinyin': 'mÃ¬ jÃ­', 'trans': 'dense'}, {'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'}, {'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'feature'}, {'word': 'è¡¨ç¤º', 'pinyin': 'biÇo shÃ¬', 'trans': 'represent'}, {'word': 'æœ‰é™', 'pinyin': 'yÇ’u xiÃ n', 'trans': 'limited'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'mark'}, {'word': 'èšåˆ', 'pinyin': 'jÃ¹ hÃ©', 'trans': 'aggregate'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅng jiÄn', 'trans': 'spatial'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantic'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'}, {'word': 'åŒºåŸŸ', 'pinyin': 'qÅ« yÃ¹', 'trans': 'region'}, {'word': 'ä¿¡æ¯', 'pinyin': 'xÃ¬n xÄ«', 'trans': 'information'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'è§£è€¦', 'pinyin': 'jiÄ› Ç’u', 'trans': 'decouple'}, {'word': 'è‡ªæ³¨æ„', 'pinyin': 'zÃ¬ zhÃ¹ yÃ¬', 'trans': 'self-attention'}, {'word': 'æ¨¡å—', 'pinyin': 'mÃ³ kuÃ i', 'trans': 'module'}, {'word': 'è·å¾—', 'pinyin': 'huÃ² dÃ©', 'trans': 'obtain'}, {'word': 'å†…å®¹', 'pinyin': 'nÃ¨i rÃ³ng', 'trans': 'content'}, {'word': 'ä¸Šä¸‹æ–‡', 'pinyin': 'shÃ ng xiÃ  wÃ©n', 'trans': 'context'}, {'word': 'è¾¨åˆ«æ€§', 'pinyin': 'biÃ n biÃ© xÃ¬ng', 'trans': 'discriminability'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¯æ˜', 'pinyin': 'zhÃ¨ng mÃ­ng', 'trans': 'prove'}, {'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'}]",
        "trans": "This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains \"content\" and \"context\" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks.",
        "update_ts": "2025-05-15 09:12"
    }
}