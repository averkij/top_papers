{
    "date": {
        "ru": "15 мая",
        "en": "May 15",
        "zh": "5月15日"
    },
    "time_utc": "2025-05-15 23:11",
    "weekday": 3,
    "issue_id": 3789,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 46,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "统一图像理解与生成的创新模型",
                    "desc": "本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04410",
            "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
            "url": "https://huggingface.co/papers/2505.04410",
            "abstract": "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
            "score": 35,
            "issue_id": 3774,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "24fee436fe24f861",
            "authors": [
                "Junjie Wang",
                "Bin Chen",
                "Yulin Li",
                "Bin Kang",
                "Yichi Chen",
                "Zhuotao Tian"
            ],
            "affiliations": [
                "International Research Institute for Artificial Intelligence, HIT, Shenzhen",
                "School of Computer Science and Technology, HIT, Shenzhen",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04410.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DeCLIP: Новый шаг к универсальному компьютерному зрению",
                    "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотного предсказания с открытым словарем. Авторы предлагают разделить модуль самовнимания CLIP на 'содержательные' и 'контекстные' признаки для повышения локальной различимости и пространственной согласованности. DeCLIP показывает значительное улучшение результатов в задачах обнаружения объектов и семантической сегментации по сравнению с существующими методами. Этот подход позволяет преодолеть ограничения предопределенных категорий в задачах компьютерного зрения."
                },
                "en": {
                    "title": "Enhancing Dense Visual Predictions with DeCLIP",
                    "desc": "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."
                },
                "zh": {
                    "title": "DeCLIP：提升视觉语言模型的密集预测能力",
                    "desc": "本论文提出了一种新的框架DeCLIP，旨在改善视觉语言模型CLIP在密集视觉预测任务中的表现。我们发现CLIP的图像标记在聚合空间或语义相关区域的信息时存在困难，导致特征缺乏局部可区分性和空间一致性。DeCLIP通过解耦自注意力模块，分别提取“内容”和“上下文”特征，从而提高局部可区分性并保持空间相关性。实验结果表明，DeCLIP在多个开放词汇密集预测任务中显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09343",
            "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
            "url": "https://huggingface.co/papers/2505.09343",
            "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
            "score": 24,
            "issue_id": 3773,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "3c249078ec32a334",
            "authors": [
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chong Ruan",
                "Damai Dai",
                "Huazuo Gao",
                "Jiashi Li",
                "Liyue Zhang",
                "Panpan Huang",
                "Shangyan Zhou",
                "Shirong Ma",
                "Wenfeng Liang",
                "Ying He",
                "Yuqing Wang",
                "Yuxuan Liu",
                "Y. X. Wei"
            ],
            "affiliations": [
                "DeepSeek-AI Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09343.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Совместное проектирование моделей и оборудования для масштабирования ИИ",
                    "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограничений современного аппаратного обеспечения при обучении больших языковых моделей (LLM). Авторы представляют ключевые инновации, включая Multi-head Latent Attention (MLA) для повышения эффективности использования памяти и архитектуру Mixture of Experts (MoE) для оптимизации баланса между вычислениями и коммуникацией. В работе также обсуждается применение смешанной точности FP8 для максимального использования возможностей оборудования и использование многоплоскостной сетевой топологии для минимизации накладных расходов на уровне кластера. На основе опыта разработки DeepSeek-V3 авторы предлагают направления для будущих исследований в области аппаратного обеспечения для ИИ."
                },
                "en": {
                    "title": "Innovating AI: Bridging Hardware and Model Design for Scalable Solutions",
                    "desc": "This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design."
                },
                "zh": {
                    "title": "硬件与模型共同设计，推动AI创新",
                    "desc": "这篇论文讨论了大型语言模型（LLMs）在硬件架构上的限制，包括内存容量、计算效率和互连带宽等问题。DeepSeek-V3模型在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型共同设计如何有效解决这些挑战。论文分析了DeepSeek-V3/R1模型架构及其AI基础设施，介绍了多头潜在注意力（MLA）、专家混合（MoE）架构和FP8混合精度训练等创新。最后，作者与学术界和工业界同行探讨了未来硬件的发展方向，强调了硬件与模型共同设计在满足AI工作负载需求中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09358",
            "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\n  Image Analysis",
            "url": "https://huggingface.co/papers/2505.09358",
            "abstract": "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io",
            "score": 13,
            "issue_id": 3777,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "88afa5d56831ceb4",
            "authors": [
                "Bingxin Ke",
                "Kevin Qu",
                "Tianfu Wang",
                "Nando Metzger",
                "Shengyu Huang",
                "Bo Li",
                "Anton Obukhov",
                "Konrad Schindler"
            ],
            "affiliations": [
                "Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09358.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#synthetic",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🌼",
                "ru": {
                    "title": "Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений",
                    "desc": "Статья представляет Marigold - семейство условных генеративных моделей и протокол дообучения, которые извлекают знания из предобученных моделей латентной диффузии, таких как Stable Diffusion. Marigold адаптирует эти модели для задач плотного анализа изображений, включая оценку монокулярной глубины, предсказание нормалей поверхности и внутреннюю декомпозицию. Модель требует минимальной модификации архитектуры предобученной модели латентной диффузии и обучается на небольших синтетических наборах данных на одном GPU в течение нескольких дней. Marigold демонстрирует передовую обобщающую способность в режиме zero-shot."
                },
                "en": {
                    "title": "Unlocking Image Analysis with Pretrained Generative Models",
                    "desc": "This paper introduces Marigold, a set of conditional generative models designed to leverage pretrained latent diffusion models for various dense image analysis tasks. By fine-tuning these models, Marigold can effectively adapt to tasks like monocular depth estimation and surface normals prediction with minimal changes to the original architecture. The approach allows for training on small synthetic datasets, making it efficient and accessible for users with limited resources. Notably, Marigold achieves impressive zero-shot generalization, showcasing its potential in data-scarce environments."
                },
                "zh": {
                    "title": "Marigold：高效的图像分析生成模型",
                    "desc": "深度学习在计算机视觉领域的成功依赖于大量标注数据集和强大的预训练模型。在数据稀缺的情况下，这些预训练模型的质量对有效的迁移学习至关重要。最近，文本到图像的生成模型，特别是使用去噪扩散的潜在空间模型，开创了一类新的基础模型，这些模型在大量带注释的图像数据集上进行训练。本文介绍了Marigold，一个条件生成模型的家族及其微调协议，能够提取预训练潜在扩散模型的知识，并将其适应于密集图像分析任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08787",
            "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations",
            "url": "https://huggingface.co/papers/2505.08787",
            "abstract": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.",
            "score": 12,
            "issue_id": 3779,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "385e98801f0b0c4a",
            "authors": [
                "Hanjung Kim",
                "Jaehyun Kang",
                "Hyolim Kang",
                "Meedeum Cho",
                "Seon Joo Kim",
                "Youngwoon Lee"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.08787.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#transfer_learning",
                    "#dataset",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "UniSkill: Обучение роботов человеческим навыкам без разметки",
                    "desc": "В статье представлен UniSkill - новый фреймворк для обучения роботов навыкам на основе видео с людьми. Он создает представления навыков, независимые от воплощения, используя масштабные видеоданные без разметки. UniSkill позволяет переносить навыки, извлеченные из видео с людьми, на политики роботов, обученные только на данных роботов. Эксперименты показали, что такой подход успешно направляет действия роботов даже для новых видеопромптов."
                },
                "en": {
                    "title": "Bridging Human-Robot Learning with UniSkill",
                    "desc": "This paper introduces UniSkill, a framework designed to help robots learn skills by observing human actions without needing labeled data. It addresses the challenge of differences in appearance and capabilities between humans and robots by using large-scale video data that captures both. UniSkill creates skill representations that are not tied to any specific embodiment, allowing robots to apply learned skills from human videos to their own tasks. The results demonstrate that robots can effectively choose actions based on human video prompts, even when those prompts are new to them."
                },
                "zh": {
                    "title": "跨体现技能学习的新突破",
                    "desc": "模仿是人类学习新任务的基本机制，通过观察和模仿专家来学习。然而，将这种能力应用于机器人面临重大挑战，因为人类和机器人的外观和物理能力存在固有差异。本文提出了UniSkill，一个新颖的框架，可以从大规模的跨体现视频数据中学习与体现无关的技能表示，而无需任何标签，从而使从人类视频提示中提取的技能能够有效转移到仅在机器人数据上训练的机器人策略中。我们的实验表明，这些跨体现技能能够成功指导机器人选择合适的动作，即使在面对未见过的视频提示时。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07849",
            "title": "SweRank: Software Issue Localization with Code Ranking",
            "url": "https://huggingface.co/papers/2505.07849",
            "abstract": "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.",
            "score": 6,
            "issue_id": 3777,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "b9c609d9756c1056",
            "authors": [
                "Revanth Gangi Reddy",
                "Tarun Suresh",
                "JaeHyeok Doo",
                "Ye Liu",
                "Xuan Phi Nguyen",
                "Yingbo Zhou",
                "Semih Yavuz",
                "Caiming Xiong",
                "Heng Ji",
                "Shafiq Joty"
            ],
            "affiliations": [
                "KAIST AI",
                "Salesforce Research",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07849.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#survey",
                    "#agents"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования",
                    "desc": "SweRank - это эффективная система для локализации проблем в программном обеспечении, использующая подход извлечения и переранжирования. Авторы создали большой датасет SweLoc, содержащий описания реальных проблем и соответствующие изменения кода из репозиториев GitHub. Эмпирические результаты показывают, что SweRank превосходит как предыдущие модели ранжирования, так и дорогостоящие системы на основе агентов, использующие закрытые языковые модели. SweLoc также демонстрирует свою полезность для улучшения существующих моделей извлечения и переранжирования в задаче локализации проблем."
                },
                "en": {
                    "title": "SweRank: Efficient Software Issue Localization with SweLoc Dataset",
                    "desc": "This paper presents SweRank, a new framework designed to improve software issue localization by efficiently retrieving and re-ranking code relevant to natural language issue descriptions. Unlike traditional models that struggle with the complexity of verbose queries, SweRank leverages a large-scale dataset called SweLoc, which contains real-world issue descriptions and their corresponding code changes. The empirical results indicate that SweRank outperforms existing models, including those based on costly closed-source large language models, in terms of accuracy and efficiency. This work not only introduces a novel approach to issue localization but also provides a valuable dataset for future research in the field."
                },
                "zh": {
                    "title": "高效软件问题定位的新方法",
                    "desc": "软件问题定位是识别与自然语言问题描述（如错误报告、功能请求）相关的代码位置（文件、类或函数）的任务，这在软件开发中至关重要但耗时。尽管最近基于大型语言模型（LLM）的代理方法显示出潜力，但由于复杂的多步骤推理和依赖于封闭源LLM，往往会导致显著的延迟和成本。传统的代码排名模型通常针对查询到代码或代码到代码的检索进行优化，但在处理冗长和描述性失败的定位查询时表现不佳。为了解决这个问题，我们提出了SweRank，一个高效且有效的软件问题定位检索与重排名框架，并构建了SweLoc，一个来自公共GitHub库的大规模数据集，包含真实问题描述及相应的代码修改。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09558",
            "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
            "url": "https://huggingface.co/papers/2505.09558",
            "abstract": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.",
            "score": 5,
            "issue_id": 3783,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "74f9831e9fa216f6",
            "authors": [
                "Shengpeng Ji",
                "Tianle Liang",
                "Yangzhuo Li",
                "Jialong Zuo",
                "Minghui Fang",
                "Jinzheng He",
                "Yifu Chen",
                "Zhengqing Liu",
                "Ziyue Jiang",
                "Xize Cheng",
                "Siqi Zheng",
                "Jin Xu",
                "Junyang Lin",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09558.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#reasoning",
                    "#open_source",
                    "#rlhf",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "WavReward: прорыв в оценке разговорных ИИ-систем",
                    "desc": "Статья представляет WavReward - модель оценки разговорных систем на основе аудио. Эта модель способна анализировать как интеллектуальные, так и эмоциональные аспекты диалоговых систем, работающих с речевым вводом. WavReward использует аудио языковые модели и механизмы обучения с подкреплением для глубокого анализа диалогов. Для обучения модели был создан датасет ChatReward-30K, охватывающий различные сценарии разговоров и акустические характеристики."
                },
                "en": {
                    "title": "WavReward: Revolutionizing Evaluation for Spoken Dialogue Systems",
                    "desc": "This paper introduces WavReward, a novel evaluation model designed specifically for spoken dialogue systems. Unlike traditional text-based models, WavReward assesses both the intelligence (IQ) and emotional understanding (EQ) of dialogue systems using audio inputs. It employs a reinforcement learning approach with multi-sample feedback to enhance its evaluation capabilities. The model demonstrates significant improvements in accuracy and user preference over existing evaluation methods, showcasing its effectiveness in various dialogue scenarios."
                },
                "zh": {
                    "title": "WavReward：提升对话系统评估的新方法",
                    "desc": "本文提出了一种新的评估模型WavReward，用于评估基于音频的对话系统的表现。WavReward结合了深度推理过程和非线性奖励机制，能够同时评估对话系统的智商和情商。我们还引入了ChatReward-30K数据集，用于训练WavReward，涵盖了对话模型的理解和生成能力。实验结果表明，WavReward在多个对话场景中显著优于现有的评估模型，提升了客观准确率和主观测试的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12894",
            "title": "CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image",
            "url": "https://huggingface.co/papers/2502.12894",
            "abstract": "Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.",
            "score": 5,
            "issue_id": 3779,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "aeb3d023043e8a18",
            "authors": [
                "Kaixin Yao",
                "Longwen Zhang",
                "Xinhao Yan",
                "Yan Zeng",
                "Qixuan Zhang",
                "Wei Yang",
                "Lan Xu",
                "Jiayuan Gu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12894.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#graphs",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Реконструкция 3D-сцен с учетом компонентов и физики из одного изображения",
                    "desc": "CAST - это новый метод реконструкции 3D-сцен из одного RGB-изображения. Он использует сегментацию объектов, анализ пространственных отношений с помощью GPT и генерацию полной геометрии каждого объекта. CAST применяет модель выравнивания для точного размещения объектов в сцене и физически корректного исправления их позиций. Метод решает проблемы окклюзий, проникновения объектов и обеспечивает реалистичное отображение физических взаимодействий в сцене."
                },
                "en": {
                    "title": "CAST: Transforming 2D Images into Coherent 3D Scenes",
                    "desc": "The paper presents CAST, a new method for reconstructing high-quality 3D scenes from a single RGB image. It begins by extracting 2D segmentations and depth information, then uses a GPT-based model to analyze how objects relate spatially within the scene. CAST generates each object's geometry while addressing occlusions and partial data through a large-scale 3D generation model. Finally, it ensures physical consistency in the scene by optimizing object poses with a physics-aware correction step, making it useful for applications in robotics."
                },
                "zh": {
                    "title": "CAST：从单张RGB图像重建高质量3D场景的创新方法",
                    "desc": "本文提出了一种名为CAST的新方法，用于从单张RGB图像中重建高质量的3D场景。该方法首先提取对象级的2D分割和相对深度信息，然后利用基于GPT的模型分析对象之间的空间关系，以实现更连贯的重建。CAST还采用了一个考虑遮挡的大规模3D生成模型，独立生成每个对象的完整几何形状，并通过MAE和点云条件来减轻遮挡和部分对象信息的影响。最后，CAST通过物理感知的校正步骤，确保生成的场景在物理上保持一致，适用于机器人等领域。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09439",
            "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
            "url": "https://huggingface.co/papers/2505.09439",
            "abstract": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.",
            "score": 4,
            "issue_id": 3778,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "95b580ea2dcf9967",
            "authors": [
                "Andrew Rouditchenko",
                "Saurabhchand Bhati",
                "Edson Araujo",
                "Samuel Thomas",
                "Hilde Kuehne",
                "Rogerio Feris",
                "James Glass"
            ],
            "affiliations": [
                "Goethe University of Frankfurt",
                "IBM Research AI",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Tuebingen AI Center/University of Tuebingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09439.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа",
                    "desc": "Исследователи представили Omni-R1 - мультимодальную языковую модель, улучшенную с помощью обучения с подкреплением. Модель достигла наилучших результатов в бенчмарке MMAU по распознаванию звуков, музыки и речи. Анализ показал, что улучшение производительности в основном связано с более качественным текстовым рассуждением. Интересно, что дообучение на текстовых данных без аудио также повысило эффективность модели в аудиозадачах."
                },
                "en": {
                    "title": "Enhancing Audio Understanding through Text-Based Reasoning",
                    "desc": "The paper introduces Omni-R1, a model that enhances the Qwen2.5-Omni multi-modal large language model (LLM) by fine-tuning it on an audio question answering dataset using the GRPO reinforcement learning method. This approach achieves state-of-the-art results on the MMAU benchmark, excelling in categories such as sounds, music, and speech. The authors found that the performance gains were largely due to improved text-based reasoning capabilities, even when audio data was not used during fine-tuning. Interestingly, they discovered that training on a text-only dataset could still boost the model's performance on audio tasks."
                },
                "zh": {
                    "title": "Omni-R1：音频问答的新突破",
                    "desc": "我们提出了Omni-R1，它在音频问答数据集上对最新的多模态大语言模型Qwen2.5-Omni进行了微调，采用了强化学习方法GRPO。这使得Omni-R1在最近的MMAU基准测试中达到了新的最先进性能。Omni-R1在声音、音乐、语音和整体平均类别上都取得了最高的准确率，无论是在Test-mini还是Test-full分割上。我们还发现，在没有音频的情况下进行微调，竟然也能有效提高音频相关的性能，这表明文本推理能力的提升对整体表现有重要影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08455",
            "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
            "url": "https://huggingface.co/papers/2505.08455",
            "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.",
            "score": 4,
            "issue_id": 3773,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "b7bc69bb40029690",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08455.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях",
                    "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Моделей (LVLM) к причинно-следственному рассуждению на основе видео. VCRBench использует процедурные видео повседневных действий с перемешанными шагами для проверки способности моделей идентифицировать и упорядочивать ключевые причинные события. Оценка современных LVLM на VCRBench показала их трудности с моделированием долгосрочных причинно-следственных зависимостей напрямую из визуальных наблюдений. Авторы предложили модульный подход Recognition-Reasoning Decomposition (RRD), который значительно повысил точность на VCRBench."
                },
                "en": {
                    "title": "Enhancing Video Causal Reasoning with RRD",
                    "desc": "This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains."
                },
                "zh": {
                    "title": "提升视频因果推理能力的关键",
                    "desc": "尽管视频理解技术有所进步，但大型视频语言模型（LVLMs）在视频基础的因果推理方面的能力仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准测试，名为视频基础的长形式因果推理（VCRBench），通过对日常活动的视频进行处理，测试LVLMs能否识别、推理并正确排序实现特定目标所需的事件。我们还提出了一种模块化的方法，称为识别-推理分解（RRD），将视频基础的因果推理分为视频识别和因果推理两个子任务，从而显著提高了模型的准确性。我们的分析表明，LVLMs在复杂的长形式因果推理任务中主要依赖语言知识。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09608",
            "title": "LightLab: Controlling Light Sources in Images with Diffusion Models",
            "url": "https://huggingface.co/papers/2505.09608",
            "abstract": "We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.",
            "score": 3,
            "issue_id": 3785,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "dbbc9a09c87e94d0",
            "authors": [
                "Nadav Magar",
                "Amir Hertz",
                "Eric Tabellion",
                "Yael Pritch",
                "Alex Rav-Acha",
                "Ariel Shamir",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "Google, Israel",
                "Google, United States of America",
                "Hebrew University of Jerusalem and Google, Israel",
                "Reichman University and Google, Israel",
                "Tel Aviv University and Google, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09608.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Прецизионное управление освещением с помощью диффузионных моделей",
                    "desc": "Авторы представляют новый метод на основе диффузии для точного контроля источников света на изображении. Модель дообучается на парах реальных фотографий и синтетических изображениях, используя фотореалистичные возможности диффузионных моделей. Метод позволяет точно контролировать интенсивность и цвет освещения. По результатам пользовательских оценок, он превосходит существующие подходы к редактированию освещения."
                },
                "en": {
                    "title": "Mastering Light: Fine-Grained Control with Diffusion Models",
                    "desc": "This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques."
                },
                "zh": {
                    "title": "精确控制光源的扩散重光照方法",
                    "desc": "我们提出了一种简单而有效的基于扩散的方法，用于对图像中的光源进行细粒度的参数控制。现有的重光照方法要么依赖多个输入视图进行逆向渲染，要么无法提供对光变化的明确控制。我们通过在一小组真实原始照片对上微调扩散模型，并结合大规模合成渲染图像，来引导其在重光照方面的真实感先验。最终，我们展示了该方法在光编辑结果上的优越性，超越了基于用户偏好的现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04793",
            "title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\n  Recognition",
            "url": "https://huggingface.co/papers/2505.04793",
            "abstract": "Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/",
            "score": 2,
            "issue_id": 3778,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "76d34246ac682dfd",
            "authors": [
                "Kailash A. Hambarde",
                "Nzakiese Mbongo",
                "Pavan Kumar MP",
                "Satish Mekewad",
                "Carolina Fernandes",
                "Gökhan Silahtaroğlu",
                "Alice Nithya",
                "Pawan Wasnik",
                "MD. Rashidunnabi",
                "Pranita Samale",
                "Hugo Proença"
            ],
            "affiliations": [
                "Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal",
                "Istanbul Medipol University, Istanbul, Turkey",
                "J.N.N. College of Engineering, Shivamogga, Karnataka, India",
                "SRM Institute of Science and Technology, Kattankulathur, India",
                "School of Computational Sciences, SRTM University, Nanded, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04793.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "DetReIDX: Стресс-тест для реидентификации людей в реальном мире",
                    "desc": "Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включает более 13 миллионов ограничивающих рамок для 509 личностей, собранных в семи университетских кампусах на трех континентах, с высотой съемки от 5,8 до 120 метров. DetReIDX специально разработан для стресс-тестирования алгоритмов реидентификации в реальных условиях, включая изменения одежды, освещения и местоположения между сессиями. Эксперименты показали, что современные методы обнаружения и реидентификации людей значительно ухудшают свою производительность при работе с данными DetReIDX."
                },
                "en": {
                    "title": "DetReIDX: A Game-Changer for Real-World Person ReID Challenges",
                    "desc": "This paper presents DetReIDX, a new dataset designed to improve person reidentification (ReID) technology in real-world scenarios. It addresses the limitations of existing datasets by incorporating significant variability factors such as changes in clothing, viewpoint, and environmental conditions. DetReIDX includes over 13 million bounding boxes from 509 identities, collected across multiple sessions in diverse locations, making it a comprehensive resource for evaluating ReID systems. The authors demonstrate that current state-of-the-art methods struggle significantly under the conditions presented by DetReIDX, highlighting the need for robust solutions in person reidentification."
                },
                "zh": {
                    "title": "DetReIDX：真实世界行人重识别的新挑战",
                    "desc": "本文介绍了一种名为DetReIDX的大规模空中-地面行人重识别（ReID）数据集，旨在测试ReID技术在真实世界条件下的表现。该数据集包含来自509个身份的1300多万个边界框，数据收集自三个大洲的七个大学校园，涵盖了不同的服装、光照和位置变化。DetReIDX的独特之处在于，它记录了至少两个不同日期的会话，能够有效评估长期行人重识别的能力。研究表明，当前最先进的方法在DetReIDX条件下的表现显著下降，检测准确率下降高达80%，Rank-1 ReID下降超过70%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08084",
            "title": "Visually Interpretable Subtask Reasoning for Visual Question Answering",
            "url": "https://huggingface.co/papers/2505.08084",
            "abstract": "Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.",
            "score": 1,
            "issue_id": 3779,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "3bcc5517eeabe122",
            "authors": [
                "Yu Cheng",
                "Arushi Goel",
                "Hakan Bilen"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08084.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VISTAR: Интерпретируемые рассуждения в мультимодальных моделях",
                    "desc": "Статья представляет VISTAR - модель для интерпретируемых рассуждений на основе подзадач в мультимодальных больших языковых моделях (MLLM). VISTAR улучшает точность рассуждений и интерпретируемость, генерируя текстовые и визуальные объяснения внутри MLLM. Модель дообучается для создания структурированных последовательностей рассуждений Subtask-of-Thought. Эксперименты показывают, что VISTAR повышает точность рассуждений, сохраняя интерпретируемость."
                },
                "en": {
                    "title": "Enhancing Visual Question Answering with VISTAR",
                    "desc": "This paper presents VISTAR, a new framework designed to improve the reasoning capabilities of multimodal large language models (MLLMs) when answering complex visual questions. VISTAR enhances interpretability by breaking down tasks into smaller subtasks and generating structured reasoning sequences, known as Subtask-of-Thought rationales. Unlike previous methods that relied on external models, VISTAR fine-tunes MLLMs directly, leading to better adaptation and accuracy on target data. Experimental results demonstrate that VISTAR not only boosts reasoning accuracy but also maintains a high level of interpretability in the model's outputs."
                },
                "zh": {
                    "title": "VISTAR：提升视觉推理的子任务驱动模型",
                    "desc": "本文介绍了一种名为VISTAR的模型，旨在提高多模态大语言模型（MLLMs）在复杂视觉问题上的推理能力。VISTAR通过生成文本和视觉解释，采用子任务驱动的训练框架，增强了解释性和推理能力。与依赖外部模型的方法不同，VISTAR对MLLMs进行微调，以生成结构化的思维子任务推理序列。实验结果表明，VISTAR在两个基准测试中均提高了推理准确性，同时保持了解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06356",
            "title": "Understanding and Mitigating Toxicity in Image-Text Pretraining\n  Datasets: A Case Study on LLaVA",
            "url": "https://huggingface.co/papers/2505.06356",
            "abstract": "Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.",
            "score": 1,
            "issue_id": 3784,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 мая",
                "en": "May 9",
                "zh": "5月9日"
            },
            "hash": "159eecb990673138",
            "authors": [
                "Karthik Reddy Kanjula",
                "Surya Guthikonda",
                "Nahid Alam",
                "Shayekh Bin Islam"
            ],
            "affiliations": [
                "Bangladesh University of Engineering and Technology",
                "Cisco Meraki",
                "Cohere for AI Community",
                "Indiana University Bloomington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06356.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#ethics",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🧹",
                "ru": {
                    "title": "Очистка данных для этичного ИИ",
                    "desc": "Исследователи изучили проблему токсичности в наборе данных LLaVA для предобучения мультимодальных моделей. Они провели анализ различных категорий токсичного контента и предложили стратегии по его устранению. В результате был создан очищенный набор данных, из которого удалено более 7500 токсичных пар изображение-текст. Авторы подчеркивают важность активного выявления и фильтрации вредоносного контента для создания более ответственных мультимодальных систем."
                },
                "en": {
                    "title": "Building Safer Multimodal Models by Mitigating Toxicity",
                    "desc": "This paper addresses the issue of toxic content in pretraining datasets used for multimodal models, specifically focusing on the LLaVA image-text dataset. The authors analyze various categories of toxicity and how they appear in both images and text. They propose strategies to reduce this harmful content, resulting in a refined dataset that eliminates over 7,500 toxic image-text pairs. The study emphasizes the importance of filtering out toxic elements to create fairer and more responsible AI systems, and the new dataset is made available for further research."
                },
                "zh": {
                    "title": "构建更安全的多模态模型",
                    "desc": "本论文研究了LLaVA图像-文本预训练数据集中有毒内容的普遍性，分析了有害内容在不同模态中的表现。我们识别了常见的有毒类别，并提出了针对性的减轻策略，最终创建了一个经过改进的去毒性数据集。该数据集移除了7531对有毒的图像-文本配对，提供了实施强大毒性检测管道的指南。我们的研究强调了主动识别和过滤有毒内容的重要性，以构建更负责任和公平的多模态系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05587",
            "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2505.05587",
            "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
            "score": 1,
            "issue_id": 3784,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "c689f8e4175cc211",
            "authors": [
                "Peihao Wang",
                "Yuehao Wang",
                "Dilin Wang",
                "Sreyas Mohan",
                "Zhiwen Fan",
                "Lemeng Wu",
                "Ruisi Cai",
                "Yu-Ying Yeh",
                "Zhangyang Wang",
                "Qiang Liu",
                "Rakesh Ranjan"
            ],
            "affiliations": [
                "Meta Reality Labs",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05587.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация 3D Gaussian Splatting: меньше точек, выше эффективность",
                    "desc": "Статья представляет теоретическую основу для улучшения контроля плотности в методе 3D Gaussian Splatting (3DGS). Авторы анализируют процесс уплотнения в 3DGS и предлагают оптимизационный подход для определения необходимых условий и параметров генерации новых гауссовых примитивов. На основе этого анализа разработан метод SteepGS, который позволяет достичь 50% сокращения количества гауссовых точек без ухудшения качества рендеринга. Предложенный подход значительно повышает эффективность и масштабируемость 3DGS, особенно для устройств с ограниченными ресурсами."
                },
                "en": {
                    "title": "Optimizing 3D Gaussian Splatting for Efficient Real-Time Rendering",
                    "desc": "This paper presents 3D Gaussian Splatting (3DGS), a technique for creating high-quality 3D views in real-time by using Gaussian primitives. It addresses the issue of excessive memory usage and slow performance caused by redundant point clouds during the densification process. The authors propose a theoretical framework to improve density control, which includes optimizing the number of Gaussian points and their opacity. The new method, SteepGS, effectively reduces the number of Gaussian points by about 50% while preserving rendering quality, making it more efficient for use on devices with limited resources."
                },
                "zh": {
                    "title": "高效紧凑的3D高斯点云控制",
                    "desc": "3D高斯点云（3DGS）是一种用于实时高分辨率新视角合成的强大技术。它通过将场景表示为高斯原语的混合体，利用GPU光栅化管道实现高效渲染和重建。为了优化场景覆盖和捕捉细节，3DGS采用了密度化算法生成额外点，但这会导致冗余点云，增加内存使用和存储需求。为了解决这个问题，我们提出了一个理论框架，改进了3DGS中的密度控制，提出了SteepGS方法，显著减少了高斯点数量，同时保持渲染质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08910",
            "title": "Behind Maya: Building a Multilingual Vision Language Model",
            "url": "https://huggingface.co/papers/2505.08910",
            "abstract": "In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.",
            "score": 0,
            "issue_id": 3784,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "617c510dcc5eefc7",
            "authors": [
                "Nahid Alam",
                "Karthik Reddy Kanjula",
                "Surya Guthikonda",
                "Timothy Chung",
                "Bala Krishna S Vegesna",
                "Abhipsha Das",
                "Anthony Susevski",
                "Ryan Sze-Yin Chan",
                "S M Iftekhar Uddin",
                "Shayekh Bin Islam",
                "Roshan Santhosh",
                "Snegha A",
                "Drishti Sharma",
                "Chen Liu",
                "Isha Chaturvedi",
                "Genta Indra Winata",
                "Ashvanth. S",
                "Snehanshu Mukherjee",
                "Alham Fikri Aji"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.08910.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Maya: мультиязычная VLM для преодоления языковых и культурных барьеров",
                    "desc": "Статья представляет Maya - открытую мультиязычную модель компьютерного зрения и обработки естественного языка (VLM). Модель обучена на наборе данных, включающем восемь языков, что расширяет её возможности для низкоресурсных языков и различных культурных контекстов. Maya улучшает понимание культурных и лингвистических аспектов в задачах, связанных со зрением и языком. Авторы предоставляют открытый доступ к коду модели для дальнейших исследований и разработок."
                },
                "en": {
                    "title": "Empowering Multilingual Understanding in Vision-Language Models",
                    "desc": "This paper presents Maya, a new open-source Multilingual Vision-Language Model (VLM) designed to improve performance in low-resource languages and diverse cultural contexts. It introduces a multilingual image-text pretraining dataset that includes eight languages, expanding upon the existing LLaVA dataset. The model aims to enhance understanding in vision-language tasks by incorporating cultural and linguistic nuances. By providing this resource, the authors hope to bridge the gap in VLM capabilities across different languages and cultures."
                },
                "zh": {
                    "title": "Maya：提升多语言视觉理解的开源模型",
                    "desc": "近年来，大型视觉语言模型（VLM）迅速发展，取得了令人瞩目的学术成绩。然而，这些模型在低资源语言和不同文化背景下的表现仍然不足。为了解决这些问题，我们推出了Maya，一个开源的多语言视觉语言模型。我们的贡献包括：基于LLaVA预训练数据集的八种语言的多语言图像-文本预训练数据集，以及支持这些语言的多语言图像-文本模型，提升了视觉语言任务中的文化和语言理解能力。"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5月14日"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。",
        "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
        "pinyin": "这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。\n\nZhè piān wénzhāng tǎolùn le chuántǒng de shìjué yùcè rènwù shòuxiàn zài yùdìngyì lèibié, ér shìjué-yǔyán móxíng (VLMs) rú CLIP zài kāifàng cíhuì rènwù zhōng biǎoxiàn chū qiánlì. Rán'ér, CLIP zài mìjī yùcè zhōng biǎoxiàn bùjiā, yīnwèi qí júbù tèzhēng biǎoshì yǒuxiàn. Yánjiū fāxiàn, CLIP de túxiàng biāojiàn nán yǐ yǒuxiào jùhé kōngjiān huò yùyì xiāngguān qūyù de xìnxī. Wèi jiějué zhè yī wèntí, zuòzhě tíchū le DeCLIP kuàngjià, tōngguò jiěkǒu zìzhùyì mókuài huòdé “nèiróng” hé “shàngxìatú” tèzhēng, fēnbié tīgāo júbù biànbiéxìng hé kōngjiān yīzhìxìng. Shíyàn zhèngmíng, DeCLIP zài duōgè kāifàng cíhuì mìjī yùcè rènwù zhōng biǎoxiàn yōuyùn.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]",
        "trans": "This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains \"content\" and \"context\" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks.",
        "update_ts": "2025-05-15 09:12"
    }
}