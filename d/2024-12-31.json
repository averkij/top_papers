{
    "date": {
        "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 31",
        "zh": "12æœˆ31æ—¥"
    },
    "time_utc": "2024-12-31 05:10",
    "weekday": 1,
    "issue_id": 1406,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.20070",
            "title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
            "url": "https://huggingface.co/papers/2412.20070",
            "abstract": "Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks, providing limited guidance on selecting datasets to enhance specific tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG)-the ability of models to understand novel combinations by recombining learned elements-as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG. Therefore, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and delivers consistent performance across different backbones, highlighting its versatility and broad applicability. Med-MAT is publicly available at https://github.com/FreedomIntelligence/Med-MAT.",
            "score": 8,
            "issue_id": 1405,
            "pub_date": "2024-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "34f9c6ec4611d6ec",
            "authors": [
                "Zhenyang Cai",
                "Junying Chen",
                "Rongsheng Wang",
                "Weihong Wang",
                "Yonglin Deng",
                "Dingjie Song",
                "Yize Chen",
                "Zixu Zhang",
                "Benyou Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.20070.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#healthcare",
                    "#open_source",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ MLLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (CG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Med-MAT Ğ¸Ğ· 106 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MLLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ CG Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CG Ğ´Ğ»Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Medical Insights with Compositional Generalization",
                    "desc": "This paper explores the use of multimodal large language models (MLLMs) in the medical field, focusing on how they can generalize from limited data. It highlights the advantages of multi-task training over single-task training, emphasizing the importance of understanding the relationships between different tasks. The authors introduce compositional generalization (CG) as a framework to enhance the model's ability to interpret new combinations of medical images. They created a dataset called Med-MAT, which consists of 106 medical datasets, and found that CG significantly improves the performance of MLLMs, especially in scenarios with scarce data."
                },
                "zh": {
                    "title": "ç»„åˆæ³›åŒ–åŠ©åŠ›åŒ»å­¦å›¾åƒç†è§£",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰é‡è¦æ½œåŠ›ï¼Œä½†åœ¨æŸäº›åŒ»å­¦é¢†åŸŸçš„æ•°æ®ä¸è¶³é™åˆ¶äº†å…¶èƒ½åŠ›ã€‚å½“å‰ç ”ç©¶è¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒä¼˜äºå•ä»»åŠ¡è®­ç»ƒï¼Œå› ä¸ºä¸åŒä»»åŠ¡å¯ä»¥ç›¸äº’ä¿ƒè¿›ï¼Œä½†å¾€å¾€å¿½è§†äº†è¿™äº›ä»»åŠ¡ä¹‹é—´çš„å†…éƒ¨å…³ç³»ã€‚æˆ‘ä»¬é‡‡ç”¨ç»„åˆæ³›åŒ–ï¼ˆCGï¼‰ä½œä¸ºæŒ‡å¯¼æ¡†æ¶ï¼Œåˆ†ææ¨¡å‹å¦‚ä½•ç†è§£æ–°ç»„åˆçš„èƒ½åŠ›ï¼Œå¹¶ç»„å»ºäº†106ä¸ªåŒ»å­¦æ•°æ®é›†ä»¥åˆ›å»ºMed-MATè¿›è¡Œå…¨é¢å®éªŒã€‚å®éªŒç»“æœç¡®è®¤ï¼ŒMLLMsèƒ½å¤Ÿåˆ©ç”¨CGç†è§£æœªè§è¿‡çš„åŒ»å­¦å›¾åƒï¼Œå¹¶ä¸”CGæ˜¯å¤šä»»åŠ¡è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„æ³›åŒ–çš„ä¸»è¦é©±åŠ¨å› ç´ ä¹‹ä¸€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21079",
            "title": "Edicho: Consistent Image Editing in the Wild",
            "url": "https://huggingface.co/papers/2412.21079",
            "abstract": "As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundamental design principle of using explicit image correspondence to direct editing. Specifically, the key components include an attention manipulation module and a carefully refined classifier-free guidance (CFG) denoising strategy, both of which take into account the pre-estimated correspondence. Such an inference-time algorithm enjoys a plug-and-play nature and is compatible to most diffusion-based editing methods, such as ControlNet and BrushNet. Extensive results demonstrate the efficacy of Edicho in consistent cross-image editing under diverse settings. We will release the code to facilitate future studies.",
            "score": 7,
            "issue_id": 1405,
            "pub_date": "2024-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "8068418a331b2086",
            "authors": [
                "Qingyan Bai",
                "Hao Ouyang",
                "Yinghao Xu",
                "Qiuyu Wang",
                "Ceyuan Yang",
                "Ka Leong Cheng",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group",
                "CUHK",
                "HKUST",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21079.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Edicho: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Edicho - Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ControlNet Ğ¸ BrushNet. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Edicho Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Edicho: Consistent Image Editing Made Easy with Diffusion Models",
                    "desc": "This paper introduces Edicho, a novel approach for consistent editing of images that addresses challenges like varying object poses and lighting. It utilizes diffusion models without the need for prior training, focusing on explicit image correspondence to guide the editing process. Key innovations include an attention manipulation module and a refined classifier-free guidance denoising strategy, which enhance the editing quality by considering pre-estimated correspondences. The method is designed to be easily integrated with existing diffusion-based editing techniques, showing strong performance across different scenarios."
                },
                "zh": {
                    "title": "Edichoï¼šæ— è®­ç»ƒä¸€è‡´æ€§å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "Edicho æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³åœ¨ä¸åŒç¯å¢ƒä¸‹è¿›è¡Œä¸€è‡´æ€§å›¾åƒç¼–è¾‘çš„æŒ‘æˆ˜ã€‚å®ƒçš„è®¾è®¡åŸåˆ™æ˜¯åˆ©ç”¨æ˜¾å¼å›¾åƒå¯¹åº”å…³ç³»æ¥æŒ‡å¯¼ç¼–è¾‘ï¼Œç¡®ä¿åœ¨ä¸åŒçš„æ‹æ‘„æ¡ä»¶ä¸‹ä¿æŒä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªæ³¨æ„åŠ›æ“ä½œæ¨¡å—å’Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ— åˆ†ç±»å™¨å¼•å¯¼å»å™ªç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é¢„ä¼°çš„å¯¹åº”å…³ç³»ã€‚Edicho å…·æœ‰å³æ’å³ç”¨çš„ç‰¹æ€§ï¼Œå…¼å®¹å¤§å¤šæ•°åŸºäºæ‰©æ•£çš„ç¼–è¾‘æ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šç§è®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18525",
            "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
            "url": "https://huggingface.co/papers/2412.18525",
            "abstract": "Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we explore the idea that CV adopts discrete and terminological task definitions (\\eg, ``image segmentation''), which may be a key barrier to zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks--due to these terminological definitions--deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ``image input to explanatory instruction to output'' triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository.",
            "score": 4,
            "issue_id": 1406,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "23f11aceae00534d",
            "authors": [
                "Yang Shen",
                "Xiu-Shen Wei",
                "Yifan Sun",
                "Yuxin Song",
                "Tao Yuan",
                "Jian Jin",
                "Heyang Xu",
                "Yazhou Yao",
                "Errui Ding"
            ],
            "affiliations": [
                "Baidu",
                "Nanjing University of Science and Technology",
                "Southeast University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18525.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#cv",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ - ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² 'Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚' Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Zero-Shot Generalization in Computer Vision with Explanatory Instructions",
                    "desc": "This paper addresses the challenge of zero-shot task generalization in Computer Vision (CV), which has not reached the levels seen in Natural Language Processing (NLP). The authors argue that the use of specific terminological definitions for tasks in CV, like 'image segmentation', limits the models' ability to generalize to new tasks. To overcome this, they propose 'Explanatory Instructions' that transform image inputs into detailed linguistic outputs, helping models understand tasks better. They introduce a large dataset of 12 million triplets and train an auto-regressive vision-language model that successfully demonstrates zero-shot capabilities for both seen and unseen tasks."
                },
                "zh": {
                    "title": "çªç ´è®¡ç®—æœºè§†è§‰çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰åœ¨é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¸è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å¯¹æ¯”ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒCVä½¿ç”¨çš„æœ¯è¯­æ€§ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰å¯èƒ½æ˜¯é˜»ç¢é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„å…³é”®å› ç´ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè§£é‡Šæ€§æŒ‡ä»¤â€ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢æ¥ç›´è§‚åœ°å®šä¹‰CVä»»åŠ¡ç›®æ ‡ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1200ä¸‡å¯¹â€œå›¾åƒè¾“å…¥ã€è§£é‡Šæ€§æŒ‡ä»¤å’Œè¾“å‡ºâ€çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªåŸºäºè‡ªå›å½’çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†å¯¹å·²è§ä»»åŠ¡çš„æŒ‡ä»¤çº§é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§çš„CVä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21037",
            "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
            "url": "https://huggingface.co/papers/2412.21037",
            "abstract": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation.",
            "score": 3,
            "issue_id": 1405,
            "pub_date": "2024-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "bb669623871df661",
            "authors": [
                "Chia-Yu Hung",
                "Navonil Majumder",
                "Zhifeng Kong",
                "Ambuj Mehrish",
                "Rafael Valle",
                "Bryan Catanzaro",
                "Soujanya Poria"
            ],
            "affiliations": [
                "NVIDIA",
                "Singapore University of Technology and Design (SUTD)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21037.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#audio",
                    "#open_source",
                    "#benchmark",
                    "#alignment",
                    "#rlhf",
                    "#small_models"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "TangoFlux: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "TangoFlux - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ (Text-to-Audio, TTA) Ñ 515 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 30 ÑĞµĞºÑƒĞ½Ğ´ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ 44,1 ĞºĞ“Ñ† Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 3,7 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU A40. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ CLAP-Ranked Preference Optimization (CRPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ TTA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. TangoFlux Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ° ĞºĞ¾Ğ´ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "TangoFlux: Revolutionizing Text-to-Audio Generation with CRPO",
                    "desc": "TangoFlux is a powerful Text-to-Audio generative model that can create high-quality audio quickly and efficiently. It addresses the challenge of aligning TTA models by introducing a new method called CLAP-Ranked Preference Optimization (CRPO), which helps generate and optimize preference data. This approach improves the model's ability to understand and produce audio that aligns with user preferences. The results show that TangoFlux not only meets but exceeds current standards in both objective and subjective evaluations, and the team has made their code and models available for further research."
                },
                "zh": {
                    "title": "TangoFluxï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†TangoFluxï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ‹¥æœ‰5.15äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªA40 GPUä¸Šä»¥3.7ç§’çš„é€Ÿåº¦ç”Ÿæˆæœ€é•¿30ç§’çš„44.1kHzéŸ³é¢‘ã€‚TTAæ¨¡å‹å¯¹é½çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯åˆ›å»ºåå¥½å¯¹çš„å›°éš¾ï¼Œå› ä¸ºTTAç¼ºä¹åƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é‚£æ ·çš„å¯éªŒè¯å¥–åŠ±æˆ–æ ‡å‡†ç­”æ¡ˆçš„ç»“æ„åŒ–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLAP-Ranked Preference Optimizationï¼ˆCRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–åå¥½æ•°æ®æ¥å¢å¼ºTTAçš„å¯¹é½ã€‚æˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨CRPOç”Ÿæˆçš„éŸ³é¢‘åå¥½æ•°æ®é›†åœ¨ç°æœ‰æ›¿ä»£æ–¹æ¡ˆä¸­è¡¨ç°æ›´ä¼˜ï¼ŒTangoFluxåœ¨å®¢è§‚å’Œä¸»è§‚åŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21139",
            "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
            "url": "https://huggingface.co/papers/2412.21139",
            "abstract": "We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to train language model based SWE agents , achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new state-of-the-art for open-weight SWE agents. To facilitate further research, we publicly release SWE-Gym, models, and agent trajectories.",
            "score": 2,
            "issue_id": 1406,
            "pub_date": "2024-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "800bb3f4c48e2cf9",
            "authors": [
                "Jiayi Pan",
                "Xingyao Wang",
                "Graham Neubig",
                "Navdeep Jaitly",
                "Heng Ji",
                "Alane Suhr",
                "Yizhe Zhang"
            ],
            "affiliations": [
                "Apple",
                "CMU",
                "UC Berkeley",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21139.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SWE-Gym: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "SWE-Gym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2438 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Python Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ SWE-Gym Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 19% Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² SWE-Bench. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Software Engineering with SWE-Gym",
                    "desc": "SWE-Gym is a novel environment designed for training software engineering agents using real-world Python tasks. It includes 2,438 task instances, each with a codebase, executable environment, unit tests, and natural language task descriptions. The paper demonstrates that language model-based agents trained in SWE-Gym can significantly improve their performance, achieving up to 19% higher resolve rates on benchmark tests. Additionally, the authors explore scaling inference through verifiers, leading to state-of-the-art results for open-weight software engineering agents, and they provide resources for further research."
                },
                "zh": {
                    "title": "SWE-Gymï¼šè½¯ä»¶å·¥ç¨‹ä»£ç†çš„æ–°èµ·ç‚¹",
                    "desc": "æˆ‘ä»¬æå‡ºäº†SWE-Gymï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè®­ç»ƒçœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»£ç†çš„ç¯å¢ƒã€‚SWE-GymåŒ…å«2438ä¸ªçœŸå®çš„Pythonä»»åŠ¡å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹éƒ½æœ‰å¯æ‰§è¡Œçš„è¿è¡Œç¯å¢ƒã€å•å…ƒæµ‹è¯•å’Œç”¨è‡ªç„¶è¯­è¨€æŒ‡å®šçš„ä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨SWE-Gymï¼Œæˆ‘ä»¬è®­ç»ƒçš„åŸºäºè¯­è¨€æ¨¡å‹çš„SWEä»£ç†åœ¨æµè¡Œçš„SWE-BenchéªŒè¯å’ŒLiteæµ‹è¯•é›†ä¸Šå®ç°äº†é«˜è¾¾19%çš„ç»å¯¹è§£å†³ç‡æå‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨SWE-Gymä¸­é‡‡æ ·çš„ä»£ç†è½¨è¿¹è®­ç»ƒéªŒè¯å™¨ï¼Œè¿›è¡Œæ¨ç†æ—¶çš„æ‰©å±•ï¼Œç»“åˆæˆ‘ä»¬å¾®è°ƒçš„SWEä»£ç†ï¼Œåœ¨SWE-BenchéªŒè¯å’ŒLiteä¸Šåˆ†åˆ«è¾¾åˆ°äº†32.0%å’Œ26.0%çš„æ–°çŠ¶æ€ï¼Œæˆä¸ºå¼€æ”¾æƒé‡SWEä»£ç†çš„æ–°æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.20993",
            "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
            "url": "https://huggingface.co/papers/2412.20993",
            "abstract": "The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solution paths, at the cost of increasing compute demands and response latencies. Existing serving systems fail to adapt to the scaling behaviors of these algorithms or the varying difficulty of queries, leading to inefficient resource use and unmet latency targets.   We present Dynasor, a system that optimizes inference-time compute for LLM reasoning queries. Unlike traditional engines, Dynasor tracks and schedules requests within reasoning queries and uses Certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustaining 3.3x higher query rates or 4.7x tighter latency SLOs in online serving.",
            "score": 1,
            "issue_id": 1406,
            "pub_date": "2024-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "7fe76ed90463d977",
            "authors": [
                "Yichao Fu",
                "Junda Chen",
                "Siqi Zhu",
                "Zheyu Fu",
                "Zhongdongming Dai",
                "Aurick Qiao",
                "Hao Zhang"
            ],
            "affiliations": [
                "Snowflake",
                "Tsinghua University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.20993.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Dynasor: ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Dynasor, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Dynasor Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Certaindex Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, ÑƒĞ´ĞµĞ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ±ĞµÑĞ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. Dynasor Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Dynasor: Smart Compute Allocation for Efficient LLM Reasoning",
                    "desc": "This paper introduces Dynasor, a system designed to optimize the compute resources used during inference for large language models (LLMs) when handling reasoning queries. It addresses the inefficiencies of existing serving systems that do not adapt to the complexity of different queries or the scaling needs of inference-time reasoning algorithms. Dynasor employs a dynamic scheduling approach that allocates compute resources based on the difficulty of the query, using a proxy called Certaindex to measure the model's certainty in its reasoning. As a result, Dynasor can significantly reduce compute usage while improving query processing rates and meeting latency targets more effectively."
                },
                "zh": {
                    "title": "Dynasorï¼šä¼˜åŒ–æ¨ç†æŸ¥è¯¢çš„è®¡ç®—æ•ˆç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Dynasorç³»ç»Ÿï¼Œå®ƒä¼˜åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æŸ¥è¯¢æ—¶çš„è®¡ç®—æ•ˆç‡ã€‚Dynasoré€šè¿‡è·Ÿè¸ªå’Œè°ƒåº¦æ¨ç†æŸ¥è¯¢ä¸­çš„è¯·æ±‚ï¼ŒåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥åº”å¯¹ä¸åŒéš¾åº¦çš„æŸ¥è¯¢ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨Certaindexä»£ç†ï¼Œæ ¹æ®æ¨¡å‹çš„ç¡®å®šæ€§æ¥è¡¡é‡æ¨ç†è¿›å±•ï¼Œä»è€ŒæŒ‡å¯¼è®¡ç®—åˆ†é…ã€‚é€šè¿‡åœ¨å¤šç§æ•°æ®é›†å’Œç®—æ³•ä¸Šæµ‹è¯•ï¼ŒDynasoråœ¨æ‰¹å¤„ç†æ—¶å‡å°‘äº†å¤šè¾¾50%çš„è®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶åœ¨åœ¨çº¿æœåŠ¡ä¸­å®ç°äº†3.3å€æ›´é«˜çš„æŸ¥è¯¢é€Ÿç‡æˆ–4.7å€æ›´ä¸¥æ ¼çš„å»¶è¿ŸæœåŠ¡æ°´å¹³ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.20005",
            "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
            "url": "https://huggingface.co/papers/2412.20005",
            "abstract": "We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code at https://github.com/zjunlp/OneKE and released a Video at http://oneke.openkg.cn/demo.mp4.",
            "score": 1,
            "issue_id": 1405,
            "pub_date": "2024-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "da8469c61421cefb",
            "authors": [
                "Yujie Luo",
                "Xiangyuan Ru",
                "Kangwei Liu",
                "Lin Yuan",
                "Mengshu Sun",
                "Ningyu Zhang",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Lanning Wei",
                "Da Zheng",
                "Haofen Wang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Tongji University",
                "ZJU-Ant Group Joint Research Center for Knowledge Graphs",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.20005.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#open_source",
                    "#benchmark",
                    "#multimodal",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OneKE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "OneKE - ÑÑ‚Ğ¾ Ğ´Ğ¾ĞºĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ÑÑ…ĞµĞ¼Ğ¾Ğ¹. ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²ĞµĞ±-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ PDF-ĞºĞ½Ğ¸Ğ³, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°ÑƒĞºĞ° Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. OneKE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "OneKE: Versatile Knowledge Extraction for Diverse Domains",
                    "desc": "OneKE is a knowledge extraction system designed to gather information from the Web and raw PDF books across various domains like science and news. It utilizes multiple agents, each responsible for specific tasks, which enhances its ability to handle different extraction scenarios effectively. The system includes a configurable knowledge base that aids in schema setup, debugging, and error correction, leading to improved performance. Empirical tests on benchmark datasets confirm OneKE's effectiveness, and case studies showcase its versatility in tackling diverse tasks."
                },
                "zh": {
                    "title": "OneKEï¼šå¤šé¢†åŸŸçŸ¥è¯†æå–çš„æ™ºèƒ½ç³»ç»Ÿ",
                    "desc": "OneKEæ˜¯ä¸€ä¸ªåŸºäºDockerçš„çŸ¥è¯†æå–ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»ç½‘ç»œå’ŒåŸå§‹PDFä¹¦ç±ä¸­æå–çŸ¥è¯†ï¼Œæ”¯æŒå¤šä¸ªé¢†åŸŸï¼ˆå¦‚ç§‘å­¦ã€æ–°é—»ç­‰ï¼‰ã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†å¤šä¸ªæ™ºèƒ½ä»£ç†ï¼Œå„è‡ªæ‰¿æ‹…ä¸åŒçš„è§’è‰²ï¼Œä»¥é€‚åº”å„ç§æå–åœºæ™¯ã€‚é…ç½®çŸ¥è¯†åº“çš„è®¾è®¡ä½¿å¾—æ¨¡å¼é…ç½®ã€é”™è¯¯è°ƒè¯•å’Œä¿®æ­£å˜å¾—æ›´åŠ é«˜æ•ˆï¼Œä»è€Œæå‡äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°ï¼ŒOneKEå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯´æ˜äº†å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„é€‚åº”æ€§å’Œå¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-30.html",
    "link_next": "2025-01-01.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "01.01",
        "en": "01/01",
        "zh": "1æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†OpenAI o1çš„çªç ´ï¼Œå¼ºè°ƒäº†å¢å¼ºæ¨ç†ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨ç†ç ”ç©¶é›†ä¸­åœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼ŒåŒ»å­¦é¢†åŸŸä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åŒ»å­¦é¢†åŸŸéœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ¥æä¾›å¯é çš„ç­”æ¡ˆï¼Œä½†éªŒè¯åŒ»å­¦æ¨ç†æ¯”æ•°å­¦æ¨ç†æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¯éªŒè¯çš„åŒ»å­¦é—®é¢˜å’Œä¸€ä¸ªåŒ»å­¦éªŒè¯å™¨æ¥æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚è¿™ç§å¯éªŒè¯æ€§ä½¿å¾—é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•è¿›ä¸€æ­¥æ¨è¿›åŒ»å­¦æ¨ç†æˆä¸ºå¯èƒ½ã€‚",
        "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le OpenAI o1 de tÅ«pÃ², qiÃ¡ngdiÃ o le zÄ“ngqiÃ¡ng tuÃ­lÇ yÇ tÃ­gÄo dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLM) de qiÃ¡nlÃ¬. RÃ¡n'Ã©r, dÃ duÅshÃ¹ tuÃ­lÇ yÃ¡njiÅ« jÃ­zhÅng zÃ i shÃ¹xuÃ© rÃ¨nwÃ¹ shÃ ng, yÄ«xuÃ© lÇngyÃ¹ rÃ©ng wÃ¨i dÃ©dÃ o chÅngfÃ¨n tÃ nsuÇ’. YÄ«xuÃ© lÇngyÃ¹ xÅ«yÃ o qiÃ¡ngdÃ  de tuÃ­lÇ nÃ©nglÃ¬ lÃ¡i tÃ­gÅng kÄ›kÃ o de dÃ¡'Ã n, dÃ n yÃ nzhÃ¨ng yÄ«xuÃ© tuÃ­lÇ bÇ shÃ¹xuÃ© tuÃ­lÇ gÃ¨ng jÃ¹ tiÇozhÃ nxÃ¬ng. WÃ¨i cÇ, zuÃ²zhÄ› tÃ­chÅ« le kÄ› yÃ nzhÃ¨ng de yÄ«xuÃ© wÃ¨ntÃ­ hÃ© yÄ«gÃ¨ yÄ«xuÃ© yÃ nzhÃ¨ngqÃ¬ lÃ¡i jiÇnchÃ¡ mÃ³xÃ­ng shÅ«chÅ« de zhÃ¨ngquÃ¨xÃ¬ng. ZhÃ¨ zhÇ’ng kÄ› yÃ nzhÃ¨ngxÃ¬ng shÇ dÃ© tÅngguÃ² liÇng jiÄ“duÃ n fÄngfÇ jÃ¬nxÃ­ng yÄ«xuÃ© tuÃ­lÇ chÃ©ngwÃ©i kÄ›nÃ©ng.",
        "vocab": "[{'word': 'çªç ´', 'pinyin': 'tÅ«pÃ²', 'trans': 'breakthrough'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'reasoning'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'large language model'}, {'word': 'é›†ä¸­', 'pinyin': 'jÃ­zhÅng', 'trans': 'focus'}, {'word': 'æ•°å­¦', 'pinyin': 'shÃ¹xuÃ©', 'trans': 'mathematics'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'åŒ»å­¦', 'pinyin': 'yÄ«xuÃ©', 'trans': 'medicine'}, {'word': 'é¢†åŸŸ', 'pinyin': 'lÇngyÃ¹', 'trans': 'field'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}, {'word': 'å¯é ', 'pinyin': 'kÄ›kÃ o', 'trans': 'reliable'}, {'word': 'ç­”æ¡ˆ', 'pinyin': 'dÃ¡'Än', 'trans': 'answer'}, {'word': 'éªŒè¯', 'pinyin': 'yÃ nzhÃ¨ng', 'trans': 'verification'}, {'word': 'æŒ‘æˆ˜æ€§', 'pinyin': 'tiÇozhÃ nxÃ¬ng', 'trans': 'challenging'}, {'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'}, {'word': 'æ¨è¿›', 'pinyin': 'tuÄ«jÃ¬n', 'trans': 'advance'}]",
        "trans": "This article introduces the breakthroughs of OpenAI o1, emphasizing the enhancement of reasoning to increase the potential of large language models (LLMs). However, most reasoning research focuses on mathematical tasks, and the medical field remains relatively unexplored. The medical field requires strong reasoning capabilities to provide reliable answers, but verifying medical reasoning is more challenging than verifying mathematical reasoning. To address this, the authors propose verifiable medical problems and a medical verifier to check the correctness of model outputs. This verifiability makes it possible to further advance medical reasoning through a two-stage method.",
        "update_ts": "2024-12-30 09:11"
    }
}