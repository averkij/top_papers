{
    "date": {
        "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 27",
        "zh": "1æœˆ27æ—¥"
    },
    "time_utc": "2026-01-27 01:56",
    "weekday": 1,
    "issue_id": 777,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.16725",
            "title": "LongCat-Flash-Thinking-2601 Technical Report",
            "url": "https://huggingface.co/papers/2601.16725",
            "abstract": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for complex tool interactions and real-world robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
            "score": 138,
            "issue_id": 757,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "72bd8c70b1c8e6af",
            "authors": [
                "Meituan LongCat Team",
                "Anchun Gui",
                "Bei Li",
                "Bingyang Tao",
                "Bole Zhou",
                "Borun Chen",
                "Chao Zhang",
                "Chao Zhang",
                "Chen Gao",
                "Chen Zhang",
                "Chengcheng Han",
                "Chenhui Yang",
                "Chuyu Zhang",
                "Cong Chen",
                "Cunguang Wang",
                "Daoru Pan",
                "Defei Bu",
                "Dengchang Zhao",
                "Di Xiu",
                "Dishan Liu",
                "Dongyu Ru",
                "Dunwei Tu",
                "Fan Wu",
                "Fengcheng Yuan",
                "Fengcun Li",
                "Gang Xu",
                "Guanyu Wu",
                "Guoyuan Lin",
                "Haibin Wang",
                "Hansi Yang",
                "Hao Yang",
                "Haonan Yan",
                "Haoxiang Ma",
                "Haoxing Wen",
                "Hongyan Hao",
                "Hongyin Tang",
                "Hongyu Zang",
                "Hongzhi Ni",
                "Hui Su",
                "Jiacheng Zhang",
                "Jiahong Zhou",
                "Jiahuan Li",
                "Jiaming Wang",
                "Jian Yang",
                "Jianfei Zhang",
                "Jianhao Xu",
                "Jianing Wang",
                "Jiapeng Zhu",
                "Jiaqi Sun",
                "Jiarong Shi",
                "Jiarui Zhao",
                "Jingang Wang",
                "Jinluan Yang",
                "Jinrui Ding",
                "Jinwei Xiao",
                "Jiyuan He",
                "Juncan Xu",
                "Kefeng Zhang",
                "Keheng Wang",
                "Li Wei",
                "Lianhui Ma",
                "Lin Qiu",
                "Lingbing Kong",
                "Lingchuan Liu",
                "Linsen Guo",
                "Mengshen Zhu",
                "Mengxia Shen",
                "Mingyang Zhu",
                "Peiguang Li",
                "Peng Pei",
                "Pengcheng Jia",
                "Pengtao Zhang",
                "Peng Zhao",
                "Qi Gu",
                "Qiong Huang",
                "Qiyuan Duan",
                "Quanchi Weng",
                "Rongxiang Weng",
                "Rongzhi Zhang",
                "Rumei Li",
                "Shanglin Lei",
                "Shengnan An",
                "Shijun Dai",
                "Shuaikang Liu",
                "Shuang Zhou",
                "Shuo Wang",
                "Songyuan Zhao",
                "Tao Liang",
                "Tianhao Hu",
                "Tianze Chen",
                "Wei Liu",
                "Wei Shi",
                "Wei Wang",
                "Weifeng Tang",
                "Wenjie Shi",
                "Wenlong Zhu",
                "Wentao Chen",
                "Wentao Shi",
                "Xi Su",
                "Xiangcheng Liu",
                "Xiandi Ma",
                "Xiangyu Xi",
                "Xiangyuan Liu",
                "Xiangzhou Huang",
                "Xiao Liu",
                "Xiaodong Cai",
                "Xiaolong Chen",
                "Xiaowei Shi",
                "Xiaoyu Li",
                "Xin Chen",
                "Xingchen Liu",
                "Xuan Huang",
                "Xuezhi Cao",
                "Xunliang Cai",
                "Yan Chen",
                "Yang Bai",
                "Yang Liu",
                "Yang Yang",
                "Yang Zheng",
                "Yaoming Wang",
                "Yaoming Zhu",
                "Yaqi Huo",
                "Yanyu Chen",
                "Yaorui Shi",
                "Yerui Sun",
                "Yi Zhang",
                "Yihao Chen",
                "Yi-Kai Zhang",
                "Yifan Lu",
                "Yifan Zhao",
                "Yitao Zhai",
                "Yongjing Yin",
                "Yongwei Zhou",
                "Youshao Xiao",
                "Yuchuan Dai",
                "Yuchen Xie",
                "Yuchen Yu",
                "Yufei Zhang",
                "Yuhuai Wei",
                "Yulei Qian",
                "Yunfan Liang",
                "Yunke Zhao",
                "Yuwei Jiang",
                "Yuxin Bian",
                "Yuxin Chen",
                "Yuxin Liu",
                "Yue Xu",
                "Yueqing Sun",
                "Zeyang Yu",
                "Zhao Yang",
                "Zhengsheng Huang",
                "Zhengyu Chen",
                "Zhijian Liu",
                "Zhikang Xia",
                "Zhimin Lin",
                "Zhiyuan Yao",
                "Zhuofan Chen",
                "Zhuowen Han",
                "Zijian Zhang",
                "Ziran Li",
                "Ziwen Wang",
                "Ziyuan Zhuang"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16725.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼: MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LongCat-Flash-Thinking-2601 â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture-of-Experts Ñ 560 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ„ÑƒĞ·Ğ¸ĞµĞ¹, Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (DORA) Ğ½Ğ° 10000+ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº ÑˆÑƒĞ¼Ğ°Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ñ€ĞµĞ¶Ğ¸Ğ¼ Heavy Thinking, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with 560 Billion Parameters!",
                    "desc": "The paper presents LongCat-Flash-Thinking-2601, a 560-billion-parameter Mixture-of-Experts (MoE) model designed for advanced reasoning tasks. It achieves top performance on various agentic benchmarks by utilizing a unified training framework that integrates domain-parallel expert training and fusion techniques. The model excels in handling complex tool interactions and demonstrates robustness in noisy real-world scenarios, thanks to its systematic approach to training and environment scaling. Additionally, it introduces a Heavy Thinking mode for enhanced reasoning capabilities during testing, allowing for deeper and broader reasoning processes."
                },
                "zh": {
                    "title": "è¶…è¶Šæ™ºèƒ½åŸºå‡†çš„æ··åˆä¸“å®¶æ¨ç†æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLongCat-Flash-Thinking-2601çš„5600äº¿å‚æ•°çš„æ··åˆä¸“å®¶æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ç»“åˆé¢†åŸŸå¹¶è¡Œä¸“å®¶è®­ç»ƒä¸èåˆï¼Œæå‡äº†å¤æ‚å·¥å…·äº¤äº’å’Œç°å®ä¸–ç•Œçš„é²æ£’æ€§ã€‚æ¨¡å‹åœ¨å¤æ‚å·¥å…·ä½¿ç”¨æ–¹é¢çš„å¼ºæ³›åŒ–èƒ½åŠ›æºäºå¯¹ç¯å¢ƒæ‰©å±•å’Œä»»åŠ¡æ„å»ºçš„æ·±å…¥æ¢ç´¢ã€‚ä¸ºäº†ä¼˜åŒ–å¤šè½®æ™ºèƒ½äº¤äº’å’Œé•¿å°¾ç”Ÿæˆï¼Œç ”ç©¶å›¢é˜Ÿæ‰©å±•äº†å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶DORAï¼Œä»¥å®ç°ç¨³å®šé«˜æ•ˆçš„å¤§è§„æ¨¡å¤šç¯å¢ƒè®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16746",
            "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
            "url": "https://huggingface.co/papers/2601.16746",
            "abstract": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.",
            "score": 63,
            "issue_id": 757,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "cbeb488f840d008d",
            "authors": [
                "Yuhang Wang",
                "Yuling Shi",
                "Mo Yang",
                "Rongrui Zhang",
                "Shilin He",
                "Heng Lian",
                "Yuting Chen",
                "Siyu Ye",
                "Kai Cai",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Douyin Group",
                "LLMSE Lab, Shanghai Jiao Tong University",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16746.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#agents",
                    "#inference",
                    "#plp",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ",
                    "desc": "SWE-Pruner â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞ¼, ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ñ‹ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ›ĞµĞ³ĞºĞ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ñ 0.6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ½Ğ¾ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (23-54% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… SWE-Bench Ğ¸ Ğ´Ğ¾ 14.84x Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Smart Context Pruning for Coding Agents",
                    "desc": "SWE-Pruner is a novel framework designed to optimize the performance of coding agents by intelligently reducing the amount of context they process. It employs task-aware pruning, allowing agents to focus on relevant information based on specific goals, similar to how human programmers skim code. This approach not only minimizes token usage but also preserves essential syntactic and logical structures, which are often lost in traditional compression methods. Evaluations show that SWE-Pruner can significantly reduce token counts while maintaining performance across various coding tasks."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”ä¸Šä¸‹æ–‡å‰ªæï¼Œæå‡ç¼–ç ä»£ç†æ•ˆç‡",
                    "desc": "SWE-Pruneræ˜¯ä¸€ç§è‡ªé€‚åº”ä¸Šä¸‹æ–‡å‰ªææ¡†æ¶ï¼Œä¸“ä¸ºç¼–ç ä»£ç†è®¾è®¡ã€‚å®ƒé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„å‰ªææ–¹æ³•ï¼Œå‡å°‘äº†ä»¤ç‰Œçš„ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†äººç±»ç¨‹åºå‘˜åœ¨å¼€å‘å’Œè°ƒè¯•è¿‡ç¨‹ä¸­â€œé€‰æ‹©æ€§æµè§ˆâ€æºä»£ç çš„æ–¹å¼ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€é€‰æ‹©ç›¸å…³çš„ä¸Šä¸‹æ–‡è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Pruneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†23-54%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸”å¯¹æ€§èƒ½å½±å“æå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14133",
            "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
            "url": "https://huggingface.co/papers/2601.14133",
            "abstract": "TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
            "score": 52,
            "issue_id": 757,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "68a0f5db6af94c86",
            "authors": [
                "Bin Yu",
                "Shijie Lian",
                "Xiaopeng Lin",
                "Yuliang Wei",
                "Zhaolong Shen",
                "Changti Wu",
                "Yuzhuo Miao",
                "Xinming Wang",
                "Bailing Wang",
                "Cong Huang",
                "Kai Chen"
            ],
            "affiliations": [
                "BUAA",
                "CASIA",
                "DeepCybo",
                "ECNU",
                "HIT",
                "HKUST(GZ)",
                "HUST",
                "ZGCA",
                "ZGCI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14133.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#robotics",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ·Ğ³ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒÑ",
                    "desc": "TwinBrainVLA Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ 'Ğ»ĞµĞ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ·Ğ³' Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ 'Ğ¿Ñ€Ğ°Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼', ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¼ĞµÑĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€Ğ°Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ·Ğ³ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñƒ Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸Ğ¾Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SimplerEnv Ğ¸ RoboCasa Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Balancing Understanding and Skill in Robot Control",
                    "desc": "TwinBrainVLA is a new approach in robot control that combines a generalist vision-language model with a specialist model to improve performance. It uses an asymmetric mixture-of-transformers mechanism to coordinate these two models, allowing the robot to maintain high-level semantic understanding while also mastering fine motor skills. The architecture features a 'Left Brain' that retains general visual reasoning and a 'Right Brain' that focuses on proprioception for better control. This innovative design helps prevent catastrophic forgetting, enabling robots to perform complex tasks while still understanding their environment."
                },
                "zh": {
                    "title": "åŒè„‘æ¨¡å‹ï¼šè¯­ä¹‰ç†è§£ä¸è¿åŠ¨æŠ€èƒ½çš„å®Œç¾ç»“åˆ",
                    "desc": "TwinBrainVLA è§£å†³äº†æœºå™¨äººæ§åˆ¶ä¸­è¯­ä¹‰ç†è§£ä¸è¿åŠ¨æŠ€èƒ½ä¹‹é—´çš„çŸ›ç›¾ã€‚å®ƒé€šè¿‡ä¸å¯¹ç§°çš„æ··åˆå˜æ¢å™¨æœºåˆ¶ï¼Œå°†é€šç”¨çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸ä¸“é—¨çš„è¿åŠ¨æ„ŸçŸ¥æ¨¡å‹è¿›è¡Œåè°ƒã€‚è¿™ä¸ªæ–°æ¶æ„åŒ…å«ä¸€ä¸ªä¿æŒé€šç”¨è¯­ä¹‰ç†è§£çš„â€œå·¦è„‘â€å’Œä¸€ä¸ªä¸“æ³¨äºèº«ä½“æ„ŸçŸ¥çš„å¯è®­ç»ƒâ€œå³è„‘â€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTwinBrainVLA åœ¨æ“ä½œæ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™äº†é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹çš„å…¨é¢è§†è§‰ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16973",
            "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
            "url": "https://huggingface.co/papers/2601.16973",
            "abstract": "Modern vision-language models exhibit significant challenges in multi-step visual interaction tasks, particularly in long-horizon perception-memory-action integration, with performance declining when handling unbounded historical contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
            "score": 24,
            "issue_id": 757,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "ee9c8cf577851d33",
            "authors": [
                "Zirui Wang",
                "Junyi Zhang",
                "Jiaxin Ge",
                "Long Lian",
                "Letian Fu",
                "Lisa Dunlap",
                "Ken Goldberg",
                "XuDong Wang",
                "Ion Stoica",
                "David M. Chan",
                "Sewon Min",
                "Joseph E. Gonzalez"
            ],
            "affiliations": [
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16973.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ VisGym - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 17 ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 46.6% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¸ 26% Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ - Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹, Ñ‡ĞµĞ¼ Ñ ÑƒÑĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ¹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ fine-tuning."
                },
                "en": {
                    "title": "Enhancing Multi-Step Visual Interaction in AI",
                    "desc": "This paper discusses the limitations of modern vision-language models (VLMs) in performing multi-step visual interaction tasks, especially when integrating perception, memory, and action over long periods. The authors introduce VisGym, a new evaluation and training environment consisting of 17 diverse tasks that test VLMs on symbolic puzzles, real-image understanding, navigation, and manipulation. Their findings indicate that current models struggle significantly in these interactive settings, with low success rates and difficulties in utilizing long historical contexts effectively. The study also suggests that providing explicit goals and feedback can improve model performance, revealing important areas for future research in enhancing multi-step visual decision-making."
                },
                "zh": {
                    "title": "æå‡è§†è§‰å†³ç­–èƒ½åŠ›çš„å…³é”®åœ¨äºå¤šæ­¥éª¤äº¤äº’",
                    "desc": "ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥éª¤è§†è§‰äº¤äº’ä»»åŠ¡ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶é—´çš„æ„ŸçŸ¥-è®°å¿†-è¡ŒåŠ¨æ•´åˆæ–¹é¢ã€‚å½“å¤„ç†æ— é™å†å²ä¸Šä¸‹æ–‡æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†VisGymï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«17ä¸ªç¯å¢ƒçš„è®­ç»ƒå’Œè¯„ä¼°å¹³å°ï¼Œæ¶µç›–ç¬¦å·è°œé¢˜ã€çœŸå®å›¾åƒç†è§£ã€å¯¼èˆªå’Œæ“ä½œç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨äº¤äº’è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ï¼ŒæˆåŠŸç‡è¾ƒä½ï¼Œä¸”åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16296",
            "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
            "url": "https://huggingface.co/papers/2601.16296",
            "abstract": "Memory-V2V enhances multi-turn video editing by maintaining cross-consistency through explicit memory mechanisms and efficient token compression in video-to-video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
            "score": 14,
            "issue_id": 757,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "02e080759bee1ddd",
            "authors": [
                "Dohun Lee",
                "Chun-Hao Paul Huang",
                "Xuelin Chen",
                "Jong Chul Ye",
                "Duygu Ceylan",
                "Hyeonho Jeong"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16296.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº",
                    "desc": "Memory-V2V â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ² ĞºĞµÑˆĞµ Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑˆĞ°Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ DiT Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 30%. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Video Editing with Memory-V2V",
                    "desc": "Memory-V2V is a novel framework designed to improve multi-turn video editing by ensuring cross-consistency across edits. It utilizes explicit memory mechanisms to retain information from previous editing steps, allowing for more coherent results in iterative editing processes. The framework incorporates efficient token compression techniques to reduce redundancy and computational load, achieving a significant speedup in processing. Through extensive testing, Memory-V2V demonstrates enhanced performance in tasks like video synthesis and long video editing, outperforming existing models while maintaining high quality."
                },
                "zh": {
                    "title": "æå‡å¤šè½®è§†é¢‘ç¼–è¾‘çš„ä¸€è‡´æ€§",
                    "desc": "Memory-V2V æ˜¯ä¸€ç§å¢å¼ºå¤šè½®è§†é¢‘ç¼–è¾‘çš„æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼çš„è®°å¿†æœºåˆ¶å’Œé«˜æ•ˆçš„ä»¤ç‰Œå‹ç¼©æ¥ä¿æŒè·¨ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤šè½®ç¼–è¾‘ä¸­ï¼Œç”¨æˆ·åœ¨ä¸åŒç¼–è¾‘æ­¥éª¤ä¹‹é—´ä¿æŒä¸€è‡´æ€§çš„é—®é¢˜ã€‚Memory-V2V åˆ©ç”¨å¤–éƒ¨ç¼“å­˜å’ŒåŠ¨æ€ä»¤ç‰ŒåŒ–ç­–ç•¥ï¼Œä½¿å½“å‰ç¼–è¾‘æ­¥éª¤èƒ½å¤Ÿå‚è€ƒä¹‹å‰çš„ç¼–è¾‘ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒMemory-V2V åœ¨è§†é¢‘ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶ä¸‹çš„é•¿è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†è·¨ä¸€è‡´æ€§ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15808",
            "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
            "url": "https://huggingface.co/papers/2601.15808",
            "abstract": "A novel self-evolving framework for Deep Research Agents that enhances performance through iterative verification and rubric-based feedback during inference, achieving significant accuracy improvements without additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
            "score": 14,
            "issue_id": 757,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "63dc3f1d7372d34d",
            "authors": [
                "Yuxuan Wan",
                "Tianqing Fang",
                "Zaitang Li",
                "Yintong Huo",
                "Wenxuan Wang",
                "Haitao Mi",
                "Dong Yu",
                "Michael R. Lyu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Singapore Management University",
                "Tencent AI Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15808.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#dataset",
                    "#inference",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Deep Research Agents), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ DeepVerifier â€” Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑÑ‹Ğ»Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8%-11% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² GAIA Ğ¸ XBench-DeepResearch Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Self-Evolving Agents: Enhancing Performance Through Iterative Feedback",
                    "desc": "This paper introduces a self-evolving framework for Deep Research Agents (DRAs) that enhances their performance through a process of iterative verification and rubric-based feedback during inference. Instead of relying solely on post-training improvements, the framework allows agents to self-improve by evaluating their outputs against predefined rubrics derived from a Failure Taxonomy. The proposed DeepVerifier module provides detailed feedback that helps the agent refine its responses without needing additional training. The results show significant accuracy improvements, demonstrating the effectiveness of this approach in automated knowledge discovery and problem-solving tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„æ·±åº¦ç ”ç©¶ä»£ç†æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºæ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆDRAï¼‰ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£éªŒè¯å’ŒåŸºäºè¯„åˆ†æ ‡å‡†çš„åé¦ˆæ¥æå‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä½¿ä»£ç†èƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè‡ªæˆ‘æ”¹è¿›å…¶ç”Ÿæˆçš„ç­”æ¡ˆï¼Œä»è€Œå®ç°æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚æˆ‘ä»¬å¼€å‘äº†DeepVerifierï¼Œä¸€ä¸ªåŸºäºè¯„åˆ†æ ‡å‡†çš„ç»“æœéªŒè¯å™¨ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶ä½œä¸ºæ’ä»¶æ¨¡å—é›†æˆï¼Œæä¾›è¯¦ç»†çš„åé¦ˆï¼Œå¸®åŠ©ä»£ç†è¿›è¡Œè¿­ä»£è‡ªæˆ‘æå‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDeepVerifieråœ¨GAIAå’ŒXBench-DeepResearchçš„æŒ‘æˆ˜æ€§å­é›†ä¸Šå®ç°äº†8%-11%çš„å‡†ç¡®ç‡æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14243",
            "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
            "url": "https://huggingface.co/papers/2601.14243",
            "abstract": "Quantized reinforcement learning training using FP8 precision faces stability issues due to numerical mismatches between training and inference phases, but a unified FP8 framework achieves significant speedups with stable convergence.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.",
            "score": 13,
            "issue_id": 759,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "d4b42f2f6e07b33b",
            "authors": [
                "Haocheng Xi",
                "Charlie Ruan",
                "Peiyuan Liao",
                "Yujun Lin",
                "Han Cai",
                "Yilong Zhao",
                "Shuo Yang",
                "Kurt Keutzer",
                "Song Han",
                "Ligeng Zhu"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14243.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ FP8 Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ FP8 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ³Ğ´Ğ° FP8 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ° BF16 Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ñ„Ğ°Ğ·Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Jet-RL â€” ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ FP8 Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¹ (Ğ´Ğ¾ 33% Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ 16% ĞºĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ†) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Jet-RL: Unifying FP8 Precision for Stable and Fast Reinforcement Learning",
                    "desc": "This paper addresses the challenges of quantized reinforcement learning (RL) training using FP8 precision, which often leads to instability due to numerical mismatches between training and inference. The authors highlight that the common practice of using BF16 precision for training and FP8 for rollout can cause significant accuracy issues, especially in long-horizon tasks. To overcome these problems, they introduce Jet-RL, a unified FP8 framework that ensures consistent precision during both training and rollout phases. Their experiments demonstrate that Jet-RL not only speeds up the training and rollout processes but also maintains stable convergence and minimal accuracy loss."
                },
                "zh": {
                    "title": "ç»Ÿä¸€FP8æ¡†æ¶ï¼Œå®ç°ç¨³å®šé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é‡åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ä½¿ç”¨FP8ç²¾åº¦æ‰€é¢ä¸´çš„ç¨³å®šæ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„æ•°å€¼ä¸åŒ¹é…ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„BF16è®­ç»ƒåŠ FP8æ¨ç†ç­–ç•¥åœ¨é•¿æ—¶é—´çš„æ¨ç†å’Œå¤æ‚ä»»åŠ¡ä¸­ä¼šå¯¼è‡´ä¸¥é‡çš„è®­ç»ƒä¸ç¨³å®šå’Œå‡†ç¡®æ€§å´©æºƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Jet-RLæ¡†æ¶ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„FP8ç²¾åº¦æµæ¥è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œä»è€Œå‡å°‘æ•°å€¼å·®å¼‚å¹¶æ¶ˆé™¤ä½æ•ˆçš„æ­¥éª¤é—´æ ¡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJet-RLåœ¨æ¨ç†å’Œè®­ç»ƒé˜¶æ®µå‡å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å®šçš„æ”¶æ•›æ€§å’Œè¾ƒå°çš„å‡†ç¡®æ€§ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16515",
            "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
            "url": "https://huggingface.co/papers/2601.16515",
            "abstract": "Diffusion Transformers for video generation are enhanced with SALAD, a method that combines linear and sparse attention branches to achieve high sparsity and speedup while maintaining quality and requiring minimal training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
            "score": 9,
            "issue_id": 758,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "0faece905ed96c1d",
            "authors": [
                "Tongcheng Fang",
                "Hanling Zhang",
                "Ruiqi Xie",
                "Zhuo Han",
                "Xin Tao",
                "Tianchen Zhao",
                "Pengfei Wan",
                "Wenbo Ding",
                "Wanli Ouyang",
                "Xuefei Ning",
                "Yu Wang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16515.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#diffusion",
                    "#inference",
                    "#optimization",
                    "#training",
                    "#video"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SALAD Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Diffusion Transformers Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (gating), Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¼Ñƒ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 90% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.72 Ñ€Ğ°Ğ·Ğ°. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ²ÑĞµĞ³Ğ¾ 2000 Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ 1600 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Speed and Quality in Video Generation with SALAD",
                    "desc": "This paper presents SALAD, a novel method for enhancing Diffusion Transformers in video generation. SALAD combines linear and sparse attention mechanisms to achieve significant speed improvements while maintaining high-quality output. The approach allows for 90% sparsity and a 1.72x increase in inference speed, making it efficient for real-time applications. Additionally, it requires minimal training data, needing only 2,000 video samples and 1,600 training steps, which is a substantial reduction compared to traditional methods."
                },
                "zh": {
                    "title": "SALADï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSALADçš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ‰©æ•£å˜æ¢å™¨åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚SALADç»“åˆäº†çº¿æ€§å’Œç¨€ç–æ³¨æ„åŠ›åˆ†æ”¯ï¼Œä»¥å®ç°é«˜ç¨€ç–æ€§å’ŒåŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒSALADåœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†1.72å€ï¼Œå¹¶ä¸”åªéœ€2000ä¸ªè§†é¢‘æ ·æœ¬å’Œ1600ä¸ªè®­ç»ƒæ­¥éª¤å³å¯å®Œæˆå¾®è°ƒã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—æ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å»¶è¿Ÿï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16276",
            "title": "GameTalk: Training LLMs for Strategic Conversation",
            "url": "https://huggingface.co/papers/2601.16276",
            "abstract": "GameTalk framework trains large language models to make strategic decisions through multi-turn dialogue by optimizing global objectives using reward signals across full conversations, outperforming untrained models in complex game scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.",
            "score": 8,
            "issue_id": 765,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "31e9765e1ccd046f",
            "authors": [
                "Victor Conchello Vendrell",
                "Max Ruiz Luyten",
                "Mihaela van der Schaar"
            ],
            "affiliations": [
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16276.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° GameTalk Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, DPO Ğ¸ STaR, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ»Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑƒĞ¼ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GameTalk ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ DPO, Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs for Strategic Dialogue Decisions",
                    "desc": "The GameTalk framework enhances large language models (LLMs) by training them to make strategic decisions through multi-turn dialogues. It focuses on optimizing global objectives rather than just single-turn tasks, allowing LLMs to engage in complex interactions that require coordination and negotiation. By using advanced fine-tuning methods that incorporate reward signals from entire conversations, GameTalk improves the decision-making capabilities of LLMs in multi-agent scenarios. The results demonstrate that this approach significantly outperforms untrained models, particularly when using reward shaping techniques like DPO."
                },
                "zh": {
                    "title": "GameTalkï¼šé€šè¿‡å¯¹è¯ä¼˜åŒ–æˆ˜ç•¥å†³ç­–",
                    "desc": "GameTalkæ¡†æ¶é€šè¿‡å¤šè½®å¯¹è¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚æ¸¸æˆåœºæ™¯ä¸­åšå‡ºæˆ˜ç•¥å†³ç­–ã€‚è¯¥æ–¹æ³•ä¼˜åŒ–äº†å…¨å±€ç›®æ ‡ï¼Œåˆ©ç”¨æ•´ä¸ªå¯¹è¯è¿‡ç¨‹ä¸­çš„å¥–åŠ±ä¿¡å·ï¼Œè¶…è¶Šäº†æœªè®­ç»ƒæ¨¡å‹çš„è¡¨ç°ã€‚ä¸ä»¥å¾€åªå…³æ³¨å•è½®ç›®æ ‡æˆ–é™æ€åŠ¨ä½œé¢„æµ‹çš„ç ”ç©¶ä¸åŒï¼ŒGameTalkå¼ºè°ƒåœ¨å®Œæ•´å¯¹è¯ä¸­ä¼˜åŒ–é•¿æœŸç›®æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGameTalkåœ¨ä¸åŒå¤æ‚åº¦çš„æ¸¸æˆä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¥–åŠ±å¡‘é€ çš„æƒ…å†µä¸‹ï¼ŒDPOæ–¹æ³•çš„æ•ˆæœæœ€ä¸ºæ˜¾è‘—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07251",
            "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
            "url": "https://huggingface.co/papers/2601.07251",
            "abstract": "MeepleLM enables human-AI collaboration in board game design by providing constructive critique through persona-specific reasoning patterns that align with player experiences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.",
            "score": 8,
            "issue_id": 758,
            "pub_date": "2026-01-12",
            "pub_date_card": {
                "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 12",
                "zh": "1æœˆ12æ—¥"
            },
            "hash": "b51c40e6a179bcf4",
            "authors": [
                "Zizhen Li",
                "Chuanhao Li",
                "Yibin Wang",
                "Yukang Feng",
                "Jianwen Sun",
                "Jiaxin Ai",
                "Fanrui Zhang",
                "Mingzhu Sun",
                "Yifei Huang",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "NKU",
                "Shanda AI Research Tokyo",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07251.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸Ğ³Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ",
                    "desc": "MeepleLM â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ°Ğ¼ Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 1,727 ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ÑƒĞ»ÑŒĞ±ÑƒĞºĞ¾Ğ² Ğ¸ 150K Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ², Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼ Mechanics-Dynamics-Aesthetics (MDA) Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ. MeopleLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (GPT-5.1, Gemini3-Pro) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµÑ‘ Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "MeepleLM: Your AI Co-Designer for Better Board Games!",
                    "desc": "MeepleLM is a machine learning model designed to enhance collaboration between humans and AI in the field of board game design. It provides constructive feedback by understanding different player personas and their unique experiences, which helps designers improve their games. The model uses a curated dataset of rulebooks and player reviews, applying Mechanics-Dynamics-Aesthetics (MDA) reasoning to connect game rules with player experiences. Experiments show that MeepleLM outperforms existing models in providing useful critiques, making it a valuable tool for game designers seeking to align their creations with player preferences."
                },
                "zh": {
                    "title": "MeepleLMï¼šæ¡Œæ¸¸è®¾è®¡ä¸­çš„äººæœºåä½œæ–°æ¨¡å¼",
                    "desc": "MeepleLM æ˜¯ä¸€ç§ä¿ƒè¿›äººæœºåä½œçš„å·¥å…·ï¼Œä¸“æ³¨äºæ¡Œæ¸¸è®¾è®¡ä¸­çš„å»ºè®¾æ€§æ‰¹è¯„ã€‚å®ƒé€šè¿‡ç‰¹å®šè§’è‰²çš„æ¨ç†æ¨¡å¼ï¼Œå¸®åŠ©è®¾è®¡å¸ˆæ›´å¥½åœ°ç†è§£ç©å®¶ä½“éªŒï¼Œä»è€Œæ”¹è¿›æ¸¸æˆè®¾è®¡ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äº†å¤§é‡çš„è§„åˆ™ä¹¦å’Œç©å®¶è¯„è®ºæ•°æ®ï¼Œç»“åˆæœºåˆ¶-åŠ¨æ€-ç¾å­¦ï¼ˆMDAï¼‰æ¨ç†ï¼Œå¡«è¡¥äº†è§„åˆ™ä¸æ¸¸æˆä½“éªŒä¹‹é—´çš„å› æœç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMeepleLM åœ¨ç¤¾åŒºå¯¹é½å’Œæ‰¹è¯„è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å•†ä¸šæ¨¡å‹ï¼Œæˆä¸ºå¯é çš„è™šæ‹Ÿæµ‹è¯•è€…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16344",
            "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
            "url": "https://huggingface.co/papers/2601.16344",
            "abstract": "DSGym presents a standardized framework for evaluating data science agents with comprehensive task suites and execution-verified training capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.",
            "score": 7,
            "issue_id": 757,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "00ecd410a7af745a",
            "authors": [
                "Fan Nie",
                "Junlin Wang",
                "Harper Hua",
                "Federico Bianchi",
                "Yongchan Kwon",
                "Zhenting Qi",
                "Owen Queen",
                "Shang Zhu",
                "James Zou"
            ],
            "affiliations": [
                "Duke University",
                "Harvard University",
                "Stanford University",
                "Together AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16344.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#synthetic",
                    "#science",
                    "#training"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DSGym Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² data science Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²: Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ DSBio Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ DSPredict Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, DSGym Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· pipeline ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ 4B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ GPT-4o Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "DSGym: A New Standard for Evaluating Data Science Agents",
                    "desc": "DSGym is a new framework designed to evaluate data science agents effectively. It addresses the shortcomings of existing benchmarks by providing a modular architecture that allows for easy addition of tasks and tools. The framework includes a curated task suite called DSGym-Tasks, which improves the quality and relevance of tasks while ensuring they are grounded in real data. Additionally, DSGym supports agent training through a verified data synthesis pipeline, demonstrating its capability to enhance the performance of models in realistic scientific scenarios."
                },
                "zh": {
                    "title": "DSGymï¼šæ•°æ®ç§‘å­¦ä»£ç†çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶",
                    "desc": "DSGymæ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ•°æ®ç§‘å­¦ä»£ç†ï¼Œæä¾›å…¨é¢çš„ä»»åŠ¡å¥—ä»¶å’Œç»è¿‡æ‰§è¡ŒéªŒè¯çš„è®­ç»ƒèƒ½åŠ›ã€‚ç°æœ‰çš„æ•°æ®ç§‘å­¦åŸºå‡†å­˜åœ¨è¯„ä¼°æ¥å£åˆ†æ•£ã€ä»»åŠ¡è¦†ç›–é¢çª„å’Œç¼ºä¹ä¸¥æ ¼æ•°æ®åŸºç¡€ç­‰é—®é¢˜ã€‚DSGymé€šè¿‡æ¨¡å—åŒ–æ¶æ„ï¼Œå…è®¸è½»æ¾æ·»åŠ ä»»åŠ¡å’Œå·¥å…·ï¼Œæˆä¸ºä¸€ä¸ªå¯æ‰©å±•çš„æµ‹è¯•å¹³å°ã€‚å®ƒè¿˜æä¾›äº†ç»è¿‡è´¨é‡è¿‡æ»¤çš„ä»»åŠ¡å¥—ä»¶ï¼Œæ”¯æŒä»£ç†åœ¨çœŸå®ç§‘å­¦ç¯å¢ƒä¸­è¿›è¡Œæ•°æ®åˆ†æçš„è§„åˆ’ã€å®æ–½å’ŒéªŒè¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16018",
            "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
            "url": "https://huggingface.co/papers/2601.16018",
            "abstract": "A framework for developing specialized Turkish legal language models through domain adaptation, featuring a pre-trained encoder model and decoder models with continual pre-training for enhanced legal text processing.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
            "score": 7,
            "issue_id": 758,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "23936bc95c8b8799",
            "authors": [
                "Ã–zgÃ¼r UÄŸur",
                "Mahmut GÃ¶ksu",
                "Mahmut Ã‡imen",
                "Musa YÄ±lmaz",
                "Esra Åavirdi",
                "Alp Talha Demir",
                "Rumeysa GÃ¼llÃ¼ce",
                "Ä°clal Ã‡etin",
                "Ã–mer Can SaÄŸbaÅŸ"
            ],
            "affiliations": [
                "NewmindAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16018.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#open_source",
                    "#transfer_learning",
                    "#small_models",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Mecellem â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¹ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ModernBERT, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 112,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ retrieval-Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğº ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ continual pre-training Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¼ curriculum learning, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 36,2% ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ perplexity Ğ½Ğ° Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Optimizing Turkish Legal Language Models with Mecellem Framework",
                    "desc": "This paper introduces the Mecellem framework, which focuses on creating specialized language models for the Turkish legal field using domain adaptation techniques. It features a pre-trained encoder model based on ModernBERT, trained on a vast Turkish corpus, achieving high retrieval performance with smaller models that are efficient in resource usage. Additionally, the framework includes decoder models that utilize continual pre-training to adapt to legal language, significantly improving performance on legal texts. The results show a notable reduction in perplexity, indicating effective adaptation to the specialized domain."
                },
                "zh": {
                    "title": "ä¸“ä¸ºåœŸè€³å…¶æ³•å¾‹å®šåˆ¶çš„è¯­è¨€æ¨¡å‹æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Mecellemæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡é¢†åŸŸé€‚åº”ç­–ç•¥ä¸ºåœŸè€³å…¶æ³•å¾‹é¢†åŸŸå¼€å‘ä¸“é—¨è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šé¦–å…ˆï¼ŒåŸºäºModernBERTçš„åŒå‘ç¼–ç å™¨ä»é›¶å¼€å§‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨äº†1127äº¿ä¸ªåœŸè€³å…¶è¯­è¯­æ–™åº“çš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æ£€æŸ¥ç‚¹é€‰æ‹©ç­–ç•¥ä¼˜åŒ–ä¸‹æ¸¸æ£€ç´¢æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œè§£ç å™¨æ¨¡å‹é€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰é€‚åº”åœŸè€³å…¶æ³•å¾‹é¢†åŸŸï¼Œé‡‡ç”¨æ§åˆ¶è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ³•å¾‹æ–‡æœ¬å¤„ç†çš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åœŸè€³å…¶æ£€ç´¢æ’è¡Œæ¦œä¸Šååˆ—å‰èŒ…ï¼Œä¸”åœ¨è®¡ç®—èµ„æºä¸Šæ›´ä¸ºé«˜æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16443",
            "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
            "url": "https://huggingface.co/papers/2601.16443",
            "abstract": "Endless Terminals introduces an autonomous pipeline for generating procedural terminal tasks that significantly improves agent performance on both synthetic and human-curated benchmarks through scalable reinforcement learning environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.",
            "score": 5,
            "issue_id": 757,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "2bf548d075a71a09",
            "authors": [
                "Kanishk Gandhi",
                "Shivam Garg",
                "Noah D. Goodman",
                "Dimitris Papailiopoulos"
            ],
            "affiliations": [
                "Microsoft Research",
                "Stanford University",
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16443.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#transfer_learning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Endless Terminals â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ PPO Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 3255 ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LLM Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Agent Training with Endless Terminals",
                    "desc": "Endless Terminals presents a novel autonomous pipeline that generates procedural terminal tasks, enhancing agent performance in reinforcement learning. This pipeline addresses the limitations of current benchmarks by creating scalable environments for training rather than just evaluation. It consists of four stages: task description generation, environment construction, completion testing, and solvability filtering, resulting in over 3,000 diverse tasks. Agents trained using this pipeline show significant performance improvements on both synthetic and human-curated benchmarks, demonstrating that simplicity in reinforcement learning can lead to effective results when environments are adequately scaled."
                },
                "zh": {
                    "title": "æ— å°½ç»ˆç«¯ï¼šæå‡æ™ºèƒ½ä½“æ€§èƒ½çš„è‡ªä¸»ä»»åŠ¡ç”Ÿæˆç®¡é“",
                    "desc": "ã€Šæ— å°½ç»ˆç«¯ã€‹ä»‹ç»äº†ä¸€ç§è‡ªä¸»ç”Ÿæˆç¨‹åºåŒ–ç»ˆç«¯ä»»åŠ¡çš„ç®¡é“ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨åˆæˆå’Œäººç±»ç­–åˆ’åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚è¯¥ç®¡é“é€šè¿‡å››ä¸ªé˜¶æ®µç”Ÿæˆå¤šæ ·åŒ–çš„ä»»åŠ¡æè¿°ã€æ„å»ºå’ŒéªŒè¯å®¹å™¨åŒ–ç¯å¢ƒã€ç”Ÿæˆå®Œæˆæµ‹è¯•å¹¶è¿‡æ»¤å¯è§£æ€§ã€‚æˆ‘ä»¬ä»ä¸­è·å¾—äº†3255ä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ–‡ä»¶æ“ä½œã€æ—¥å¿—ç®¡ç†ã€æ•°æ®å¤„ç†ã€è„šæœ¬ç¼–å†™å’Œæ•°æ®åº“æ“ä½œã€‚å°½ç®¡è®­ç»ƒè¿‡ç¨‹ç®€å•ï¼Œä½¿ç”¨åŸºç¡€çš„PPOç®—æ³•ï¼Œæ¨¡å‹åœ¨ã€Šæ— å°½ç»ˆç«¯ã€‹ä¸Šè®­ç»ƒåæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†åœ¨å¯æ‰©å±•ç¯å¢ƒä¸­ç®€å•çš„å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå–å¾—æˆåŠŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13606",
            "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
            "url": "https://huggingface.co/papers/2601.13606",
            "abstract": "ChartVerse is a framework that synthesizes complex charts and reliable reasoning data using novel metrics and answer-first paradigms to improve vision-language model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.",
            "score": 5,
            "issue_id": 767,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "ab49116784fa741d",
            "authors": [
                "Zheng Liu",
                "Honglin Lin",
                "Chonghan Qin",
                "Xiaoyang Wang",
                "Xin Gao",
                "Yu Li",
                "Mengzhang Cai",
                "Yun Zhu",
                "Zhanping Zhong",
                "Qizhi Pei",
                "Zhuoshi Pan",
                "Xiaoran Shang",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang",
                "Lijun Wu"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13606.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#hallucinations",
                    "#reasoning",
                    "#cv",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚Ğ²ĞµÑ‚-Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´",
                    "desc": "ChartVerse â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Rollout Posterior Entropy Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Â«Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ĞµÂ»: Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ChartVerse-SFT-600K Ğ¸ ChartVerse-RL-40K Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´ÑˆÑƒÑ ÑĞ²Ğ¾ĞµĞ³Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ChartVerse: Elevating Chart Reasoning for Vision Language Models",
                    "desc": "ChartVerse is a new framework that creates complex charts and reliable reasoning data to enhance the performance of Vision Language Models (VLMs). It addresses the problem of low-quality training data by introducing a novel metric called Rollout Posterior Entropy (RPE) to measure chart complexity, allowing for the generation of diverse and intricate charts. Additionally, it employs an answer-first approach for question generation, ensuring that the reasoning process is rigorous and consistent. The framework has led to the creation of large datasets and has shown impressive results, outperforming existing models in chart reasoning tasks."
                },
                "zh": {
                    "title": "ChartVerseï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚å›¾è¡¨æ¨ç†èƒ½åŠ›",
                    "desc": "ChartVerseæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ–°é¢–çš„åº¦é‡æ ‡å‡†å’Œä»¥ç­”æ¡ˆä¸ºå…ˆçš„èŒƒå¼ï¼Œåˆæˆå¤æ‚å›¾è¡¨å’Œå¯é çš„æ¨ç†æ•°æ®ï¼Œä»¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ•°æ®é›†çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åˆæˆå›¾è¡¨çš„ç®€å•æ€§å’Œé—®ç­”å¯¹çš„å¹»è§‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥Rollout Posterior Entropyï¼ˆRPEï¼‰åº¦é‡å›¾è¡¨å¤æ‚æ€§ï¼ŒChartVerseèƒ½å¤Ÿè‡ªä¸»åˆæˆå¤šæ ·åŒ–çš„é«˜å¤æ‚åº¦å›¾è¡¨ã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒChartVerse-8Båœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶æ•™å¸ˆæ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11258",
            "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
            "url": "https://huggingface.co/papers/2601.11258",
            "abstract": "A novel framework called Parametric Skill Transfer (PaST) is presented that enables efficient knowledge adaptation in large language models by combining supervised fine-tuning with skill vector injection, demonstrating superior performance in question answering and tool-use tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face the \"knowledge cutoff\" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.",
            "score": 4,
            "issue_id": 762,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "83b14e33e01e826a",
            "authors": [
                "Pingzhi Tang",
                "Yiding Wang",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Yuanpei College, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11258.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#transfer_learning",
                    "#training",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Parametric Skill Transfer (PaST), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ supervised fine-tuning Ğ¸ reinforcement learning Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ supervised fine-tuning Ñ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¸ĞµĞ¼ baseline'Ğ¾Ğ² Ğ½Ğ° 8-10 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Efficient Knowledge Adaptation with Parametric Skill Transfer",
                    "desc": "The paper introduces a new method called Parametric Skill Transfer (PaST) that helps large language models (LLMs) learn new information more effectively. It combines supervised fine-tuning with a technique called skill vector injection, allowing models to adapt their knowledge without losing their ability to answer questions or make decisions. The authors found that the updates from fine-tuning and reinforcement learning are almost independent, which led to the development of PaST for better skill transfer. Experiments show that PaST significantly improves performance on various tasks, including question answering and tool use, demonstrating its efficiency and versatility."
                },
                "zh": {
                    "title": "å‚æ•°æŠ€èƒ½è½¬ç§»ï¼šé«˜æ•ˆçŸ¥è¯†é€‚åº”çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºå‚æ•°æŠ€èƒ½è½¬ç§»ï¼ˆPaSTï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒå’ŒæŠ€èƒ½å‘é‡æ³¨å…¥ï¼Œå®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆçŸ¥è¯†é€‚åº”ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ¨¡å‹åœ¨çŸ¥è¯†æ›´æ–°æ—¶çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡é—®ç­”å’Œå·¥å…·ä½¿ç”¨ä»»åŠ¡çš„è¡¨ç°ã€‚é€šè¿‡ä»æºé¢†åŸŸæå–é¢†åŸŸæ— å…³çš„æŠ€èƒ½å‘é‡ï¼ŒPaSTå¯ä»¥åœ¨è½»é‡çº§å¾®è°ƒåï¼Œå°†çŸ¥è¯†æ“ä½œæŠ€èƒ½çº¿æ€§æ³¨å…¥ç›®æ ‡æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPaSTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œè·¨é¢†åŸŸè½¬ç§»èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15715",
            "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
            "url": "https://huggingface.co/papers/2601.15715",
            "abstract": "RebuttalAgent is a novel framework that applies Theory of Mind to academic rebuttal, utilizing a ToM-Strategy-Response pipeline with supervised fine-tuning and reinforcement learning for improved automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.",
            "score": 1,
            "issue_id": 762,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "17444721f50259b1",
            "authors": [
                "Zhitao He",
                "Zongwei Lyu",
                "Yi R Fung"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15715.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#science",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#alignment",
                    "#rl",
                    "#training",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RebuttalAgent â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ mind (ToM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ°, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° â€” ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Rebuttal-RM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 100K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Academic Rebuttal with Theory of Mind",
                    "desc": "RebuttalAgent is a groundbreaking framework that enhances academic rebuttal by incorporating Theory of Mind (ToM) principles. It utilizes a ToM-Strategy-Response pipeline to understand the mental states of reviewers, develop persuasive strategies, and generate contextually relevant responses. The framework is trained using a two-stage process that includes supervised fine-tuning for strategic planning and reinforcement learning for continuous improvement. With the help of a specialized evaluator, Rebuttal-RM, the system demonstrates superior performance in automated evaluations compared to existing models, achieving higher consistency with human preferences."
                },
                "zh": {
                    "title": "å¿ƒæ™ºç†è®ºé©±åŠ¨çš„å­¦æœ¯åé©³æ¡†æ¶",
                    "desc": "RebuttalAgentæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¿ƒæ™ºç†è®ºï¼ˆTheory of Mindï¼‰æ¥å¤„ç†å­¦æœ¯åé©³é—®é¢˜ã€‚å®ƒé€šè¿‡ToM-ç­–ç•¥-å“åº”ï¼ˆToM-Strategy-Responseï¼‰ç®¡é“ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡è‡ªåŠ¨è¯„ä¼°çš„æ•ˆæœã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ¨¡æ‹Ÿè¯„å®¡è€…çš„å¿ƒç†çŠ¶æ€ï¼Œåˆ¶å®šè¯´æœç­–ç•¥ï¼Œå¹¶ç”ŸæˆåŸºäºç­–ç•¥çš„å›åº”ã€‚é€šè¿‡æ„å»ºRebuttalBenchæ•°æ®é›†å¹¶è¿›è¡Œä¸¤é˜¶æ®µè®­ç»ƒï¼ŒRebuttalAgentåœ¨è‡ªåŠ¨åŒ–æŒ‡æ ‡ä¸Šå¹³å‡æå‡äº†18.3%ï¼Œå¹¶åœ¨å¤šæºåé©³æ•°æ®çš„è¯„ä¼°ä¸­è¶…è¶Šäº†äººç±»åå¥½çš„è¯„åˆ†ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13118",
            "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
            "url": "https://huggingface.co/papers/2601.13118",
            "abstract": "Research derives and evaluates prompt optimization guidelines for code generation tasks in software engineering, identifying 10 specific improvement patterns related to input/output specification, conditions, examples, and clarity.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.",
            "score": 1,
            "issue_id": 760,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "ba5b27928e3f784b",
            "authors": [
                "Alessandro Midolo",
                "Alessandro Giagnorio",
                "Fiorella Zampetti",
                "Rosalia Tufano",
                "Gabriele Bavota",
                "Massimiliano Di Penta"
            ],
            "affiliations": [
                "Dipartimento di Matematica Informatica, University of Catania",
                "Software Institute USI UniversitÃ  della Svizzera italiana",
                "University of Sannio"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13118.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ 10 ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ĞºĞ°ÑĞ°ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…/Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 50 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Optimizing Prompts for Better Code Generation",
                    "desc": "This paper focuses on improving code generation tasks in software engineering by providing specific guidelines for prompt optimization. It identifies ten key patterns that enhance the clarity and effectiveness of prompts, such as better input/output specifications and the inclusion of examples. The authors used a test-driven approach to refine prompts and evaluated their impact through feedback from software practitioners. The findings suggest that these guidelines can significantly aid developers in utilizing Large Language Models more effectively for code generation."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æç¤ºï¼Œæå‡ä»£ç ç”Ÿæˆæ•ˆç‡",
                    "desc": "æœ¬ç ”ç©¶æå‡ºå¹¶è¯„ä¼°äº†é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä¸­ä»£ç ç”Ÿæˆä»»åŠ¡çš„æç¤ºä¼˜åŒ–æŒ‡å—ï¼Œè¯†åˆ«å‡º10ç§ä¸è¾“å…¥/è¾“å‡ºè§„èŒƒã€æ¡ä»¶ã€ç¤ºä¾‹å’Œæ¸…æ™°åº¦ç›¸å…³çš„å…·ä½“æ”¹è¿›æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€‚å½“çš„æç¤ºå·¥ç¨‹å¯ä»¥å¸®åŠ©å¼€å‘è€…æ”¹å–„ä»£ç ç”Ÿæˆçš„æç¤ºã€‚æˆ‘ä»¬é‡‡ç”¨è¿­ä»£çš„æµ‹è¯•é©±åŠ¨æ–¹æ³•è‡ªåŠ¨ä¼˜åŒ–ä»£ç ç”Ÿæˆæç¤ºï¼Œå¹¶åˆ†æç»“æœä»¥è¯†åˆ«å‡ºèƒ½å¤Ÿæé«˜æµ‹è¯•é€šè¿‡ç‡çš„æç¤ºæ”¹è¿›é¡¹ã€‚é€šè¿‡å¯¹50åä»ä¸šè€…çš„è¯„ä¼°ï¼Œå‘ç°ä»–ä»¬å¯¹æç¤ºæ”¹è¿›æ¨¡å¼çš„ä½¿ç”¨å’Œæ„ŸçŸ¥çš„æœ‰æ•ˆæ€§å¹¶ä¸æ€»æ˜¯ä¸å®é™…ä½¿ç”¨æƒ…å†µä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16451",
            "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
            "url": "https://huggingface.co/papers/2601.16451",
            "abstract": "VISTA-PATH is an interactive, class-aware pathology segmentation model that integrates visual context, semantic descriptions, and expert feedback to enable precise multi-class segmentation and clinical interpretation in digital pathology.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.",
            "score": 0,
            "issue_id": 776,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "775026c5d1dd5c5c",
            "authors": [
                "Peixian Liang",
                "Songhao Li",
                "Shunsuke Koga",
                "Yutong Li",
                "Zahra Alipour",
                "Yucheng Tang",
                "Daguang Xu",
                "Zhi Huang"
            ],
            "affiliations": [
                "Emory University",
                "Georgia Institute of Technology",
                "NVIDIA Corporation",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16451.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#healthcare"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ: foundation model Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "VISTA-PATH â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ¸ÑÑ‚Ğ¾Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚ĞºĞ°Ğ½ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 1.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¼ 93 Ñ‚Ğ¸Ğ¿Ğ° Ñ‚ĞºĞ°Ğ½ĞµĞ¹ Ğ¸Ğ· 9 Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ†ĞµĞ»Ñ‹Ğµ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Tumor Interaction Score Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Transforming Pathology Segmentation with Interactive AI",
                    "desc": "VISTA-PATH is a novel interactive model designed for precise segmentation of pathology images, integrating visual context, semantic descriptions, and expert feedback. It addresses the limitations of traditional segmentation models by treating the task as dynamic and interactive rather than static. The model is trained on a large dataset of over 1.6 million image-mask-text triplets, enabling it to perform multi-class segmentation effectively across various tissue types. VISTA-PATH not only enhances segmentation accuracy but also supports clinical interpretation, making it a valuable tool in computational pathology."
                },
                "zh": {
                    "title": "VISTA-PATHï¼šæå‡ç—…ç†å›¾åƒåˆ†å‰²çš„äº¤äº’æ€§ä¸ä¸´åºŠæ„ä¹‰",
                    "desc": "VISTA-PATHæ˜¯ä¸€ç§äº¤äº’å¼çš„ã€ç±»æ„ŸçŸ¥çš„ç—…ç†åˆ†å‰²æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ•°å­—ç—…ç†å­¦ä¸­çš„å¤šç±»åˆ†å‰²ç²¾åº¦ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ä¸Šä¸‹æ–‡ã€è¯­ä¹‰æè¿°å’Œä¸“å®¶åé¦ˆï¼Œä½¿å¾—åˆ†å‰²ç»“æœæ›´å…·ä¸´åºŠæ„ä¹‰ã€‚é€šè¿‡å¤§è§„æ¨¡çš„ç—…ç†åˆ†å‰²æ•°æ®é›†ï¼ŒVISTA-PATHåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æ”¯æŒåŠ¨æ€çš„äººæœºåä½œã€‚æœ€ç»ˆï¼ŒVISTA-PATHé€šè¿‡æå‡ºçš„è‚¿ç˜¤ç›¸äº’ä½œç”¨è¯„åˆ†ï¼ˆTISï¼‰æ”¹å–„äº†ç»„ç»‡å¾®ç¯å¢ƒåˆ†æï¼Œä¸æ‚£è€…ç”Ÿå­˜ç‡æœ‰æ˜¾è‘—å…³è”ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-26.html",
    "link_next": "2026-01-28.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "26.01",
        "en": "01/26",
        "zh": "1æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 7,
        "#cv": 3,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 5,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 8,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 4,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 1
    }
}